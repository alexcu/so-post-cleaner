title,id,title,body,accepted_answer_id,answer_count,comment_count,community_owned_date,creation_date,favorite_count,last_activity_date,last_edit_date,last_editor_display_name,last_editor_user_id,owner_display_name,owner_user_id,post_type_id,score,tags,view_count
Json_format.parse() function throws error 'utf-8' codec can't decode byte 0xa1 in position 11: invalid start byte,50805054,Json_format.parse() function throws error 'utf-8' codec can't decode byte 0xa1 in position 11: invalid start byte,"<p>I am trying to perform OCR on PDF files using google cloud vision, to do some basic testing i am using the sample code provided in google documentation in the below link, but the line of code provided below is throwing unicode decode error given below, can anyone please help me fix this, i did an extensive search and tried different approaches but i am unable to fix it.</p>

<pre><code>response = json_format.Parse(json_string, vision.types.AnnotateFileResponse())
</code></pre>

<p><a href=""https://cloud.google.com/vision/docs/pdf"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/pdf</a></p>

<p>Error: UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa1 in position 11: invalid start byte</p>",,0,2,,2018-06-11 19:49:46.200 UTC,,2018-06-11 19:49:46.200 UTC,,,,,5326115,1,1,python|google-cloud-platform,83
How to prevent inappropriate content from uploading to AWS with Rails?,54876804,How to prevent inappropriate content from uploading to AWS with Rails?,"<p>I have a Rails 5 app that allows users to upload images to their profiles using the new ActiveStorage with AWS S3 Storage process. </p>

<p>I've been searching for a way to detect inappropriate content / explicit images in the uploads so I could prevent the user from displaying those on their accounts, but I'm not sure how I could accomplish this.</p>

<p>I don't want to have to moderate the content uploads. I know there are ways to allow users to ""flag as inappropriate"". I would prefer to not allow explicit content to be uploaded at all.</p>

<p>I figure the best solution would be for the Rails app to detect the explicit content and put in a placeholder image instead of the user's inappropriate image.</p>

<p>One idea was AWS Rekognition. Has anybody successfully implemented a solution for this problem?</p>",,2,0,,2019-02-26 00:30:52.140 UTC,,2019-03-27 10:01:15.317 UTC,2019-02-27 18:09:36.920 UTC,,5113026,,5113026,1,0,ruby-on-rails|amazon-s3|paperclip|image-recognition|rails-activestorage,39
Detecting content in Google Cloud Vision for .NET does nothing/hangs app,48999636,Detecting content in Google Cloud Vision for .NET does nothing/hangs app,"<p>I just started playing around with Google Cloud Vision a bit. I wanted to detect text in an image. Inspired by the official docs (e.g. <a href=""https://cloud.google.com/vision/docs/detecting-text"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/detecting-text</a> and <a href=""https://cloud.google.com/docs/authentication/production"" rel=""nofollow noreferrer"">https://cloud.google.com/docs/authentication/production</a>) I</p>

<ul>
<li>created a new project,</li>
<li>attached the Vision API to it,</li>
<li>created a service account and downloaded the credentials/key-JSON file,</li>
<li>set up an VS project and got all relevant packages from NuGET.</li>
</ul>

<p>My code looks like this:</p>

<pre><code>using System;
using System.Windows;
using Google.Apis.Auth.OAuth2;
using Google.Cloud.Vision.V1;
using Grpc.Auth;

//...

private void Button_Click(object sender, RoutedEventArgs e)
{
    // Load an image from a local file.
    var image = Image.FromFile(@""C:\!\myimage.png"");
    var credential = GoogleCredential.FromFile(@""C:\!\credentials.json"");

    var channel = new Grpc.Core.Channel(@""https://vision.googleapis.com/v1/images:annotate"",credential.ToChannelCredentials());

    var client = ImageAnnotatorClient.Create(channel);

    var response = client.DetectText(image); // &lt;-- Nothing happens, app hangs, why?
    foreach (var annotation in response)
    {
        if (annotation.Description != null)
            Console.WriteLine(annotation.Description);
    }
}

//...
</code></pre>

<p>While stepping through the code, the app hangs at <code>var response = client.DetectText(image);</code> (no exception or anything). The same happens, if I use other methods (e.g. <code>DetectLogos(image)</code> or <code>DetectLabels(image)</code>). When checking CPU usage and network traffic nothing important happens (before or after the relevant line of code).</p>

<p>What am I doing wrong here?</p>

<p>Thanks!</p>",49137894,3,0,,2018-02-27 01:11:23.643 UTC,,2019-01-07 21:57:31.760 UTC,2018-03-06 20:50:48.433 UTC,,9446581,,3552711,1,0,c#|.net|grpc|google-cloud-vision,773
Increase confidence score of handwritten text detected using google vision api,56250899,Increase confidence score of handwritten text detected using google vision api,"<p>I have to process a bunch of digital scanned documents which contain information as a form(mostly insurance, legal stuff). They are 90% printed text and 10% handwritten.
I used Google Vision API to extract information from them. It gave accurate results for printed texts with high confidence but handwritten parts were not always detected correctly.</p>

<p>So, is there any way to increase confidence of handwritten parts or can I customize API to do this?</p>",,1,0,,2019-05-22 06:51:39.890 UTC,,2019-05-22 08:28:15.233 UTC,2019-05-22 06:55:40.923 UTC,,6622587,,11189391,1,0,python|python-3.x|google-cloud-platform|google-vision,31
Generate barcode or qrcode using google vision api,41400421,Generate barcode or qrcode using google vision api,"<p>I am using google vision API to scan the barcodes and qrcodes. Now I want to give one more facility to the users that user can generate text, url, phone, vcard etc barcodes/qrcodes.</p>

<p>So anybody knows how to achieve this? Because there are lots of app on google play store those are doing the same things.</p>",41400491,1,1,,2016-12-30 17:46:56.187 UTC,3,2016-12-30 18:06:14.453 UTC,,,,,792480,1,2,android|google-play-services|google-vision,3462
Google Cloud Vision API - Error creating Grpc.Core.Channel,52414541,Google Cloud Vision API - Error creating Grpc.Core.Channel,"<p>I am trying to use Google Cloud Vision V1 Api's ImageAnnotatorClient class. I am following the example at <a href=""https://googleapis.github.io/google-cloud-dotnet/docs/Google.Cloud.Vision.V1/api/Google.Cloud.Vision.V1.ImageAnnotatorClient.html"" rel=""nofollow noreferrer"">https://googleapis.github.io/google-cloud-dotnet/docs/Google.Cloud.Vision.V1/api/Google.Cloud.Vision.V1.ImageAnnotatorClient.html</a> under the </p>

<blockquote>
  <p>Create(ServiceEndpoint, ImageAnnotatorSettings)</p>
</blockquote>

<p>header. I am using C# and trying to build a classic console application. I am using GRPC.Core version 1.15.0, and Google.Cloud.Vision.V1 version 1.2.0 from Nuget. I get a compile error </p>

<blockquote>
  <p>'GoogleCredential' does not contain a definition for
  'ToChannelCredentials' and no extension method 'ToChannelCredentials'
  accepting a first argument of type 'GoogleCredential' could be found</p>
</blockquote>

<p>The below is my code:</p>

<pre><code>GoogleCredential credential = GoogleCredential
    .FromFile(@""C:\Users\...\12345.json"")
    .CreateScoped(ImageAnnotatorClient.DefaultScopes);
            Google.Cloud.Vision.V1.Image image1 = Google.Cloud.Vision.V1.Image.FromFile(@""c:\Users\....\Image14b.png"");

            Channel channel = new Channel(
    ImageAnnotatorClient.DefaultEndpoint.Host, ImageAnnotatorClient.DefaultEndpoint.Port, credential.ToChannelCredentials());
            ImageAnnotatorClient client = ImageAnnotatorClient.Create(channel);

            IReadOnlyList&lt;EntityAnnotation&gt; textAnnotations = client.DetectText(image1);
</code></pre>

<p>I get error at the line below:</p>

<pre><code>        Channel channel = new Channel(
ImageAnnotatorClient.DefaultEndpoint.Host, ImageAnnotatorClient.DefaultEndpoint.Port, credential.ToChannelCredentials());
</code></pre>

<p>Any hints please?</p>",,1,0,,2018-09-19 21:39:00.280 UTC,,2018-09-27 08:19:20.007 UTC,,,,,1200114,1,0,c#|google-cloud-platform|google-cloud-vision|google-client,146
Alternate way to set credentials for GOOGLE_APPLICATION_CREDENTIALS,52925404,Alternate way to set credentials for GOOGLE_APPLICATION_CREDENTIALS,"<p>I've a Go app that uses the <em>Google Vision API</em> and <em>Google Video intelligence</em> API.
To enter my credentials, I set the environment variable called <code>GOOGLE_APPLICATION_CREDENTIALS</code>. To do so, I assign a file path to this variable that points to the directory where my credentials are stored in.</p>

<p><em>Problem:</em></p>

<p>My credentials are <em>not</em> initially saved in a file. Instead they are assigned to a  string variable inside my app.
As a workaround, I store that value to a temporary file and then assign it's path to <code>GOOGLE_APPLICATION_CREDENTIALS</code>, like described above.</p>

<p><em>Question:</em></p>

<p>Is it possible to set API credentials for <code>cloud.google.com/go/vision/apiv1</code> without this file?</p>",,1,0,,2018-10-22 08:45:14.953 UTC,,2018-10-22 13:17:16.137 UTC,2018-10-22 13:17:16.137 UTC,,9818506,,629960,1,0,go|google-vision,76
Is it possible to select particular text using Google's vision API?,49340214,Is it possible to select particular text using Google's vision API?,"<p>I am designing an app where i scan the text using the camera and use that text to fetch more details. To do that i am using Google's vision API. But by default the API reads all the text that is available on the image as shown below. </p>

<p><a href=""https://i.stack.imgur.com/OCw3q.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OCw3q.jpg"" alt=""enter image description here""></a></p>

<p>As you can see from the above image the app is recognizing all the text that is available in front of the camera. But i would like to just scan <strong>""Hello World""</strong> from the camera. Is it possible to use some kind of touch event just to focus on the desired text</p>

<p>Please find the code used for text recognition</p>

<pre><code>    private void startCameraSource() {
//Create the TextRecognizer
final TextRecognizer textRecognizer = new TextRecognizer.Builder(getApplicationContext()).build();

    if (!textRecognizer.isOperational()) {
    Log.w(TAG, ""Detector dependencies not loaded yet"");
} else {

    //Initialize camerasource to use high resolution and set Autofocus on.
    mCameraSource = new CameraSource.Builder(getApplicationContext(), textRecognizer)
            .setFacing(CameraSource.CAMERA_FACING_BACK)
            .setRequestedPreviewSize(1280, 1024)
            .setAutoFocusEnabled(true)
            .setRequestedFps(2.0f)
            .build();

    /**
     * Add call back to SurfaceView and check if camera permission is granted.
     * If permission is granted we can start our cameraSource and pass it to surfaceView
     */
    mCameraView.getHolder().addCallback(new SurfaceHolder.Callback() {
        @Override
        public void surfaceCreated(SurfaceHolder holder) {
            try {

                if (ActivityCompat.checkSelfPermission(getApplicationContext(),
                        Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {

                    ActivityCompat.requestPermissions(MainActivity.this,
                            new String[]{Manifest.permission.CAMERA},
                            requestPermissionID);
                    return;
                }
                mCameraSource.start(mCameraView.getHolder());
            } catch (IOException e) {
                e.printStackTrace();
            }
        }

        @Override
        public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {
        }

        @Override
        public void surfaceDestroyed(SurfaceHolder holder) {
            mCameraSource.stop();
        }
    });

    //Set the TextRecognizer's Processor.
    textRecognizer.setProcessor(new Detector.Processor&lt;TextBlock&gt;() {
        @Override
        public void release() {
        }

        /**
         * Detect all the text from camera using TextBlock and the values into a stringBuilder
         * which will then be set to the textView.
         * */
        @Override
        public void receiveDetections(Detector.Detections&lt;TextBlock&gt; detections) {
            final SparseArray&lt;TextBlock&gt; items = detections.getDetectedItems();
            if (items.size() != 0 ){

                mTextView.post(new Runnable() {
                    @Override
                    public void run() {
                        StringBuilder stringBuilder = new StringBuilder();
                        for(int i=0;i&lt;items.size();i++){
                            TextBlock item = items.valueAt(i);
                            stringBuilder.append(item.getValue());
                            stringBuilder.append(""\n"");
                        }
                        mTextView.setText(stringBuilder.toString());
                    }
                });
            }
        }
    });
}
}
</code></pre>",,0,2,,2018-03-17 18:05:45.147 UTC,,2018-03-17 18:05:45.147 UTC,,,,,4729925,1,0,android|google-vision|text-recognition,215
How to refer a local file for Azure emotion api,43425391,How to refer a local file for Azure emotion api,"<p>I am trying to refer a local jpg file for using in Azure Emotion API.
To do this, I refer my file through ""file:///"" like below. </p>

<pre><code>body = ""{'url': 'file:///Users/jonghkim/dev_jhk/Research/Crowdfunding/Face_Analysis/me.jpg'}""
</code></pre>

<p>But the response says ""Invalid image URL."" How could I fix it?</p>

<p>{""error"":{""code"":""InvalidUrl"",""message"":""Invalid image URL.""}}</p>

<p>Whole code looks like below.</p>

<pre><code>########### Python 2.7 #############
import httplib, urllib, base64

headers = {
    # Request headers. Replace the placeholder key below with your subscription key.
    'Content-Type': 'application/json',
    'Ocp-Apim-Subscription-Key': '***********************',
}

params = urllib.urlencode({
})

# Replace the example URL below with the URL of the image you want to analyze.
body = ""{'url': 'file:///Users/jonghkim/dev_jhk/Research/Crowdfunding/Face_Analysis/me.jpg'}""

try:
    conn = httplib.HTTPSConnection('westus.api.cognitive.microsoft.com')
    conn.request(""POST"", ""/emotion/v1.0/recognize?%s"" % params, body, headers)
    response = conn.getresponse()
    data = response.read()
    print(data)
    conn.close()
except Exception as e:
    print(""[Errno {0}] {1}"".format(e.errno, e.strerror))
</code></pre>",43426556,2,0,,2017-04-15 11:17:29.497 UTC,,2017-06-29 12:47:46.937 UTC,2017-04-15 12:22:00.983 UTC,,272109,,3001616,1,0,python|azure,259
"aws: error: argument command: Invalid choice, valid choices are: this is the error showing with some number of commands",47430886,"aws: error: argument command: Invalid choice, valid choices are: this is the error showing with some number of commands","<pre><code>aws rekognition index-faces \
--image '{""S3Object"":{""Bucket"":""mybucketname"",""Name"":""S3ObjectKey""}}' \
--collection-id ""image1"" \
--region us-east-1 \
--profile adminuser
</code></pre>

<p>That is my code which I'm running on terminal, I tried but don't know why is that error coming. What changes should I do??</p>",,1,2,,2017-11-22 09:22:18.473 UTC,,2018-02-17 17:28:22.793 UTC,2017-11-23 22:35:41.577 UTC,,1695906,,8315773,1,1,amazon-web-services|amazon-s3,1573
Determining which Google Vision API to use - image text or document text OCR,56301560,Determining which Google Vision API to use - image text or document text OCR,"<p>I have been using Google Vision API to read text off several hundred thousand images. Some of the images are memes or sparse captions or scattered graffiti, while some are close to dense documents. I have used both the image-text reader and well as the document text detect on all images, and some returned text renditions in both services.
How do I determine which result is the best to retain and which one can be discarded?</p>

<p>I was hoping to go by measuring token lengths after cleaning the texts and retaining the longer texts, but it feels very oversimplified and unbankable</p>",,0,0,,2019-05-25 03:55:52.627 UTC,,2019-05-25 03:55:52.627 UTC,,,,,11553368,1,-1,google-api|ocr|text-extraction|google-cloud-vision|vision,18
React App Proxy Error: Could Not Proxy Request,51833429,React App Proxy Error: Could Not Proxy Request,"<p>I am working on a group project that was deployed on Heroku, but now I need to do further work on it and am trying to get it to run locally again. It is a React app that uses MongoDB. I cloned the repo and did npm install in both the root folder and the client folder. When I enter npm run dev, the page comes up, but the the functionality is not working. When I try to sign in, sign up, or submit photos I get errors: </p>

<p>From the terminal:</p>

<pre><code>[1] Proxy error: Could not proxy request /api/account/signin from localhost:3000 to http://localhost:3001/.
[1] See https://nodejs.org/api/errors.html#errors_common_system_errors for more information (ECONNREFUSED).
</code></pre>

<p>From the console:</p>

<pre><code>POST http://localhost:3000/api/account/signin 500 (Internal Server Error)
Uncaught (in promise) Error: Request failed with status code 500
    at createError (createError.js:16)
    at settle (settle.js:18)
    at XMLHttpRequest.handleLoad (xhr.js:77)
</code></pre>

<p>I looked at the error log and it seems like the server is refusing to make the connection. I can't figure out what the problem is. The thing that confuses me the most is that my other team member also cloned the repo and claims everything is working for him. It was also functional while deployed on Heroku. Is there a particular setting on my computer that could be causing a problem? My firewall is turned off. In case it is an issue with the code, here is some of it:</p>

<p><strong>server.js:</strong></p>

<pre><code>const express = require('express');
const app = express();
const mongoose = require('mongoose');
const bodyParser = require('body-parser');  

const PORT = process.env.PORT || 3001;

app.use(bodyParser.json());
app.use(bodyParser.urlencoded({ extended: true }));

if (process.env.NODE_ENV === ""production"") {
    app.use(express.static(""client/build""));
    const path = require('path');
    app.get('/', (req, res) =&gt; {
      res.sendFile(path.resolve(__dirname, 'client', 'build', 
'index.html'));
    });
}

mongoose.connect(
  process.env.MONGODB_URI || ""mongodb://localhost:27017/leafy"",
  {
    useNewUrlParser: true
  }
);

require(""./routes/api-routes.js"")(app);

app.listen(PORT, function () {
    console.log(""App listening on PORT: "" + PORT);
});
</code></pre>

<p><strong>package.json:</strong></p>

<pre><code>{
  ""name"": ""ai-img-recog"",
  ""version"": ""1.0.0"",
  ""description"": ""AI-powered image recognition"",
  ""main"": ""server.js"",
  ""scripts"": {
    ""server"": ""node server.js"",
    ""test"": ""echo \""Error: no test specified\"" &amp;&amp; exit 1"",
    ""start"": ""node server.js"",
    ""client"": ""npm run start --prefix client"",
    ""dev"": ""concurrently \""node server.js\"" \""npm run client\"""",
    ""heroku-postbuild"": ""NPM_CONFIG_PRODUCTION=false npm install --prefix client &amp;&amp; npm run build --prefix client""
  },
  ""repository"": {
    ""type"": ""git"",
    ""url"": ""git+https://github.com/deasydoesit/ai-img-recog.git""
  },
  ""keywords"": [
    ""AI"",
    ""visual analysis"",
    ""image recognition""
  ],
  ""author"": ""Christina"",
  ""license"": ""MIT"",
  ""bugs"": {
    ""url"": ""https://github.com/deasydoesit/ai-img-recog/issues""
  },
  ""homepage"": ""https://github.com/deasydoesit/ai-img-recog#readme"",
  ""dependencies"": {
    ""aws-sdk"": ""^2.279.1"",
    ""axios"": ""^0.18.0"",
    ""bcrypt"": ""^2.0.1"",
    ""body-parser"": ""^1.18.3"",
    ""concurrently"": ""^3.6.0"",
    ""create-react-app"": ""^1.5.2"",
    ""dotenv"": ""^6.0.0"",
    ""express"": ""^4.16.3"",
    ""mongoose"": ""^5.2.4"",
    ""multer"": ""^1.3.1"",
    ""multer-s3"": ""^2.7.0"",
    ""path"": ""^0.12.7"",
    ""watson-developer-cloud"": ""^3.7.0""
  }
}
</code></pre>

<p><strong>api-routes.js</strong></p>

<pre><code>require('dotenv').config();
const aws = require('aws-sdk');
const multerS3 = require('multer-s3');
const multer = require('multer');
const VisualRecognitionV3 = require('watson-developer-cloud/visual-recognition/v3');
const db = require('../models');
const mongoose = require('mongoose');


// Initialize Mongo
const MONGODB_URI = process.env.MONGODB_URI || ""mongodb://localhost/leafy"";
mongoose.Promise = Promise;
mongoose.connect(MONGODB_URI);
//---*

// Initiaize Watson Visual Recognition
const visualRecognition = new VisualRecognitionV3({
  version: process.env.WATSON_VERSION,
  iam_apikey: process.env.WATSON_APIKEY
});
//---*

// Initialize AWS
aws.config.update({
  secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,
  accessKeyId: process.env.AWS_ACCESS_KEY_ID,
  region: 'us-east-1'
});

const s3 = new aws.S3();

let upload = multer({
    storage: multerS3({
        s3: s3,
        bucket: ""leafy-me/public"",
        key: function (req, file, cb) {
            console.log(file);
            let path = ""https://s3.amazonaws.com/leafy-me/public/"";
            let newImage = file.fieldname + Date.now() + "".jpg"";
            path += newImage;
            cb(null, newImage);
        }
    })
});  
//---*

module.exports = function (app) {

  // Function 
  app.get('/api/user_trees/:token', function(req, res) {
    console.log(req.params.token);
    console.log(mongoose.Types.ObjectId(req.params.token));
    db.UserSession.find({_id: mongoose.Types.ObjectId(req.params.token)}) //req.params.token
        .then(function(session) {
          db.Post.find({user_id: session[0].userId})
            .then(function(trees) {
              console.log(""hello"");
              res.send(trees);
            })
            .catch(function(err) {
              return res.json(err);
            });
          })
          .catch(function(err) {
            return res.json(err);
          });
    });

  // Route for image upload to AWS, Watson processing, etc.
  app.post('/api/image/image-upload/:token', upload.single('photo'), function(req, res, next) {

      let token = req.params.token; 
      let user_id = '';
      db.UserSession.find({_id: mongoose.Types.ObjectId(req.params.token)})
        .then(function(res) {
          console.log(res);
          user_id = res[0].userId;
          console.log(res[0].userId);
        })
      // Set up Watson parameters

      let image_url =  req.file.location;
      const classifier_ids = [""trees_447821576""];
      const threshold = 0.6;

      let params = {
          url: image_url,
          classifier_ids: classifier_ids,
          threshold: threshold
      };
      //---*

      visualRecognition.classify(params, function(err, response) { // Watson request
          if (err) {
            console.log(err);
          }
          else //get Watson results back
            console.log(JSON.stringify(response, null, 2));
            let trees = response.images[0].classifiers[0].classes; // Access Watson returned tree types
            if (trees.length === 0) { // If there are no tree types, respond client that the image isn't recognized
              res.send(""Image not recognized"");
            } else if (trees.length === 1) { // If there is one tree type, make a database entry and return tree data to client
            // Mongo storage
              let result = {};
              result.path = image_url;
              result.name = trees[0].class;
              console.log(user_id);
              db.Tree.find({name: result.name})
                  .then(function(tree) {
                      result.user_id = user_id;
                      result.sciName = tree[0].sciName;
                      result.range = tree[0].range;
                      db.Post.create(result)
                          .then(function(dbPost) {
                              console.log(dbPost)
                              res.send(dbPost);
                          })
                          .catch(function(err) {
                              return res.json(err);
                          });
                  })
            //---*
              } else { // If there are more than one tree types identified, ask client for help.
                  res.send(""Please pick one of these images"");
              }
      });
  });

  // --------------sign up------------------------------------------------------------
  app.post('/api/account/signup', (req, res, next) =&gt; {
    const { body } = req;
    const {
      username,
      password
    } = body;

    if (!username) {
      return res.send({
        success: false,
        message: ""Username required.""
      });
    }

    if (!password) {
      return res.send({
        success: false,
        message: ""Password required.""
      });
    }

    db.User.find({
      username: username
    }, (err, previousUsers) =&gt; {
      if (err) {
        return res.send({
          success: false,
          message: ""Error""
        });
      } else if (previousUsers.length &gt; 0) {
        return res.send({
          success: false,
          message: ""Username is taken.""
        })
      }

      const newUser = new db.User();
      newUser.username = username;
      newUser.password = newUser.generateHash(password);
      newUser.save((err, user) =&gt; {
        if (err) {
          return res.send({
            success: false,
            message: ""Server error""
          })
        }
        return res.send({
          success: true,
          message: ""Sign Up successful!""
        })
      })
    })
  });

  // --------------sign in -----------------------------------------------------------
  app.post('/api/account/signin', (req, res, next) =&gt; {
    const { body } = req;
    const {
      username,
      password
    } = body;

    if (!username) {
      return res.send({
        success: false,
        message: ""Username required.""
      });
    }

    if (!password) {
      return res.send({
        success: false,
        message: ""Password required.""
      });
    }

    db.User.find({
      username: username
    }, (err, users) =&gt; {
      if (err) {
        return res.send({
          success: false,
          message: ""Server Error""
        });
      }
      if (users.length != 1) {
        return res.send({
          success: false,
          message: ""Invalid""
        })
      }

      const user = users[0];
      if (!user.validPassword(password)) {
        return res.send({
          success: false,
          message: ""Invalid""
        })
      }

      const userSession = new db.UserSession();
      userSession.userId = user._id;
      userSession.save((err, doc) =&gt; {
        if (err) {
          return res.send({
            success: false,
            message: ""Server Error""
          });
        }
        console.log(doc);
        return res.send({
          success: true,
          message: ""Sign In successful"",
          token: doc._id
        });
      });
    });
  });

  // --------------verify--------------------------------------------------------------
  app.get('/api/account/verify', (req, res, next) =&gt; {

    const { query } = req;
    const { token } = query;

    db.UserSession.find({
      _id: token,
      isDeleted: false
    }, (err, sessions) =&gt; {
      if (err) {
        return res.send({
          success: false,
          message: ""Server Error""
        })
      }

      if (sessions.length != 1) {
        return res.send({
          success: false,
          message: ""Invalid""
        })
      }

      else {
        return res.send({
          success: true,
          message: 'good'
        })
      }
    })
  })

  // ---------------logout-------------------------------------------------------------
  app.get('/api/account/logout', (req, res, next) =&gt; {
    const { query } = req;
    const { token } = query;

    db.UserSession.findOneAndUpdate({
      _id: token,
      isDeleted: false
    }, {
        $set: { isDeleted: true }
      }, null, (err, sessions) =&gt; {
        if (err) {
          return res.send({
            success: false,
            message: ""Server Error""
          })
        }

        return res.send({
          success: true,
          message: 'good'
        })
      })
  })
};
</code></pre>

<p>I would greatly appreciate any advice offered to fix this problem. I've been working on this for several days and can't make it work. I am fairly new to coding -- and completely new to React -- and I'm getting very discouraged.
Thank you in advance.</p>

<p><strong>UPDATE</strong>
Thanks for the quick responses! After looking through my files I realized I didn't have a .env file, so there were no keys for the Watson api or AWS. I added one and the proxy error went away. But now I'm still having other errors.</p>

<p>In the app, you fill out the sign up form and then it takes you to the sign in page. I add my name and password to the sign up page, but I don't think it is taking in the information. When I try to sign in, my name and/or password is apparently invalid. (The only restrictions on passwords in the model is that it must be 6 characters.) When I try to sign in I get the following in the console:</p>

<pre><code>{data: {…}, status: 200, statusText: ""OK"", headers: {…}, config: {…}, …}
config: 
adapter: ƒ xhrAdapter(config)
data: ""{""username"":""Christina"",""password"":""password""}""
headers: {Accept: ""application/json, text/plain, */*"", Content-Type: ""application/json;charset=utf-8""}
maxContentLength: -1
method: ""post""
timeout: 0
transformRequest: {0: ƒ}0: ƒ 
transformResponse: {0: ƒ}
url: ""/api/account/signin""
validateStatus: ƒ validateStatus(status)
xsrfCookieName: ""XSRF-TOKEN""
xsrfHeaderName: ""X-XSRF-TOKEN""
__proto__: Object
data: 
message: ""Invalid""
success: false
__proto__: Object
headers: 
connection: ""close""
content-length: ""37""
content-type: ""application/json; charset=utf-8""
date: ""Tue, 14 Aug 2018 20:03:01 GMT""
etag: ""W/""25-GgaVhntYazB/MPzwqX72WRtisKI""""
vary: ""Accept-Encoding""
x-powered-by: ""Express""
. . . 
SignIn.js:68 false
</code></pre>

<p>Now I'm wondering if this is a problem with the database.</p>

<p>Here is the sign-in page:</p>

<pre><code>import React, { Component } from ""react"";
import { Redirect } from ""react-router"";
import { setInStorage } from '../../utils/storage';
import Input from ""../../components/Input"";
import API from ""../../utils/API"";
import Header from ""../../components/Header"";
import Footer from ""../../components/Footer"";

import ""./SignIn.css"";


class SignIn extends Component {
  constructor(props) {
      super(props);
      this.state = {
          token: '',
          signInUser: '',
          signInPass: '',
          signInError: '',
          fireRedirect: false
      }

    this.HandleInputChangeSignInPass = this.HandleInputChangeSignInPass.bind(this);
    this.HandleInputChangeSignInUser = this.HandleInputChangeSignInUser.bind(this);
    this.onSignIn = this.onSignIn.bind(this);
  }

  HandleInputChangeSignInUser(event) {
    this.setState({
      signInUser: event.target.value
    });
  }

  HandleInputChangeSignInPass(event) {
    this.setState({
      signInPass: event.target.value
    });
  }

  onSignIn(e) {
    e.preventDefault()
    const {
      signInUser,
      signInPass
    } = this.state

    let siObj = {
      username: signInUser,
      password: signInPass
    }

    API.signIn(siObj)
      .then(json =&gt; {
        console.log(json)
        if (json.data.success === true) {
          console.log(json.data.token);
          console.log(json.data);
          setInStorage('the_main-app', { token: json.data.token });
          this.setState({
            signInError: json.data.message,
            isLoading: false,
            signInUser: '',
            signInPass: '',
            token: json.data.token
          });
          this.setState({ fireRedirect: true });
        } else {
          console.log(json.data.success);
          this.setState({
            signInError: json.message,
            isLoading: false
          })
          this.setState({ signInError: true });
        }

      });
  }

  render() {
    const {
      signInUser,
      signInPass,
      fireRedirect
    } = this.state;
      return (
        &lt;div className=""signInPage""&gt;
          {/* &lt;Container&gt; */}
            &lt;Header/&gt;
            &lt;form className=""signIn-form""&gt;
                &lt;h3 className=""signin-heading""&gt; Hello &lt;/h3&gt;
                &lt;Input
                    type=""text""
                    placeholder=""Username""
                    value={signInUser}
                    onChange={this.HandleInputChangeSignInUser}/&gt;
                &lt;Input
                    type=""password""
                    placeholder=""Password""
                    value={signInPass}
                    onChange={this.HandleInputChangeSignInPass}/&gt;
                &lt;br /&gt;
                &lt;button type=""button"" className=""btn btn-success"" id=""signin"" onClick={this.onSignIn}&gt;Sign In&lt;/button&gt;
                &lt;br&gt;&lt;/br&gt;
                  {
                    this.state.signInError ? &lt;p id=""error""&gt;Invalid Username or Password&lt;/p&gt; : &lt;br/&gt;
                  }
            &lt;/form&gt;
            {fireRedirect &amp;&amp; (
              &lt;Redirect to={'/profile'} /&gt;
            )}
            &lt;Footer /&gt;

        {/* &lt;/Container&gt; */}
      &lt;/div&gt;
    );
  }

}


export default SignIn;
</code></pre>

<p>Any suggestions would be greatly welcomed.</p>

<p><strong>Update:</strong></p>

<p>Everything is working now! Yay!!!</p>",,1,5,,2018-08-14 03:46:19.470 UTC,,2018-08-14 23:27:19.303 UTC,2018-08-14 23:27:19.303 UTC,,9738049,,9738049,1,1,reactjs|proxy,2043
CompareFaces Throws Error When Using Cropped Image,49762291,CompareFaces Throws Error When Using Cropped Image,"<p>I am using Amazon Rekognition to compare faces between images loaded into memory in python, but when I try to pass cropped images, it throws an error:
    <code>InvalidParameterException: An error occurred (InvalidParameterException) when calling the CompareFaces operation: Request has Invalid Parameters</code></p>

<p>an example of the code being used is:</p>

<pre><code>ret, frame = vid.read()
frame = cv2.resize(frame1, (0,0), fx=.5, fy=.5)
frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
content1 = cv2.imencode('.jpg',frame[:,0:540,:])[1].tostring()
content2 = cv2.imencode('.jpg',frame[:,1:540,:])[1].tostring()
awsclient=boto3.client('rekognition','us-west-2',
                    aws_access_key_id='********************',
                    aws_secret_access_key='*****************')
awsresponse = awsclient.compare_faces(SourceImage={'Bytes': content1},
                                  TargetImage={'Bytes': content2})
</code></pre>

<p>The last line throws the error. However, if I submit:</p>

<pre><code>awsresponse = awsclient.compare_faces(SourceImage={'Bytes': content1},
                                  TargetImage={'Bytes': content1})
</code></pre>

<p>There is no error and faces are matched. Does anybody have ideas as to the reason for this error?</p>

<p>A full trace for this error:</p>

<pre><code>---------------------------------------------------------------------------
InvalidParameterException                 Traceback (most recent call last)
&lt;ipython-input-227-60437107e6df&gt; in &lt;module&gt;()
      8                         aws_secret_access_key='***********')
      9 awsresponse = awsclient.compare_faces(SourceImage={'Bytes': content1},
---&gt; 10                                   TargetImage={'Bytes': content2})

/usr/local/lib/python3.5/dist-packages/botocore/client.py in _api_call(self, *args, **kwargs)
    312                     ""%s() only accepts keyword arguments."" % py_operation_name)
    313             # The ""self"" in this scope is referring to the BaseClient.
--&gt; 314             return self._make_api_call(operation_name, kwargs)
    315 
    316         _api_call.__name__ = str(py_operation_name)

/usr/local/lib/python3.5/dist-packages/botocore/client.py in _make_api_call(self, operation_name, api_params)
    610             error_code = parsed_response.get(""Error"", {}).get(""Code"")
    611             error_class = self.exceptions.from_code(error_code)
--&gt; 612             raise error_class(parsed_response, operation_name)
    613         else:
    614             return parsed_response

InvalidParameterException: An error occurred (InvalidParameterException) when calling the CompareFaces operation: Request has Invalid Parameters
</code></pre>",,0,3,,2018-04-10 20:23:05.240 UTC,,2018-04-10 20:33:11.100 UTC,2018-04-10 20:33:11.100 UTC,,1468229,,1468229,1,1,python|amazon-web-services|amazon-rekognition,83
How can I use Google Natural Processing API to extract emails from a text?,50219443,How can I use Google Natural Processing API to extract emails from a text?,"<p>I want to extract emails from a text which I get from the business cards using Google Vision API. How can I do that using Natural Language API?
(I'm using Python)</p>",50222330,1,0,,2018-05-07 17:21:08.993 UTC,0,2018-05-08 08:09:09.857 UTC,2018-05-08 08:09:09.857 UTC,,4482491,,5627572,1,-1,python|google-cloud-platform|google-natural-language,70
cUrl with json string and file upload,46962077,cUrl with json string and file upload,"<p>how do I have to ""translate"" the following curl command into a valid php curl function?</p>

<pre><code>curl -X POST 
     -F ""images_file=@fruitbowl.jpg"" 
     -F parameters=%7B%22classifier_ids%22%3A%5B%22testtype_205919966%22%5D%2C%22threshold%22%3A0%7D  
     'https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classify?api_key={key}&amp;version=2016-05-20'""
</code></pre>

<p>It seems that I'm doing something wrong and I can't figure out the problem:</p>

<pre><code>$method = 'POST'
$url = 'https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classify?api_key=&lt;myApiKey&gt;&amp;version=2016-05-20'
$data = array(
    array(&lt;file-information&gt;),
    array(&lt;json-string&gt;),
)
$header = array(
            'Content-Type: application/json',
            'Content-Length: ' . strlen(&lt;json-string&gt;),
                )
        )

public function send($method, $url, $data = null, $header = null)
{
    $curl = curl_init();

    switch ($method) {
        case ""POST"":
            curl_setopt($curl, CURLOPT_POST, 1);

            if ($data) {
                $postData = $this-&gt;renderPostData($data);
                curl_setopt($curl, CURLOPT_POSTFIELDS, $postData);
            }
            break;
    }

    if($header) {
        curl_setopt($curl, CURLOPT_HEADER, 1);
        curl_setopt($curl,CURLOPT_HTTPHEADER,$header);
    }

    curl_setopt($curl, CURLOPT_URL, $url);
    curl_setopt($curl, CURLOPT_RETURNTRANSFER, 1);
    $response = curl_exec($curl);
}

protected function renderPostData($data)
{
    $postData = array();
    foreach ($data as $file) {
        if ($file['isFile']) {
            if(pathinfo($file['path'], PATHINFO_EXTENSION) == 'zip'){
                $postData[$file['name']] = new \CURLFile($file['path'], 'application/zip', $file['name']);
            }
            else {
                $postData[$file['name']] = new \CURLFile($file['path'], 'application/octet-stream', $file['name']);
            }
        } else {
            // this contains the json encoded string
            $postData[$file['name']] = $file['path'];
        }
    }

    return $postData;
}
</code></pre>

<p>I tried several variations and the Watson Visual Recognition API error is now:</p>

<blockquote>
  <p>{
      ""custom_classes"": 0,
      ""images"": [
          {
              ""error"": {
                  ""description"": ""Invalid image data. Supported formats are JPG and PNG."",
                  ""error_id"": ""input_error""
              }
          }
      ],
      ""images_processed"": 1
  }</p>
</blockquote>

<p>before it was:</p>

<blockquote>
  <p>{
      ""error"": {
          ""code"": 400,
          ""description"": ""Invalid JSON content received. Unable to parse."",
          ""error_id"": ""parameter_error""
      },
      ""images_processed"": 0
  }</p>
</blockquote>

<p>Thank you for your help! </p>",,1,4,,2017-10-26 19:11:32.353 UTC,,2017-10-27 11:16:30.943 UTC,,,,,1185351,1,0,php|json|curl|watson|visual-recognition,387
watson visual recognition pricing/costing clarifications,38480586,watson visual recognition pricing/costing clarifications,"<p>The documentation for the Watson Visual Recognition Services indicates that the costs for the service are</p>

<pre><code>$0.25/Training image
$0.004 for classification per image per custom class
$10 for storage per custom class per month
</code></pre>

<p>So if I have 1 custom classifier with 1000 classes trained with 50 images each. Then the costs would be</p>

<pre><code>$0.25 * 50000 = $12500 for initial training
$10 * 1000 = $10000 per month for storage
$4 per classification call if tested against all 1000 classes in the classifier
</code></pre>

<p>is my understanding correct? The $4 per call seems too high. Is the cost per class (1000 in this case) or per custom classifier (1 in this case)?</p>

<p>If I later add more training images (say additional 500 images), would the $0.25 per training image be charged for only these additional images ($0.25 * 500 = $125) or would it instead be $0.25 * 50500 = $12625?</p>",38482304,1,0,,2016-07-20 11:53:59.663 UTC,,2016-07-20 13:12:06.690 UTC,,,,,6250587,1,0,ibm-cloud|ibm-watson|visual-recognition,249
Returning formatted text from GCP Vision PDF results,56266707,Returning formatted text from GCP Vision PDF results,"<p>I finally got my script to submit PDF document to Google Storage and then extract Text using Google Vision for PDF, as described in <a href=""https://cloud.google.com/vision/docs/pdf"" rel=""nofollow noreferrer"">documentation</a>.</p>

<p>The data is returned in a huge JSON file. There's one node that contains test, but it's no longer formatted. Only line breaks are delineated with <code>\n</code>. I don't really care so much about the line breaks, as much as paragraphs.</p>

<p>How can I return it formatted? Are there any libraries that would work with GCP to enhance JSON output?</p>",,0,5,,2019-05-23 00:45:11.177 UTC,2,2019-05-26 22:46:46.187 UTC,,,,,434218,1,2,php|pdf|google-vision|pdftotext|pdf-to-html,69
"AWS rekognition x,y formula",44225909,"AWS rekognition x,y formula","<p>I am attempting to find the x,y coordinates for the nose of a person in a photo with AWS rekognition, im using the javascript SDK and am getting returned the values as a ratio of the size of the picture. This is clearly stated in the documentation and I have no problem with that.</p>

<p>What I am after is a formula to find the exact x,y of the nose ""landmark"" from the perspective of the whole image, not the bounding box. below is my output from rekognition.</p>

<pre><code>{ FaceDetails: 
   [ { BoundingBox: 
        { Width: 0.6399999856948853,
          Height: 0.47999998927116394,
          Left: 0.1644444465637207,
          Top: 0.17666666209697723 },
       Landmarks: 
        [ { Type: 'eyeLeft',
            X: 0.36238425970077515,
            Y: 0.3900916874408722 },
          { Type: 'eyeRight', X: 0.5580493807792664, Y: 0.362303763628006 },
          { Type: 'nose', X: 0.4164798855781555, Y: 0.4511926472187042 },
          { Type: 'mouthLeft',
            X: 0.42259901762008667,
            Y: 0.5591621994972229 },
          { Type: 'mouthRight',
            X: 0.5580134391784668,
            Y: 0.5394133925437927 } ],
       Pose: 
        { Roll: -9.781778335571289,
          Yaw: -20.029239654541016,
          Pitch: 10.893087387084961 },
       Quality: { Brightness: 59.32780456542969, Sharpness: 99.9980239868164 },
       Confidence: 99.99403381347656 } ] }
</code></pre>

<p>I have an image that is 2576x1932 is there some formula that can be applied here to just give me the x,y of the nose in the picture. currently it gives the x,y of the nose from inside the bounding box (i think). My math skill is not really up to this one.</p>

<p>From the documentation: </p>

<p>Boundingbox: </p>

<blockquote>
  <p>The top and left values returned are ratios of the overall image size.
  For example, if the input image is 700x200 pixels, and the top-left
  coordinate of the bounding box is 350x50 pixels, the API returns a
  left value of 0.5 (350/700) and a top value of 0.25 (50/200).</p>
</blockquote>

<p>Landmark:</p>

<blockquote>
  <p>x-coordinate from the top left of the landmark expressed as the ration
  of the width of the image. For example, if the images is 700x200 and
  the x-coordinate of the landmark is at 350 pixels, this value is 0.5.</p>
</blockquote>",44232960,2,0,,2017-05-28 09:45:37.500 UTC,,2018-09-12 07:53:41.223 UTC,2017-05-28 10:59:39.660 UTC,,7386839,,7386839,1,0,javascript|amazon-web-services|numbers|amazon-rekognition,268
"IBM Watson Visual recognition. Is it possible to get X,Y coordinates from an specific object?",45468418,"IBM Watson Visual recognition. Is it possible to get X,Y coordinates from an specific object?","<p>I'm starting with an university project and I'm looking for a tool that help me to find the coordinates(X,Y) in pixels from an specific objects in an image(I'm not talking about text). I'm trying to know if IBM Watson Visual recognition could help me out to get this achieve, or if you know any other tool that could work better. </p>

<p>Thank you. </p>",45471592,2,0,,2017-08-02 18:28:30.490 UTC,1,2017-08-02 21:50:58.637 UTC,,,,,4533743,1,0,api|ibm-watson|visual-recognition,222
Google App Engine Error: Fetch in a thread that is neither the original request thread nor a thread created by ThreadManager,44832036,Google App Engine Error: Fetch in a thread that is neither the original request thread nor a thread created by ThreadManager,"<p>App Engine gives the error: </p>

<pre><code>com.google.apphosting.api.ApiProxy$CallNotFoundException: Can't make API call urlfetch.Fetch in a thread that is neither the original request thread nor a thread created by ThreadManager
</code></pre>

<p>when I make call to Google Vision API inside Callable in async Servlet.
How to make it working?</p>

<p>servlet:</p>

<pre><code>public class OcrForTextServlet extends HttpServlet {
    @Override
    public void doPost(HttpServletRequest req, HttpServletResponse response) {
            byte[] file = extractFile(req);
            String[] languages = req.getParameterValues(""language"");
            ExecutorService executor = Executors.newFixedThreadPool(2);
            Future&lt;String&gt; result = executor.submit(new OcrCallableTask(file, languages));
            executor.shutdown();
            response.getWriter().write(result.get()); //ERROR HERE
</code></pre>

<p>Full stack trace:</p>

<pre><code>[INFO] java.util.concurrent.ExecutionException: com.google.apphosting.api.ApiProxy$CallNotFoundException: Can't make API call urlfetch.Fetch in a thread that is neither the original request thread nor a thread created by ThreadManager
[INFO]  at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[INFO]  at java.util.concurrent.FutureTask.get(FutureTask.java:192)
[INFO]  at ocrme_backend.servlets.ocr.OcrForTextServlet.doPost(OcrForTextServlet.java:49)
[INFO]  at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
[INFO]  at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
[INFO]  at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
[INFO]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)
[INFO]  at com.google.appengine.api.socket.dev.DevSocketFilter.doFilter(DevSocketFilter.java:74)
[INFO]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
[INFO]  at com.google.appengine.tools.development.ResponseRewriterFilter.doFilter(ResponseRewriterFilter.java:134)
[INFO]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
[INFO]  at com.google.appengine.tools.development.HeaderVerificationFilter.doFilter(HeaderVerificationFilter.java:34)
[INFO]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
[INFO]  at com.google.appengine.api.blobstore.dev.ServeBlobFilter.doFilter(ServeBlobFilter.java:63)
[INFO]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
[INFO]  at com.google.apphosting.utils.servlet.TransactionCleanupFilter.doFilter(TransactionCleanupFilter.java:48)
[INFO]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
[INFO]  at com.google.appengine.tools.development.jetty9.StaticFileFilter.doFilter(StaticFileFilter.java:122)
[INFO]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
[INFO]  at com.google.appengine.tools.development.DevAppServerModulesFilter.doDirectRequest(DevAppServerModulesFilter.java:366)
[INFO]  at com.google.appengine.tools.development.DevAppServerModulesFilter.doDirectModuleRequest(DevAppServerModulesFilter.java:349)
[INFO]  at com.google.appengine.tools.development.DevAppServerModulesFilter.doFilter(DevAppServerModulesFilter.java:116)
[INFO]  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1751)
[INFO]  at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)
[INFO]  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
[INFO]  at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:524)
[INFO]  at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
[INFO]  at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
[INFO]  at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
[INFO]  at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
[INFO]  at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
[INFO]  at com.google.appengine.tools.development.jetty9.DevAppEngineWebAppContext.doScope(DevAppEngineWebAppContext.java:112)
[INFO]  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
[INFO]  at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
[INFO]  at com.google.appengine.tools.development.jetty9.JettyContainerService$ApiProxyHandler.handle(JettyContainerService.java:596)
[INFO]  at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
[INFO]  at org.eclipse.jetty.server.Server.handle(Server.java:534)
[INFO]  at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
[INFO]  at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
[INFO]  at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
[INFO]  at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
[INFO]  at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
[INFO]  at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
[INFO]  at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
[INFO]  at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
[INFO]  at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
[INFO]  at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
[INFO]  at java.lang.Thread.run(Thread.java:745)
[INFO] Caused by: com.google.apphosting.api.ApiProxy$CallNotFoundException: Can't make API call urlfetch.Fetch in a thread that is neither the original request thread nor a thread created by ThreadManager
[INFO]  at com.google.apphosting.api.ApiProxy$CallNotFoundException.foreignThread(ApiProxy.java:844)
[INFO]  at com.google.apphosting.api.ApiProxy.makeSyncCall(ApiProxy.java:116)
[INFO]  at com.google.appengine.api.urlfetch.URLFetchServiceImpl.fetch(URLFetchServiceImpl.java:40)
[INFO]  at com.google.apphosting.utils.security.urlfetch.URLFetchServiceStreamHandler$Connection.fetchResponse(URLFetchServiceStreamHandler.java:543)
[INFO]  at com.google.apphosting.utils.security.urlfetch.URLFetchServiceStreamHandler$Connection.getInputStream(URLFetchServiceStreamHandler.java:422)
[INFO]  at com.google.apphosting.utils.security.urlfetch.URLFetchServiceStreamHandler$Connection.getResponseCode(URLFetchServiceStreamHandler.java:275)
[INFO]  at com.google.api.client.http.javanet.NetHttpResponse.&lt;init&gt;(NetHttpResponse.java:37)
[INFO]  at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94)
[INFO]  at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972)
[INFO]  at com.google.api.client.auth.oauth2.TokenRequest.executeUnparsed(TokenRequest.java:283)
[INFO]  at com.google.api.client.auth.oauth2.TokenRequest.execute(TokenRequest.java:307)
[INFO]  at com.google.api.client.auth.oauth2.Credential.executeRefreshToken(Credential.java:570)
[INFO]  at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.executeRefreshToken(GoogleCredential.java:362)
[INFO]  at com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:489)
[INFO]  at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStreamUser(GoogleCredential.java:772)
[INFO]  at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:257)
[INFO]  at com.google.api.client.googleapis.auth.oauth2.DefaultCredentialProvider.getCredentialUsingWellKnownFile(DefaultCredentialProvider.java:249)
[INFO]  at com.google.api.client.googleapis.auth.oauth2.DefaultCredentialProvider.getDefaultCredentialUnsynchronized(DefaultCredentialProvider.java:117)
[INFO]  at com.google.api.client.googleapis.auth.oauth2.DefaultCredentialProvider.getDefaultCredential(DefaultCredentialProvider.java:91)
[INFO]  at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:213)
[INFO]  at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:191)
[INFO]  at ocrme_backend.ocr.OCRProcessorImpl.getVisionService(OCRProcessorImpl.java:40)
[INFO]  at ocrme_backend.ocr.OCRProcessorImpl.&lt;init&gt;(OCRProcessorImpl.java:32)
[INFO]  at ocrme_backend.servlets.ocr.OcrCallableTask.doStaff(OcrCallableTask.java:27)
[INFO]  at ocrme_backend.servlets.ocr.OcrCallableTask.call(OcrCallableTask.java:39)
[INFO]  at ocrme_backend.servlets.ocr.OcrCallableTask.call(OcrCallableTask.java:14)
[INFO]  at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[INFO]  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[INFO]  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[INFO]  ... 1 more
</code></pre>

<p>API call makes the error:</p>

<pre><code>/**
 * Connects to the Vision API using Application Default Credentials.
 */
public static Vision getVisionService() throws IOException, GeneralSecurityException {
    GoogleCredential credential =
            GoogleCredential.getApplicationDefault().createScoped(VisionScopes.all());
    com.google.api.client.json.JsonFactory jsonFactory = JacksonFactory.getDefaultInstance();
    return new Vision.Builder(GoogleNetHttpTransport.newTrustedTransport(), jsonFactory, credential)
            .setApplicationName(APPLICATION_NAME)
            .build();
}
</code></pre>

<p>I am using the last version of javax.servlet-api (3.1.0), GAE (1.9.52) and Java 8. I need to obtain the result from the async part.
How can I do this?
Thank you for any help.</p>

<p><strong>UPDATE:</strong><br>
I tried to <code>use com.google.appengine.api.ThreadManager</code> as mentioned in error message but it gives the same error. Here is my updated servlet:</p>

<pre><code>@Override
public void doPost(HttpServletRequest req, HttpServletResponse response) {

    try {
        byte[] file = extractFile(req);
        String[] languages = req.getParameterValues(""language"");

        ThreadFactory factory = ThreadManager.currentRequestThreadFactory();
        ExecutorService service = Executors.newCachedThreadPool(factory);
        Future&lt;String&gt; result =
                service.submit(new OcrCallableTask(file, languages));
        response.getWriter().write(result.get()); //ERROR HERE
</code></pre>

<p>Next test passed OK:</p>

<pre><code>public class OcrCallableTaskTest {
    @Test
    public void testCall() throws Exception {
        ExecutorService service = Executors.newFixedThreadPool(2);
        Future&lt;String&gt; result = service.submit(new OcrCallableTask(FileProvider.getFile(), null));
        Assert.assertTrue(result.get() != null);
        Assert.assertTrue(result.get().length() &gt; 0);
    }
}
</code></pre>

<p><strong>UPDATE 2:</strong> <br>
(Reply to the proposing do staff in request's thread.) <br>
Realy I don't need extra thread for my servlet. It is only the attempt to fix the error.
I have the same error if I don't use multithreading in my app:</p>

<pre><code>[INFO] com.google.api.gax.grpc.ApiException: io.grpc.StatusRuntimeException: UNAUTHENTICATED
[INFO]  at com.google.api.gax.grpc.ExceptionTransformingCallable$ExceptionTransformingFuture.onFailure(ExceptionTransformingCallable.java:108)
[INFO]  at com.google.api.core.ApiFutures$1.onFailure(ApiFutures.java:52)
[INFO]  at com.google.common.util.concurrent.Futures$6.run(Futures.java:1310)
[INFO]  at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:457)
[INFO]  at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156)
[INFO]  at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145)
[INFO]  at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:202)
[INFO]  at io.grpc.stub.ClientCalls$GrpcFuture.setException(ClientCalls.java:463)
[INFO]  at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:439)
[INFO]  at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:428)
[INFO]  at io.grpc.internal.ClientCallImpl.access$100(ClientCallImpl.java:76)
[INFO]  at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:514)
[INFO]  at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$700(ClientCallImpl.java:431)
[INFO]  at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:546)
[INFO]  at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:52)
[INFO]  at io.grpc.internal.SerializingExecutor$TaskRunner.run(SerializingExecutor.java:152)
[INFO]  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[INFO]  at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[INFO]  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
[INFO]  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
[INFO]  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[INFO]  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[INFO]  at java.lang.Thread.run(Thread.java:745)
[INFO] Caused by: io.grpc.StatusRuntimeException: UNAUTHENTICATED
[INFO]  at io.grpc.Status.asRuntimeException(Status.java:540)
[INFO]  ... 15 more
[INFO] Caused by: com.google.apphosting.api.ApiProxy$CallNotFoundException: Can't make API call urlfetch.Fetch in a thread that is neither the original request thread nor a thread created by ThreadManager
[INFO]  at com.google.apphosting.api.ApiProxy$CallNotFoundException.foreignThread(ApiProxy.java:844)
[INFO]  at com.google.apphosting.api.ApiProxy.makeSyncCall(ApiProxy.java:116)
[INFO]  at com.google.appengine.api.urlfetch.URLFetchServiceImpl.fetch(URLFetchServiceImpl.java:40)
[INFO]  at com.google.apphosting.utils.security.urlfetch.URLFetchServiceStreamHandler$Connection.fetchResponse(URLFetchServiceStreamHandler.java:543)
[INFO]  at com.google.apphosting.utils.security.urlfetch.URLFetchServiceStreamHandler$Connection.getInputStream(URLFetchServiceStreamHandler.java:422)
[INFO]  at com.google.apphosting.utils.security.urlfetch.URLFetchServiceStreamHandler$Connection.getResponseCode(URLFetchServiceStreamHandler.java:275)
[INFO]  at com.google.api.client.http.javanet.NetHttpResponse.&lt;init&gt;(NetHttpResponse.java:37)
[INFO]  at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94)
[INFO]  at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972)
[INFO]  at com.google.auth.oauth2.UserCredentials.refreshAccessToken(UserCredentials.java:207)
[INFO]  at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:149)
[INFO]  at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:135)
[INFO]  at io.grpc.auth.GoogleAuthLibraryCallCredentials$1.run(GoogleAuthLibraryCallCredentials.java:110)
[INFO]  ... 7 more
</code></pre>

<p>code of servlet - no multithreading:</p>

<pre><code> @Override
    public void doPost(HttpServletRequest req, HttpServletResponse response) {
        try {
            byte[] file = extractFile(req);
            String[] languages = req.getParameterValues(""language"");

            OCRProcessor processor = new OCRProcessorImpl();
            String jsonResult;

            if (languages == null || languages.length &lt;= 0) { 
                jsonResult = processor.ocrForText(file);
</code></pre>

<p>next test passed ok:</p>

<pre><code>@Test
public void doOCR() throws Exception {
    byte[] file = FileProvider.getImageFile().getFile();
    String result = ocrProcessor.ocrForText(file);
    assertNotNull(result);
    assertTrue(result.length() &gt; 0);
}
</code></pre>

<p>It seams (GAE + API calls) not compatible with Servlet architecture. Thank you for any advices. </p>",,1,6,,2017-06-29 17:59:06.587 UTC,3,2017-07-11 06:29:28.047 UTC,2017-07-10 08:40:04.077 UTC,,3627736,,3627736,1,10,java|multithreading|google-app-engine|servlets,1644
Google Cloud Vision API limit and usage,42421317,Google Cloud Vision API limit and usage,"<p>In my project I need to use the Google Vision API in order to know if an image uploaded by the user is rated as adult content or not.</p>

<p>In their documentation page we have a pricing table <a href=""https://cloud.google.com/vision/pricing"" rel=""nofollow noreferrer"">Google Vision API Pricing</a> in which we can see there is a free plan in which you have some limits. In order to start using this I needed to join the free trial and set a billing account.</p>

<p>My questions are the following:</p>

<ol>
<li>When the limits are reached, am I going to be billed by Google? Or the service will be unavailable until I accept to be billed for that?</li>
<li>As I have joined to a free trial, is this API usage limited to the trial period (60 days), or is it free (limited) even when the trial period has ended?</li>
</ol>",42487643,1,3,,2017-02-23 16:31:47.203 UTC,,2017-02-28 19:53:03.260 UTC,2017-02-28 19:53:03.260 UTC,,2898636,,2898636,1,5,google-api|google-cloud-vision,861
"How to fix ""Error encountered while training. Error in Watson Visual Recognition service: Unexpected response code when creating API Key token: 400""",55464541,"How to fix ""Error encountered while training. Error in Watson Visual Recognition service: Unexpected response code when creating API Key token: 400""","<p>I'm developing a project with Watson Visual Recognition, I create classes and upload the zip files in order to train the model. When I click ""Train Model"" the following error appears:
Error encountered while training.
Error in Watson Visual Recognition service: Unexpected response code when creating API Key token: 400.
How do I fix this problem?</p>

<p>I renamed the classes with names with no spaces.
Zip file names don't contain spaces.
I deleted and remade the project.</p>

<p>I expect the output after clicking ""Train Model"" is ""Training complete. Your model training is complete. Click here to view and test your model.""</p>",,0,0,,2019-04-01 22:38:40.960 UTC,,2019-04-01 22:38:40.960 UTC,,,,,11295487,1,0,ibm-watson|api-key|visual-recognition,36
Google Vision API: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target,37508678,Google Vision API: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target,"<p>I've been testing an application example using Google Vision API.</p>

<p>I have the code from their github, and created the relevant project and credentials file (something Google Cloud requests) and tried to run the code.</p>

<p>I get the following:</p>

<blockquote>
  <p>sun.security.validator.ValidatorException: PKIX path building failed:
  sun.security.provider.certpath.SunCertPathBuilderException: unable to
  find valid certification path to requested target</p>
</blockquote>

<p>I've tried to add the relevant SSL certificates to my JRE's cacerts, I've restarted eclipse and an admin, but I still get this.</p>

<p><strong><em>EDIT</em></strong>
I've also tried to manually set the ssl properties as follows:</p>

<pre><code>        System.setProperty(""javax.net.ssl.keyStore"", ""C:/path/to/cacerts"");
        System.setProperty(""javax.net.ssl.keyStorePassword"",""cacerts_pass"");
        System.setProperty(""javax.net.ssl.trustStore"", ""C:/path/to/cacerts"");
        System.setProperty(""javax.net.ssl.trustStorePassword"",""cacerts_pass"");
</code></pre>

<p>Any ideas to resolve this issue, please?</p>

<p>If there's anything I should provide, please let me know.
Thanks in advance.</p>",,0,23,,2016-05-29 10:17:30 UTC,,2017-01-13 08:31:15.570 UTC,2016-06-08 15:22:41.820 UTC,,5231007,,1067083,1,0,java|ssl-certificate|google-cloud-vision,476
textRecognizer.detect(frame); it returns size as 0,52848248,textRecognizer.detect(frame); it returns size as 0,"<p>I am using google vision API and trying to get the text from the captured image.
I have set the captured image in an image view and then I am trying to get the text from the image. but I am getting SparseArray of size 0. what can be the problem. Here is my java code.</p>

<pre><code>    public class MainActivity extends AppCompatActivity {


    ImageView imgPic;
    TextView tvText;
    Button btnClick, btnCapture;
    private int REQUEST_IMAGE_CAPTURE = 1;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);

        imgPic = findViewById(R.id.img_pic);
        tvText = findViewById(R.id.tv_text);
        btnClick = findViewById(R.id.btn_click);
        btnCapture = findViewById(R.id.btn_capture);

        btnCapture.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {

                Intent takePictureIntent = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);
                if (takePictureIntent.resolveActivity(getPackageManager()) != null) {
                    startActivityForResult(takePictureIntent, REQUEST_IMAGE_CAPTURE);
                }


            }
        });


        btnClick.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {

Bitmap bitmap;
 if (imgPic.getDrawable() != null &amp;&amp; imgPic.getDrawable() instanceof BitmapDrawable) {
                bitmap = ((BitmapDrawable) imgPic.getDrawable()).getBitmap();
            }

                TextRecognizer textRecognizer = new TextRecognizer.Builder(getApplicationContext()).build();
                if (!textRecognizer.isOperational()) {

                    Toast.makeText(MainActivity.this, ""could not get the text"", Toast.LENGTH_SHORT).show();
                } else {
                    if (bitmap != null) {
                        Frame frame = new Frame.Builder().setBitmap(bitmap).build();
                        SparseArray&lt;TextBlock&gt; items = textRecognizer.detect(frame);
                        StringBuilder sb = new StringBuilder();
                        for (int i = 0; i &lt; items.size(); i++) {
                            TextBlock myItem = items.valueAt(i);
                            Log.e(""hello"", (String) myItem.getValue());
                            sb.append(myItem.getValue());
                            sb.append(""\n"");

                        }

                        tvText.setText(sb.toString());
                    } else {
                        Toast.makeText(MainActivity.this, ""returned null"", Toast.LENGTH_SHORT).show();

                    }

                }
            }
        });


    }

    @Override
    protected void onActivityResult(int requestCode, int resultCode, Intent data) {
        super.onActivityResult(requestCode, resultCode, data);
        if (requestCode == REQUEST_IMAGE_CAPTURE &amp;&amp; resultCode == RESULT_OK) {
            Bundle extras = data.getExtras();
            Bitmap imageBitmap = (Bitmap) extras.get(""data"");
            imgPic.setImageBitmap(imageBitmap);


        }

    }
}
</code></pre>

<p>here is my main activity xml file.</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;

&lt;LinearLayout xmlns:android=""http://schemas.android.com/apk/res/android""
    xmlns:app=""http://schemas.android.com/apk/res-auto""
    xmlns:tools=""http://schemas.android.com/tools""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""
    android:orientation=""vertical""
    tools:context="".MainActivity""&gt;
    &lt;Button
        android:id=""@+id/btn_capture""
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:layout_marginTop=""20dp""
        android:text=""capture""
        /&gt;


    &lt;ImageView
        android:id=""@+id/img_pic""
        android:layout_width=""370dp""
        android:layout_height=""300dp"" /&gt;

    &lt;TextView
        android:textColor=""@color/colorAccent""
        android:id=""@+id/tv_text""
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:text=""Hello World!""
        android:textSize=""25sp""
        app:layout_constraintBottom_toBottomOf=""parent""
        app:layout_constraintLeft_toLeftOf=""parent""
        app:layout_constraintRight_toRightOf=""parent""
        app:layout_constraintTop_toTopOf=""parent"" /&gt;

    &lt;Button
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:text=""click here""
        android:id=""@+id/btn_click""/&gt;
&lt;/LinearLayout&gt;
</code></pre>

<p>The main thing is when i set image manually in an imageView it shows the result but when i capture the image by my self and then i try to get the text i am not getting the results i am always gets a 0 sized array.</p>",52848406,1,0,,2018-10-17 06:07:49.790 UTC,,2018-10-17 17:11:53.903 UTC,2018-10-17 17:11:53.903 UTC,,9633929,,9633929,1,0,java|android|bitmap|google-vision,66
Google Vision Accent Character Set NodeJs,38860919,Google Vision Accent Character Set NodeJs,"<p>I'm trying to use Google Vision service with NodeJs. However when I request a text detection of an image, it gives only English Alphabet characters (characters without accents) which is not enough for me. How can I get the UTF-32 characters?</p>

<p>For example: the real text ""öğrenci"" but the service returns ""ogrenci""</p>",,2,0,,2016-08-09 21:43:12.827 UTC,,2017-06-15 17:44:14.990 UTC,,,,,2604150,1,0,node.js|special-characters|google-cloud-platform|google-vision,232
Google Video Intelligence - even preview doesn't work,54979768,Google Video Intelligence - even preview doesn't work,"<p>I try to use google cloud video intelligence demo on their site:
<a href=""https://cloud.google.com/video-intelligence/"" rel=""nofollow noreferrer"">https://cloud.google.com/video-intelligence/</a>
and it works perfectly fine with their predefine demos to choose. 
When i try to use my own location the video loads forever. <strong>Even if I just download their sample video (which worked) and upload it in my bucket.</strong></p>

<p>I checked the path correctness many times over. It's simple and fine. Anyone could suggest some way to investigate it?</p>",,0,0,,2019-03-04 08:58:58.750 UTC,,2019-05-29 05:21:42.973 UTC,,,,,7064147,1,0,google-cloud-platform|video-intelligence-api,33
Does Google's mobile vision support languages other than the English?,42220293,Does Google's mobile vision support languages other than the English?,"<p>I've used Google's vision API to extract text (only English) from any given images in one of my android application. The application fetches every libraries needed, online, only once at the time of installation and then can extract the English text out of an image without the need of the internet after that. Is it possible to achieve the same result with languages other than English?
Lately, I came to know about the Google's Cloud Vision API which does support different languages but it requires the internet every time you want to scan images. So, to be precise, I just want to know if I can extract texts of any other languages from an image just adding this line in the app dependencies and if yes, then how?</p>

<pre><code>compile 'com.google.android.gms:play-services-vision:10.0.1'
</code></pre>",42220686,1,0,,2017-02-14 07:33:22.443 UTC,,2017-02-14 07:57:22.270 UTC,,,,,4387975,1,0,android|google-vision,502
Training Watson Visual Recognition classifier,44215222,Training Watson Visual Recognition classifier,"<p>I am trying to evaluate the training function of the Watson visual Recognition API.
Has anyone some experience with costumizing classifers for Visual Recognition?
I have some expierence myself with training the classifier and found some infomation in this blog:
<a href=""http://christopher5106.github.io/computer/vision/2016/12/23/ibm-watson-bluemix-visual-api-to-create-custom-classifier.html"" rel=""nofollow noreferrer"">http://christopher5106.github.io/computer/vision/2016/12/23/ibm-watson-bluemix-visual-api-to-create-custom-classifier.html</a></p>

<p>What I really would like to know is how much pictures do I need of an object to classify it with an accuracy of 75%?
How long does it take to get such a result?</p>

<p>Thank you in advance for your help.</p>",44255009,1,1,,2017-05-27 09:31:29.077 UTC,,2017-05-30 06:29:37.063 UTC,,,,,7913856,1,1,ibm-watson|watson,206
aws sagemaker for detecting text in an image,54521080,aws sagemaker for detecting text in an image,"<p>I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.</p>

<p>I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.</p>",54524071,1,0,,2019-02-04 17:11:02.230 UTC,,2019-02-04 20:46:42.943 UTC,,,,,4104481,1,3,amazon-web-services|amazon-sagemaker,195
Can OCR be performed on a video using Google Cloud Vision or Video intelligence API?,47749413,Can OCR be performed on a video using Google Cloud Vision or Video intelligence API?,"<p>I have used Google's Vision OCR a lot and it is really very accurate. I was wondering if I can do the OCR on a video file or video stream. Say, I have some surveillance video and I want to get all the text throughout that video. In Google's Video intelligence API, I can only get the labels, which I am guessing is using label detection API of Google Vision. I think there might be challenges to OCR on every frame of video, but still wanted to try to start a discussion on how it can be done. There might not be a perfect solution, but even if we get 50% of it, it's better than nothing. </p>",,3,4,,2017-12-11 09:07:26.567 UTC,2,2019-04-06 23:42:58.297 UTC,,,,,9082805,1,3,video|google-cloud-platform|ocr|google-cloud-vision,852
Pipfile failing for valid dependencies (six 1.12.0),55930223,Pipfile failing for valid dependencies (six 1.12.0),"<p>A valid Pipfile which builds on some machines isn't building on others, and dies with a dependency error.</p>

<p>We see this error when running <code>pipenv install --dev</code> even though it looks like <code>six</code> version 1.12.0 should match the set of constraints:</p>

<pre><code>ERROR: ERROR: Could not find a version that matches six&gt;=1.10.0,&gt;=1.12,&gt;=1.2,&gt;=1.5.2,&gt;=1.6.1,&gt;=1.7.0,&gt;=1.9.0,~=1.10.0
Tried: 0.9.0, 0.9.1, 0.9.2, 1.0.0, 1.1.0, 1.2.0, 1.3.0, 1.4.0, 1.4.1, 1.5.0, 1.5.0, 1.5.1, 1.5.1, 1.5.2, 1.5.2, 1.6.0, 1.6.0, 1.6.1, 1.6.1, 1.7.0, 1.7.0, 1.7.1, 1.7.1, 1.7.2, 1.7.2, 1.7.3, 1.7.3, 1.8.0, 1.8.0, 1.9.0, 1.9.0, 1.10.0, 1.10.0, 1.11.0, 1.11.0, 1.12.0, 1.12.0
S
</code></pre>

<p>The original Pipfile is:</p>

<pre><code>[[source]]

url = ""https://pypi.python.org/simple""
verify_ssl = true


[dev-packages]

pytest = ""*""
ipdb = ""*""
django-environ = ""*""
responses = ""*""
django-extensions = ""*""
django = ""&lt;=1.11.7""
pytest-cov = ""*""
pytest-django = ""*""
""flake8"" = ""*""
pytest-xdist = ""*""


[packages]

grpcio = ""*""
pytest = ""*""
requests = ""*""
pytest-django = ""*""
""psycopg2"" = ""*""
arrow = ""*""
retrying = ""*""
djangorestframework = ""*""
""autopep8"" = ""*""
dj-environ = ""*""
pytz = ""*""
pillow = ""*""
raven = ""*""
colorlog = ""*""
django-extensions = ""*""
redis = ""&lt;3.0.0""
rq = ""&lt;0.11.0""
keen = ""*""
pymongo = ""*""
django-simple-history = ""*""
python-stdnum = ""*""
dictdiffer = ""*""
shippo = ""*""
shopifyapi = ""*""
mypy = ""*""
boto = ""*""
google-cloud-vision = ""*""
twilio = ""*""
django-phonenumber-field = ""*""
python-bcrypt = ""*""
phonenumbers = ""*""
core = {editable = true,path = "".""}
""tinys3"" = ""*""
dialogflow = ""*""
python-twitter = ""*""
sentry-sdk = ""*""
""oauth2client"" = ""*""
tenacity = ""*""
fakeredis = ""*""


[requires]

python_version = ""3.6""
</code></pre>

<p>We've tried a number of options, such as <code>pipenv install --ignore-pipfile</code> to use a <code>Pipfile.lock</code> from a machine that appeared to work, but it didn't install anything.  Running <code>pipenv graph</code> didn't produce any output. Running <code>pipenv install --skip-lock</code> failed with the original error.</p>",,0,0,,2019-05-01 00:20:40.617 UTC,,2019-05-01 00:20:40.617 UTC,,,,,1865272,1,0,pipenv,13
bundle update --source my_gem' updates more than just the specified gem,52451363,bundle update --source my_gem' updates more than just the specified gem,"<p>I created a ruby gem. And I made a change to it. But I did not change any of its development or runtime dependencies. After I made a change to the gem and pushed it to git, I then run <code>bundle update --source</code> on the Rails project that is using the gem:</p>

<pre><code>bundle update --source my_gem
</code></pre>

<p>My expectation is that it will just update my_gem and nothing else. However, I found it is updating several other gems in Gemfile.lock of my Rails project:</p>

<pre><code>-    google-cloud-core (1.2.6)
+    google-cloud-core (1.2.7)

-    google-cloud-env (1.0.4)
+    google-cloud-env (1.0.5)

-    google-cloud-vision (0.30.3)
+    google-cloud-vision (0.30.4)

-    signet (0.9.1)
+    signet (0.9.2)
</code></pre>

<p>Now yes my gem depends on google cloud. However, I did not update google cloud in my gem. I just updated one line of code in my gem itself. Why is it updating other gems and how can I prevent this?</p>",,1,0,,2018-09-21 21:41:58.100 UTC,,2018-09-21 21:44:39.113 UTC,,,,,4501354,1,0,ruby|bundler,122
Aws Rekognition Stream Processor Kinesis aways fails,51365884,Aws Rekognition Stream Processor Kinesis aways fails,"<p>When I start the stream processor it fails.
I create the processor and I check (list stream processors), the initial status is STOPPED, then I execute the command to start stream processor, I check again and the status returned is FAILED.</p>

<p>I'm using: 
- Raspberry Pi and Camera Pi;
- AWS SDK for PHP 3.x (<a href=""https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.Rekognition.RekognitionClient.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.Rekognition.RekognitionClient.html</a>)</p>

<p>More info:</p>

<h2>CREATE COLLECTION = OK</h2>

<pre><code>$result = $client-&gt;createCollection([
    'CollectionId' =&gt; 'novaColecao',
]);
</code></pre>

<h2>CREATE STREAM PROCESSOR = OK</h2>

<pre><code>$result = $client-&gt;createStreamProcessor([
    'Input' =&gt; [ // REQUIRED
        'KinesisVideoStream' =&gt; [
            'Arn' =&gt; 'arn:aws:kinesisvideo:us-east-1:086417584256:stream/streamA/1531689874652',
        ],
    ],
    'Name' =&gt; 'store-processor', // REQUIRED
    'Output' =&gt; [ // REQUIRED
        'KinesisDataStream' =&gt; [
            'Arn' =&gt; 'arn:aws:kinesis:us-east-1:086417584256:stream/dataA',
        ],
    ],
    'RoleArn' =&gt; 'arn:aws:iam::086417584256:role/funcaoLerGravarFluxoKinesis', // REQUIRED
    'Settings' =&gt; [ // REQUIRED
        'FaceSearch' =&gt; [
            'CollectionId' =&gt; 'novaColecao',
            'FaceMatchThreshold' =&gt; 85.5,
        ],
    ],
]);
</code></pre>

<h2>START STREAM PROCESSOR = FAILED</h2>

<pre><code>$result = $client-&gt;startStreamProcessor([
    'Name' =&gt; 'store-processor', // REQUIRED
]);
</code></pre>

<p>Print Screens folder: <a href=""https://drive.google.com/drive/folders/1JwzzUcyvkkFrqWtI_IofcwI2xkQZ_sNf?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/drive/folders/1JwzzUcyvkkFrqWtI_IofcwI2xkQZ_sNf?usp=sharing</a> </p>

<p>1 - Kinesis video stream: <a href=""https://drive.google.com/file/d/1w4BIY4LTWpFzDML9wT9nCxTzFcMTdfyg/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1w4BIY4LTWpFzDML9wT9nCxTzFcMTdfyg/view?usp=sharing</a>
2 - ""streamA"" (1): <a href=""https://drive.google.com/file/d/1vy5PRuA6dUxr-GaZQOKSWA4Jcq5HyeDS/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1vy5PRuA6dUxr-GaZQOKSWA4Jcq5HyeDS/view?usp=sharing</a> 
3 - ""streamA"" (2): <a href=""https://drive.google.com/file/d/11Zwb3TMKGuZ90xdXXYJ8SQE2ToE__dYV/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/11Zwb3TMKGuZ90xdXXYJ8SQE2ToE__dYV/view?usp=sharing</a>
4 - ""dataA"": <a href=""https://drive.google.com/file/d/17PeknCK7stklEJoDP6X0wHwmt-Wd825J/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/17PeknCK7stklEJoDP6X0wHwmt-Wd825J/view?usp=sharing</a>
5 - Role ARN: <a href=""https://drive.google.com/file/d/10qQcnIkWvMpmVAbYb6SoLafzhxLmG-r-/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/10qQcnIkWvMpmVAbYb6SoLafzhxLmG-r-/view?usp=sharing</a>
6 - Stream Processor Failed: <a href=""https://drive.google.com/file/d/1vrOOh7NVqkMFwDp3lMy3j0mTpm0-l2xs/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1vrOOh7NVqkMFwDp3lMy3j0mTpm0-l2xs/view?usp=sharing</a> </p>

<p>What am I doing wrong?</p>",,0,0,,2018-07-16 16:00:02.193 UTC,,2018-07-16 16:00:02.193 UTC,,,,,5374304,1,0,live-streaming|amazon-kinesis|amazon-rekognition,112
How to Improve TEXT_DETECTION of Google Vision API for specific language,40013910,How to Improve TEXT_DETECTION of Google Vision API for specific language,"<p>I am interested in TEXT_DETECTION of Google Vision API, it works impressively. But it seems that TEXT_DETECTION only gives exactly result when the text is in English. In my case, i want to use TEXT_DETECTION in a quite narrow context, for example detection text on ads banners in specific language (in Vietnamese for my case). Can i train the machine on my own data collection to get more exactly result? And how to implement this? </p>

<p>Beside TEXT_DETECTION of Google Vision API, Google also has Google's Optical Character Recognition (OCR) software using dependencies of Tesseract. As i known, they have different algorithms to detect text. I used both Google Docs and TEXT_DETECTION of Google Vision API to read text (in Vietnamse) from a picture. Google Docs gave a good result but Vision API didn't. Why Google Vision API does not inherit advantages of Google OCR?</p>

<p><strong>I want to say something more about Google Vision API Text Detection, maybe any Google Expert here and can read this</strong>. As Google announced, their TEXT_DETECTION was fantastic: ""<em>Even though the words in this image were slanted and unclear, the OCR extracts the words and their positions correctly. It even picks up the word ""beacon"" on the presenter's t-shirt</em>"". But for some of my pics, what happened was really funny. For example with <a href=""http://rongbay2.vcmedia.vn/thumb_max/up_new/2012/10/01/521383/201210095217_sam_2028.jpg"" rel=""nofollow"">this pic</a>, even the words ""Kem Oxit"" are very big in center of pic, it was not recognized. Or in <a href=""http://rongbay2.vcmedia.vn/thumb_max/up_new/2012/10/01/521383/201210095204_sam_2026.jpg"" rel=""nofollow"">this pic</a>, the red text ""HOA CHAT NGOC VIET"" in center of pic was not recognized too. There must be something wrong with the text detection algorithm. </p>",,2,0,,2016-10-13 06:37:46.147 UTC,2,2016-10-26 21:47:49.100 UTC,2016-10-14 16:56:08.950 UTC,,7011711,,7011711,1,2,google-cloud-platform|google-vision,6488
AWS Kinesis and Rekognition at 1 FPS,51588864,AWS Kinesis and Rekognition at 1 FPS,"<p>According to the documentation AWS Rekognition is processing maximum 1FPS. But Amazon tutorial use GStreamer (on linux) to stream 30FPS from a camera to Kinesis Stream. And there is an heavy lag (few seconds) from camera to rekognition.</p>

<p>Is there a better way than ""Kinesis Stream > Rekognition > Kinesis Data"" to use AWS Rekognition ?  Is it possible to ""put"" a Frame with a simple REST API to perform recognition ?</p>

<p>Thanks</p>",,0,0,,2018-07-30 07:20:33.577 UTC,,2018-07-30 07:20:33.577 UTC,,,,,149838,1,0,amazon-web-services|computer-vision|amazon-kinesis|amazon-rekognition,61
Google Vision API specify JSON file,45372938,Google Vision API specify JSON file,"<p>I'm trying to authenticate to Google Vision API using a JSON file. Normally, I do it using the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environmental variable which specifies the path to the JSON file itself.</p>

<p>However, I am required to specify this in my application itself and authenticate using the JSON file contents.</p>

<p>Now, I have tried to specify <code>CallSettings</code> to then pass it in as a parameter to the <code>ImageAnnotatorClient.Create</code> method. Sure enough, a <code>CallSettings</code> object can be created perfectly by reading the authentication info from the JSON file, but passing it in as a parameter to <code>ImageAnnotatorClient</code> seems to make no difference as the <code>ImageAnnotatorClient.Create</code> method is still looking for the environmental variable and throws an <code>InvalidOperation</code> exception, specifying that the environmental variable cannot be found.</p>

<p>Any idea how I can get the desired behavior?</p>

<p><a href=""https://cloud.google.com/vision/docs/reference/libraries"" rel=""nofollow noreferrer"">Google Vision Docs</a></p>",,1,0,,2017-07-28 11:57:53.847 UTC,,2018-12-12 05:55:08.180 UTC,2017-07-28 12:14:57.857 UTC,,852243,,7098811,1,2,c#|google-cloud-vision|google-vision,553
Failed to retrieve list of IAM roles and policies. (Loading Roles Error),52805384,Failed to retrieve list of IAM roles and policies. (Loading Roles Error),"<p>I am receiving this error message when trying to upload to an AWS Lambda.  This is from the <a href=""https://docs.aws.amazon.com/toolkit-for-visual-studio/latest/user-guide/lambda-rekognition-example.html"" rel=""nofollow noreferrer"">AWS Example</a>  - example</p>

<p>In particular it says IAM is not authorized to perform iam:ListRoles nor iam:ListPolicies.</p>

<p>I checked my IAM user's AWS Lambda ListFunctions in the AWS policy simulator which says it is working , although I do not know if this is relevant to my problem.</p>

<ul>
<li>thanks
<a href=""https://i.stack.imgur.com/eU8Kj.png"" rel=""nofollow noreferrer"">Error Message</a>
<a href=""https://i.stack.imgur.com/UZ2fL.png"" rel=""nofollow noreferrer"">Policy Simulator</a></li>
</ul>",52806104,1,0,,2018-10-14 17:43:39.973 UTC,,2018-10-14 19:01:46.813 UTC,,,,,5181022,1,0,amazon-web-services|aws-lambda|amazon-iam,273
Aws rekognition passing image using variable not working,46509380,Aws rekognition passing image using variable not working,"<pre><code>$rekognition = new RekognitionClient([
'version' =&gt; 'latest',
'region'  =&gt; 'us-west-2',
'credentials' =&gt; [
    'key'    =&gt; '....................',
    'secret' =&gt; '....................'
]
]);




$source2=json_encode($source);
echo $source2;

$result3=$rekognition-&gt;compareFaces([
'SimilarityThreshold' =&gt; 70.05,
'SourceImage' =&gt; [ 

    'S3Object' =&gt; [
        'Bucket' =&gt; 'krishrekog',
        'Name' =&gt; ""david.jpg"",             //Here is the problem

    ],
],
'TargetImage' =&gt; [ 

    'S3Object' =&gt; [
        'Bucket' =&gt; 'krishrekog',
        'Name' =&gt; 'target.jpg',

    ],
],
]);

$similar=0;



 foreach($result3['FaceMatches'] as $d){
   $similar=$d['Similarity'].""&lt;br&gt;"";
} 

if($similar&gt;75){
echo ""Matching faces!!"";
}
else{
echo ""Not Matching Faces!!"";
}
</code></pre>

<p>In the above code if I provide the source image as david.jpg it will work with no errors,but if I store image name in a variable and use json_encode and send it as source image.It throws a big error.What am I doing wrong?</p>",,0,2,,2017-10-01 05:02:21.273 UTC,,2017-10-01 05:02:21.273 UTC,,,,,7588051,1,0,php|json|amazon-web-services|amazon-rekognition,147
Face API - Variation in # of Emotion Attributes,53961202,Face API - Variation in # of Emotion Attributes,"<p>I am using the Microsoft Face API to detect faces and emotions in Android. I have a <code>TreeMap</code> whose key is the probability of an emotion attribute with the value being the attributes name like so:</p>

<pre><code>TreeMap&lt;Double, String&gt; treeMap = new TreeMap&lt;&gt;();
treeMap.put(person.faceAttributes.emotion.happiness, ""Happiness"");
treeMap.put(person.faceAttributes.emotion.anger, ""Anger"");
treeMap.put(person.faceAttributes.emotion.disgust, ""Disgust"");
treeMap.put(person.faceAttributes.emotion.sadness, ""Sadness"");
treeMap.put(person.faceAttributes.emotion.neutral, ""Neutral"");
treeMap.put(person.faceAttributes.emotion.surprise, ""Surprise"");
treeMap.put(person.faceAttributes.emotion.fear, ""Fear"");
</code></pre>

<p><em>(person is an object of the type <code>Face</code>)</em></p>

<p>What I want to do is rank those 7 emotions from most likely to least likely but the problem is that <strong>the size of <code>treeMap</code> varies</strong>. For example, when I choose to get the emotion attributes of a face that is <strong>smiling</strong>, <code>treeMap.size()</code> returns <strong>2</strong>. When I choose to get the emotion attributes of a face that is mainly <strong>neutral</strong>, <code>treeMap.size()</code> returns <strong>3</strong>.</p>

<p>Does anyone know why this behavior is occurring even though I have added 7 key-value pairs to <code>treeMap</code> using <code>treeMap.put()</code>? I have found that the key-value pairs which <em>do</em> exist in <code>treeMap</code> are the most likely emotions, but even if the other attributes were to be 0, why aren't they still being added to <code>treeMap</code>.</p>",54391050,2,0,,2018-12-28 16:03:36.507 UTC,,2019-01-27 17:44:34.980 UTC,,,,,10589041,1,0,android|azure|attributes|microsoft-cognitive|face-api,80
"Python POST request error ""image format unsupported"" using Microsoft Face API",39551502,"Python POST request error ""image format unsupported"" using Microsoft Face API","<p>I'm trying to send a binary image file to test the Microsoft Face API. Using POSTMAN works perfectly and I get back a <code>faceId</code> as expected. However, I try to transition that to Python code and it's currently giving me this error: </p>

<pre><code>{""error"": {""code"": ""InvalidImage"", ""message"": ""Decoding error, image format unsupported.""}}
</code></pre>

<p>I read this <a href=""https://stackoverflow.com/questions/36120746/decoding-error-image-format-unsupported-in-microsoft-face-api"">SO post</a> but it doesn't help. Here's my code for sending requests. I'm trying to mimic what POSTMAN is doing such as labeling it with the header <code>application/octet-stream</code> but it's not working. Any ideas?</p>

<pre><code>url = ""https://api.projectoxford.ai/face/v1.0/detect""

headers = {
  'ocp-apim-subscription-key': ""&lt;key&gt;"",
  'content-type': ""application/octet-stream"",
  'cache-control': ""no-cache"",
}

data = open('IMG_0670.jpg', 'rb')
files = {'IMG_0670.jpg': ('IMG_0670.jpg', data, 'application/octet-stream')}

response = requests.post(url, headers=headers, files=files)

print(response.text)
</code></pre>",39552568,1,0,,2016-09-17 20:38:16.940 UTC,2,2016-09-19 21:05:31.233 UTC,2017-05-23 12:00:52.550 UTC,,-1,,3903274,1,1,python|azure|python-requests|azure-api-apps|microsoft-cognitive,911
JSON Parsing Error Microsoft Emotion API ios,37718233,JSON Parsing Error Microsoft Emotion API ios,"<blockquote>
  <p>I am having problem in using the Microsoft Emotion API i have read the <a href=""https://dev.projectoxford.ai/docs/services/5639d931ca73072154c1ce89/operations/563b31ea778daf121cc3a5fa"" rel=""nofollow"">documentation</a> but not able to use it. Whenever i use the below code it gives Bad body JSON parsing error. I am not able to detect whats the problem in code.</p>
</blockquote>

<p>i have created a method 
- (IBAction)clickToGenerateEmotion:(id)sender </p>

<pre><code>NSString* path = @""https://api.projectoxford.ai/emotion/v1.0/recognize"";
NSArray* array = @[
                    @""entities=true"",
                   ];

NSString* string = [array componentsJoinedByString:@""&amp;""];
path = [path stringByAppendingFormat:@""?%@"", string];

NSLog(@""%@"", path);

UIImage *yourImage= _image;
NSData *imageData = UIImagePNGRepresentation(yourImage);

NSMutableURLRequest* _request = [NSMutableURLRequest requestWithURL:[NSURL URLWithString:path]];
[_request setHTTPMethod:@""POST""];
[_request setValue:@""application/json"" forHTTPHeaderField:@""Content-Type""];
[_request setValue:@""9b118d1587ce40899598b48a6c29b51a"" forHTTPHeaderField:@""Ocp-Apim-Subscription-Key""];

 NSDictionary *params = @{@""\""url\"""" : @""\""http://engineering.unl.edu/images/staff/Kayla_Person-small.jpg\""""};

  NSMutableArray *pairs = [[NSMutableArray alloc] initWithCapacity:0];
for (NSString *key in params) {
     [pairs addObject:[NSString stringWithFormat:@""%@=%@"", key, params[key]]];
   }

NSString *requestParams = [pairs componentsJoinedByString:@""&amp;""];


[_request setHTTPBody:imageData];

NSURLResponse *response = nil;
NSError *error = nil;
NSData* _connectionData = [NSURLConnection sendSynchronousRequest:_request returningResponse:&amp;response error:&amp;error];
NSLog(@""responseData = %@"", [[NSString alloc] initWithData:_connectionData encoding:NSUTF8StringEncoding]);
if (nil != error)
{
    NSLog(@""Error: %@"", error);
}
else
{
    NSError* error = nil;
    NSMutableDictionary* json = nil;
    NSString* dataString = [[NSString alloc] initWithData:_connectionData encoding:NSUTF8StringEncoding];
    NSLog(@""%@"", dataString);

    if (nil != _connectionData)
    {
        json = [NSJSONSerialization JSONObjectWithData:_connectionData options:NSJSONReadingMutableContainers error:&amp;error];
    }

    if (error || !json)
    {
        NSLog(@""Could not parse loaded json with error:%@"", error);
    }

    NSLog(@""%@"", json);
    _connectionData = nil;
}
`
</code></pre>

<h1>Thanks in advance!!!</h1>",,0,3,,2016-06-09 06:20:26.187 UTC,1,2016-12-10 23:39:59.700 UTC,2016-12-10 23:39:59.700 UTC,,6090608,,5772601,1,0,ios|iphone|json|microsoft-cognitive,212
How to get a bitmap image in ruby?,47838580,How to get a bitmap image in ruby?,"<p>The google vision API requires a bitmap sent as an argument. I am trying to convert a png from a URL to a bitmap to pass to the google api:</p>

<pre><code>require ""google/cloud/vision""
PROJECT_ID = Rails.application.secrets[""project_id""]
KEY_FILE = ""#{Rails.root}/#{Rails.application.secrets[""key_file""]}""
google_vision = Google::Cloud::Vision.new project: PROJECT_ID, keyfile: KEY_FILE
img = open(""https://www.google.com/images/branding/googlelogo/2x/googlelogo_color_272x92dp.png"").read
image = google_vision.image img
ArgumentError: string contains null byte
</code></pre>

<p>This is the source code processing of the gem:</p>

<pre><code>    def self.from_source source, vision = nil
      if source.respond_to?(:read) &amp;&amp; source.respond_to?(:rewind)
        return from_io(source, vision)
      end
      # Convert Storage::File objects to the URL
      source = source.to_gs_url if source.respond_to? :to_gs_url
      # Everything should be a string from now on
      source = String source
      # Create an Image from a HTTP/HTTPS URL or Google Storage URL.
      return from_url(source, vision) if url? source
      # Create an image from a file on the filesystem
      if File.file? source
        unless File.readable? source
          fail ArgumentError, ""Cannot read #{source}""
        end
        return from_io(File.open(source, ""rb""), vision)
      end
      fail ArgumentError, ""Unable to convert #{source} to an Image""
    end
</code></pre>

<p><a href=""https://github.com/GoogleCloudPlatform/google-cloud-ruby"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/google-cloud-ruby</a></p>

<p>Why is it telling me string contains null byte? How can I get a bitmap in ruby? </p>",47839046,3,4,,2017-12-15 19:14:35.763 UTC,,2017-12-15 21:23:07.477 UTC,2017-12-15 19:49:39.523 UTC,,4501354,,4501354,1,-1,ruby-on-rails|ruby|bitmap|google-cloud-vision,354
How to use Aws Rekognition to improve text base image quality?,54586779,How to use Aws Rekognition to improve text base image quality?,"<p>I am creating an API which will take a text based image(Business card) as an input and return image with improved quality by remove noise and applying some image filter. For this I want to use AWS Rekognition or any othre php library.
I have read AWS Rekognition documentation but I have not found any api to just improve image quality.
There are various php Libraries are available but I can not find a proper solution for this problem.
Please let me know a solution where a text based image quality can be improved either using php library or AWS Rekognition .
Following is sample image of Business Card of which we want to improve.
<a href=""https://i.stack.imgur.com/HvoJR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HvoJR.png"" alt=""enter image description here""></a></p>",,0,0,,2019-02-08 06:01:22.427 UTC,1,2019-02-08 06:01:22.427 UTC,,,,,9496018,1,0,php|image|image-processing|php-gd|amazon-rekognition,25
Unable to fetch the complete data using vision api,55095799,Unable to fetch the complete data using vision api,"<pre><code> $web_detection = $vision-&gt;image($imageData, ['WEB_DETECTION']);
 $imageFeatures[] = $web_detection;
 $data = [];
 $results = $vision-&gt;annotate($web_detection);
</code></pre>

<p>I am using this piece of code to fetch the data from vision API for matching pages, but I always got only 10 results, whereas in google vision official website the dataset is large than the same.</p>",,1,0,,2019-03-11 05:42:20.680 UTC,,2019-03-12 20:12:20.570 UTC,2019-03-11 09:09:31.297 UTC,,10727652,,10727652,1,0,php|google-app-engine|google-cloud-platform|google-cloud-vision|google-vision,39
Rekognition InvalidS3ObjectException Error in SearchFacesByImage (400 Bad Request.. Unable to get object metadata from S3),51959009,Rekognition InvalidS3ObjectException Error in SearchFacesByImage (400 Bad Request.. Unable to get object metadata from S3),"<p>I'm getting below error when I execute Rekognition using Image or Video (s3 source).</p>

<blockquote>
  <p>"" Error executing ""SearchFacesByImage"" on ""<a href=""https://rekognition.us-east-1.amazonaws.com"" rel=""nofollow noreferrer"">https://rekognition.us-east-1.amazonaws.com</a>""; AWS HTTP error: Client error: POST <a href=""https://rekognition.us-east-1.amazonaws.com"" rel=""nofollow noreferrer"">https://rekognition.us-east-1.amazonaws.com</a> resulted in a 400 Bad Request response: {""__type"":""InvalidS3ObjectException"",""Code"":""InvalidS3ObjectException"",""Logref"":""xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"","" (truncated...) InvalidS3ObjectException (client): Unable to get object metadata from S3. Check object key, region and/or access permissions. - {""__type"":""InvalidS3ObjectException"",""Code"":""InvalidS3ObjectException.</p>
</blockquote>

<p>My as code Below.</p>

<pre><code>$credentials = new Credentials('xxxxxxxxx', 'xxxxxxxxxxxx');

$rekognitionClient = RekognitionClient::factory(array(
            'region' =&gt; ""us-east-1"",
            'version' =&gt; 'latest',
            'credentials' =&gt; $credentials,
            'http' =&gt; [ 'verify' =&gt; false ]
 ));

$result = $rekognitionClient-&gt;searchFacesByImage([
                    'CollectionId' =&gt; ''.$collection_id.'', // REQUIRED
                    'FaceMatchThreshold' =&gt; 25.0,
                    'Image' =&gt; [ // REQUIRED
                            'S3Object' =&gt; [
                                'Bucket' =&gt; 'facetracking',
                                'Name' =&gt; 'test.jpg',
                                'Key' =&gt; 'test.jpg',
                                'Version' =&gt; 'latest',
                            ],
                    ], 'MaxFaces' =&gt; 2,
                ]);
</code></pre>

<p>When I'm uploading into S3, Getting an object from S3 and creating a collection in Rekognition All Its works fine but I can't execute searchFacesByImage (Source: S3) and startFaceSearch in Laravel PHP.</p>

<p>Also tired after bucket policy setup in S3 like below.</p>

<pre><code>{
    ""Version"": ""2012-10-17"",
    ""Id"": ""PolicyXXXXXX"",
    ""Statement"": [
        {
            ""Sid"": ""StmtXXXXXX"",
            ""Effect"": ""Allow"",
            ""Principal"": {
                ""AWS"": ""arn:aws:iam::XXXXXXXX:root""
            },
            ""Action"": [
                ""s3:GetObject"",
                ""s3:GetObjectAcl"",
                ""s3:GetObjectTagging"",
                ""s3:GetObjectTorrent"",
                ""s3:GetObjectVersion"",
                ""s3:GetObjectVersionAcl""
            ],
            ""Resource"": [
                ""arn:aws:s3:::&lt;MyBucketName&gt;/*"",
                ""arn:aws:s3:::&lt;MyBucketName&gt;""
            ]
        }
    ]
}
</code></pre>",51960013,1,0,,2018-08-22 02:13:19.633 UTC,,2018-08-22 04:43:39.367 UTC,,,,,8368982,1,0,php|laravel|amazon-web-services|laravel-5|amazon-s3,335
Does Google Vision have Swift Documentation for Facial Recognition,45133845,Does Google Vision have Swift Documentation for Facial Recognition,"<p>I'm trying to use the Google Vision Cocoapod module to recognize faces in my app.
When looking through the documentation, I could only find it in objective-C:
<a href=""https://developers.google.com/vision/ios/detect-faces-tutorial"" rel=""nofollow noreferrer"">https://developers.google.com/vision/ios/detect-faces-tutorial</a></p>

<p>Does there exist a swift version, and if so, where can I find it?
If there is no swift version, how can I go about converting this code to swift?</p>

<p>I don't only want to do facial detection. I also want to do landmark detection, which is why I'm not using the native IOS facial detection api.</p>",,1,0,,2017-07-16 22:31:08.857 UTC,1,2017-11-20 14:14:26.547 UTC,2017-07-16 22:38:52.743 UTC,,2251258,,2251258,1,1,ios|swift|google-vision,501
I am having a problem building my Node app with webpack,53335695,I am having a problem building my Node app with webpack,"<p>Let me describe my problem. I have developed a Node.js application with ES6, it is a REST API using several Node modules especially from google-cloud because I am using Google Cloud Vision and Translate APIs. </p>

<p>Until now there is no problem, everything works as expected, but things got wrong when I wanted to run it as a service on a Windows Server. I found a way to do it <a href=""https://stackoverflow.com/questions/10547974/how-to-install-node-js-as-windows-service"">here</a> using the Node module ""node-windows"". </p>

<p>I made the service script like in that post and the service got installed and shown in the Windows services list, but when I click to start it stops immediately. </p>

<p>After some analyzing I remembered that I am using ES6 that needs to be transpiled to ES5 to work like a standard Node script, so I thought that building my whole app with webpack will solve that for me, but not exactly, I got my bundle.js generated with webpack without any error (just some warnings), then when I try to run it with <code>node ./bundle.js</code> it returns errors like :</p>

<p><code>Error: The include '/protos/google/cloud/vision/v1/image_annotator.proto' was not found.</code></p>

<p>Though I made a rule in my webpack config file to support .proto files.</p>

<p>This is my <strong>webpack.config.js</strong> :</p>

<pre><code>module.exports = {
  target: ""node"",
  module: {
    rules: [
      {
        test: /\.js$/,
        exclude: /node_modules/,
        use: {
          loader: ""babel-loader""
        }
      },
      {
        test: /\.json$/,
        exclude: /node_modules/,
        use: {
          loader: ""json-loader""
        }
      },
      {
      test: /\.proto$/,
          use: {
            loader: ""pbf-loader""
          }
      },
      {
        test: /\.html$/,
        use: {
          loader: ""html-loader""
        }
      }
    ]
  }
};
</code></pre>

<p>At this level, I have no idea how to make those google-cloud .proto files to be integrated in my bundel.js, can someone please guide me ? thanks.</p>

<p>This is the code from grpc.js inside the @google-cloud module that tries to resolve the .proto files paths:</p>

<pre><code>GoogleProtoFilesRoot.prototype.resolvePath = function (originPath, includePath) {
        originPath = path.normalize(originPath);
        includePath = path.normalize(includePath);
        // Fully qualified paths don't need to be resolved.
        if (path.isAbsolute(includePath)) {
            if (!fs.existsSync(includePath)) {
                throw new Error('The include `' + includePath + '` was not found.');
            }
            return includePath;
        }
        if (COMMON_PROTO_FILES.indexOf(includePath) &gt; -1) {
            return path.join(googleProtoFilesDir, includePath);
        }
        return GoogleProtoFilesRoot._findIncludePath(originPath, includePath);
    };
</code></pre>",,0,6,,2018-11-16 10:15:08.667 UTC,,2018-11-16 11:22:19.520 UTC,2018-11-16 11:22:19.520 UTC,,9731901,,9731901,1,1,javascript|node.js|webpack,82
Google Vision API in background service?,54788081,Google Vision API in background service?,"<p>I'm trying to use the google vision api facetracker sample ( <a href=""https://github.com/googlesamples/android-vision/tree/master/visionSamples/FaceTracker"" rel=""nofollow noreferrer"">https://github.com/googlesamples/android-vision/tree/master/visionSamples/FaceTracker</a> )
to detect and track faces without showing the preview (at least detect, tracking is not necessary).</p>

<p>The main problem is that if I delete the ""preview"" I don't receive the callback in ""onNewItem(int faceId, Face item)""  (function in FaceTrackerActivity in the github link provided).</p>

<p>I've been lookin through SO and I didn't find anything similar to this question, also it's not necessary that I use ""google vision api"", if you know another system that can achieve this objective it's perfect.</p>

<p>Thanks for reading.</p>",,0,2,,2019-02-20 13:57:02.860 UTC,,2019-02-20 14:01:38.667 UTC,2019-02-20 14:01:38.667 UTC,,9364229,,9364229,1,0,android|preview|face-detection|google-vision|face,27
Google Cloud Vision text detection issues with text language management,36774015,Google Cloud Vision text detection issues with text language management,"<p>I've encountered the following issues with the text detection feature of Google Cloud Vision:</p>

<p>1) I submit the same image using the same Python code to Google Cloud Vision, from 2 different machines (the Windows-based development machine and the Linux-based ""production"" machine) but I get 2 different outputs. Same image, same code, same libraries, but the extracted text is different.</p>

<p>2) I get two differently detected locales for the two differently detected texts. My original text is Italian text mixed with digits. On the development machine, the detected locale is ""zh"" (Chinese). On the ""production"" machine, the detected locale is ""fil"". There isn't any ""fil"" code in <a href=""https://cloud.google.com/translate/v2/using_rest#language-params"" rel=""nofollow"">https://cloud.google.com/translate/v2/using_rest#language-params</a> so I don't know what it is (Filipino?). In any case, I get a better result on the development machine when the detected locale is ""zh"". So... same image, same code, but differently detected locale and differently detected text.</p>

<p>3) Therefore I try to force the ""it"" or ""zh"" locale using the ImageContext languageHints annotation <a href=""https://cloud.google.com/vision/reference/rest/v1/images/annotate#AnnotateImageRequest"" rel=""nofollow"">https://cloud.google.com/vision/reference/rest/v1/images/annotate#AnnotateImageRequest</a> and now there's the funny thing: if I set languageHints to ['it'] on the development machine, I get almost nothing as output from Google Cloud Vision. If I set it to ['ja'] (Japanese), Google Cloud Vision says that the text locale is ""it"" (!!) and I get some decent results (!!!). But if I set ['ja'] on the ""production"" machine, Google Cloud Vision says that the text locale is ""oc"" (?). So... same image, same code, but differently detected locale and differently detected text. Moreover, the detected locale and text don't follow what I set with languageHints, but when I change the languageHints parameter, the detected locale (and text) also changes in an unpredictable way.</p>

<p>Do you have any hint? Thanks.</p>",,0,0,,2016-04-21 15:23:13.920 UTC,,2016-04-21 15:23:13.920 UTC,,,,,6236183,1,1,google-cloud-vision,882
Amazon Rekognition not returning labels for image,52322574,Amazon Rekognition not returning labels for image,"<p>I have a whole bunch of aerial photos. For some unknown reason Rekognition does not return labels on some of them, even though they are very similar to each other. Shrinking the size of the photo does make it work, but the photo is already well below 15 mb limit. I think this is a bug on Rekognition. Has anyone else run into this?</p>",,0,0,,2018-09-13 22:30:13.407 UTC,,2018-09-13 22:30:13.407 UTC,,,,,1580359,1,0,amazon-rekognition,28
How I do text detection from image using google vision api?,39925488,How I do text detection from image using google vision api?,"<p>I new the Android code side and I have a project.</p>

<p>I want text detection from ımage using google vision api, but I cannot.</p>

<p>I search the internet but I can't find enough information and I know I should use Json(AsynTack).</p>

<p>Just want this not face detection,logo detection.</p>

<p>How can I do this, can you suggest anything?</p>",,3,0,,2016-10-07 20:41:25.987 UTC,,2018-10-28 14:37:39.120 UTC,2018-10-28 14:37:39.120 UTC,,1033581,,5770137,1,0,android|json|android-asynctask|google-cloud-vision,2450
How to use AWS Rekognition service to detect text in image with javascript,51630466,How to use AWS Rekognition service to detect text in image with javascript,<p>i want to use Aws rekognition service to detect text in image with java script..Kindly tell me procedure to do it.</p>,,1,1,,2018-08-01 10:00:40.890 UTC,,2018-08-01 12:01:39.203 UTC,2018-08-01 12:01:39.203 UTC,,174777,,10165077,1,-4,amazon-web-services|aws-sdk|amazon-rekognition,313
How to decrease the time gap between detections in Rekognition?,51156207,How to decrease the time gap between detections in Rekognition?,"<p>I am using Rekognition's <a href=""https://docs.aws.amazon.com/rekognition/latest/dg/API_StartPersonTracking.html"" rel=""nofollow noreferrer"">StartPersonTracking</a> and <a href=""https://docs.aws.amazon.com/rekognition/latest/dg/API_GetPersonTracking.html"" rel=""nofollow noreferrer"">GetPersonTracking</a> to analise a video with some people. The result is a json file, something like this:</p>

<pre><code>""Persons"": [{
        ""Timestamp"": 0, 
        ""Person"": {
            ""BoundingBox"": {
                ""Width"": 0.0520833320915699, 
                ""Top"": 0.3125, 
                ""Left"": 0.40625, 
                ""Height"": 0.09444444626569748
            }, 
            ""Index"": 4
        }
    }, 
    {
        ""Timestamp"": 33, 
        ""Person"": {
            ""BoundingBox"": {
                ""Width"": 0.20624999701976776, 
                ""Top"": 0.4611110985279083, 
                ""Left"": 0.17499999701976776, 
                ""Height"": 0.3986110985279083
            }, 
            ""Index"": 1
        }
    }, 
    {
        ""Timestamp"": 33, 
        ""Person"": {
            ""BoundingBox"": {
                ""Width"": 0.22291666269302368, 
                ""Top"": 0.3194444477558136, 
                ""Left"": 0.7708333134651184, 
                ""Height"": 0.5444444417953491
            }, 
            ""Index"": 2
        }
    }, 
    {
        ""Timestamp"": 33, 
        ""Person"": {
            ""BoundingBox"": {
                ""Width"": 0.12083332985639572, 
                ""Top"": 0.36666667461395264, 
                ""Left"": 0.4520833194255829, 
                ""Height"": 0.24722221493721008
            }, 
            ""Index"": 3
        }
    }, 
    {
        ""Timestamp"": 33, 
        ""Person"": {
            ""BoundingBox"": {
                ""Width"": 0.0520833320915699, 
                ""Top"": 0.3125, 
                ""Left"": 0.40625, 
                ""Height"": 0.09444444626569748
            }, 
            ""Index"": 4
        }
    }, 
    {
        ""Timestamp"": 100, 
        ""Person"": {
            ""BoundingBox"": {
                ""Width"": 0.24375000596046448, 
                ""Top"": 0.2888889014720917, 
                ""Left"": 0.5833333134651184, 
                ""Height"": 0.5847222208976746
            }, 
            ""Index"": 0
        }
    },...
</code></pre>

<p>Each item has its timestamp, so we can track each person throughout the video. The issue is that the gap between two detections can be quite large. Is there a known way to decrease the gap, i.e increasing the detection density?</p>

<p>I couldnt find anything in the docs, nor in php/java SDKs</p>",,0,0,,2018-07-03 13:44:46.970 UTC,,2018-07-04 11:32:57.150 UTC,2018-07-04 11:32:57.150 UTC,,4911877,,4911877,1,0,amazon-web-services|cloud|artificial-intelligence|amazon-rekognition,18
Android Studio - Google Vision for android 6,46470476,Android Studio - Google Vision for android 6,"<p>I want add flashlight control to my code on android studio (for android 6).
I'm using google vision and thi is my actually code:</p>

<p>Android Manifest:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>        &lt;uses-permission android:name=""android.permission.CAMERA"" /&gt;
        &lt;uses-permission android:name=""android.permission.FLASHLIGHT""/&gt;
        
        &lt;uses-feature android:name=""android.hardware.camera"" /&gt;
        &lt;uses-feature android:name=""android.hardware.camera.flash"" /&gt;</code></pre>
</div>
</div>
</p>

<p>Gradle:</p>

<pre><code>    compile 'com.google.android.gms:play-services-vision:11.0.4'
</code></pre>

<p>XML file:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>    &lt;SurfaceView
        android:id=""@+id/surface_view""
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:foregroundGravity=""center"" /&gt;

    &lt;TextView
        android:id=""@+id/barcode_value""
        android:layout_width=""0dp""
        android:layout_height=""0dp""
        android:text=""No Barcode""
        android:textSize=""20sp""
        android:visibility=""gone""
        tools:layout_editor_absoluteX=""119dp""
        tools:layout_editor_absoluteY=""474dp"" /&gt;</code></pre>
</div>
</div>
</p>

<p>Activity:</p>

<blockquote>
  <p>public class Qr3 extends AppCompatActivity{</p>

<pre><code>        private BarcodeDetector barcodeDetector;
        private CameraSource cameraSource;
        private SurfaceView cameraView;
        private TextView barcodeValue;
        private boolean flashavAilable = false;
        private boolean hasFlash;
        private boolean isFlashOn;

        #Override
        protected void onCreate(Bundle savedInstanceState) {
            super.onCreate(savedInstanceState);
      setContentView(R.layout.activity_qr3);
      //start verify flashLight
            hasFlash = getApplicationContext().getPackageManager().hasSystemFeature(PackageManager.FEATURE_CAMERA_FLASH);
            if (!hasFlash) {
    //alert ""noflash exist""
    else{
      //alert ""flash exist""
            flashavAilable = true;

    }//end verify flashLight

int permission = ContextCompat.checkSelfPermission(this,
                Manifest.permission.CAMERA);
        if (permission != PackageManager.PERMISSION_GRANTED) {makeRequest();}

            cameraView = (SurfaceView) findViewById(R.id.surface_view);
            barcodeValue = (TextView) findViewById(R.id.barcode_value);

            barcodeDetector = new BarcodeDetector.Builder(this)
                    .setBarcodeFormats(Barcode.ALL_FORMATS)
                    .build();
            cameraSource = new CameraSource.Builder(this, barcodeDetector)
                    .setAutoFocusEnabled(true)
                    .setRequestedFps(60.0f)
                    .setFacing(CameraSource.CAMERA_FACING_BACK)
                    .build();
            cameraView.getHolder().addCallback(new SurfaceHolder.Callback() {
                @Override
                public void surfaceCreated(SurfaceHolder holder) {
                    try {
                        cameraSource.start(cameraView.getHolder());
                    } catch (IOException ex) {
                        ex.printStackTrace();
                    }
                }
                #Override
                public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {}
                #Override
                public void surfaceDestroyed(SurfaceHolder holder) {cameraSource.stop();}});
    #Override
                public void receiveDetections(Detector.Detections&lt;Barcode&gt; detections) {
                    final SparseArray&lt;Barcode&gt; barcodes = detections.getDetectedItems();
                    if (barcodes.size() != 0) {
                        barcodeValue.post(new Runnable() {
                            #Override
                            public void run() {
                                //Update barcode value to TextView
                                barcodeValue.setText(barcodes.valueAt(0).displayValue);
                                cameraSource.stop();
                                AlertDialog.Builder dlgAlert = new  
  AlertDialog.Builder(Qr3.this);
  dlgAlert.setMessage(barcodes.valueAt(0).displayValue);
 dlgAlert.setTitle(""Value:"");
</code></pre>
  
  <p>dlgAlert.setPositiveButton(""Again"", newDialogInterface.OnClickListener() {
                                          Override
                                          public void onClick(DialogInterface >dialog, int which) {
                                              try {
                                                  //noinspection MissingPermission
                                                  cameraSource.start(cameraView.getHolder());
                                              } catch (IOException ex) {
                                                  ex.printStackTrace();
                                              }
                                          }
                                      });
                                      dlgAlert.setNegativeButton(""End"", new DialogInterface.OnClickListener() {
                                          #Override
                                          public void onClick(DialogInterface dialog, int which) {finish();}
                                      });
                                      dlgAlert.setCancelable(true);
                                      dlgAlert.create().show();
                                  }
                              });
                          }
                      }
                  });
          }//end onCreate
              //ascolta click pulsanti del volume per accendere o spegnere flash
              #Override
              public boolean dispatchKeyEvent(KeyEvent event) {
                  int action = event.getAction();
                  int keyCode = event.getKeyCode();
                  switch (keyCode) {
                      case KeyEvent.KEYCODE_VOLUME_UP:
                          if (flashavAilable ) {
                              if (action == KeyEvent.ACTION_DOWN) {
                                  turnOnFlash();
                              }
                          }
                          return true;
                      case KeyEvent.KEYCODE_VOLUME_DOWN:
                          if (flashavAilable ) {
                              if (action == KeyEvent.ACTION_DOWN) {
                                  turnOffFlash();
                              }
                          }
                          return true;
                      default:
                          //riabilita fullscreen
                          return super.dispatchKeyEvent(event);
                  }
              }
          private void turnOnFlash() {
                  if (!isFlashOn) {
                      //turn on flashlight &lt;-------------------------------------HELP
                      isFlashOn = true;
                      cameraSource.stop();
                  }
              }
              private void turnOffFlash() {
                  if (isFlashOn) {
                      //turn off flashlight &lt;-------------------------------------HELP
                      isFlashOn = false;
                  }
              }
          }</p>
</blockquote>

<p>How may I control the flashLight?</p>

<p>(can anyone fix the formatting of the code in my question?)</p>

<p>Thanks</p>",,0,0,,2017-09-28 13:33:01.880 UTC,,2017-09-28 13:33:01.880 UTC,,,,,8573066,1,2,android|android-6.0-marshmallow|android-studio-2.3|google-vision,172
"Google Cloud Vision API, identifying a snake in long grass",46289477,"Google Cloud Vision API, identifying a snake in long grass","<p>When I run the following image through the Google Cloud Vision API it see's the grass but not the snake. What can I do to improve object detection?</p>

<p><a href=""https://i.stack.imgur.com/a1k3l.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a1k3l.jpg"" alt=""Snake in long grass""></a></p>",,1,0,,2017-09-18 23:17:34.043 UTC,,2017-09-21 22:06:59.240 UTC,,,,,3488432,1,0,google-cloud-platform|google-cloud-vision|vision-api,161
"Can not get camera output, face detection android",43361706,"Can not get camera output, face detection android","<p>I am trying face detection and adding mask(graphic overlay) using google vision api ,the problem is i could not get the ouptut from camera after detecting and adding mask.so far I have tried this solution from github , <a href=""https://github.com/googlesamples/android-vision/issues/24"" rel=""nofollow noreferrer"">https://github.com/googlesamples/android-vision/issues/24</a> ,based on this issue i have added a custom detector class,
<a href=""https://stackoverflow.com/questions/32299947/mobile-vision-api-concatenate-new-detector-object-to-continue-frame-processing/32314136"">Mobile Vision API - concatenate new detector object to continue frame processing</a> .  and added this on mydetector class <a href=""https://stackoverflow.com/questions/32412197/how-to-create-bitmap-from-grayscaled-byte-buffer-image/33054982#33054982"">How to create Bitmap from grayscaled byte buffer image?</a> .</p>

<p>MyDetectorClass</p>

<pre><code>class MyFaceDetector extends Detector&lt;Face&gt; 
{
    private Detector&lt;Face&gt; mDelegate;

    MyFaceDetector(Detector&lt;Face&gt; delegate) {
        mDelegate = delegate;
    }

    public SparseArray&lt;Face&gt; detect(Frame frame) {
        // *** add your custom frame processing code here
        ByteBuffer byteBuffer = frame.getGrayscaleImageData();
        byte[] bytes = byteBuffer.array();
        int w = frame.getMetadata().getWidth();
        int h = frame.getMetadata().getHeight();
        YuvImage yuvimage=new YuvImage(bytes, ImageFormat.NV21, w, h, null);
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        yuvimage.compressToJpeg(new Rect(0, 0, w, h), 100, baos); // Where 100 is the quality of the generated jpeg
        byte[] jpegArray = baos.toByteArray();
        Bitmap bitmap = BitmapFactory.decodeByteArray(jpegArray, 0, jpegArray.length);
        Log.e(""got bitmap"",""bitmap val "" + bitmap);
        return mDelegate.detect(frame);
    }

    public boolean isOperational() {
        return mDelegate.isOperational();
    }

    public boolean setFocus(int id) {
        return mDelegate.setFocus(id);
    }
}
</code></pre>

<p>frame processing</p>

<pre><code>public SparseArray&lt;Face&gt; detect(Frame frame) 
{
    // *** add your custom frame processing code here
    ByteBuffer byteBuffer = frame.getGrayscaleImageData();
    byte[] bytes = byteBuffer.array();
    int w = frame.getMetadata().getWidth();
    int h = frame.getMetadata().getHeight();
    YuvImage yuvimage=new YuvImage(bytes, ImageFormat.NV21, w, h, null);
    ByteArrayOutputStream baos = new ByteArrayOutputStream();
    yuvimage.compressToJpeg(new Rect(0, 0, w, h), 100, baos); // Where 100 is the quality of the generated jpeg
    byte[] jpegArray = baos.toByteArray();
    Bitmap bitmap = BitmapFactory.decodeByteArray(jpegArray, 0, jpegArray.length);
    Log.e(""got bitmap"",""bitmap val "" + bitmap);
    return mDelegate.detect(frame);
}
</code></pre>

<p>i am getting a rotated bitmap ,that is without the mask (graphic overlay) i have added .How can i get the camera output with mask .</p>

<p>Thanks in advance.</p>",,1,0,,2017-04-12 06:17:46.457 UTC,,2017-04-20 15:55:43.677 UTC,2017-05-23 12:02:36.157 UTC,,-1,,4997636,1,0,android|face-detection|google-vision,959
Amazon Rekognition Text detection,50343162,Amazon Rekognition Text detection,"<p>I've been unable to find the URL to make the API call for AWS Rekognition for text detection. I found <a href=""https://docs.aws.amazon.com/rekognition/latest/dg/API_Reference.html"" rel=""nofollow noreferrer"">this</a> documentation for headers and parameters to be sent, but there is no Base URL mentioned in the post.</p>

<p>Is it available somewhere else?</p>",,1,0,,2018-05-15 05:54:48.147 UTC,,2018-05-15 06:31:38.170 UTC,2018-05-15 06:31:38.170 UTC,,174777,,8218789,1,0,amazon-web-services|aws-sdk|amazon-rekognition,224
How to use Google Vision API in python program?,47906157,How to use Google Vision API in python program?,"<p>I am trying to run the most basic text detection and OCR (Optical Character Recognition) program of Google Vision API in python.
My source code is taken from the Google Cloud tutorial for this API and it is the following:</p>

<pre><code>import  io
from google.cloud import vision
from google.cloud.vision import types

def detect_text(file):
    """"""Detects text in the file.""""""
    client = vision.ImageAnnotatorClient()

    with io.open(file, 'rb') as image_file:
        content = image_file.read()

    image = types.Image(content=content)

    response = client.text_detection(image=image)
    texts = response.text_annotations
    print('Texts:')

    for text in texts:
        print('\n""{}""'.format(text.description))

        vertices = (['({},{})'.format(vertex.x, vertex.y)
                    for vertex in text.bounding_poly.vertices])

        print('bounds: {}'.format(','.join(vertices)))

file_name = ""prescription.jpg""
detect_text(file_name)
</code></pre>

<p>However I get the following error:</p>

<pre><code>google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or
explicitly create credential and re-run the application. For more
information, please see
https://developers.google.com/accounts/docs/application-default-credentials.
</code></pre>

<p>This is weird because:</p>

<p>1) I created a new service account </p>

<p>2) I added <code>export GOOGLE_APPLICATION_CREDENTIALS=""/Users/User/PycharmProjects/GCP-OCR/Trial-e046e4bc6ce1.json""</code> to my .bash_profile (I put the json file at the Pycharm file of this project)
Perhaps the only weird thing is that the private key at the json file is around 20 lines while I would expect to be around 1 line.</p>

<p>How can I fix this bug and make the program running?</p>

<p>By the way the problem is solved if I simply add </p>

<pre><code>import os
os.environ[""GOOGLE_APPLICATION_CREDENTIALS""] = ""/Users/User/PycharmProjects/GCP-OCR/Trial-e046e4bc6ce1.json""
</code></pre>

<p>to my source code.</p>",,1,0,,2017-12-20 12:32:14.980 UTC,,2019-02-22 10:04:25.397 UTC,2017-12-20 17:10:24.537 UTC,,9024698,,9024698,1,2,python|google-cloud-platform|google-cloud-vision,1077
Why did my Facial Detection Script suddenly stop working?,55252358,Why did my Facial Detection Script suddenly stop working?,"<p>I am using Google Cloud Vision to detect faces within images. Earlier today, my code was working perfectly fine. This code is supposed to create a JSON string explaining if the image has a face, how certain Google vision is, and if there is an exception. However, now it is giving me an error message that I am finding hard to debug. The code is seen below:</p>

<pre><code>import io
import os
import json

# Imports the Google Cloud client library
from google.cloud import vision
from google.cloud.vision import types

class Face_Detector:

# Constructor for Face Detector
def __init__(self, im):
    self.im_file = im

# Creates String in Json format given whether a face was detected
# the confidence if yes, and the name of an exception if one was triggered
# (f_d is false if an exception is triggered)
# (con is None if f_d is false)
# (ex is None if an exception is not triggered)
def formatJson(self, f_d, con, ex):
    # Generates json String
    dic = { 
        ""face_detected"" :   f_d,
         ""confidence""   :   con,
         ""exception""    :   ex
        }
    jsonA = json.dumps(dic);

    return jsonA

# Checks image to see if a face exists
def detect_face(self):

    # vairbles to be used in Json

    # is face detected
    f_d = False
    # with what ertainty
    con = None
    # if there is an exception, displays which one
    ex = None

    try:
        # Creates an environment variable
        credential_path = ""C:\\Users\\Nick\\Desktop\\Proj1-7f68a23c3dd0.json""
        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path

        # Uses object field to define local variable
        im_file = self.im_file

        # Checks for valid file type
        if(not (im_file[-4:] == "".jpg"" or im_file[-4:] == "".png"" or im_file[-5:] == "".jpeg"")):
            ex = ""Invalid file type.""
            return Face_Detector.formatJson(self, f_d, con, ex)

        # Checks for images that are too large or too small
        im_size = os.stat(im_file).st_size
        if(im_size == 0):
            ex = ""Image file size of 0.""
            return Face_Detector.formatJson(self, f_d, con, ex)
        elif(im_size &gt; 10485760):
            ex = ""Exceeds image file size limit of 10MB.""
            return Face_Detector.formatJson(self, f_d, con, ex)

        # Instantiates a client
        client = vision.ImageAnnotatorClient()

        # Concatenates the service key address and the image file address
        file_name = os.path.join(
            os.path.dirname(""C:\\Users\\Nick\\Desktop\\&lt;ProjectName&gt;""),
            im_file)

        # Loads the image into RAM
        with io.open(file_name, 'rb') as image_file:
            content = image_file.read()

        # Makes data type compatable with the Google Cloud API
        image = types.Image(content=content)

        # Performs face detection
        response = client.face_detection(image=image)
        faces = response.face_annotations

        # Checks for the existance of face and confidence
        for face in faces:
            f_d = True
            con = face.detection_confidence
        return Face_Detector.formatJson(self, f_d, con, ex)

    except FileNotFoundError:
        ex = ""File Not Found.""
        return Face_Detector.formatJson(self, f_d, con, ex)


im = r""C:\\Users\\Nick\\Desktop\\Images\\Face2.jpg""
i = Face_Detector(im)
print(i.detect_face())
</code></pre>

<p>As of now, it seems to be producing this error:</p>

<pre><code>Traceback (most recent call last):
File ""C:\Users\Nick\AppData\Local\Programs\Python\Python36\lib\site-packages\google\api_core\grpc_helpers.py"", line 57, in error_remapped_callable
    return callable_(*args, **kwargs)
  File ""C:\Users\Nick\AppData\Local\Programs\Python\Python36\lib\site-packages\grpc\_channel.py"", line 549, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File ""C:\Users\Nick\AppData\Local\Programs\Python\Python36\lib\site-packages\grpc\_channel.py"", line 466, in _end_unary_response_blocking
    raise _Rendezvous(state, None, None, deadline)
grpc._channel._Rendezvous: &lt;_Rendezvous of RPC that terminated with:
    status = StatusCode.UNAVAILABLE
    details = ""Getting metadata from plugin failed with error: ('invalid_grant: Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values and use a clock with skew to account for clock differences between systems.', '{\n  ""error"": ""invalid_grant"",\n  ""error_description"": ""Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values and use a clock with skew to account for clock differences between systems.""\n}')""
    debug_error_string = ""{""created"":""@1553027287.140000000"",""description"":""Getting metadata from plugin failed with error: ('invalid_grant: Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values and use a clock with skew to account for clock differences between systems.', '{\n  ""error"": ""invalid_grant"",\n  ""error_description"": ""Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values and use a clock with skew to account for clock differences between systems.""\n}')"",""file"":""src/core/lib/security/credentials/plugin/plugin_credentials.cc"",""file_line"":79,""grpc_status"":14}""
&gt;

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""C:\Users\Nick\Desktop\Python Stuff\Face_Detector.py"", line 97, in &lt;module&gt;
    print(i.detect_face())
  File ""C:\Users\Nick\Desktop\Python Stuff\Face_Detector.py"", line 81, in detect_face
    response = client.face_detection(image=image)
  File ""C:\Users\Nick\AppData\Local\Programs\Python\Python36\lib\site-packages\google\cloud\vision_helpers\decorators.py"", line 101, in inner
    response = self.annotate_image(request, retry=retry, timeout=timeout)
  File ""C:\Users\Nick\AppData\Local\Programs\Python\Python36\lib\site-packages\google\cloud\vision_helpers\__init__.py"", line 72, in annotate_image
    r = self.batch_annotate_images([request], retry=retry, timeout=timeout)
  File ""C:\Users\Nick\AppData\Local\Programs\Python\Python36\lib\site-packages\google\cloud\vision_v1\gapic\image_annotator_client.py"", line 234, in batch_annotate_images
    request, retry=retry, timeout=timeout, metadata=metadata
  File ""C:\Users\Nick\AppData\Local\Programs\Python\Python36\lib\site-packages\google\api_core\gapic_v1\method.py"", line 143, in __call__
    return wrapped_func(*args, **kwargs)
  File ""C:\Users\Nick\AppData\Local\Programs\Python\Python36\lib\site-packages\google\api_core\grpc_helpers.py"", line 59, in error_remapped_callable
    six.raise_from(exceptions.from_grpc_error(exc), exc)
  File ""&lt;string&gt;"", line 3, in raise_from
google.api_core.exceptions.ServiceUnavailable: 503 Getting metadata from plugin failed with error: ('invalid_grant: Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values and use a clock with skew to account for clock differences between systems.', '{\n  ""error"": ""invalid_grant"",\n  ""error_description"": ""Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values and use a clock with skew to account for clock differences between systems.""\n}')
[Finished in 1.6s]
</code></pre>

<p>Could someone help me with this issue? This is my first time using Google Cloud Vision.</p>",55264102,1,3,,2019-03-20 01:39:00.013 UTC,,2019-03-20 15:09:05.700 UTC,,,,,11229461,1,1,python|google-cloud-platform|google-vision,36
OCR app crashes giving an error: jni_helper.cc:110 Bitmap is of the wrong format: 4,45116880,OCR app crashes giving an error: jni_helper.cc:110 Bitmap is of the wrong format: 4,"<p>I am using Google's vision API to detect text from an image. I set it in an imageview and put a button and on clicking the button it processes it and displays the text. This works fine if I click a picture or upload one from the gallery. I put a crop option as well and when I press that and set the image on clicking the process button the app crashes. This is my log:</p>

<pre><code>A/native: jni_helper.cc:110 Bitmap is of the wrong format: 4
07-15 15:14:35.511 6965-6965/com.cameradetect A/native: terminating.
07-15 15:14:35.512 6965-6965/com.cameradetect A/libc: Fatal signal 6 (SIGABRT), code -6 in tid 6965 (om.cameradetect)
07-15 15:14:35.614 1368-1368/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
07-15 15:14:35.614 1368-1368/? A/DEBUG: Build fingerprint: 'Android/sdk_google_phone_x86_64/generic_x86_64:6.0/MASTER/3738108:userdebug/test-keys'
07-15 15:14:35.614 1368-1368/? A/DEBUG: Revision: '0'
07-15 15:14:35.614 1368-1368/? A/DEBUG: ABI: 'x86_64'
07-15 15:14:35.614 1368-1368/? A/DEBUG: pid: 6965, tid: 6965, name: om.cameradetect  &gt;&gt;&gt; com.cameradetect &lt;&lt;&lt;
07-15 15:14:35.615 1368-1368/? A/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------
07-15 15:14:35.623 1368-1368/? A/DEBUG: Abort message: 'jni_helper.cc:110 Bitmap is of the wrong format: 4
                                    '
07-15 15:14:35.624 1368-1368/? A/DEBUG:     rax 0000000000000000  rbx 00007fce9d1a52c0  rcx ffffffffffffffff  rdx 0000000000000006
07-15 15:14:35.624 1368-1368/? A/DEBUG:     rsi 0000000000001b35  rdi 0000000000001b35
07-15 15:14:35.624 1368-1368/? A/DEBUG:     r8  0000000000000003  r9  0000000000000003  r10 0000000000000008  r11 0000000000000206
07-15 15:14:35.624 1368-1368/? A/DEBUG:     r12 0000000000001b35  r13 0000000000000006  r14 00007fff88f913c8  r15 0000000012ff9970
07-15 15:14:35.624 1368-1368/? A/DEBUG:     cs  0000000000000033  ss  000000000000002b
07-15 15:14:35.624 1368-1368/? A/DEBUG:     rip 00007fce9ccfb447  rbp 0000000000000009  rsp 00007fff88f90c88  eflags 0000000000000206
07-15 15:14:35.626 1368-1368/? A/DEBUG: backtrace:
07-15 15:14:35.626 1368-1368/? A/DEBUG:     #00 pc 0000000000088447  /system/lib64/libc.so (tgkill+7)
07-15 15:14:35.626 1368-1368/? A/DEBUG:     #01 pc 0000000000085b11  /system/lib64/libc.so (pthread_kill+65)
07-15 15:14:35.626 1368-1368/? A/DEBUG:     #02 pc 000000000002e841  /system/lib64/libc.so (raise+17)
07-15 15:14:35.626 1368-1368/? A/DEBUG:     #03 pc 00000000000288fd  /system/lib64/libc.so (abort+61)
07-15 15:14:35.626 1368-1368/? A/DEBUG:     #04 pc 00000000001bd2d1  /data/data/com.google.android.gms/files/com.google.android.gms.vision/ocr/libs/x86_64/libocr.so
07-15 15:14:35.626 1368-1368/? A/DEBUG:     #05 pc 00000000001bd84d  /data/data/com.google.android.gms/files/com.google.android.gms.vision/ocr/libs/x86_64/libocr.so
07-15 15:14:35.607 1368-1368/? W/debuggerd64: type=1400 audit(0.0:58): avc: denied { search } for name=""com.google.android.gms"" dev=""dm-0"" ino=114710 scontext=u:r:debuggerd:s0 tcontext=u:object_r:app_data_file:s0:c512,c768 tclass=dir permissive=0
07-15 15:14:35.627 1368-1368/? A/DEBUG:     #06 pc 000000000004c7ec  /data/data/com.google.android.gms/files/com.google.android.gms.vision/ocr/libs/x86_64/libocr.so
07-15 15:14:35.627 1368-1368/? A/DEBUG:     #07 pc 0000000000118bf0  /data/data/com.google.android.gms/app_chimera/m/00000002/oat/x86_64/DynamiteModulesB_GmsCore_prodmnc_alldpi_release.odex (offset 0x332000)
</code></pre>

<p>Here is my crop function:</p>

<pre><code>private void cropImage() {

    try{
        Intent crop =  new Intent(""com.android.camera.action.CROP"");
        crop.setDataAndType(selectImage, ""image/*"");
        crop.putExtra(""crop"", true);
        crop.putExtra(""outputX"", 180);
        crop.putExtra(""outputY"", 180);
        crop.putExtra(""aspectX"", 3);
        crop.putExtra(""aspectY"", 4);
        crop.putExtra(""scaleUpIfNeeded"", true);
        crop.putExtra(""return-data"", true);
        startActivityForResult(crop, CROP_PIC);

    }
    catch(ActivityNotFoundException e){
        e.printStackTrace();
        Toast.makeText(MainActivity.this, ""Error your phone does not support cropping!"", Toast.LENGTH_LONG).show();
    }
}
</code></pre>

<p>Here is my code to set the image into the imageView in the <code>onActivityResult</code> method </p>

<pre><code>if(requestCode == CROP_PIC &amp;&amp; data != null){
        bitmap = (Bitmap) data.getExtras().get(""data"");
        imageView.setImageBitmap(bitmap);
    }
</code></pre>

<p>The image gets set and I can see it but pressing process gives me the error</p>",46105271,1,0,,2017-07-15 09:56:23.803 UTC,,2017-09-07 21:20:50.713 UTC,,,,,8079810,1,1,android|bitmap|ocr|google-vision,134
Watson Visual Recognition Custom Classifiers in Node Red with Base64 Encoded Images,47392166,Watson Visual Recognition Custom Classifiers in Node Red with Base64 Encoded Images,"<p>I am currently working on an use case where I want to show how simple it is to train Watson Visual Recognition.</p>

<p>The images I get are base64 encoded and I know that there is an base64 node to create a binary buffer of the string/image. </p>

<p>Visual Recognition wants at least 20 images to be ready for classification. So it needs two times 10 images in a zip-folder (binary buffer of the zip folder). Now I have the problem when I use the ZIP node in Node-Red that it only can create a ZIP buffer of image buffers and visual recognition wants a zip buffer of images and not of image buffers.</p>

<p>I don't know how to custom the classifiers when I only have access to the base64 string of the images, because they get uploaded with Skype and I can't get them in png or jpg format.</p>",,0,2,,2017-11-20 12:34:34.097 UTC,,2017-11-20 12:34:34.097 UTC,,,,,8832449,1,0,image|zip|node-red|watson|visual-recognition,124
Cannot execute learning task. : Could not train classifier in IBM custom model,54803618,Cannot execute learning task. : Could not train classifier in IBM custom model,"<p>I created a new customer image classifier in IBM watson visual recognition. I created a 5 classes and upload training images for corresponding classes. </p>

<p>It successfully uploaded and created as a asset. </p>

<p>After uploading files, I started training a model. </p>

<p>It shows status is failed.</p>

<p>when I went look into details it shows,</p>

<blockquote>
  <p>Cannot execute learning task. : Could not train classifier. Verify there are at least 10 positive training images for each class, and at least 10 other unique training images (inluding optional negative_examples). There is a minimum of 1 positive class. Not enough samples for training, class: daisy has only 0 samples</p>
</blockquote>

<p>But daisy class has more more than 10 samples.</p>

<p>I have attached my screenshots for your reference.</p>

<p><a href=""https://i.stack.imgur.com/1BBZa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1BBZa.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/guC2f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/guC2f.png"" alt=""enter image description here""></a></p>

<p>The above screenshot explanation describes an error. But it clearly shows i have 3458 samples. and daisy class has 615 samples.</p>

<p>I don't have any clue to solve this. What should I try now? any help would be appreciable. </p>

<blockquote>
  <p>Error encountered while training.</p>
  
  <blockquote>
    <p>Error in Watson Visual Recognition service: Classifier must be in ready state to retrain. DefaultCustomModel_1967236334 is currently failed</p>
  </blockquote>
</blockquote>",,0,2,,2019-02-21 09:31:28.260 UTC,,2019-02-21 09:55:07.283 UTC,2019-02-21 09:55:07.283 UTC,,4684861,,4684861,1,0,image-processing|ibm-cloud|ibm-watson|data-science-experience,65
How to run python app on server through browser,50500341,How to run python app on server through browser,"<p>I just started using PYTHON and now i want to run a google vision cloud app on the server but I'm not sure how to start. 
I do have a server up and running at <a href=""http://18.217.137.107"" rel=""nofollow noreferrer"">http://18.217.137.107</a> and the app source code looks like <a href=""https://cloud.google.com/vision/docs/detecting-landmarks#vision-landmark-detection-python"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/detecting-landmarks#vision-landmark-detection-python</a>.
Any help would be greatly appreciated.</p>",,1,2,,2018-05-24 03:10:39.430 UTC,,2018-07-30 16:31:03.380 UTC,,,,,9394885,1,0,python|google-cloud-vision,72
Gradle error when using Google's latest API,35757052,Gradle error when using Google's latest API,"<p>I'm using Google Vision API's sample to make barcode reader in a webview of existing project.
<a href=""https://github.com/googlesamples/android-vision/tree/master/visionSamples/barcode-reader"" rel=""nofollow"">https://github.com/googlesamples/android-vision/tree/master/visionSamples/barcode-reader</a></p>

<p>this is my error message:</p>

<blockquote>
  <p>Error:(36, 13) Failed to resolve: com.google.android.support:design:23.0.1</p>
</blockquote>

<p>This is build.gradle file:
<div class=""snippet"" data-lang=""js"" data-hide=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>apply plugin: 'com.android.application'

android {
    compileSdkVersion 23
    buildToolsVersion ""21.1.2""

    defaultConfig {
        applicationId ""com.example.harry""
        minSdkVersion 15
        targetSdkVersion 23
        versionCode 1
        versionName ""1.0""
    }
    buildTypes {
        release {
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
        }
    }

    packagingOptions {
        exclude 'META-INF/DEPENDENCIES'
        exclude 'META-INF/LICENSE'
        exclude 'META-INF/LICENSE.txt'
        exclude 'META-INF/license.txt'
        exclude 'META-INF/NOTICE'
        exclude 'META-INF/NOTICE.txt'
        exclude 'META-INF/notice.txt'
        exclude 'META-INF/ASL2.0'
    }
}

dependencies {
    compile fileTree(include: ['*.jar'], dir: 'libs')
    compile 'com.android.support:support-v4:23.0.1'
    compile 'com.google.android.gms:play-services:8.1.0'
    compile 'com.google.android.support:design:23.0.1'
    compile files('libs/android-support-v4.jar')
    compile files('libs/httpclient-4.3.6.jar')
    compile files('libs/commons-codec-1.3.jar')
    compile files('libs/httpclient-4.1-beta1.jar')
    compile files('libs/apache-mime4j-0.6.jar')
    compile files('libs/httpclient-4.1.jar')
    compile files('libs/httpcore-4.1.jar')
    compile project(':..:ExpandableButtonMenu:library')

}</code></pre>
</div>
</div>
</p>

<p>when I created BarcodeCaptureActivity.java,
I get tons of errors...
BarcodeCaptureActivity.java:
<div class=""snippet"" data-lang=""js"" data-hide=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>package com.example.harry;

import android.Manifest;
import android.annotation.SuppressLint;
import android.app.Activity;
import android.app.AlertDialog;
import android.app.Dialog;
import android.content.Context;
import android.content.DialogInterface;
import android.content.Intent;
import android.content.IntentFilter;
import android.content.pm.PackageManager;
import android.hardware.Camera;
import android.os.Build;
import android.os.Bundle;
import android.support.annotation.NonNull;
import android.support.v4.app.ActivityCompat;
import android.support.v7.app.ActionBarActivity;
import android.util.Log;
import android.view.GestureDetector;
import android.view.MotionEvent;
import android.view.ScaleGestureDetector;
import android.view.View;
import android.widget.Toast;


import com.google.android.gms.common.ConnectionResult;
import com.google.android.gms.common.GoogleApiAvailability;
import com.google.android.gms.common.api.CommonStatusCodes;
import com.google.android.gms.samples.vision.barcodereader.ui.camera.CameraSource;
import com.google.android.gms.samples.vision.barcodereader.ui.camera.CameraSourcePreview;

import com.google.android.gms.samples.vision.barcodereader.ui.camera.GraphicOverlay;
import com.google.android.gms.vision.MultiProcessor;
import com.google.android.gms.vision.barcode.Barcode;
import com.google.android.gms.vision.barcode.BarcodeDetector;

import java.io.IOException;

public class BarcodeCaptureActivity extends ActionBarActivity {

    private static final String TAG = ""Barcode-reader"";

    // intent request code to handle updating play services if needed.
    private static final int RC_HANDLE_GMS = 9001;

    // permission request codes need to be &lt; 256
    private static final int RC_HANDLE_CAMERA_PERM = 2;

    // constants used to pass extra data in the intent
    public static final String AutoFocus = ""AutoFocus"";
    public static final String UseFlash = ""UseFlash"";
    public static final String BarcodeObject = ""Barcode"";

    private CameraSource mCameraSource;
    private CameraSourcePreview mPreview;
    private GraphicOverlay&lt;BarcodeGraphic&gt; mGraphicOverlay;

    // helper objects for detecting taps and pinches.
    private ScaleGestureDetector scaleGestureDetector;
    private GestureDetector gestureDetector;

    /**
     * Initializes the UI and creates the detector pipeline.
     */
    @Override
    public void onCreate(Bundle icicle) {
        super.onCreate(icicle);
        setContentView(R.layout.barcode_capture);

        mPreview = (CameraSourcePreview) findViewById(R.id.preview);
        mGraphicOverlay = (GraphicOverlay&lt;BarcodeGraphic&gt;) findViewById(R.id.graphicOverlay);

        // read parameters from the intent used to launch the activity.
        boolean autoFocus = getIntent().getBooleanExtra(AutoFocus, false);
        boolean useFlash = getIntent().getBooleanExtra(UseFlash, false);

        // Check for the camera permission before accessing the camera.  If the
        // permission is not granted yet, request permission.
        int rc = ActivityCompat.checkSelfPermission(this, Manifest.permission.CAMERA);
        if (rc == PackageManager.PERMISSION_GRANTED) {
            createCameraSource(autoFocus, useFlash);
        } else {
            requestCameraPermission();
        }

        gestureDetector = new GestureDetector(this, new CaptureGestureListener());
        scaleGestureDetector = new ScaleGestureDetector(this, new ScaleListener());

        Snackbar.make(mGraphicOverlay, ""Tap to capture. Pinch/Stretch to zoom"",
                Snackbar.LENGTH_LONG)
                .show();
    }

    /**
     * Handles the requesting of the camera permission.  This includes
     * showing a ""Snackbar"" message of why the permission is needed then
     * sending the request.
     */
    private void requestCameraPermission() {
        Log.w(TAG, ""Camera permission is not granted. Requesting permission"");

        final String[] permissions = new String[]{Manifest.permission.CAMERA};

        if (!ActivityCompat.shouldShowRequestPermissionRationale(this,
                Manifest.permission.CAMERA)) {
            ActivityCompat.requestPermissions(this, permissions, RC_HANDLE_CAMERA_PERM);
            return;
        }

        final Activity thisActivity = this;

        View.OnClickListener listener = new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                ActivityCompat.requestPermissions(thisActivity, permissions,
                        RC_HANDLE_CAMERA_PERM);
            }
        };

        Snackbar.make(mGraphicOverlay, R.string.permission_camera_rationale,
                Snackbar.LENGTH_INDEFINITE)
                .setAction(R.string.ok, listener)
                .show();
    }

    @Override
    public boolean onTouchEvent(MotionEvent e) {
        boolean b = scaleGestureDetector.onTouchEvent(e);

        boolean c = gestureDetector.onTouchEvent(e);

        return b || c || super.onTouchEvent(e);
    }

    /**
     * Creates and starts the camera.  Note that this uses a higher resolution in comparison
     * to other detection examples to enable the barcode detector to detect small barcodes
     * at long distances.
     *
     * Suppressing InlinedApi since there is a check that the minimum version is met before using
     * the constant.
     */
    @SuppressLint(""InlinedApi"")
    private void createCameraSource(boolean autoFocus, boolean useFlash) {
        Context context = getApplicationContext();

        // A barcode detector is created to track barcodes.  An associated multi-processor instance
        // is set to receive the barcode detection results, track the barcodes, and maintain
        // graphics for each barcode on screen.  The factory is used by the multi-processor to
        // create a separate tracker instance for each barcode.
        BarcodeDetector barcodeDetector = new BarcodeDetector.Builder(context).build();
        BarcodeTrackerFactory barcodeFactory = new BarcodeTrackerFactory(mGraphicOverlay);
        barcodeDetector.setProcessor(
                new MultiProcessor.Builder&lt;&gt;(barcodeFactory).build());

        if (!barcodeDetector.isOperational()) {
            // Note: The first time that an app using the barcode or face API is installed on a
            // device, GMS will download a native libraries to the device in order to do detection.
            // Usually this completes before the app is run for the first time.  But if that
            // download has not yet completed, then the above call will not detect any barcodes
            // and/or faces.
            //
            // isOperational() can be used to check if the required native libraries are currently
            // available.  The detectors will automatically become operational once the library
            // downloads complete on device.
            Log.w(TAG, ""Detector dependencies are not yet available."");

            // Check for low storage.  If there is low storage, the native library will not be
            // downloaded, so detection will not become operational.
            IntentFilter lowstorageFilter = new IntentFilter(Intent.ACTION_DEVICE_STORAGE_LOW);
            boolean hasLowStorage = registerReceiver(null, lowstorageFilter) != null;

            if (hasLowStorage) {
                Toast.makeText(this, R.string.low_storage_error, Toast.LENGTH_LONG).show();
                Log.w(TAG, getString(R.string.low_storage_error));
            }
        }

        // Creates and starts the camera.  Note that this uses a higher resolution in comparison
        // to other detection examples to enable the barcode detector to detect small barcodes
        // at long distances.
        CameraSource.Builder builder = new CameraSource.Builder(getApplicationContext(), barcodeDetector)
                .setFacing(CameraSource.CAMERA_FACING_BACK)
                .setRequestedPreviewSize(1600, 1024)
                .setRequestedFps(15.0f);

        // make sure that auto focus is an available option
        if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.ICE_CREAM_SANDWICH) {
            builder = builder.setFocusMode(
                    autoFocus ? Camera.Parameters.FOCUS_MODE_CONTINUOUS_PICTURE : null);
        }

        mCameraSource = builder
                .setFlashMode(useFlash ? Camera.Parameters.FLASH_MODE_TORCH : null)
                .build();
    }

    /**
     * Restarts the camera.
     */
    @Override
    protected void onResume() {
        super.onResume();
        startCameraSource();
    }

    /**
     * Stops the camera.
     */
    @Override
    protected void onPause() {
        super.onPause();
        if (mPreview != null) {
            mPreview.stop();
        }
    }

    /**
     * Releases the resources associated with the camera source, the associated detectors, and the
     * rest of the processing pipeline.
     */
    @Override
    protected void onDestroy() {
        super.onDestroy();
        if (mPreview != null) {
            mPreview.release();
        }
    }

    /**
     * Callback for the result from requesting permissions. This method
     * is invoked for every call on {@link #requestPermissions(String[], int)}.
     * &lt;p&gt;
     * &lt;strong&gt;Note:&lt;/strong&gt; It is possible that the permissions request interaction
     * with the user is interrupted. In this case you will receive empty permissions
     * and results arrays which should be treated as a cancellation.
     * &lt;/p&gt;
     *
     * @param requestCode  The request code passed in {@link #requestPermissions(String[], int)}.
     * @param permissions  The requested permissions. Never null.
     * @param grantResults The grant results for the corresponding permissions
     *                     which is either {@link PackageManager#PERMISSION_GRANTED}
     *                     or {@link PackageManager#PERMISSION_DENIED}. Never null.
     * @see #requestPermissions(String[], int)
     */
    @Override
    public void onRequestPermissionsResult(int requestCode,
                                           @NonNull String[] permissions,
                                           @NonNull int[] grantResults) {
        if (requestCode != RC_HANDLE_CAMERA_PERM) {
            Log.d(TAG, ""Got unexpected permission result: "" + requestCode);
            super.onRequestPermissionsResult(requestCode, permissions, grantResults);
            return;
        }

        if (grantResults.length != 0 &amp;&amp; grantResults[0] == PackageManager.PERMISSION_GRANTED) {
            Log.d(TAG, ""Camera permission granted - initialize the camera source"");
            // we have permission, so create the camerasource
            boolean autoFocus = getIntent().getBooleanExtra(AutoFocus,false);
            boolean useFlash = getIntent().getBooleanExtra(UseFlash, false);
            createCameraSource(autoFocus, useFlash);
            return;
        }

        Log.e(TAG, ""Permission not granted: results len = "" + grantResults.length +
                "" Result code = "" + (grantResults.length &gt; 0 ? grantResults[0] : ""(empty)""));

        DialogInterface.OnClickListener listener = new DialogInterface.OnClickListener() {
            public void onClick(DialogInterface dialog, int id) {
                finish();
            }
        };

        AlertDialog.Builder builder = new AlertDialog.Builder(this);
        builder.setTitle(""Multitracker sample"")
                .setMessage(R.string.no_camera_permission)
                .setPositiveButton(R.string.ok, listener)
                .show();
    }

    /**
     * Starts or restarts the camera source, if it exists.  If the camera source doesn't exist yet
     * (e.g., because onResume was called before the camera source was created), this will be called
     * again when the camera source is created.
     */
    private void startCameraSource() throws SecurityException {
        // check that the device has play services available.
        int code = GoogleApiAvailability.getInstance().isGooglePlayServicesAvailable(
                getApplicationContext());
        if (code != ConnectionResult.SUCCESS) {
            Dialog dlg =
                    GoogleApiAvailability.getInstance().getErrorDialog(this, code, RC_HANDLE_GMS);
            dlg.show();
        }

        if (mCameraSource != null) {
            try {
                mPreview.start(mCameraSource, mGraphicOverlay);
            } catch (IOException e) {
                Log.e(TAG, ""Unable to start camera source."", e);
                mCameraSource.release();
                mCameraSource = null;
            }
        }
    }

    /**
     * onTap is called to capture the oldest barcode currently detected and
     * return it to the caller.
     *
     * @param rawX - the raw position of the tap
     * @param rawY - the raw position of the tap.
     * @return true if the activity is ending.
     */
    private boolean onTap(float rawX, float rawY) {

        //TODO: use the tap position to select the barcode.
        BarcodeGraphic graphic = mGraphicOverlay.getFirstGraphic();
        Barcode barcode = null;
        if (graphic != null) {
            barcode = graphic.getBarcode();
            if (barcode != null) {
                Intent data = new Intent();
                data.putExtra(BarcodeObject, barcode);
                setResult(CommonStatusCodes.SUCCESS, data);
                finish();
            }
            else {
                Log.d(TAG, ""barcode data is null"");
            }
        }
        else {
            Log.d(TAG,""no barcode detected"");
        }
        return barcode != null;
    }

    private class CaptureGestureListener extends GestureDetector.SimpleOnGestureListener {

        @Override
        public boolean onSingleTapConfirmed(MotionEvent e) {

            return onTap(e.getRawX(), e.getRawY()) || super.onSingleTapConfirmed(e);
        }
    }

    private class ScaleListener implements ScaleGestureDetector.OnScaleGestureListener {

        /**
         * Responds to scaling events for a gesture in progress.
         * Reported by pointer motion.
         *
         * @param detector The detector reporting the event - use this to
         *                 retrieve extended info about event state.
         * @return Whether or not the detector should consider this event
         * as handled. If an event was not handled, the detector
         * will continue to accumulate movement until an event is
         * handled. This can be useful if an application, for example,
         * only wants to update scaling factors if the change is
         * greater than 0.01.
         */
        @Override
        public boolean onScale(ScaleGestureDetector detector) {
            return false;
        }

        /**
         * Responds to the beginning of a scaling gesture. Reported by
         * new pointers going down.
         *
         * @param detector The detector reporting the event - use this to
         *                 retrieve extended info about event state.
         * @return Whether or not the detector should continue recognizing
         * this gesture. For example, if a gesture is beginning
         * with a focal point outside of a region where it makes
         * sense, onScaleBegin() may return false to ignore the
         * rest of the gesture.
         */
        @Override
        public boolean onScaleBegin(ScaleGestureDetector detector) {
            return true;
        }

        /**
         * Responds to the end of a scale gesture. Reported by existing
         * pointers going up.
         * &lt;p/&gt;
         * Once a scale has ended, {@link ScaleGestureDetector#getFocusX()}
         * and {@link ScaleGestureDetector#getFocusY()} will return focal point
         * of the pointers remaining on the screen.
         *
         * @param detector The detector reporting the event - use this to
         *                 retrieve extended info about event state.
         */
        @Override
        public void onScaleEnd(ScaleGestureDetector detector) {
            mCameraSource.doZoom(detector.getScaleFactor());
        }
    }
}
}</code></pre>
</div>
</div>
</p>

<p>why can't android studio recognize import in java?</p>

<p>I'm stuck here for 2 days...
please help me!!!!</p>",35757934,1,4,,2016-03-02 20:06:06.080 UTC,,2016-03-03 20:47:32.513 UTC,2016-03-02 20:39:29.617 UTC,,6009679,,6009679,1,2,java|android|android-studio|android-gradle|google-vision,3919
Find match face from the list of faces in local storage using Azure Face API,54733517,Find match face from the list of faces in local storage using Azure Face API,"<p>I am reading the documents and API from Azure page but I am still not sure if my though it correct here.</p>

<p>Scenario</p>

<p>We have around 1M ID photos in our local storage. Each ID contain only one single person.</p>

<p>We would like to implement the basic validation when taking the ID photo .. the small app will then using the Azure Face API to look through those 1M ID photos that we have and return the matcged photo or return if we have the same person in our ID storage or not.</p>

<p>To do the avove, I believe we need to write the software to do things below </p>

<ol>
<li>Upload all the photos into Azure </li>
<li>Create Large FaceList? </li>
<li>Train the model</li>
<li>Then we can do the face identify or face similar </li>
</ol>

<p>Are the steps above correct?</p>

<p>If I use the method above that mean I need to use 'face storage' for persisted face Id right?</p>

<p>1.Is there a way to avoid cost of face storage this? As it will cost a lot to keep 1M images</p>

<ol start=""2"">
<li>When I do verify how many transactions will it be counted? Is it counted as 1? </li>
</ol>

<p>I am thinking about using Container Cognitive as well  so it can run locally and using the storage on the local instead.</p>

<p>Will that help me in saving the face storage cost? As when I run container the storage should not need to be paid. I will only need to pay for transaction fee such as detect, verify</p>

<p>I am welcome any comments pretty new in this field please guide me</p>",54746783,1,0,,2019-02-17 13:01:31.677 UTC,,2019-02-18 11:56:35.573 UTC,,,,,10809685,1,0,c#|face-recognition|azure-cognitive-services|face|face-api,97
How to upload and analyze thousands of videos in Video Indexer?,56178717,How to upload and analyze thousands of videos in Video Indexer?,"<p>I'm about to use Azure Video Indexer to analyze thousands of videos, and I'm trying to understand what is the best way to do this in reasonable time. Is there a way to upload and then analyze multiple files in parallel? And how can I estimate how much time it is going to take?</p>",,0,2,,2019-05-17 02:17:07.580 UTC,,2019-05-17 02:17:07.580 UTC,,,,,3017075,1,-2,azure|microsoft-cognitive|azure-cognitive-services,28
"Unable to get object metadata from S3. Check object key, region and/or access permissions in aws Rekognition",50181484,"Unable to get object metadata from S3. Check object key, region and/or access permissions in aws Rekognition","<pre><code>import boto3

if __name__ == ""__main__"":

    bucket='MyBucketName'
sourceFile='pic1.jpg'
targetFile='pic2.jpg'

client=boto3.client('rekognition','us-east-1')

response=client.compare_faces(SimilarityThreshold=70,
                              SourceImage={'S3Object':{'Bucket':bucket,'Name':sourceFile}},
                              TargetImage={'S3Object':{'Bucket':bucket,'Name':targetFile}})

for faceMatch in response['FaceMatches']:
    position = faceMatch['Face']['BoundingBox']
    confidence = str(faceMatch['Face']['Confidence'])
    print('The face at ' +
           str(position['Left']) + ' ' +
           str(position['Top']) +
           ' matches with ' + confidence + '% confidence')
</code></pre>

<p>I am trying to compare two images present in my bucket but no matter which region i select i always get the following error:-</p>

<p>botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the CompareFaces operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.</p>

<p>My bucket's region is us-east-1 and I have configured the same in my code.
what am I doing wrong?</p>",53471781,3,4,,2018-05-04 18:52:53.653 UTC,,2018-11-25 20:45:12.013 UTC,2018-05-04 19:43:31.770 UTC,,7032512,,9742957,1,1,python|amazon-web-services|amazon-s3|face-detection,3085
Google Vision: setRequestPreviewSize no effect,48384087,Google Vision: setRequestPreviewSize no effect,"<p>I am working on an Android application with an embedded QR code scanner using the Google Vision API. The scanner functions, but the SurfaceView that acts as camera preview is stretched vertically. The degree of distortion is different for different emulated devices.</p>

<p>As I understand it, you would use <code>mCameraSource.setRequestedPreviewSize(w,h)</code> to set the correct size. <code>w</code> and <code>h</code> I have set as <code>Resources.getSystem().getDisplayMetrics().widthPixels</code> and <code>Resources.getSystem().getDisplayMetrics().heightPixels</code>, respectively. However, I have noticed that regardless of what numbers I parse as width and height, there are no changes in the way it displays.</p>

<p>However, resizing the SurfaceView on which it is displayed does have an effect on the distortion. For one particular emulated Android device I can statically set the right width and height. For different devices, however, with a slightly different pixel w:h ratio, the distortion can become quite large.</p>

<p>I have read various solutions on StackOverflow, but most use the <code>CameraPreview</code> instead of the <code>CameraSource.Builder</code>.</p>

<p>My code thus far is (part of <code>ScannerActivity.java</code>):</p>

<pre><code>private SurfaceView svCamera;
private BarcodeDetector barcodeDetector;
private CameraSource cameraSource;

@Override

protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.activity_scanner);
    initViews();
    initListeners();

    barcodeDetector = new BarcodeDetector.Builder(this)
                .setBarcodeFormats(Barcode.QR_CODE)
                .build();


    cameraSource = new CameraSource.Builder(this, barcodeDetector)
                .setRequestedPreviewSize(Resources.getSystem().getDisplayMetrics().widthPixels,     Resources.getSystem().getDisplayMetrics().heightPixels)
                .setAutoFocusEnabled(true)
                .build();

    svCamera.getHolder().addCallback(new SurfaceHolder.Callback() {
        @Override
        public void surfaceCreated(SurfaceHolder surfaceHolder) {
        requestPermission();
        try {
                if (ActivityCompat.checkSelfPermission(ScannerActivity.this, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {
                   return;
                }
                cameraSource.start(svCamera.getHolder());
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
         @Override
    public void surfaceChanged(SurfaceHolder surfaceHolder, int i, int i1, int i2) {

        }
         @Override
        public void surfaceDestroyed(SurfaceHolder surfaceHolder) {
            cameraSource.stop();
        }
    });

   barcodeDetector.setProcessor(new Detector.Processor&lt;Barcode&gt;() {
       @Override
        public void release() {
            scanned = true;
        }
         @Override
        public void receiveDetections(Detector.Detections&lt;Barcode&gt; detections) {
              ...
        }
    });
} }   
</code></pre>

<p>Can someone help me with setting preview size?</p>

<hr>

<p><strong>The way I fixed it</strong> with help of Alex Cohn's answer:</p>

<pre><code>DisplayMetrics displayMetrics = new DisplayMetrics();
getWindowManager().getDefaultDisplay().getMetrics(displayMetrics);
int width = displayMetrics.widthPixels;    
int height = displayMetrics.heightPixels;
...
cameraSource.setRequestedPreviewSize(1280, 720); // Hardcoded, for now.
</code></pre>

<p>And I set the size of the <code>SurfaceView</code> with:</p>

<pre><code>svCamera.setLayoutParams(new RelativeLayout.LayoutParams(width, width/9*16));
</code></pre>

<p>If I remember I will update this to a non-hardcoded version.</p>",,1,0,,2018-01-22 14:42:12.893 UTC,,2018-01-22 17:49:49.543 UTC,2018-01-22 17:49:49.543 UTC,,9251944,,9251944,1,2,java|android|android-camera|google-vision,406
Get number of trained faces in PersonGroup,55382899,Get number of trained faces in PersonGroup,<p>Is their any API from which we can get the number of person trained in a particular  <code>PersonGroup</code> in <code>Microsoft Face API</code> ?</p>,,1,0,,2019-03-27 17:07:51.493 UTC,,2019-03-28 10:35:36.983 UTC,,,,,4881193,1,0,microsoft-cognitive,18
Convert BufferedImage to aws...rekognition.model.Image,41527687,Convert BufferedImage to aws...rekognition.model.Image,"<p>I'm playing around with the Amazon Rekognition. I found a really <a href=""https://github.com/sarxos/webcam-capture"" rel=""nofollow noreferrer"">nice/easy library</a> to take an image from my webcam which works like this:</p>

<pre><code>BufferedImage bufImg = webcam.getImage();
</code></pre>

<p>I'm then trying to convert this <code>BufferedImage</code> to a <code>com.amazonaws.services.rekognition.model.Image</code> , which is what must be submitted to the Rekognition library. This is what I'm doing:</p>

<pre><code>byte[] imgBytes = ((DataBufferByte) bufImg.getData().getDataBuffer()).getData();
ByteBuffer byteBuffer = ByteBuffer.wrap(imgBytes);
return new Image().withBytes(byteBuffer);
</code></pre>

<p>However when I try and do some API call to Rekognition with the <code>Image</code>, I get an Exception:</p>

<pre><code>com.amazonaws.services.rekognition.model.InvalidImageFormatException: Invalid image encoding (Service: AmazonRekognition; Status Code: 400; Error Code: InvalidImageFormatException; Request ID: X)
</code></pre>

<p>The <a href=""http://docs"" rel=""nofollow noreferrer"">docs</a> state that the Java SDK will automatically base64 encode the bytes. In case, something weird was happening, I tried base64 encoding the bytes before converting:</p>

<pre><code>imgBytes = Base64.getEncoder().encode(imgBytes);
</code></pre>

<p>However, the same Exception ensues.</p>

<p>Any ideas? :)</p>",41528266,1,0,,2017-01-07 23:20:02.890 UTC,1,2017-01-08 00:45:09.713 UTC,,,,,2620746,1,1,java|amazon-web-services|bufferedimage|amazon-rekognition,546
Tidy way to batch calls to Google Vision using purr and RoogleVision?,55251217,Tidy way to batch calls to Google Vision using purr and RoogleVision?,"<p>I have a directory with images, and I'd like to query the Google Vision API for each and store the aggregate output in one tibble.</p>

<p>I tried what seemed like an easy solution: if <code>getGoogleVisionResponse(""file1.png"")</code> works, then all I need is:</p>

<pre><code>files &lt;- dir(""image-path"")
map(files, getGoogleVisionResponse)
</code></pre>

<p>Only to get: 
<code>Error in file(con, ""rb"") : cannot open the connection</code></p>

<p>I found this answer, which involves writing a function from scratch, but that seems untidy and overkill, no?
<a href=""https://stackoverflow.com/questions/55155238/create-variable-and-dataset-in-a-loop-r"">Create variable and dataset in a loop? (R)</a></p>

<p>I also found <a href=""https://github.com/cloudyr/RoogleVision/pull/12"" rel=""nofollow noreferrer"">this pull request</a> that aimed to address this, but it did not get merged.</p>",55251585,1,0,,2019-03-19 23:03:01.797 UTC,,2019-03-19 23:48:46.807 UTC,2019-03-19 23:44:41 UTC,,3495574,,3495574,1,0,r|purrr|google-vision,22
Vision API: How to get JSON-output,52169264,Vision API: How to get JSON-output,"<p>I'm having trouble saving the output given by the Google Vision API. I'm using Python and testing with a demo image. I get the following error:</p>

<pre><code>TypeError: [mid:...] + is not JSON serializable
</code></pre>

<p>Code that I executed: </p>

<pre><code>import io
import os
import json
# Imports the Google Cloud client library
from google.cloud import vision
from google.cloud.vision import types

# Instantiates a client
vision_client = vision.ImageAnnotatorClient()


# The name of the image file to annotate
file_name = os.path.join(
    os.path.dirname(__file__),
    'demo-image.jpg') # Your image path from current directory

# Loads the image into memory
with io.open(file_name, 'rb') as image_file:
    content = image_file.read()
    image = types.Image(content=content)

# Performs label detection on the image file
response = vision_client.label_detection(image=image)
labels = response.label_annotations


print('Labels:')
for label in labels:
    print(label.description, label.score, label.mid)

with open('labels.json', 'w') as fp:
   json.dump(labels, fp)
</code></pre>

<p>the output appears on the screen, however I do not know exactly how I can save it. Anyone have any suggestions?</p>",,4,0,,2018-09-04 15:06:35.260 UTC,,2019-04-03 05:13:30.973 UTC,,,,,10233443,1,2,google-api|google-cloud-platform|google-vision,438
Google Cloud Vision - How to Switch Feature Type?,52684763,Google Cloud Vision - How to Switch Feature Type?,"<p>I found this project on GitHub that is an application with the Google Cloud Vision API: <a href=""https://github.com/GoogleCloudPlatform/cloud-vision/tree/master/android"" rel=""nofollow noreferrer"">GitHub</a>.</p>

<p>I've edited a few things to avoid compatibility errors, and now it perfectly works.</p>

<p>There is only one thing that I would like to do, change the feature type by ""LABEL_DETECTION"" to ""LOGO_DETECTION"" or anything else.</p>

<p>This is the entire code of MainActibity.java: <a href=""https://pastebin.com/FjbH1n1p"" rel=""nofollow noreferrer"">Pastebin</a>.</p>

<p>Go to the line 169 <code>labelDetection.setType(""LABEL_DETECTION"");</code>, if I try to change that line by ""LABEL_DETECTION"" to anything else, for example ""LOGO_DETECTION"", the application doesn't work anymore; it tells me ""nothing found"".</p>

<p>Is there a way to change the feature type?</p>",,0,0,,2018-10-07 01:33:51.797 UTC,,2018-10-07 04:03:32.150 UTC,2018-10-07 04:03:32.150 UTC,,4283581,,8102699,1,0,android|android-studio|google-cloud-vision,30
Can IBM Cloud Watson recognize the same person across multiple images?,50358189,Can IBM Cloud Watson recognize the same person across multiple images?,"<p>I want to create a gallery service that clusters images based on different characteristics, chief among them being faces matched across multiple images.</p>

<p>I've been considering the IBM Cloud for this, but i can't find a definitive yes or no answer to whether Watson supports Face recognition (on top of detection) so the same person is identified across multiple images, like <a href=""https://aws.amazon.com/blogs/machine-learning/build-your-own-face-recognition-service-using-amazon-rekognition/"" rel=""nofollow noreferrer"">AWS Rekognition</a> and <a href=""https://azure.microsoft.com/en-us/services/cognitive-services/face/"" rel=""nofollow noreferrer"">Azure CognitiveServices Face API</a> do.</p>

<p>The concrete scenario i want to implement is this: Given photos A.jpg and B.jpg Watson should be able to tell that A.jpg has a face corresponding to person X, and B.jpg has another face that looks similar to the one in A.jpg. Ideally, it should do this automatically and give me face id values for each detected face.</p>

<p>Has anyone tackled this with Watson before? Is it doable in a simple manner without much code or ML techniques on top of the vanilla Watson face detection? </p>",,1,0,,2018-05-15 19:51:05.213 UTC,,2018-05-15 20:35:47.610 UTC,2018-05-15 20:35:47.610 UTC,,528131,,528131,1,0,ibm-cloud,62
Face API Attribute Emotion Gives Null Object Error,53938721,Face API Attribute Emotion Gives Null Object Error,"<p>I am working with the Microsoft Face API to detect attributes of faces such as age, gender, and emotion. The following code is working for me: <code>faces[position].faceAttributes.age</code> and I am able to get the estimated age. <em>(<code>faces[]</code> is an array of the type <code>Face</code> )</em></p>

<p>However, <strong>when I try to get the probability that the face is happy, I am running into the following error:</strong></p>

<blockquote>
  <p>Attempt to read from field 'double com.microsoft.projectoxford.face.contract.Emotion.happiness' on a null object reference.</p>
</blockquote>

<p>This is how I am getting the probability that the person is <strong>happy</strong>:<br>
    <code>faces[position].faceAttributes.emotion.happiness</code></p>

<p>Similarly, when I try:  <code>faces[position].faceAttributes.emotion</code>, it returns <code>null</code>. </p>

<p>I know that <code>faces[position].faceAttributes</code> is not <code>null</code> because it works for other attributes like age and gender but I am unable to figure out why it is not working for emotions. Does anyone know why this is occurring and what I can do to get it to work?</p>

<hr>

<h2>Update:</h2>

<p>For those who are experiencing the same problem, in the <code>AsnycTask</code> where you are processing the faces, you must include the attributes you wish to detect otherwise it says that it is a null object reference when you refer to them later. Initially, I had <code>FaceServiceClient.FaceAttributeType.Smile</code> and that was why it was giving me an error when trying to determine emotions. The following code goes in the <code>doInBackground</code> method:</p>

<pre><code>FaceServiceClient.FaceAttributeType[] faceAttr = new FaceServiceClient.FaceAttributeType[]{
    FaceServiceClient.FaceAttributeType.HeadPose,
    FaceServiceClient.FaceAttributeType.Age,
    FaceServiceClient.FaceAttributeType.Gender,
    FaceServiceClient.FaceAttributeType.Emotion,
    FaceServiceClient.FaceAttributeType.FacialHair
};
</code></pre>",53942455,1,0,,2018-12-27 01:05:08.183 UTC,,2018-12-27 14:06:46.207 UTC,2018-12-27 14:06:46.207 UTC,,10589041,,10589041,1,0,android|azure|null|microsoft-cognitive|face-api,150
Can I use Amazon Rekognition without an S3 bucket?,51034435,Can I use Amazon Rekognition without an S3 bucket?,"<p>I want to use the Firebase with the Amazon Rekognition is it possible to use?</p>

<p>I read Class for Rekognition for Node.js it has the S3 command in the code.</p>",,1,1,,2018-06-26 03:23:19.160 UTC,,2018-06-26 06:25:54.110 UTC,2018-06-26 05:33:59.583 UTC,,174777,,9992071,1,1,amazon-web-services|amazon-s3|amazon-rekognition,352
google vision OCR api returns no response,50642047,google vision OCR api returns no response,"<p>I am calling the google vision OCR api from a spring boot maven project to extract text from an image.</p>

<pre><code>public class TestGoogleVision {

BufferedWriter writer = null;

public static void main(String args[]) throws IOException, Exception {
    List&lt;String&gt; output=new ArrayList&lt;&gt;();

 GoogleCredentials credentials = ServiceAccountCredentials.fromStream(new FileInputStream(""/Users/ummulkiram 1/Documents/UK.json""))
           .createScoped(Lists.newArrayList(""https://www.googleapis.com/auth/cloud-platform""));

    ImageAnnotatorSettings settings =
            ImageAnnotatorSettings.newBuilder().setCredentialsProvider
                    (FixedCredentialsProvider.create(credentials)).build();

    System.out.println(""Setting::::::::"" + settings);

    try (ImageAnnotatorClient vision = ImageAnnotatorClient.create(settings)) {

        System.out.println(""CLIENT::::::: "" + vision);
        List&lt;AnnotateImageRequest&gt; requests = new ArrayList&lt;&gt;();
        Feature feat = Feature.newBuilder().setType(Type.TEXT_DETECTION).build();

        String[] fileNames = {""/Users/ummulkiram 1/Documents/img.jpg""};


        for(String fileName : fileNames) {

            // Reads the image file into memory
            Path path = Paths.get(fileName);
            byte[] data = Files.readAllBytes(path);
            ByteString imgBytes = ByteString.copyFrom(data);

            // Builds the image annotation request

            Image img = Image.newBuilder().setContent(imgBytes).build();

            AnnotateImageRequest request = AnnotateImageRequest.newBuilder().addFeatures(feat).setImage(img).build();
            requests.add(request);
        }

        // Performs text detection on the image file
        System.out.println(""line 80"");
        BatchAnnotateImagesResponse response = vision.batchAnnotateImages(requests);
        System.out.println(""line 82"");
        List&lt;AnnotateImageResponse&gt; responses = response.getResponsesList();

        for (AnnotateImageResponse resp : responses) {
            if (resp.hasError()) {
                System.out.printf(""Error: %s\n"", resp.getError().getMessage());
                return;
            }
            if (resp.getTextAnnotationsList() != null) {
                int count = 0;
                System.out.println(""List size""+resp.getTextAnnotationsList().size());
                //writer = new BufferedWriter(new FileWriter(""CHOLA MS GEN INSURANCE"", true));
                for (EntityAnnotation ea : resp.getTextAnnotationsList()) {
                    output.add(ea.getDescription());
                    System.out.println(ea.getDescription());

                    System.out.println(""XXXXXXXXX"");

                    break;
                }
            }
        }
    }
    System.out.println(output);
}
}
</code></pre>

<p>The call on the line below(i.e below line 80) does not return a response.</p>

<pre><code>BatchAnnotateImagesResponse response = vision.batchAnnotateImages(requests);
</code></pre>

<p>It gets stuck for a very long time after which it throws the below exception.</p>

<pre><code>a Caused by: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded: -538368540044 ns from now
at io.grpc.Status.asRuntimeException(Status.java:526)
</code></pre>

<p>The console has the below logs printed. Kindly help me  resolve the issue.</p>

<pre><code>    /Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/bin/java ""-javaagent:/Applications/IntelliJ IDEA CE.app/Contents/lib/idea_rt.jar=55016:/Applications/IntelliJ IDEA CE.app/Contents/bin"" -Dfile.encoding=UTF-8 -classpath ""/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/lib/tools.jar:/Users/ummulkiram 1/development/TestApp/target/classes:/Users/ummulkiram 1/.m2/repository/org/springframework/boot/spring-boot-starter-web/1.5.3.RELEASE/spring-boot-starter-web-1.5.3.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/boot/spring-boot-starter/1.5.3.RELEASE/spring-boot-starter-1.5.3.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/boot/spring-boot/1.5.3.RELEASE/spring-boot-1.5.3.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/boot/spring-boot-autoconfigure/1.5.3.RELEASE/spring-boot-autoconfigure-1.5.3.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/boot/spring-boot-starter-logging/1.5.3.RELEASE/spring-boot-starter-logging-1.5.3.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/ch/qos/logback/logback-classic/1.1.11/logback-classic-1.1.11.jar:/Users/ummulkiram 1/.m2/repository/ch/qos/logback/logback-core/1.1.11/logback-core-1.1.11.jar:/Users/ummulkiram 1/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.25/jcl-over-slf4j-1.7.25.jar:/Users/ummulkiram 1/.m2/repository/org/slf4j/jul-to-slf4j/1.7.25/jul-to-slf4j-1.7.25.jar:/Users/ummulkiram 1/.m2/repository/org/slf4j/log4j-over-slf4j/1.7.25/log4j-over-slf4j-1.7.25.jar:/Users/ummulkiram 1/.m2/repository/org/yaml/snakeyaml/1.17/snakeyaml-1.17.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/boot/spring-boot-starter-tomcat/1.5.3.RELEASE/spring-boot-starter-tomcat-1.5.3.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/apache/tomcat/embed/tomcat-embed-core/8.5.14/tomcat-embed-core-8.5.14.jar:/Users/ummulkiram 1/.m2/repository/org/apache/tomcat/embed/tomcat-embed-el/8.5.14/tomcat-embed-el-8.5.14.jar:/Users/ummulkiram 1/.m2/repository/org/apache/tomcat/embed/tomcat-embed-websocket/8.5.14/tomcat-embed-websocket-8.5.14.jar:/Users/ummulkiram 1/.m2/repository/org/hibernate/hibernate-validator/5.3.5.Final/hibernate-validator-5.3.5.Final.jar:/Users/ummulkiram 1/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/ummulkiram 1/.m2/repository/org/jboss/logging/jboss-logging/3.3.1.Final/jboss-logging-3.3.1.Final.jar:/Users/ummulkiram 1/.m2/repository/com/fasterxml/classmate/1.3.3/classmate-1.3.3.jar:/Users/ummulkiram 1/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.8.8/jackson-databind-2.8.8.jar:/Users/ummulkiram 1/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.8.0/jackson-annotations-2.8.0.jar:/Users/ummulkiram 1/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.8.8/jackson-core-2.8.8.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/spring-web/4.3.8.RELEASE/spring-web-4.3.8.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/spring-aop/4.3.8.RELEASE/spring-aop-4.3.8.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/spring-beans/4.3.8.RELEASE/spring-beans-4.3.8.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/spring-context/4.3.8.RELEASE/spring-context-4.3.8.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/spring-webmvc/4.3.8.RELEASE/spring-webmvc-4.3.8.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/spring-expression/4.3.8.RELEASE/spring-expression-4.3.8.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/boot/spring-boot-starter-jdbc/1.5.3.RELEASE/spring-boot-starter-jdbc-1.5.3.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/apache/tomcat/tomcat-jdbc/8.5.14/tomcat-jdbc-8.5.14.jar:/Users/ummulkiram 1/.m2/repository/org/apache/tomcat/tomcat-juli/8.5.14/tomcat-juli-8.5.14.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/spring-jdbc/4.3.8.RELEASE/spring-jdbc-4.3.8.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/spring-tx/4.3.8.RELEASE/spring-tx-4.3.8.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/com/h2database/h2/1.4.194/h2-1.4.194.jar:/Users/ummulkiram 1/.m2/repository/org/bgee/log4jdbc-log4j2/log4jdbc-log4j2-jdbc4.1/1.16/log4jdbc-log4j2-jdbc4.1-1.16.jar:/Users/ummulkiram 1/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/spring-core/4.3.8.RELEASE/spring-core-4.3.8.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/apache/httpcomponents/httpclient/4.5.3/httpclient-4.5.3.jar:/Users/ummulkiram 1/.m2/repository/org/apache/httpcomponents/httpcore/4.4.6/httpcore-4.4.6.jar:/Users/ummulkiram 1/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/Users/ummulkiram 1/.m2/repository/com/google/cloud/google-cloud-vision/1.31.0/google-cloud-vision-1.31.0.jar:/Users/ummulkiram 1/.m2/repository/com/google/cloud/google-cloud-core/1.31.0/google-cloud-core-1.31.0.jar:/Users/ummulkiram 1/.m2/repository/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar:/Users/ummulkiram 1/.m2/repository/com/google/http-client/google-http-client/1.23.0/google-http-client-1.23.0.jar:/Users/ummulkiram 1/.m2/repository/com/google/code/findbugs/jsr305/3.0.1/jsr305-3.0.1.jar:/Users/ummulkiram 1/.m2/repository/com/google/api/api-common/1.5.0/api-common-1.5.0.jar:/Users/ummulkiram 1/.m2/repository/com/google/api/gax/1.25.0/gax-1.25.0.jar:/Users/ummulkiram 1/.m2/repository/org/threeten/threetenbp/1.3.3/threetenbp-1.3.3.jar:/Users/ummulkiram 1/.m2/repository/com/google/auth/google-auth-library-oauth2-http/0.9.1/google-auth-library-oauth2-http-0.9.1.jar:/Users/ummulkiram 1/.m2/repository/com/google/http-client/google-http-client-jackson2/1.19.0/google-http-client-jackson2-1.19.0.jar:/Users/ummulkiram 1/.m2/repository/com/google/protobuf/protobuf-java-util/3.5.1/protobuf-java-util-3.5.1.jar:/Users/ummulkiram 1/.m2/repository/com/google/code/gson/gson/2.8.0/gson-2.8.0.jar:/Users/ummulkiram 1/.m2/repository/com/google/api/grpc/proto-google-common-protos/1.11.0/proto-google-common-protos-1.11.0.jar:/Users/ummulkiram 1/.m2/repository/com/google/api/grpc/proto-google-iam-v1/0.12.0/proto-google-iam-v1-0.12.0.jar:/Users/ummulkiram 1/.m2/repository/com/google/cloud/google-cloud-core-grpc/1.31.0/google-cloud-core-grpc-1.31.0.jar:/Users/ummulkiram 1/.m2/repository/com/google/auth/google-auth-library-credentials/0.9.1/google-auth-library-credentials-0.9.1.jar:/Users/ummulkiram 1/.m2/repository/com/google/protobuf/protobuf-java/3.5.1/protobuf-java-3.5.1.jar:/Users/ummulkiram 1/.m2/repository/io/grpc/grpc-protobuf/1.10.1/grpc-protobuf-1.10.1.jar:/Users/ummulkiram 1/.m2/repository/io/grpc/grpc-protobuf-lite/1.10.1/grpc-protobuf-lite-1.10.1.jar:/Users/ummulkiram 1/.m2/repository/io/grpc/grpc-context/1.10.1/grpc-context-1.10.1.jar:/Users/ummulkiram 1/.m2/repository/com/google/api/gax-grpc/1.25.0/gax-grpc-1.25.0.jar:/Users/ummulkiram 1/.m2/repository/com/google/api/grpc/proto-google-cloud-vision-v1/1.13.0/proto-google-cloud-vision-v1-1.13.0.jar:/Users/ummulkiram  1/.m2/repository/io/grpc/grpc-netty-shaded/1.10.1/grpc-netty-shaded-1.10.1.jar:/Users/ummulkiram 1/.m2/repository/io/grpc/grpc-core/1.10.1/grpc-core-1.10.1.jar:/Users/ummulkiram 1/.m2/repository/com/google/errorprone/error_prone_annotations/2.1.2/error_prone_annotations-2.1.2.jar:/Users/ummulkiram 1/.m2/repository/io/opencensus/opencensus-api/0.11.0/opencensus-api-0.11.0.jar:/Users/ummulkiram 1/.m2/repository/io/opencensus/opencensus-contrib-grpc-metrics/0.11.0/opencensus-contrib-grpc-metrics-0.11.0.jar:/Users/ummulkiram 1/.m2/repository/io/grpc/grpc-stub/1.10.1/grpc-stub-1.10.1.jar:/Users/ummulkiram 1/.m2/repository/io/grpc/grpc-auth/1.10.1/grpc-auth-1.10.1.jar:/Users/ummulkiram 1/.m2/repository/com/google/guava/guava/21.0/guava-21.0.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/boot/spring-boot-starter-actuator/1.5.3.RELEASE/spring-boot-starter-actuator-1.5.3.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/boot/spring-boot-actuator/1.5.3.RELEASE/spring-boot-actuator-1.5.3.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/cloud/spring-cloud-starter/1.2.2.RELEASE/spring-cloud-starter-1.2.2.RELEASE.jar:/Users/ummulkiram  1/.m2/repository/org/springframework/security/spring-security-crypto/4.2.2.RELEASE/spring-security-crypto-4.2.2.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/cloud/spring-cloud-commons/1.2.2.RELEASE/spring-cloud-commons-1.2.2.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/springframework/security/spring-security-rsa/1.0.3.RELEASE/spring-security-rsa-1.0.3.RELEASE.jar:/Users/ummulkiram 1/.m2/repository/org/bouncycastle/bcpkix-jdk15on/1.55/bcpkix-jdk15on-1.55.jar:/Users/ummulkiram 1/.m2/repository/org/bouncycastle/bcprov-jdk15on/1.55/bcprov-jdk15on-1.55.jar:/Users/ummulkiram 1/.m2/repository/org/assertj/assertj-core/2.6.0/assertj-core-2.6.0.jar:/Users/ummulkiram 1/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar"" com.bayview.TestGoogleVision
inpstream::::::::java.io.BufferedInputStream@1de0aca6
Setting::::::::ImageAnnotatorSettings{executorProvider=InstantiatingExecutorProvider{executorThreadCount=4}, transportChannelProvider=com.google.api.gax.grpc.InstantiatingGrpcChannelProvider@59e84876, credentialsProvider=FixedCredentialsProvider{credentials=ServiceAccountCredentials{clientId=103654972067458963122, clientEmail=uk-974@spherical-gate-204306.iam.gserviceaccount.com, privateKeyId=e053f4b02858a4389953cdfefe2d3321edc19952, transportFactoryClassName=com.google.auth.oauth2.OAuth2Utils$DefaultHttpTransportFactory, tokenServerUri=https://accounts.google.com/o/oauth2/token, scopes=[https://www.googleapis.com/auth/cloud-platform], serviceAccountUser=null}}, headerProvider=com.google.api.gax.rpc.NoHeaderProvider@61a485d2, internalHeaderProvider=com.google.api.gax.rpc.ApiClientHeaderProvider@39fb3ab6, clock=com.google.api.core.NanoClock@6276ae34, endpoint=vision.googleapis.com:443, watchdogProvider=com.google.api.gax.rpc.InstantiatingWatchdogProvider@7946e1f4, watchdogCheckInterval=PT10S}
14:15:54.181 [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.logging.InternalLoggerFactory - Using SLF4J as the default logging framework
14:15:54.201 [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.PlatformDependent - Platform: MacOS
14:15:54.202 [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.PlatformDependent0 - -Dio.netty.noUnsafe: false
14:15:54.204 [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.PlatformDependent0 - Java version: 8
14:15:54.207 [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.PlatformDependent0 - jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
14:15:54.207 [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.PlatformDependent0 - java.nio.DirectByteBuffer.&lt;init&gt;(long, int): available
14:15:54.207 [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.PlatformDependent - sun.misc.Unsafe: available
14:15:54.207 [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.PlatformDependent - -Dio.netty.tmpdir: /var/folders/2n/dp_swhjs5mqb17cn0vr33yq00000gq/T (java.io.tmpdir)
14:15:54.207 [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.PlatformDependent - -Dio.netty.bitMode: 64 (sun.arch.data.model)
14:15:54.209 [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.PlatformDependent - -Dio.netty.noPreferDirect: false
14:15:54.209 [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.PlatformDependent - -Dio.netty.maxDirectMemory: 3817865216 bytes
14:15:54.209 [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.PlatformDependent - -Dio.netty.uninitializedArrayAllocationThreshold: -1
14:15:54.210 [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.CleanerJava6 - java.nio.ByteBuffer.cleaner(): available
14:15:54.211 [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.NativeLibraryLoader - -Dio.netty.native.workdir: /var/folders/2n/dp_swhjs5mqb17cn0vr33yq00000gq/T (io.netty.tmpdir)
14:15:54.214 [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.NativeLibraryLoader - Unable to load the library 'io_grpc_netty_shaded_netty_tcnative_osx_x86_64', trying other loading mechanism.
java.lang.UnsatisfiedLinkError: no io_grpc_netty_shaded_netty_tcnative_osx_x86_64 in java.library.path
    at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867)
    at java.lang.Runtime.loadLibrary0(Runtime.java:870)
    at java.lang.System.loadLibrary(System.java:1122)
    at io.grpc.netty.shaded.io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38)
    at io.grpc.netty.shaded.io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:243)
    at io.grpc.netty.shaded.io.netty.handler.ssl.OpenSsl.loadTcNative(OpenSsl.java:421)
    at io.grpc.netty.shaded.io.netty.handler.ssl.OpenSsl.&lt;clinit&gt;(OpenSsl.java:89)
    at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:155)
    at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:147)
    at com.google.api.gax.rpc.ClientContext.create(ClientContext.java:151)
    at com.google.cloud.vision.v1.stub.GrpcImageAnnotatorStub.create(GrpcImageAnnotatorStub.java:84)
    at com.google.cloud.vision.v1.stub.ImageAnnotatorStubSettings.createStub(ImageAnnotatorStubSettings.java:120)
    at com.google.cloud.vision.v1.ImageAnnotatorClient.&lt;init&gt;(ImageAnnotatorClient.java:136)
    at com.google.cloud.vision.v1.ImageAnnotatorClient.create(ImageAnnotatorClient.java:117)
    at com.bayview.TestGoogleVision.main(TestGoogleVision.java:57)
    Suppressed: java.lang.UnsatisfiedLinkError: no io_grpc_netty_shaded_netty_tcnative_osx_x86_64 in java.library.path
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867)
        at java.lang.Runtime.loadLibrary0(Runtime.java:870)
        at java.lang.System.loadLibrary(System.java:1122)
        at io.grpc.netty.shaded.io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at io.grpc.netty.shaded.io.netty.util.internal.NativeLibraryLoader$1.run(NativeLibraryLoader.java:263)
        at java.security.AccessController.doPrivileged(Native Method)
        at io.grpc.netty.shaded.io.netty.util.internal.NativeLibraryLoader.loadLibraryByHelper(NativeLibraryLoader.java:255)
        at io.grpc.netty.shaded.io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:233)
        ... 21 common frames omitted
14:15:54.241 [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.NativeLibraryLoader - Successfully loaded the library /var/folders/2n/dp_swhjs5mqb17cn0vr33yq00000gq/T/libio_grpc_netty_shaded_netty_tcnative_osx_x86_649072795634898251757.dylib
14:15:54.241 [main] DEBUG io.grpc.netty.shaded.io.netty.handler.ssl.OpenSsl - netty-tcnative using native library: BoringSSL
14:15:54.364 [main] DEBUG io.grpc.netty.shaded.io.netty.util.ResourceLeakDetector - -Dio.grpc.netty.shaded.io.netty.leakDetection.level: simple
14:15:54.377 [main] DEBUG io.grpc.netty.shaded.io.netty.util.ResourceLeakDetectorFactory - Loaded default ResourceLeakDetector: io.grpc.netty.shaded.io.netty.util.ResourceLeakDetector@3b2cf7ab
14:15:54.390 [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.InternalThreadLocalMap - -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
14:15:54.446 [main] DEBUG io.grpc.netty.shaded.io.netty.util.ResourceLeakDetectorFactory - Loaded default ResourceLeakDetector: io.grpc.netty.shaded.io.netty.util.ResourceLeakDetector@147ed70f
14:15:54.517 [main] DEBUG io.grpc.netty.shaded.io.netty.util.Recycler - -Dio.netty.recycler.maxCapacityPerThread: 32768
14:15:54.517 [main] DEBUG io.grpc.netty.shaded.io.netty.util.Recycler - -Dio.netty.recycler.linkCapacity: 16
14:15:54.517 [main] DEBUG io.grpc.netty.shaded.io.netty.util.Recycler - -Dio.netty.recycler.ratio: 8
14:15:54.567 [main] DEBUG io.grpc.netty.shaded.io.netty.handler.ssl.CipherSuiteConverter - Cipher suite mapping: TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 =&gt; ECDHE-ECDSA-CHACHA20-POLY1305
14:15:54.567 [main] DEBUG io.grpc.netty.shaded.io.netty.handler.ssl.CipherSuiteConverter - Cipher suite mapping: SSL_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 =&gt; ECDHE-ECDSA-CHACHA20-POLY1305
14:15:54.568 [main] DEBUG io.grpc.netty.shaded.io.netty.handler.ssl.CipherSuiteConverter - Cipher suite mapping: TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 =&gt; ECDHE-RSA-AES128-SHA256
c.netty.shaded.io.netty.handler.ssl.CipherSuiteConverter - Cipher suite mapping: TLS_PSK_WITH_AES_128_CBC_SHA =&gt; PSK-AES128-CBC-SHA
14:15:54.572 [main] DEBUG io.grpc.netty.shaded.io.netty.handler.ssl.OpenSsl - Supported protocols (OpenSSL): [[SSLv2Hello, TLSv1, TLSv1.1, TLSv1.2]] 
14:15:54.572 [main] DEBUG io.grpc.netty.shaded.io.netty.handler.ssl.OpenSsl - Default cipher suites (OpenSSL): [TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_PSK_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,  TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_256_CBC_SHA]
14:15:54.727 [main] DEBUG io.grpc.netty.shaded.io.netty.channel.MultithreadEventLoopGroup - -Dio.netty.eventLoopThreads: 16
14:15:54.761 [main] DEBUG io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop - -Dio.netty.noKeySetOptimization: false
14:15:54.761 [main] DEBUG io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop - -Dio.netty.selectorAutoRebuildThreshold: 512
14:15:54.772 [main] DEBUG io.grpc.netty.shaded.io.netty.util.internal.PlatformDependent - org.jctools-core.MpscChunkedArrayQueue: available
CLIENT::::::: com.google.cloud.vision.v1.ImageAnnotatorClient@55b53d44
line 80
14:15:55.202 [grpc-default-executor-0] DEBUG io.grpc.netty.shaded.io.netty.channel.DefaultChannelId - -Dio.netty.processId: 62389 (auto-detected)
14:15:55.206 [grpc-default-executor-0] DEBUG io.grpc.netty.shaded.io.netty.util.NetUtil - Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
14:15:55.207 [grpc-default-executor-0] DEBUG io.grpc.netty.shaded.io.netty.util.NetUtil - Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
14:15:55.275 [grpc-default-worker-ELG-2-3] DEBUG io.grpc.netty.shaded.io.netty.util.ResourceLeakDetectorFactory - Loaded default ResourceLeakDetector: io.grpc.netty.shaded.io.netty.util.ResourceLeakDetector@7d6a4305
14:15:55.637 [grpc-default-worker-ELG-2-3] DEBUG io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler - [id: 0x028b11ee, L:/192.168.1.101:55036 - R:vision.googleapis.com/172.217.160.170:443] HANDSHAKEN: TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256
14:15:55.650 [grpc-default-worker-ELG-2-3] DEBUG io.grpc.netty.shaded.io.grpc.netty.NettyClientHandler - [id: 0x028b11ee, L:/192.168.1.101:55036 - R:vision.googleapis.com/172.217.160.170:443] OUTBOUND SETTINGS: ack=false settings={ENABLE_PUSH=0, MAX_CONCURRENT_STREAMS=0, INITIAL_WINDOW_SIZE=1048576, MAX_HEADER_LIST_SIZE=8192}
14:15:55.651 [grpc-default-worker-ELG-2-3] DEBUG io.grpc.netty.shaded.io.grpc.netty.NettyClientHandler - [id: 0x028b11ee, L:/192.168.1.101:55036 - R:vision.googleapis.com/172.217.160.170:443] OUTBOUND WINDOW_UPDATE: streamId=0 windowSizeIncrement=983041
14:15:55.703 [grpc-default-worker-ELG-2-3] DEBUG io.grpc.netty.shaded.io.grpc.netty.NettyClientHandler - [id: 0x028b11ee, L:/192.168.1.101:55036 - R:vision.googleapis.com/172.217.160.170:443] INBOUND SETTINGS: ack=true
14:19:55.643 [grpc-default-worker-ELG-2-3] DEBUG io.grpc.netty.shaded.io.grpc.netty.NettyClientHandler - [id: 0x028b11ee, L:/192.168.1.101:55036 - R:vision.googleapis.com/172.217.160.170:443] INBOUND GO_AWAY: lastStreamId=0 errorCode=0 length=17 bytes=73657373696f6e5f74696d65645f6f7574
Exception in thread ""main"" com.google.api.gax.rpc.DeadlineExceededException: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded: -538368540044 ns from now
    at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:51)
    at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:72)
    at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:60)
    at com.google.api.gax.grpc.GrpcExceptionCallable$ExceptionTransformingFuture.onFailure(GrpcExceptionCallable.java:95)
    at com.google.api.core.ApiFutures$1.onFailure(ApiFutures.java:61)
    at com.google.common.util.concurrent.Futures$4.run(Futures.java:1126)
    at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:399)
    at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:902)
    at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:813)
    at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:677)
    at io.grpc.stub.ClientCalls$GrpcFuture.setException(ClientCalls.java:492)
    at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:467)
    at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:41)
    at io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:684)
    at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:41)
    at io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:391)
    at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:475)
    at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:557)
    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:478)
    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:590)
    at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
    at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded: -538368540044 ns from now
    at io.grpc.Status.asRuntimeException(Status.java:526)
    ... 19 more

Process finished with exit code 1
</code></pre>",,1,0,,2018-06-01 11:06:16.463 UTC,1,2018-06-12 18:43:43.967 UTC,,,,,4507518,1,3,google-vision,450
Google Cloud Vision API 'Request Admission Denied',37796918,Google Cloud Vision API 'Request Admission Denied',"<p>I am new to Google Cloud Vision API. I am doing OCR on images primarily for bills and receipts.</p>

<p>For a few images it is working fine, but when I try some other images it gives me this error:</p>

<pre><code>Error:  { [Error: Request Admission Denied.]
  code: 400,
  errors:
   [ { message: 'Request Admission Denied.',
       domain: 'global',
       reason: 'badRequest' } ] }
</code></pre>

<p>This is my code:</p>

<pre><code>// construct parameters
const req = new vision.Request({
image: new vision.Image('./uploads/reciept.png'),
features: [
new vision.Feature('TEXT_DETECTION', 1)
]
})

vision.annotate(req).then((res) =&gt; {
// handling response
//console.log(res.responses[0].textAnnotations);
var desc=res.responses[0].textAnnotations;
var descarr=[];
for (i = 0; i &lt; desc.length; i++) { 
descarr.push(desc[i].description);
}
</code></pre>",38131991,4,1,,2016-06-13 18:38:25.013 UTC,,2016-08-16 14:05:27.817 UTC,2016-06-13 19:08:46.020 UTC,,5764553,,1688432,1,5,ocr|gcloud|google-cloud-vision|google-vision,1413
NameError: name 'creds' is not defined,48748566,NameError: name 'creds' is not defined,"<p>I got an error,NameError: name 'creds' is not defined .I wanna use Google Cloud Vision API.I set up various things in Google Cloud and I downloaded google-cloud-sdk-180.0.0-darwin-x86_64.tar.gz ,and I run command <code>./google-cloud-sdk/bin/gcloud init</code>,it was successful.I wrote test.py </p>

<pre><code>import io
import os

# Imports the Google Cloud client library
from google.cloud import vision
from google.cloud.vision import types

# Instantiates a client
client = vision.ImageAnnotatorClient(credentials=creds,)

# The name of the image file to annotate
file_name = os.path.join(
    os.path.dirname(__file__),
    'cat.jpg')

# Loads the image into memory
with io.open(file_name, 'rb') as image_file:
    content = image_file.read()
</code></pre>

<p>and when I run this codes,</p>

<pre><code>Traceback (most recent call last):
    client = vision.ImageAnnotatorClient(credentials=creds,)
NameError: name 'creds' is not defined
</code></pre>

<p>the error happens.
I wrote codes by seeing <a href=""https://github.com/GoogleCloudPlatform/google-cloud-python/blob/master/docs/vision/index.rst#id8"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/google-cloud-python/blob/master/docs/vision/index.rst#id8</a> ,so I rewrote </p>

<pre><code>client = vision.ImageAnnotatorClient()
</code></pre>

<p>error happens google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or
explicitly create credential and re-run the application. For more
information, please see
<a href=""https://developers.google.com/accounts/docs/application-default-credentials"" rel=""nofollow noreferrer"">https://developers.google.com/accounts/docs/application-default-credentials</a>. . I really cannot understand why this error happens.I installed config.json &amp; index.js &amp; package.json in same directory as test.py but same error happens.I run command  <code>gcloud components update &amp;&amp; gcloud components install beta</code> but zsh: command not found: gcloud error happens.How should I fix this?What is wrong in my codes?</p>",48748923,1,2,,2018-02-12 14:19:22.037 UTC,,2018-02-12 14:39:27.253 UTC,2018-02-12 14:20:47.933 UTC,,5320906,,9287271,1,1,python|google-cloud-platform|google-vision,830
React state updating but not rendering on child component,51088778,React state updating but not rendering on child component,"<p>I know the state is updating because 1. the 'Loading...' is going away, I can console log this.state.images to see the array. However when the state updates and the loading goes the searchbar shows up but the Card's within CardList do not.</p>

<p>They do show up when I search for a correct string, but not before.</p>

<p>If I pass this.state.images to CardList they show up perfectly. However when I move to the filteredImages they only show up when filtered.</p>

<p>Any ideas? Thanks in advance.</p>

<pre><code>class App extends Component {

  constructor() {
    super();
    this.state = {
      images:[],
      searchfield: ''
    }
  }

  getLabels = (image) =&gt; {
    const AuthKey = key.key;
    const res = axios.post(`https://vision.googleapis.com/v1/images:annotate?key=${AuthKey}`, {
      requests: [
        {
          image:{
            source:{
              imageUri: `http://storage.googleapis.com/${image}`
            }
          },
          features:[
            {
              type:""LABEL_DETECTION"",
              maxResults:10
            }
          ]
        }
      ]
    });

    res.then(function (response) {
      const results = response.data.responses[0].labelAnnotations;
      const ex = results.map(result =&gt; {
        return result.description;
      }); 
      return ex;
    });

    return res;

  };

  componentDidMount() {
      imageFiles.imageFiles.forEach(img =&gt; {
        this.getLabels(img).then(result =&gt; {
          const results = result.data.responses[0].labelAnnotations;
          const labels = results.map(result =&gt; {
          return result.description;
        });
        //Add new values to the state 
        this.setState({images:[...this.state.images, {img, labels}]});
      });
    })
  }

  onSearchChange = (event) =&gt; {
    this.setState({searchfield: event.target.value});
  }

  render() {
    const filteredImages = this.state.images.filter(image =&gt; {
      return image.labels.includes(this.state.searchfield.toLowerCase());
    });
    // Create an array of objects to store the image path and the labels detected from Google Vision
    if (this.state.images.length === 0) {
      return &lt;h1&gt;Loading...&lt;/h1&gt;
    } else {
      return (
        &lt;Grid className=""App""&gt;
          &lt;SearchBox searchChange={this.onSearchChange}/&gt;
          &lt;CardList images={filteredImages} /&gt;
        &lt;/Grid&gt;
      )}
  }

}

export default App;
</code></pre>",51095069,1,6,,2018-06-28 17:45:41.553 UTC,,2018-06-29 08:34:46.447 UTC,,,,,1624927,1,0,javascript|reactjs|setstate,81
Firebase Cloud function & Cloud Vision API: TypeError: vision.detectText is not a function,47946770,Firebase Cloud function & Cloud Vision API: TypeError: vision.detectText is not a function,"<p>I try to use the Cloud Vision API in a Firebase Cloud function to OCR an image stored in Firebase Storage.</p>

<p>I import the Google Cloud vision client library as follow</p>

<pre><code>const vision = require('@google-cloud/vision');
</code></pre>

<p>and then I call </p>

<pre><code>vision.detectText({ source: { imageUri: 'gs://xxxx.appspot.com/yyyy.JPG' } }) 
</code></pre>

<p>However I get an error </p>

<p>TypeError: vision.detectText is not a function</p>

<p>Initially I used </p>

<pre><code>vision.textDetection({ source: { imageUri: ... } })
</code></pre>

<p>from this example <a href=""https://cloud.google.com/vision/docs/reference/libraries#client-libraries-install-nodejs"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/reference/libraries#client-libraries-install-nodejs</a> but I got the exact same error. I then read that textDetection has been replaced by detectText but no more success</p>

<p>Thanks in advance</p>",47947217,1,0,,2017-12-22 20:00:09.083 UTC,,2017-12-22 20:51:34.080 UTC,2017-12-22 20:51:34.080 UTC,,807126,,3371862,1,2,node.js|firebase|google-cloud-functions|google-cloud-vision,716
Google Cloud Vision produce duplicate files in android studio,53515301,Google Cloud Vision produce duplicate files in android studio,"<p>I'm using android studio and cloud vision in order to detect faces features in a picture. When compiling, I get this error (About cloud vision V 1.53):</p>

<pre><code>Duplicate files copied in APK META-INF/INDEX.LIST
    File1: /home/dragonklavier/.gradle/caches/modules-2/files-2.1/com.google.cloud/google-cloud-core-grpc/1.53.0/f86ad24d0f605abab1bd8020f2ab357125be404d/google-cloud-core-grpc-1.53.0.jar
    File2: /home/dragonklavier/.gradle/caches/modules-2/files-2.1/com.google.cloud/google-cloud-vision/1.53.0/e54a24754ed11d72bfd9f0b8d0d078dca3c7533f/google-cloud-vision-1.53.0.jar
    File3: /home/dragonklavier/.gradle/caches/modules-2/files-2.1/com.google.cloud/google-cloud-core/1.53.0/3f87dfdae359ce1cdb738d4feac6569792f54ee9/google-cloud-core-1.53.0.jar
</code></pre>

<p>My Gradles dependencies are:</p>

<pre><code>dependencies {
    compile fileTree(dir: 'libs', include: ['*.jar'])
    androidTestCompile('com.android.support.test.espresso:espresso-core:2.2.2', {
        exclude group: 'com.android.support', module: 'support-annotations'
        exclude group: 'com.google.code.findbugs'
    })
    testCompile 'junit:junit:4.12'
    compile 'com.google.android.gms:play-services-appindexing:9.0.0'
    compile 'com.google.android.gms:play-services-auth:9.0.0'
    compile 'com.android.support:appcompat-v7:25.1.1'
    compile 'com.android.support:design:25.1.1'

   compile 'com.google.cloud:google-cloud-vision:1.53.0'



   compile 'net.sourceforge.owlapi:owlapi-distribution:3.4.3'

    //compile 'com.android.billingclient:billing:1.1'


    compile('com.sangcomz:FishBun:0.6.4@aar') {
        transitive = true
    }
    compile 'com.github.PhilJay:MPAndroidChart:v3.0.1'
}
</code></pre>",,1,0,,2018-11-28 08:41:31.190 UTC,,2018-11-28 08:54:15.007 UTC,,,,,10691858,1,0,android|api|gradle,30
Why converting C# HTTP Request to Java did not work?,54513958,Why converting C# HTTP Request to Java did not work?,"<p>I am trying to use the Microsoft Custom Vision<sup><a href=""https://azure.microsoft.com/de-de/services/cognitive-services/custom-vision-service/"" rel=""nofollow noreferrer"">know more</a></sup>. I need to make an HTTP request to send the image to be analyzed. I successfully made a request from C# so I know the information is correct. </p>

<p>However, when I tried to make the same request in Java I received an HTTP 400 error. </p>

<blockquote>
  <p>I believe I did not handle the request correctly in Java. Is that true?</p>
</blockquote>

<p>Following are the snippets.</p>

<h3>C#:</h3>

<pre><code>var client = new HttpClient();
client.DefaultRequestHeaders.Add(""Prediction-Key"", PredicitionKey);
using (var content = new 
ByteArrayContent(byteData))
{
  content.Headers.ContentType = new MediaTypeHeaderValue(""application/octet-stream"");
  response = await client.PostAsync(url, content);
  Console.WriteLine(await response.Content.ReadAsStringAsync());
}
</code></pre>

<h3>Java:</h3>

<pre><code>HttpURLConnection connection = (HttpURLConnection) url.openConnection();
connection.setRequestProperty(""Prediction-Key"", predicitionKey);
connection.setRequestProperty(""Content-Type"", ""application/octet-stream"");
connection.setDoInput(true);
connection.setDoOutput(true);
connection.getOutputStream().write(data.getData());
connection.connect();

Reader in = new BufferedReader(new InputStreamReader(connection.getInputStream(), ""UTF-8""));
</code></pre>",54517151,1,2,,2019-02-04 10:14:12.583 UTC,,2019-02-04 13:27:14.257 UTC,2019-02-04 10:37:47.573 UTC,,1386680,,8939032,1,-1,java|c#|http-headers,41
listCollection returns empty when using javascript sdk while it was created through python using boto3,55666678,listCollection returns empty when using javascript sdk while it was created through python using boto3,"<p>I created a collection using boto3 with following code:</p>

<pre><code>rekognition = boto3.client('rekognition',
        aws_access_key_id=""__myclientId"",
        aws_secret_access_key=""__secret""
)
rekognition.create_collection(
     CollectionId='myPhotos'
)
</code></pre>

<p>which is appearing as created when I fetch it using the following code in python boto3:</p>

<pre><code>response = rekognition.list_collections()
print(response)
</code></pre>

<p>But when I try to fetch the same collection using Javascript SDK ""aws-sdk"" in nodeJs using following code I get empty results:</p>

<pre><code>const AWS = require('aws-sdk');
AWS.config.update({
            accessKeyId : ""__myclientId"",
            secretAccessKey : ""__secret"",
            region: ""us-east-1""
});

let rekognition = new AWS.Rekognition();
rekognition.listCollections({}, function (err, data)
{
     console.log(err, data);
});
</code></pre>

<p><strong>RESPONSE JS:</strong></p>

<pre><code>{ CollectionIds: [], FaceModelVersions: [] }
</code></pre>",,1,0,,2019-04-13 14:40:19.157 UTC,,2019-04-13 18:00:19.003 UTC,,,,,606669,1,0,aws-sdk|boto3|amazon-rekognition,10
IBM Watson Visual Recognition in Powershell,49416747,IBM Watson Visual Recognition in Powershell,"<p>I am attempting to integrate Watson Visual Recognition into a powershell script, I have my free account set up and everything works form curl in a docker container.  But I cannot for the life of me figure out how to get it to work from Powershell.</p>

<p>The example curl command is</p>

<pre><code>curl ""https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classify?api_key={api-key}&amp;url=https://watson-developer-cloud.github.io/doc-tutorial-downloads/visual-recognition/fruitbowl.jpg&amp;version=2016-05-20""
</code></pre>

<p>where <code>{api-key}</code> is replaced with an actual api key</p>

<p>As this is just hitting a URL I expected I should be able to use</p>

<pre><code>Invoke-RestMethod -Uri ""https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classify?api_key={api-key}&amp;url=https://watson-developer-cloud.github.io/doc-tutorial-downloads/visual-recognition/fruitbowl.jpg&amp;version=2016-05-20""
</code></pre>

<p>However <code>Invoke-RestMethod</code> returns</p>

<pre><code>Invoke-RestMethod : The underlying connection was closed: An unexpected error occurred on a send.
At line:1 char:1
+ Invoke-RestMethod -Uri ""https://gateway-a.watsonplatform.net/visual-r ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : InvalidOperation: (System.Net.HttpWebRequest:HttpWebRequest) [Invoke-RestMethod], WebException
    + FullyQualifiedErrorId : WebCmdletWebResponseException,Microsoft.PowerShell.Commands.InvokeRestMethodCommand
</code></pre>

<p>What am I missing in my <code>Invoke-RestMethod</code> commands?  Do I need to specify some sort of headers or something?</p>

<p>Documentation link <a href=""https://www.ibm.com/watson/developercloud/visual-recognition/api/v3/curl.html?curl#data-collection"" rel=""nofollow noreferrer"">https://www.ibm.com/watson/developercloud/visual-recognition/api/v3/curl.html?curl#data-collection</a></p>",49417277,1,0,,2018-03-21 21:23:07.773 UTC,,2018-03-21 22:04:05.037 UTC,,,,,4378874,1,0,powershell|curl|watson,119
Google Vision - RPC deadline exceeded. Image processing error,43069723,Google Vision - RPC deadline exceeded. Image processing error,"<p>We are using Google Vision API to extract text from image. Suddenly, since this morning, Google API returns the below error for few images and empty text(with HTTP 200 status code) for others.</p>

<blockquote>
  <p>Status: 4, Message: image-annotator::RPC deadline exceeded.: Image
  processing error!</p>
</blockquote>

<p>Can someone explain why we are getting that error and how we can rectify it?</p>",,0,5,,2017-03-28 12:34:31.057 UTC,,2017-03-29 14:30:54.210 UTC,2017-03-29 14:30:54.210 UTC,,5231007,,710817,1,1,google-cloud-platform|ocr|google-cloud-vision,173
Not able to generate correct AWS signature in Google App script,55683585,Not able to generate correct AWS signature in Google App script,"<p>I am trying to access image rekognition service of AWS using google-App-script, for this, I am trying to generate an AWS signature for API call, but response showing an error message.</p>

<p><code>{""__type"":""InvalidSignatureException"",""message"":""The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.""}</code></p>

<p>when I am trying with postman using authorization (AWS signature) it is working fine. </p>

<p>below is my google app script code in which I am calling the main function</p>

<pre><code>function main(){
    var headers=AwsHeader_('ListCollections');

  var t=UrlFetchApp.fetch(""https://rekognition.us-east-1.amazonaws.com"", {
      method: ""POST"",
      muteHttpExceptions:true,
    headers:headers
    });

  Logger.log(""======&gt;&gt;""+t);
}


function hexSignature(signature){
    var signatureStr = '';
  Logger.log(signature.length);

    for (i = 0; i &lt; signature.length; i++) {
      var byte = signature[i];
      Logger.log(""byte""+byte);
      if (byte &lt; 0)
        byte += 256;
      var byteStr = byte.toString(16);
      // Ensure we have 2 chars in our byte, pad with 0
      if (byteStr.length == 1) byteStr = '0'+byteStr;
      signatureStr += byteStr;

      Logger.log(""signatureStr""+signatureStr);
    }  
  return signatureStr;
}


function get_signature(datestamp){

  var secret=""XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"";
  var region=""us-east-1"";
  var service=""rekognition"";


  var kdate = Utilities.computeHmacSignature(Utilities.MacAlgorithm.HMAC_SHA_256, datestamp, ""AWS4""+secret);
  var kregion = Utilities.computeHmacSignature(Utilities.MacAlgorithm.HMAC_SHA_256, region, kdate);
  var kservice = Utilities.computeHmacSignature(Utilities.MacAlgorithm.HMAC_SHA_256, service, kregion);
  var ksigning = Utilities.computeHmacSignature(Utilities.MacAlgorithm.HMAC_SHA_256, ""aws4_request"", kservice);

  var signature=hexSignature(ksigning);
  return signature;

}

function AwsHeader_(service){

  var header={};
  var date=new Date();

  var formattedDate = Utilities.formatDate(date, ""GMT"", ""yyyyMMdd'T'HHmmss'Z'"");
  var onlydate=Utilities.formatDate(date, ""GMT"", ""yyyyMMdd"")


  var sign=get_signature(onlydate);

  header['Authorization']=""AWS4-HMAC-SHA256 Credential=XXXXXXXXXX/""+onlydate+""/us-east-1/rekognition/aws4_request, SignedHeaders=cache-control;content-length;content-type;host;postman-token;x-amz-date;x-amz-target, Signature=""+sign;
  header['X-Amz-Date']=formattedDate;
  header['Content-Type']=""application/x-amz-json-1.1"";
  header['X-Amz-Target']=""RekognitionService.""+service;
    return header;
}



</code></pre>

<p>I am not able to figure out what I am doing wrong in the code, or it is the wrong method of generating the signature, please help.</p>",,0,2,,2019-04-15 06:28:25.423 UTC,,2019-04-15 06:37:00.417 UTC,2019-04-15 06:37:00.417 UTC,,10722131,,10722131,1,1,amazon-web-services|google-apps-script|signature|aws-service-catalog,26
How to start the stream processor in Amazon Rekognition,51336137,How to start the stream processor in Amazon Rekognition,"<p>I followed the AWS Rekognition Developer Guide and wrote a stream processor using CreateStreamProcessor in Java.</p>

<pre><code>import com.amazonaws.services.rekognition.AmazonRekognition;
import com.amazonaws.services.rekognition.AmazonRekognitionClientBuilder;
import com.amazonaws.services.rekognition.model.*;

public class StreamProcessor {
    private String streamProcessorName;
    private String kinesisVideoStreamArn;
    private String kinesisDataStreamArn;
    private String roleArn;
    private String collectionId;
    private float matchThreshold;
    private AmazonRekognition rekognitionClient = AmazonRekognitionClientBuilder.defaultClient();

public void createStreamProcessor() {
    KinesisVideoStream kinesisVideoStream = new KinesisVideoStream().withArn(kinesisVideoStreamArn);
    StreamProcessorInput streamProcessorInput = new StreamProcessorInput().withKinesisVideoStream(kinesisVideoStream);
    KinesisDataStream kinesisDataStream = new KinesisDataStream().withArn(kinesisDataStreamArn);
    StreamProcessorOutput streamProcessorOutput = new StreamProcessorOutput().withKinesisDataStream(kinesisDataStream);
    FaceSearchSettings faceSearchSettings = new FaceSearchSettings().withCollectionId(collectionId)
            .withFaceMatchThreshold(matchThreshold);
    StreamProcessorSettings streamProcessorSettings = new StreamProcessorSettings().withFaceSearch(faceSearchSettings);
    CreateStreamProcessorResult createStreamProcessorResult = rekognitionClient.createStreamProcessor(
            new CreateStreamProcessorRequest().withInput(streamProcessorInput).withOutput(streamProcessorOutput)
                    .withSettings(streamProcessorSettings).withRoleArn(roleArn).withName(streamProcessorName));
    System.out.println(""StreamProcessorArn - "" +
            createStreamProcessorResult.getStreamProcessorArn());
}

public void startStreamProcessor() {
    StartStreamProcessorResult startStreamProcessorResult = rekognitionClient.startStreamProcessor(
            new StartStreamProcessorRequest().withName(streamProcessorName));
}

public void stopStreamProcessorSample() {
    StopStreamProcessorResult stopStreamProcessorResult = rekognitionClient.stopStreamProcessor(
            new StopStreamProcessorRequest().withName(streamProcessorName));
}

public void deleteStreamProcessorSample() {
    DeleteStreamProcessorResult deleteStreamProcessorResult = rekognitionClient.deleteStreamProcessor(
            new DeleteStreamProcessorRequest().withName(streamProcessorName));
}

public void describeStreamProcessorSample() {
    DescribeStreamProcessorResult describeStreamProcessorResult = rekognitionClient.describeStreamProcessor(
            new DescribeStreamProcessorRequest().withName(streamProcessorName));
    System.out.println(""Arn - "" + describeStreamProcessorResult.getStreamProcessorArn());
    System.out.println(""Input kinesisVideo stream - "" + describeStreamProcessorResult.getInput()
            .getKinesisVideoStream().getArn());
    System.out.println(""Output kinesisData stream - "" + describeStreamProcessorResult.getOutput()
            .getKinesisDataStream().getArn());
    System.out.println(""RoleArn - "" + describeStreamProcessorResult.getRoleArn());
    System.out.println(""CollectionId - "" + describeStreamProcessorResult.getSettings().getFaceSearch()
            .getCollectionId());
    System.out.println(""Status - "" + describeStreamProcessorResult.getStatus());
    System.out.println(""Status message - "" + describeStreamProcessorResult.getStatusMessage());
    System.out.println(""Creation timestamp - "" + describeStreamProcessorResult.getCreationTimestamp());
    System.out.println(""Last updatClient rekognitionClient = new AmazonRekognitionClient()e timestamp - ""
            + describeStreamProcessorResult.getLastUpdateTimestamp());
}

public void listStreamProcessorSample() {
    ListStreamProcessorsResult listStreamProcessorsResult = rekognitionClient.listStreamProcessors(
            new ListStreamProcessorsRequest().withMaxResults(100));
    for (com.amazonaws.services.rekognition.model.StreamProcessor streamProcessor :
            listStreamProcessorsResult.getStreamProcessors()) {
        System.out.println(""StreamProcessor name - "" + streamProcessor.getName());
        System.out.println(""Status - "" + streamProcessor.getStatus());
    }
}
</code></pre>

<p>}</p>

<p>But I can't figure out how to start the stream processor? Do I have to simply write the main method and call <code>createStreamProcessor()</code> function? Or do I have to do something else: like the guide mentioned something as <code>StartStreamProcessor</code>?</p>",,1,0,,2018-07-14 07:05:04.103 UTC,,2018-09-21 21:50:06.120 UTC,2018-07-14 18:09:25.067 UTC,,7534249,,7534249,1,0,amazon-web-services|amazon-rekognition,50
How to run django application on google app engine without using gunicorn,43129395,How to run django application on google app engine without using gunicorn,"<p>I have gone through all the documentation provided for the running Django Application on app engine.
I have a Django Application where I am using Vision and Storage clients and my app name is pvd.
I have been constantly getting below errors in error logs.</p>

<pre><code>A  [2017-03-30 22:08:07 +0000] [1] [CRITICAL] WORKER TIMEOUT (pid:7) 
A  [2017-03-30 22:08:07 +0000] [7] [INFO] Worker exiting (pid: 7) 
A  [2017-03-30 22:08:07 +0000] [9] [INFO] Booting worker with pid: 9 
A  [2017-03-30 22:12:35 +0000] [1] [CRITICAL] WORKER TIMEOUT (pid:9) 
A  [2017-03-30 22:12:35 +0000] [9] [INFO] Worker exiting (pid: 9) 
A  [2017-03-30 22:12:36 +0000] [11] [INFO] Booting worker with pid: 11 
A  [2017-03-30 22:13:03 +0000] [1] [INFO] Handling signal: term 
A  [2017-03-30 22:13:03 +0000] [7] [INFO] Worker exiting (pid: 7) 
A  [2017-03-30 22:13:03 +0000] [1] [INFO] Shutting down: Master*
</code></pre>

<p>Below is my app.yaml</p>

<pre><code>runtime: python
env: flex
entrypoint: gunicorn -b :$PORT pythonvision.wsgi

runtime_config:
  python_version: 3
</code></pre>

<p>Below is my requirement.txt</p>

<pre><code>Django==1.10.6
google-cloud-storage==0.23.1
google-cloud-vision==0.23.1
gunicorn==19.7.0
</code></pre>

<p>For deploying I am using:</p>

<pre><code>gcloud app deploy
</code></pre>

<p>What am I doing wrong?</p>",,2,0,,2017-03-30 22:26:35.013 UTC,1,2017-06-19 04:23:29 UTC,,,,,3012946,1,1,python|django|google-app-engine|google-cloud-platform|gunicorn,848
installing google-cloud using pip fails,43124732,installing google-cloud using pip fails,"<p>Im trying to install the <code>google-cloud</code> python package, and I encounter the foloowing meesage whenn the installation fails:</p>

<pre><code> pip install --upgrade google-cloud
    Collecting google-cloud
      Using cached google_cloud-0.23.0-py2.py3-none-any.whl
    Collecting google-cloud-dns&lt;0.24dev,&gt;=0.23.0 (from google-cloud)
      Using cached google_cloud_dns-0.23.0-py2.py3-none-any.whl
    Collecting google-cloud-monitoring&lt;0.24dev,&gt;=0.23.0 (from google-cloud)
      Using cached google_cloud_monitoring-0.23.0-py2.py3-none-any.whl
    Collecting google-cloud-error-reporting&lt;0.24dev,&gt;=0.23.0 (from google-cloud)
      Using cached google_cloud_error_reporting-0.23.2-py2.py3-none-any.whl
    Collecting google-cloud-bigquery&lt;0.24dev,&gt;=0.23.0 (from google-cloud)
      Using cached google_cloud_bigquery-0.23.0-py2.py3-none-any.whl
    Collecting google-cloud-bigtable&lt;0.24dev,&gt;=0.23.0 (from google-cloud)
      Using cached google_cloud_bigtable-0.23.1-py2.py3-none-any.whl
    Collecting google-cloud-runtimeconfig&lt;0.24dev,&gt;=0.23.0 (from google-cloud)
      Using cached google_cloud_runtimeconfig-0.23.0-py2.py3-none-any.whl
    Collecting google-cloud-pubsub&lt;0.24dev,&gt;=0.23.0 (from google-cloud)
      Using cached google_cloud_pubsub-0.23.0-py2.py3-none-any.whl
    Collecting google-cloud-datastore&lt;0.24dev,&gt;=0.23.0 (from google-cloud)
      Using cached google_cloud_datastore-0.23.0-py2.py3-none-any.whl
    Collecting google-cloud-core&lt;0.24dev,&gt;=0.23.1 (from google-cloud)
      Using cached google_cloud_core-0.23.1-py2.py3-none-any.whl
    Collecting google-cloud-resource-manager&lt;0.24dev,&gt;=0.23.0 (from google-cloud)
      Using cached google_cloud_resource_manager-0.23.0-py2.py3-none-any.whl
    Collecting google-cloud-vision&lt;0.24dev,&gt;=0.23.0 (from google-cloud)
      Using cached google_cloud_vision-0.23.3-py2.py3-none-any.whl
    Collecting google-cloud-translate&lt;0.24dev,&gt;=0.23.0 (from google-cloud)
      Using cached google_cloud_translate-0.23.0-py2.py3-none-any.whl
    Collecting google-cloud-logging&lt;0.24dev,&gt;=0.23.0 (from google-cloud)
      Using cached google_cloud_logging-0.23.1-py2.py3-none-any.whl
    Collecting google-cloud-language&lt;0.24dev,&gt;=0.23.0 (from google-cloud)
      Using cached google_cloud_language-0.23.1-py2.py3-none-any.whl
    Collecting google-cloud-speech&lt;0.24dev,&gt;=0.23.0 (from google-cloud)
      Using cached google_cloud_speech-0.23.0-py2.py3-none-any.whl
    Collecting google-cloud-spanner&lt;0.24dev,&gt;=0.23.1 (from google-cloud)
      Using cached google_cloud_spanner-0.23.1-py2.py3-none-any.whl
    Collecting google-cloud-storage&lt;0.24dev,&gt;=0.23.0 (from google-cloud)
      Using cached google_cloud_storage-0.23.1-py2.py3-none-any.whl
    Collecting gapic-google-cloud-error-reporting-v1beta1&lt;0.16dev,&gt;=0.15.0 (from google-cloud-error-reporting&lt;0.24dev,&gt;=0.23.0-&gt;google-cloud)
      Using cached gapic-google-cloud-error-reporting-v1beta1-0.15.3.tar.gz
        Complete output from command python setup.py egg_info:
        Traceback (most recent call last):
          File ""&lt;string&gt;"", line 1, in &lt;module&gt;
          File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
          File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
          File ""&lt;frozen importlib._bootstrap&gt;"", line 664, in _load_unlocked
          File ""&lt;frozen importlib._bootstrap&gt;"", line 634, in _load_backward_compatible
          File ""c:\anaconda3\lib\site-packages\setuptools-23.0.0-py3.5.egg\setuptools\__init__.py"", line 11, in &lt;module&gt;
          File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
          File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
          File ""&lt;frozen importlib._bootstrap&gt;"", line 664, in _load_unlocked
          File ""&lt;frozen importlib._bootstrap&gt;"", line 634, in _load_backward_compatible
          File ""c:\anaconda3\lib\site-packages\setuptools-23.0.0-py3.5.egg\setuptools\extern\__init__.py"", line 1, in &lt;module&gt;
          File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
          File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
          File ""&lt;frozen importlib._bootstrap&gt;"", line 664, in _load_unlocked
          File ""&lt;frozen importlib._bootstrap&gt;"", line 634, in _load_backward_compatible
          File ""c:\anaconda3\lib\site-packages\setuptools-23.0.0-py3.5.egg\pkg_resources\__init__.py"", line 2927, in &lt;module&gt;
          File ""c:\anaconda3\lib\site-packages\setuptools-23.0.0-py3.5.egg\pkg_resources\__init__.py"", line 2913, in _call_aside
          File ""c:\anaconda3\lib\site-packages\setuptools-23.0.0-py3.5.egg\pkg_resources\__init__.py"", line 2952, in _initialize_master_working_set
          File ""c:\anaconda3\lib\site-packages\setuptools-23.0.0-py3.5.egg\pkg_resources\__init__.py"", line 956, in subscribe
          File ""c:\anaconda3\lib\site-packages\setuptools-23.0.0-py3.5.egg\pkg_resources\__init__.py"", line 2952, in &lt;lambda&gt;
          File ""c:\anaconda3\lib\site-packages\setuptools-23.0.0-py3.5.egg\pkg_resources\__init__.py"", line 2515, in activate
          File ""c:\anaconda3\lib\site-packages\setuptools-23.0.0-py3.5.egg\pkg_resources\__init__.py"", line 2097, in declare_namespace
          File ""c:\anaconda3\lib\site-packages\setuptools-23.0.0-py3.5.egg\pkg_resources\__init__.py"", line 2047, in _handle_ns
          File ""c:\anaconda3\lib\site-packages\setuptools-23.0.0-py3.5.egg\pkg_resources\__init__.py"", line 2066, in _rebuild_mod_path
        AttributeError: '_NamespacePath' object has no attribute 'sort'

        ----------------------------------------
    Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\OFIRAR~1\AppData\Local\Temp\pip-build-j5a_32a2\gapic-google-cloud-error-reporting-v1beta1\
</code></pre>

<p>what could cause this? I installed other packages without problems.</p>",43124857,1,0,,2017-03-30 17:34:59.677 UTC,,2017-03-30 17:41:32.173 UTC,,,,,4809113,1,1,python|pip,1354
Android vision - Face detector dependencies are not yet available,37327891,Android vision - Face detector dependencies are not yet available,"<p>I am trying to test the google mobile vision api, for face detection, so I started with the demos from <a href=""https://github.com/googlesamples/android-vision/tree/master/visionSamples"" rel=""nofollow noreferrer"">GitHub mobile vision</a>. I tried both apps, FaceTracker and photo-demo, and the same issue rises with downloading the native face detector library. </p>

<p>For Nexus 5x, Galaxy S6 Edge not working, Galaxy S4, Galaxy Alpha is working.</p>

<p>What I see in the logs, when not working is:</p>

<p>For the app:</p>

<pre><code>05-19 17:52:07.301 14909-14909/com.google.android.gms.samples.vision.face.photo W/System: ClassLoader referenced unknown path: /data/app/com.google.android.gms.samples.vision.face.photo-1/lib/arm64
05-19 17:52:07.725 14909-14909/com.google.android.gms.samples.vision.face.photo W/System: ClassLoader referenced unknown path: /data/app/com.google.android.gms.samples.vision.face.photo-1/lib/arm64
05-19 17:52:08.043 14909-14909/com.google.android.gms.samples.vision.face.photo D/ChimeraCfgMgr: Reading stored module config
05-19 17:52:08.068 14909-14909/com.google.android.gms.samples.vision.face.photo W/System: ClassLoader referenced unknown path: /data/user/0/com.google.android.gms/app_chimera/m/00000001/n/arm64-v8a
05-19 17:52:08.077 14909-14909/com.google.android.gms.samples.vision.face.photo D/ChimeraFileApk: Primary ABI of requesting process is arm64-v8a
05-19 17:52:08.079 14909-14909/com.google.android.gms.samples.vision.face.photo D/ChimeraFileApk: Classloading successful. Optimized code found.
05-19 17:52:08.105 14909-14909/com.google.android.gms.samples.vision.face.photo I/FaceDetectorCreatorImpl: Requesting download for vision face detector
05-19 17:52:08.107 14909-14909/com.google.android.gms.samples.vision.face.photo W/FaceDetectorHandle: Native face detector not yet available.  Reverting to no-op detection.
05-19 17:52:08.253 14909-14909/com.google.android.gms.samples.vision.face.photo W/PhotoViewerActivity: Face detector dependencies are not yet available.
05-19 17:52:08.263 14909-14941/com.google.android.gms.samples.vision.face.photo D/OpenGLRenderer: Use EGL_SWAP_BEHAVIOR_PRESERVED: true
05-19 17:52:08.312 14909-14941/com.google.android.gms.samples.vision.face.photo I/Adreno: QUALCOMM build                   : 63c06b2, I8366cd0437
                                                                                          Build Date                       : 12/06/15
                                                                                          OpenGL ES Shader Compiler Version: XE031.05.13.02
                                                                                          Local Branch                     : mybranch17112971
                                                                                          Remote Branch                    : quic/LA.BF64.1.2.9_v2
                                                                                          Remote Branch                    : NONE
                                                                                          Reconstruct Branch               : NOTHING
05-19 17:52:08.317 14909-14941/com.google.android.gms.samples.vision.face.photo I/OpenGLRenderer: Initialized EGL, version 1.4
</code></pre>

<p>After doing some digging in the logs, by applying the vision filter for tags I see:</p>

<pre><code>05-19 17:52:08.124 10421-10526/? I/Vision: Attempting to open download_details.json in com.google.android.gms.vision
05-19 17:52:08.125 10421-10526/? W/Vision: Failed to open download_details.json in com.google.android.gms.vision: android.content.pm.PackageManager$NameNotFoundException: com.google.android.gms.vision
05-19 17:52:08.126 10421-10526/? I/Vision: Attempting to open download_details.json in com.google.android.gms.policy_ccocr_vision
05-19 17:52:08.127 10421-10526/? W/Vision: Failed to open download_details.json in com.google.android.gms.policy_ccocr_vision: android.content.pm.PackageManager$NameNotFoundException: com.google.android.gms.policy_ccocr_vision
05-19 17:52:08.127 10421-10526/? W/Vision: Reading download details from hard-coded Java
05-19 17:52:08.443 10421-10526/? E/Vision: Download Failed: Status{statusCode=Download errored: The download was configured incorrectly., resolution=null}
</code></pre>

<p>Similar questions:</p>

<ol>
<li><a href=""https://stackoverflow.com/questions/32099530/google-vision-barcode-library-not-found"">Google Vision barcode library not found</a></li>
<li><a href=""https://stackoverflow.com/questions/32062803/facedetectorhandle-native-face-detector-not-yet-available-reverting-to-no-op-d"">FaceDetectorHandle﹕ Native face detector not yet available. Reverting to no-op detection</a></li>
</ol>

<p>Tried the solution with freeing up space, clearing cache, no result.</p>

<p>What can I do next ? Is it possible to manually download the libraries ? Does anyone have a clear indication why is the library not downloading ?</p>

<p><strong><em>EDIT</em></strong></p>

<p>So it looks like there is an issue with the Mobile Vision api <a href=""https://github.com/googlesamples/android-vision/issues/98"" rel=""nofollow noreferrer"">https://github.com/googlesamples/android-vision/issues/98</a>. Hope the developers come with a solution soon.</p>

<p><strong><em>EDIT</em></strong></p>

<p>The issue has been resolved by updating the play service library.</p>",,2,6,,2016-05-19 15:32:50.097 UTC,,2018-05-04 17:42:16.637 UTC,2017-05-23 12:02:05.500 UTC,,-1,,1532642,1,2,android|google-play-services|google-vision,4456
Google Vision OCR Multiple Text Detection,53613704,Google Vision OCR Multiple Text Detection,"<p>I'm triying to multiple language text detection with google cloud vision. But I have a problem. 
If I send the request text detection api endpoint this url;</p>

<pre><code>https://vision.googleapis.com/v1/images:annotate?key=XxxXX
</code></pre>

<p>and this body;</p>

<pre><code>{
  ""requests"": [
    {
      ""image"": {
        ""source"": {
          ""imageUri"": ""image_url""
        }
      },
      ""features"": [
        {
          ""type"": ""DOCUMENT_TEXT_DETECTION"",
          ""maxResults"": 1
        }
      ],
      ""ImageContext"": {
        ""languageHints"": [
          ""tr"", ""en""
        ]
      }
    }
  ]
}
</code></pre>

<p>I'm getting the this error code;</p>

<pre><code>{
""error"": {
    ""code"": 400,
    ""message"": ""Invalid JSON payload received. Unknown name \""image_context\"" at 'requests[0]': Cannot find field."",
    ""status"": ""INVALID_ARGUMENT"",
    ""details"": [
        {
            ""@type"": ""type.googleapis.com/google.rpc.BadRequest"",
            ""fieldViolations"": [
                {
                    ""field"": ""requests[0]"",
                    ""description"": ""Invalid JSON payload received. Unknown name \""image_context\"" at 'requests[0]': Cannot find field.""
                }
            ]
        }
    ]
}
</code></pre>

<p>}</p>

<p>What is a problem?</p>",53617976,1,0,,2018-12-04 13:07:29.070 UTC,,2018-12-04 16:58:19.910 UTC,,,,,8139146,1,2,google-cloud-platform|ocr|google-cloud-vision,96
Get label percentages from Google Vision API,45917756,Get label percentages from Google Vision API,"<p>I would like to use the Google Vision API for label detection. For this I am using a .NET library. This is my code:</p>

<pre><code>        var client = ImageAnnotatorClient.Create();
        // Load the image file into memory
        var image = Image.FromFile(""trui3.jpg"");
        // Performs label detection on the image file
        var response = client.DetectLabels(image);
        foreach (var annotation in response)
        {
            if (annotation.Description != null)
                Console.WriteLine(annotation.Description);
        }
        Console.ReadKey();
</code></pre>

<p>It works very well. It displays all the labels. But on the Google <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">website</a> it also displays the percentages of the labels. See the image for an example.</p>

<p>How can I achieve this by using the .NET library?</p>

<p><a href=""https://i.stack.imgur.com/5Ao72.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5Ao72.jpg"" alt=""Vision API from Google website.""></a></p>",45918763,1,0,,2017-08-28 11:19:04.167 UTC,,2017-08-28 12:15:26.230 UTC,,,,user7273169,,1,0,c#|google-cloud-vision,531
VNDetectFaceRectanglesRequest always returns 0 for yaw/pitch/roll,55043291,VNDetectFaceRectanglesRequest always returns 0 for yaw/pitch/roll,"<p>I'm trying to get the pitch / yaw / roll of a face in an image using the Vision framework but always get 0 for all values. Images should be very easy to process (mostly forward looking portraits).</p>

<p>I've successfully got these values by using Amazon Rekognition on them, so the images themselves aren't the issue. (I need to do a batch of about 70,000 so using rekogniton for them all will get expensive and slow.)</p>

<p>This is the request code:</p>

<pre><code>let faceLandmarksRequest = VNDetectFaceRectanglesRequest(completionHandler: handleRectangles)

let requestHandler = VNImageRequestHandler(cgImage: cgImage!, orientation: CGImagePropertyOrientation.right ,options: [:])

do {
    try requestHandler.perform([faceLandmarksRequest])
} catch {
    print(error)
}
</code></pre>

<p>And here's the handler code:</p>

<pre><code>func handleRectangles(request: VNRequest, errror: Error?) {

    guard let observations = request.results as? [VNFaceObservation] else {
        fatalError(""unexpected result type!"")
    }

    for face in observations {

        print(""\(face.yaw))"") // always zero

    }
}
</code></pre>

<p>Any help appreciated :)</p>",,0,5,,2019-03-07 12:01:14.040 UTC,,2019-03-07 12:01:14.040 UTC,,,,,594898,1,1,swift|swift4,54
AWS : Running Rekognition on DeepLens device,53744481,AWS : Running Rekognition on DeepLens device,"<p>I am creating a DeepLens project to recognise people, when one of select group of people are scanned by the camera.</p>

<p>The project uses a lambda, which processes the images and triggers the 'rekognition' aws api.</p>

<p>On AWS lambda console ( which has 1.8.9 boto version ), I get following issue when I try to call an AWS python API:</p>

<p>Note : <em>img_str</em> is a byte array</p>

<pre><code>img_str = cv2.imencode('.jpg', frame)[1].tostring()
image = { 'Bytes': img_str }
response = rekognition.search_faces_by_image(CollectionId = 'TestingCollection', Image = { ""Bytes"" : image } )
</code></pre>

<p><strong>First error</strong> : sendall() argument 1 must be string or buffer, not dict</p>

<p><strong>Reason in my understanding</strong> : { ""Bytes"" : image } is a Json and NOT a string</p>

<p><strong>My Solution</strong> : Make the json a string ( not sure whether I can concatenate img_str ( a byte array )</p>

<pre><code>image = '{ ""Bytes"" :' + img_str + '}'
response = rekognition.search_faces_by_image(CollectionId = 'TestingCollection', Image = { ""Bytes"" : image } )
</code></pre>

<p><strong>Now error</strong> : Error in face detection lambda: 'ascii' codec can't decode byte 0xff in position 52: ordinal not in range(128)</p>

<p><strong>Question</strong>
How do I concatenate a byte array (img_str) with strings without losing the array ?</p>

<p>Can i convert <em>image</em> variable to string WITHOUT <em>getting the can't decode byte 0xff</em> exception ? or</p>

<p>Can we do something else to overcome this issue ?</p>

<p>Thanks in advance guys !!</p>",,1,0,,2018-12-12 13:47:40.007 UTC,,2019-01-27 17:55:42.897 UTC,,,,,2555384,1,2,python|aws-lambda|boto3|amazon-rekognition|amazon-deeplens,118
Google Vision API and language detection,50785134,Google Vision API and language detection,"<p>I am trying to perform OCR on images with some regional language which are supported by Google Vision API.  However, I am not able to specify multiple languages to be extracted from the image like en ----english, hi------hindi.  Below given is my code:</p>

<pre><code>import os
import sys
import json
import time
import base64
import urllib
import urllib2

def main():
if len(sys.argv) != 2:
   print 'Usage: python {} [api-key]'.format(sys.argv[0])
   return
to_check = ('inputs', 'processed', 'outputs')
if any([not os.path.exists(x) for x in to_check]):
  print 'The script expects the following folders to exist: {}'.format(', '.join(to_check))

answer = None

while answer is None or answer.lower() not in ['y', 'n']:
  answer = raw_input('Do you want the script to create them? [yn] ').lower()

if answer == 'n':
  return
for folder in to_check:
  if not os.path.exists(folder):
    os.makedirs(folder)

print 'Using API key: {}\n'.format(sys.argv[1])

before = []

while True:
  files = os.listdir('inputs')
  added = [f for f in files if f not in before]
  removed = [f for f in before if f not in files]

  print 'Added:', added
  print 'Removed:', removed
  print 'Processing new files..'

  for file in added:
          parts = os.path.splitext(os.path.basename(file))
          text_path = os.path.join('outputs', '.'.join(parts[:-1]) + 
          '.txt')
          processed_path = os.path.join('processed', 
          os.path.basename(file))

if os.path.exists(text_path):
    print('Output file already exists, just moving image.')
    os.rename(os.path.join('inputs', file), processed_path)
  else:
    process(os.path.join('inputs', file), text_path)
    os.rename(os.path.join('inputs', file), processed_path)

print('---')

before = files
time.sleep(5)

def process(fname, output):
  print('Processing {}'.format(fname))

url = 'https://vision.googleapis.com/v1/images:annotate?' + 
urllib.urlencode({'key': sys.argv[1]
})

payload = json.dumps(get_payload([fname], 5)).encode('utf-8')

request = urllib2.Request(url)
request.add_header('Content-Type', 'application/json')
request.add_header('Content-Length', len(payload))

try:
  response = json.loads(urllib2.urlopen(request, payload).read())
except urllib2.HTTPError as e:
  print('Wrong api key. Please check it.')
  print(e.read())
  sys.exit(1)
  return

text_response = response['responses'][0]

text = text_response['fullTextAnnotation']['text']

with open(output, 'wb+') as file:

  file.write(text.encode('utf-8'))

print('Done! Text written to {}'.format(output))

def get_payload(paths, max_results):
   requests = []
   for path in paths:
     with open(path, 'rb') as file:
       content = {'content': 
       base64.b64encode(file.read()).decode('utf-8')}
     requests.append({
       'image': content,
  'features': [{'type': 'TEXT_DETECTION', 'maxResults': max_results}]
})
return {'requests': requests}

if __name__ == '__main__':
  main()
</code></pre>",,1,2,,2018-06-10 15:18:35.923 UTC,,2018-06-11 13:04:20.213 UTC,2018-06-10 15:21:29.803 UTC,,2326911,,4180988,1,0,python|google-vision,263
Boto3 and Python3: Can't convert 'bytes' object to str implicitly,49249692,Boto3 and Python3: Can't convert 'bytes' object to str implicitly,"<p>I am trying to use AWS Rekognition, detect_text API. I am using Boto3 along with Python 3.</p>

<p>Here is my relevant code:</p>

<pre><code>with open(file_path, 'rb') as file:
  data = file.read()

response = self._rekognition.detect_text(Image={'Bytes': data})
</code></pre>

<p>This code worked with Python2.7 but is failing with Python3. I am getting the following error:</p>

<pre><code>File ""..."", line 39, in extract_text
response = self._rekognition.detect_text(Image={'Bytes': data})
...
...
k_date = self._sign(('AWS4' + key).encode('utf-8'),
TypeError: Can't convert 'bytes' object to str implicitly
</code></pre>

<p>Any ideas what I need to change here.</p>",49249767,1,2,,2018-03-13 06:33:50.030 UTC,,2018-03-13 06:39:45.040 UTC,,,,,3371460,1,0,python|boto3|amazon-rekognition,860
Google Cloud Vision AutoML poor prediction performance,53671813,Google Cloud Vision AutoML poor prediction performance,"<p>I have trained a custom Google Cloud Vision model using AutoML. The purpose of this model is to classify a single label for a given image.</p>

<p>I have implemented a client to send HTTP prediction requests to their REST API. This works perfectly fine, however the time it takes to get a response is 13 seconds. This seems extremely slow and inefficient to me. I am sure that this is caused by Google, since I timed the method calls (uploading the raw image data could take some time, but using the same image on their pre-trained Cloud Vision network is a lot faster).</p>

<p>Did anyone else run into this problem and found a solution for this? Or is it better to just train my own model using Tensorflow/Pytorch with transfer leaning on e.g. Imagenet and build an API around that.</p>",,1,1,,2018-12-07 14:47:52.453 UTC,,2019-01-16 09:23:50.590 UTC,,,,,4530508,1,1,cloud|vision|automl,59
AWS Rekognition and s3 calling subfolders in Python Lambda,50258562,AWS Rekognition and s3 calling subfolders in Python Lambda,"<p>I'm having trouble figuring out how to access a certain folder within a bucket in s3 using Python</p>

<p>Let's say I'm trying to access this folder in the bucket which contains a bunch of images that I want to run rekognition on:
""myBucket/subfolder/images/""</p>

<p>In /images/ folder there are:  </p>

<pre><code>one.jpg  
two.jpg  
three.jpg  
four.jpg  
</code></pre>

<p>I want to run rekognition's detect_labels on this folder. However, I can't seem to access this folder but if I change the bucket_name to just the root folder (""myBucket""/), then I can access just that folder.</p>

<pre><code>bucket_name = ""myBucket/subfolder/images/""  
rekognition = boto3.client('rekognition')  
s3 = boto3.resource('s3')  
bucket = s3.Bucket(name=bucket_name)  
</code></pre>",50267190,3,0,,2018-05-09 17:01:16.550 UTC,,2019-02-28 01:09:59.570 UTC,2018-05-10 06:52:56.747 UTC,,174777,,2624768,1,1,python|amazon-web-services|amazon-s3|aws-lambda|aws-cli,244
Azure Detection with Custom Vision and OCR text detection,54365930,Azure Detection with Custom Vision and OCR text detection,"<p>i'm studying the Azure Custom Vision service for object detection, but I would like also to extrapolate text information within a tagged image zone.
Is it possible with Custom Vision?
If not, is it in the service roadmap?
Thank you</p>",,0,2,,2019-01-25 13:06:16.373 UTC,,2019-01-25 13:06:16.373 UTC,,,,,1788990,1,0,ocr|microsoft-cognitive,60
"SVG: using path element to create an area with ""holes""",44446544,"SVG: using path element to create an area with ""holes""","<p>I'm trying to use SVG path element to define an area with ""holes"". I would like to use these areas for highlighting of some words of text in an image. </p>

<p><a href=""https://i.stack.imgur.com/9OGuF.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9OGuF.jpg"" alt=""image with text""></a></p>

<p><a href=""https://i.stack.imgur.com/VWUgC.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VWUgC.jpg"" alt=""image with text and word highlighting""></a></p>

<p>My goal is to present results from text extraction from an image using the OCR (<a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">Google Cloud Vision API</a>). Results will be displayed in my Angular application in form of table with words from extracted text with ability to highlight/show selected word in an image from the user.</p>

<p>Using the OCR I got bounding box for each word of extracted text. </p>

<p>This is how I solved highlighting:</p>

<pre><code>&lt;svg height=""300"" width=""300""&gt;
  &lt;image xlink:href=""http://www.downloadclipart.net/thumb/17283-ok-icon-vector-thumb.png"" x=""0"" y=""0"" height=""300"" width=""300""/&gt;
  &lt;path fill=""#ddd"" opacity=""0.7"" fill-rule=""evenodd"" d=""M0 0 H300 V300 H0 Z M10 10 H100 V100 H10 Z"" style=""visibility:visible""/&gt;
  &lt;path fill=""#ddd"" opacity=""0.7"" fill-rule=""evenodd"" d=""M0 0 H300 V300 H0 Z M150 150 H200 V200 H150 Z"" style=""visibility:hidden""/&gt;
&lt;/svg&gt;
</code></pre>

<p>Everything works fine. I have problem only with overlapping bounding boxes. I have an utility that converts image width and height and bounding boxes to the ""d"" attribute of path element.</p>

<pre><code>public static String example(int width, int height, List&lt;Box&gt; boxes) {

    StringBuilder sb = new StringBuilder(""M0 0"")
            .append("" H"").append(width)
            .append("" V"").append(height)
            .append("" H0 Z"");

    boxes.forEach((box) -&gt; {
        sb.append("" M"").append(box.getLeft())
                .append("" "").append(box.getTop())
                .append("" H"").append(box.getRight())
                .append("" V"").append(box.getBottom())
                .append("" H"").append(box.getLeft())
                .append("" Z"");
    });

    return sb.toString();
}
</code></pre>

<p>But if boxes overlap, I got result like this</p>

<pre><code>&lt;svg height=""200"" width=""200""&gt;
  &lt;path fill=""red"" fill-rule=""evenodd"" d=""M0 0 H200 V200 H0 Z M40 40 H100 V100 H40 Z M10 10 H50 V50 H10 Z"" /&gt;
&lt;/svg&gt;
</code></pre>

<p><a href=""https://i.stack.imgur.com/M2wsh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M2wsh.png"" alt=""current result""></a></p>

<p>and I want this</p>

<p><a href=""https://i.stack.imgur.com/ZsltN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZsltN.png"" alt=""iresult i want""></a></p>

<p>My question is if there is a better way how to define SVG path element to get result I want.</p>",44450708,1,4,,2017-06-08 22:28:05.797 UTC,,2017-06-09 06:25:48.560 UTC,2017-06-08 22:35:27.633 UTC,,2598453,,2598453,1,1,svg,988
AWS Rekognition Detect Labels Error in android studio,44258407,AWS Rekognition Detect Labels Error in android studio,"<p>Following error message is shown when I implement these codes...</p>

<pre><code>CognitoCachingCredentialsProvider credentialsProvider = new CognitoCachingCredentialsProvider(
            getApplicationContext.getApplicationContext(),
            ""us-west-2:..."",// Identity Pool ID
            Regions.US_WEST_2 // Region
    );

    AmazonRekognitionClient rekognitionClient = new AmazonRekognitionClient(credentialsProvider); &lt;--Error Here

DetectLabelsRequest request = new DetectLabelsRequest()
            .withImage(new Image()
                    .withBytes(imageBytes))
            .withMaxLabels(10)
            .withMinConfidence(77F);
</code></pre>

<p>Console Messsage: AmazonWebServiceClient: {cognito-identity, us-west-2} was not found in region metadata, trying to construct an endpoint using the standard pattern for this region: 'cognito-identity.us-west-2.amazonaws.com'.<br>
CognitoCachingCredentialsProvider: Loading credentials from SharedPreferences
CognitoCachingCredentialsProvider: No valid credentials found in SharedPreferences</p>

<p>Error Message:</p>

<pre><code>java.lang.NoSuchMethodError: No static method isInRegionOptimizedModeEnabled()Z in class Lcom/amazonaws/SDKGlobalConfiguration; or its super classes (declaration of 'com.amazonaws.SDKGlobalConfiguration' appears in /data/app/com.example.name.app-1/base.apk:classes11.dex)
                                                                              at com.amazonaws.ClientConfigurationFactory.getConfig(ClientConfigurationFactory.java:35)
                                                                              at com.amazonaws.services.rekognition.AmazonRekognitionClient.&lt;init&gt;(AmazonRekognitionClient.java:210)
                                                                              at com.example.name.app.DetectLabelsExampleImageBytes.main(DetectLabelsExampleImageBytes.java:67)
                                                                              at com.example.name.app.PhotoActivity.detectHash(PhotoActivity.java:186)
                                                                              at com.example.name.app.PhotoActivity.onActivityResult(PhotoActivity.java:158)
                                                                              at android.app.Activity.dispatchActivityResult(Activity.java:6470)
                                                                              at android.app.ActivityThread.deliverResults(ActivityThread.java:3716)
                                                                              at android.app.ActivityThread.handleSendResult(ActivityThread.java:3763)
                                                                              at android.app.ActivityThread.-wrap16(ActivityThread.java)
                                                                              at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1403)
                                                                              at android.os.Handler.dispatchMessage(Handler.java:102)
                                                                              at android.os.Looper.loop(Looper.java:148)
                                                                              at android.app.ActivityThread.main(ActivityThread.java:5443)
                                                                              at java.lang.reflect.Method.invoke(Native Method)
                                                                              at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:728)
                                                                              at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:618)
</code></pre>",,0,2,,2017-05-30 09:26:37.800 UTC,,2017-06-03 14:38:28.057 UTC,2017-06-03 14:38:28.057 UTC,,5443681,,5443681,1,1,amazon-web-services|android-studio|amazon-rekognition,294
Parse responses (batch) from Cloud Vision API in Python,48857882,Parse responses (batch) from Cloud Vision API in Python,"<p>I would like to pass a large number of requests to the Google Cloud Vision API for label detection - in Python. It all goes well. My problem is: how to read results?</p>

<p>Here's an example of a request: </p>

<pre><code>{'image': {'source': {'image_uri': 'my obj on gs'}},
 'features': [{'type': vision.enums.Feature.Type.LABEL_DETECTION}],}
</code></pre>

<p>and the code:</p>

<pre><code>from google.cloud import vision

client = vision.ImageAnnotatorClient()
responses = client.batch_annotate_images(requests)
</code></pre>

<p>I send a list of requests with <a href=""http://google-cloud-python.readthedocs.io/en/latest/vision/gapic/v1/api.html"" rel=""nofollow noreferrer"">batch_annotate_images</a> and get <a href=""https://developers.google.com/resources/api-libraries/documentation/vision/v1/java/latest/com/google/api/services/vision/v1/model/BatchAnnotateImagesResponse.html"" rel=""nofollow noreferrer"">google.cloud.vision_v1.types.BatchAnnotateImagesResponse</a>. I can't iterate through the response, there is also no method the interface that would be an obvious candidate. Here's what's available:</p>

<pre><code>['ByteSize',
 'Clear',
 'ClearExtension',
 'ClearField',
 'CopyFrom',
 'DESCRIPTOR',
 'DiscardUnknownFields',
 'Extensions',
 'FindInitializationErrors',
 'FromString',
 'HasExtension',
 'HasField',
 'IsInitialized',
 'ListFields',
 'MergeFrom',
 'MergeFromString',
 'ParseFromString',
 'RESPONSES_FIELD_NUMBER',
 'RegisterExtension',
 'SerializePartialToString',
 'SerializeToString',
 'SetInParent',
 'WhichOneof]
</code></pre>

<p>The only thing I could come up with is:</p>

<pre><code>import json
from google.protobuf.json_format import MessageToJson

json_result = MessageToJson(responses)
dict_result = json.loads(json_result)
</code></pre>

<p>It does the trick, but seems to be rather convoluted when compared to labelling a <a href=""https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/vision/cloud-client/detect/detect.py#L123"" rel=""nofollow noreferrer"">single photo</a>. </p>

<p>Is there a better way?</p>

<p>If not, I assume order of my requests passed in a list to <code>batch_annotate_images</code> matches exactly the response I get? </p>",,1,0,,2018-02-18 23:51:06.377 UTC,,2018-03-12 16:39:36.100 UTC,,,,,1397946,1,0,python|protocol-buffers|google-cloud-vision,277
Can't import google.cloud.vision,51894464,Can't import google.cloud.vision,"<p>I'm using Anaconda, and I'm trying to use google cloud vision, but I cannot import google cloud vision. I can import google cloud, but it throws an error below. </p>

<pre><code>from google.cloud import vision
ImportError: cannot import name 'vision'
</code></pre>

<p>What module should I import with anaconda? 
(I've already imported <code>google-api-core</code>, <code>google-auth</code>, <code>google-cloud-bigquery</code>, <code>google-cloud-core</code>, <code>google-cloud-sdk</code>, <code>google-cloud-storage</code>, <code>google-resumable-media</code>, <code>google-resumable-media</code>, <code>googleapis-common-protos</code>)</p>

<p>Could anyone solve this? Thanks in advance.</p>",51894617,1,0,,2018-08-17 11:29:35.390 UTC,,2018-08-17 11:38:32.053 UTC,,,,,10233761,1,1,python|google-cloud-platform|anaconda|google-cloud-vision,1657
Is it possible for real time access to mobile device camera?,45376533,Is it possible for real time access to mobile device camera?,"<p>So I'm wondering if it's possible for a website to be able A) access the mobile device's camera and B) have real time facial recognition capabilities? Essentially what Snapchat does, albeit much simpler, but in a web application opposed to a mobile application?</p>

<p>I already know the answer to (A), as found here: <a href=""https://davidwalsh.name/demo/iphone-camera.php"" rel=""nofollow noreferrer"">https://davidwalsh.name/demo/iphone-camera.php</a></p>

<p>And I even found an example that uses Amazon Rekognition, as found here: <a href=""https://hackernoon.com/building-a-face-recognition-web-app-in-under-an-hour-345aa91487c"" rel=""nofollow noreferrer"">https://hackernoon.com/building-a-face-recognition-web-app-in-under-an-hour-345aa91487c</a> </p>

<p>Only nuisance with the rekognition example I found was that it seems to take the picture <strong>AND THEN</strong> do the recognition, I'm looking more for something to do it while the camera is up (so you point the camera to someones face, and it does the magic there).</p>

<p><strong>Disclaimer</strong>: I am not asking anyone to do any work for me here. I know I'm not providing any code samples, and that's because I'm just in the research phase and wanted to see if anyone here has any input on what I'm trying to achieve. </p>

<p>Something tells me this may not be possible, from my google searches I didn't quite find anything that I'm looking for, but close. </p>",,0,7,,2017-07-28 14:52:02.777 UTC,,2017-07-28 14:52:02.777 UTC,,,,,6891201,1,0,javascript|mobile,37
Google VISION API read barcode in textview,46354536,Google VISION API read barcode in textview,"<p>I am trying to integrate Google Vision API in my code but having hard time converting value in to text view. Also the camera surface view doesn't get destroyed. </p>

<p>Manifest Meta-data -</p>

<pre><code>&lt;meta-data
        android:name=""com.google.android.gms.vision.DEPENDENCIES""
        android:value=""barcode"" /&gt;
</code></pre>

<p>Current Value readed by below code: </p>

<pre><code>com.google.android.gms.vision.barcode.Barcode@eeb8638
</code></pre>

<p>Expected Barcode Text : <code>047754732276</code></p>

<p>Code:</p>

<pre><code>barcodeDetector =
                new BarcodeDetector.Builder(transactionActivity)
                        .setBarcodeFormats(Barcode.ALL_FORMATS)
                        .build();
        cameraSource = new CameraSource.Builder(transactionActivity, barcodeDetector)
                .setAutoFocusEnabled(true)
                .setRequestedPreviewSize(1600, 1024)
                .build();

        cameraView.getHolder().addCallback(new SurfaceHolder.Callback() {
            @Override
            public void surfaceCreated(SurfaceHolder holder) {
                try {
                    //noinspection MissingPermission
                    if(ContextCompat.checkSelfPermission(transactionActivity, android.Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED){
                        ActivityCompat.requestPermissions(transactionActivity, new String[]{Manifest.permission.CAMERA}, PERMISSION_REQUEST);
                    }
                    cameraSource.start(cameraView.getHolder());
                } catch (IOException ex) {
                    ex.printStackTrace();
                }
            }

            @Override
            public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {
            }

            @Override
            public void surfaceDestroyed(SurfaceHolder holder) {
                cameraSource.stop();
            }
        });
        barcodeDetector.setProcessor(new Detector.Processor() {
            @Override
            public void release() {
            }

            @Override
            public void receiveDetections(Detector.Detections detections) {
                final SparseArray barcodes = detections.getDetectedItems();
                if (barcodes.size() != 0) {
                    imei.post(new Runnable() {
                        @Override
                        public void run() {
                            cameraSource.stop();
                            //Update barcode value to TextView
                            imei.setText(String.valueOf((barcodes.valueAt(0).displayValue)));
                        }
                    });
                }
            }
        });
</code></pre>",,1,2,,2017-09-21 23:13:48.090 UTC,,2017-10-01 23:12:10.183 UTC,2017-09-22 14:27:18.450 UTC,,2220092,,2220092,1,1,android|barcode-scanner|android-vision,870
Microsoft-Cognitive Request per second,49562890,Microsoft-Cognitive Request per second,"<p>Tenho uma questão sobre a limitação da Face API da Microsoft Azure, existe uma limitação no plano standard que deixa fazer apenas 10 requisições por segundo. Estamos com problema quanto a esse número por não atender a escalabilidade visto que queremos resultados sob demanda em um tempo aceitável, existe alguma possibilidade de ser feito um plano diferente do standard com mais requisições por segundo?</p>

<p>I have a question about the limitation of the Microsoft Azure Face API, there is a limitation in the standard plan that leaves only 10 requests per second. We have a problem with this number because it does not meet the scalability since we want results on demand in an acceptable time, is there any possibility of being made a different plan from the standard with more requisitions per second?</p>",,1,0,,2018-03-29 18:32:13.480 UTC,,2018-03-29 18:57:47.447 UTC,,,,,9571472,1,0,microsoft-cognitive,46
Deploy Azure Face API for IoT Edge,51556793,Deploy Azure Face API for IoT Edge,"<p>Is it possible to deploy Azure Face API trained model to IoT Edge like Custom Vision?</p>

<p>If it is, please answer me how to do that?</p>",54314747,2,2,,2018-07-27 11:14:12.977 UTC,,2019-01-22 18:56:12.870 UTC,,,,,9493188,1,0,microsoft-cognitive|azure-cognitive-services|face-api|azure-iot-edge,312
Google cloud: insufficient authentication scopes,50275116,Google cloud: insufficient authentication scopes,"<p>I am having difficulties sending requests to my spring boot application deployed in my Google Cloud Kubernetes cluster. My application receives a photo and sends it to the Google Vision API. I am using the provided client library (<a href=""https://cloud.google.com/vision/docs/libraries#client-libraries-install-java"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/libraries#client-libraries-install-java</a>) as explained here <a href=""https://cloud.google.com/vision/docs/auth"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/auth</a>:</p>

<blockquote>
  <p>If you're using a client library to call the Vision API, use Application Default Credentials (ADC). Services using ADC look for credentials within a GOOGLE_APPLICATION_CREDENTIALS environment variable. Unless you specifically wish to have ADC use other credentials (for example, user credentials), we recommend you set this environment variable to point to your service account key file.</p>
</blockquote>

<p>On my local machine everyting works fine, I have a docker container with an env. varialbe GOOGLE_APPLICATION_CREDENTIALS pointing to my service account key file. </p>

<p>I do not have this variable in my cluster. This is the response I am getting from my application in the Kubernetes cluster:</p>

<pre><code>{
    ""timestamp"": ""2018-05-10T14:07:27.652+0000"",
    ""status"": 500,
    ""error"": ""Internal Server Error"",
    ""message"": ""io.grpc.StatusRuntimeException: PERMISSION_DENIED: Request had insufficient authentication scopes."",
    ""path"": ""/image""
}
</code></pre>

<p>What I am doing wrong? Thx in advance!</p>",,3,0,,2018-05-10 14:20:37.780 UTC,1,2018-05-11 17:24:21.970 UTC,,,,,1725624,1,0,kubernetes|google-cloud-platform|google-vision,846
JavaScript face recognition API,18142659,JavaScript face recognition API,"<p>I would like to know if there are good face detection &amp; recognition APIs around for JavaScript (either client side or server side). I've read some of the StackOverflow related questions (like <a href=""https://stackoverflow.com/questions/7291065/any-library-for-face-recognition-in-javascript"">this one</a>), but either they are old or the answers do not satisfy me.</p>

<p>I'm searching for an API capable of detecting a face (e.g. bounding box around the face) and recognise it (e.g. telling me with some probability who is the person in the picture). For what concerns the database of faces, I will take care of it.</p>

<p>The only resource I know is <a href=""http://rekognition.com/"" rel=""nofollow noreferrer"">rekognition</a> and looks promising. I would like to have more options though.</p>

<p>Thanks a bunch!</p>",,1,0,,2013-08-09 08:17:48.077 UTC,0,2018-09-07 21:05:48.933 UTC,2017-05-23 11:43:51.717 UTC,,-1,,617461,1,1,javascript|api|face-detection|face-recognition,2312
How to read bytes from a image url (jpeg) and encode in base64 on Python 2.7?,43926563,How to read bytes from a image url (jpeg) and encode in base64 on Python 2.7?,"<p>I am using the following code:</p>

<pre><code>import urllib, cStringIO
from PIL import Image  
url='https://www.google.com/images/branding/googlelogo/1x/googlelogo_color_272x92dp.png'
file = cStringIO.StringIO(urllib.urlopen(url).read())
img = Image.open(file)
</code></pre>

<p>based on: <a href=""https://stackoverflow.com/questions/7391945/how-do-i-read-image-data-from-a-url-in-python"">How do I read image data from a URL in Python?</a></p>

<p>Now I need to base64 encode it for posting to Google Cloud Vision API. How to do that?</p>

<p>Or does the Google Cloud vision API work with image urls?</p>",43926648,1,0,,2017-05-11 22:27:17.937 UTC,,2018-05-18 11:50:08.963 UTC,2017-05-23 12:02:39.190 UTC,,-1,,376742,1,2,python|google-cloud-vision,1059
How do I extract the 'Roll' Data from this Kinesis shard using Python?,49471062,How do I extract the 'Roll' Data from this Kinesis shard using Python?,"<p>how am I able to extract just the ""roll"" from this list which I got from Amazon Kinesis/Rekognition using Python 2.7?</p>

<p>{u'FaceSearchResponse': [{u'DetectedFace': {u'BoundingBox': {u'Width': 0.10875, u'Top': 0.08555555, u'Left': 0.775, u'Height': 0.19333333}, u'Confidence': 99.82224, u'Pose': {u'Yaw': 39.53371, u'Roll': 10.791267, u'Pitch': -1.0082194}, u'Quality': {u'Sharpness': 99.93052, u'Brightness': 44.374504}, u'Landmarks': [{u'Y': 0.17006741, u'X': 0.81887186, u'Type': u'eyeLeft'}, {u'Y': 0.18348174, u'X': 0.8479081, u'Type': u'eyeRight'}, {u'Y': 0.21523575, u'X': 0.8444541, u'Type': u'nose'}, {u'Y': 0.2389706, u'X': 0.81935763, u'Type': u'mouthLeft'}, {u'Y': 0.2415149, u'X': 0.83268094, u'Type': u'mouthRight'}]}, u'MatchedFaces': []}], u'StreamProcessorInformation': {u'Status': u'RUNNING'}, u'InputInformation': {u'KinesisVideo': {u'ServerTimestamp': 1521934266.557, u'FrameOffsetInSeconds': 0.035999998450279236, u'StreamArn': u'arn:aws:kinesisvideo:us-east-1:086906171606:stream/AmazonRekognitionVS/1520802835146', u'FragmentNumber': u'91343852333181789275940108114159018792280348730', u'ProducerTimestamp': 1521934266.294}}}</p>

<pre><code>out = kinesis.get_records(shard_it,limit=2)
for o in out[""Records""]: 
a = json.loads(o[""Data""]) 
b = a[""FaceSearchResponse""] 
c = b[""DetectedFace""] 
print c
</code></pre>",,0,4,,2018-03-24 23:36:27.783 UTC,,2018-03-25 00:48:33.437 UTC,2018-03-25 00:48:33.437 UTC,,9546305,,9546305,1,0,python-2.7|amazon|amazon-kinesis|amazon-rekognition,62
AWS Rekognition - How to index multiple images belonging to the same face?,55940804,AWS Rekognition - How to index multiple images belonging to the same face?,"<p>This document - <a href=""https://docs.aws.amazon.com/rekognition/latest/dg/recommendations-facial-input-images.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/rekognition/latest/dg/recommendations-facial-input-images.html</a> recommends indexing 5 faces of a person straight-on.</p>

<p>But indexFaces takes 1 image at a time.</p>

<p>After indexing first image, when indexing the second image, how do I tell rekognition that it belongs to the same person?</p>

<p>How do I tell rekognition that these 5 images belong to the same person?</p>",,1,0,,2019-05-01 18:22:21.540 UTC,,2019-05-01 22:07:16.710 UTC,,,,,810583,1,1,amazon-web-services|amazon-rekognition,40
"Google Vision API text detection Python example uses project: ""google.com:cloudsdktool"" and not my own project",38048320,"Google Vision API text detection Python example uses project: ""google.com:cloudsdktool"" and not my own project","<p>I am working on the python example for Cloud Vision API from <a href=""https://github.com/GoogleCloudPlatform/cloud-vision/tree/master/python/text"" rel=""nofollow"">github repo</a>.</p>

<p>I have already setup the project and activated the service account with its key. I have also called the <code>gcloud auth</code> and entered my credentials.</p>

<p>Here is my code (as derived from the python example of Vision API text detection):</p>

<pre><code>import base64
import os
import re
import sys

from googleapiclient import discovery
from googleapiclient import errors
import nltk
from nltk.stem.snowball import EnglishStemmer
from oauth2client.client import GoogleCredentials
import redis

DISCOVERY_URL = 'https://{api}.googleapis.com/$discovery/rest?version={apiVersion}'  # noqa
BATCH_SIZE = 10


class VisionApi:
    """"""Construct and use the Google Vision API service.""""""

    def __init__(self, api_discovery_file='/home/saadq/Dev/Projects/TM-visual-search/credentials-key.json'):
        self.credentials = GoogleCredentials.get_application_default()
        print self.credentials.to_json()
        self.service = discovery.build(
            'vision', 'v1', credentials=self.credentials,
            discoveryServiceUrl=DISCOVERY_URL)
        print DISCOVERY_URL

    def detect_text(self, input_filenames, num_retries=3, max_results=6):
        """"""Uses the Vision API to detect text in the given file.
        """"""
        images = {}
        for filename in input_filenames:
            with open(filename, 'rb') as image_file:
                images[filename] = image_file.read()

        batch_request = []
        for filename in images:
            batch_request.append({
                'image': {
                    'content': base64.b64encode(
                            images[filename]).decode('UTF-8')
                },
                'features': [{
                    'type': 'TEXT_DETECTION',
                    'maxResults': max_results,
                }]
            })
        request = self.service.images().annotate(
            body={'requests': batch_request})

        try:
            responses = request.execute(num_retries=num_retries)
            if 'responses' not in responses:
                return {}
            text_response = {}
            for filename, response in zip(images, responses['responses']):
                if 'error' in response:
                    print(""API Error for %s: %s"" % (
                            filename,
                            response['error']['message']
                            if 'message' in response['error']
                            else ''))
                    continue
                if 'textAnnotations' in response:
                    text_response[filename] = response['textAnnotations']
                else:
                    text_response[filename] = []
            return text_response
        except errors.HttpError as e:
            print(""Http Error for %s: %s"" % (filename, e))
        except KeyError as e2:
            print(""Key error: %s"" % e2)



vision = VisionApi()
print vision.detect_text(['test_article.png'])
</code></pre>

<p>This is the error message I am getting:</p>

<pre><code>Http Error for test_article.png: &lt;HttpError 403 when requesting https://vision.googleapis.com/v1/images:annotate?alt=json returned ""Google Cloud Vision API has not been used in project google.com:cloudsdktool before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/vision.googleapis.com/overview?project=google.com:cloudsdktool then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.""&gt;
</code></pre>

<p>I want to be able to use my own project for the example and not the default (google.com:cloudsdktool).</p>",38077836,2,0,,2016-06-27 07:38:35.983 UTC,,2016-12-07 23:37:06.640 UTC,2016-06-27 15:52:58.230 UTC,,5231007,,5279545,1,1,python|google-cloud-platform|google-cloud-vision|google-cloud-console,1342
AWS Rekognition call index-faces on all images in an S3 bucket instead of one by one?,46287595,AWS Rekognition call index-faces on all images in an S3 bucket instead of one by one?,"<p>I am testing Image Recognition from was. So far good. What I am having problems with is indexing faces in the CLI. I can index one at the time, but, I would like to tell AWS to index all faces in a bucket. To index a face one at the time I call this:</p>

<pre><code>aws rekognition index-faces --image ""S3Object={Bucket=bname,Name=123.jpg}"" --collection-id ""myCollection"" --detection-attributes ""ALL"" --external-image-id ""myImgID""
</code></pre>

<p>How do I tell it to index all images in the ""name"" bucket? </p>

<pre><code>I tried this:
aws rekognition index-faces --image ""S3Object={Bucket=bname}"" --collection-id ""myCollection"" --detection-attributes ""ALL"" --external-image-id ""myImgID""
</code></pre>

<p>no luck.</p>",46305082,2,0,,2017-09-18 20:20:56.527 UTC,,2018-03-26 10:31:42.003 UTC,,,,,728246,1,0,amazon-s3|amazon-rekognition,883
Text extraction - line-by-line,42391009,Text extraction - line-by-line,"<p>I am using Google Vision API, primarily to extract texts. I works fine, but for specific cases where I would need the API to scan the enter line, spits out the text before moving to the next line. However, it appears that the API is using some kind of logic that makes it scan top to bottom on the left side and moving to right side and doing a top to bottom scan. I would have liked if the API read left-to-right, move down and so on.</p>

<p>For example, consider the image:</p>

<p><a href=""https://i.stack.imgur.com/wk3t1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wk3t1.png"" alt=""enter image description here""></a></p>

<p>The API returns the text like this:</p>

<pre><code>“ Name DOB Gender: Lives In John Doe 01-Jan-1970 LA ”
</code></pre>

<p>Whereas, I would have expected something like this:</p>

<pre><code>“ Name: John Doe DOB: 01-Jan-1970 Gender: M Lives In: LA ”
</code></pre>

<p>I suppose there is a way to define the block size or margin setting (?) to read the image/scan line by line? </p>

<p>Thanks for your help.
Alex</p>",,3,0,,2017-02-22 12:06:41.330 UTC,0,2019-01-26 16:00:02.837 UTC,2018-08-09 08:44:19.403 UTC,,51280,,7604576,1,5,google-vision,2297
Why am I not able to run this curl command?,54077087,Why am I not able to run this curl command?,"<p>I tried to create a custom model for my IBM Watson Visual Recognition API, by following the IBM's docs. I'm stuck at this point.</p>",54084382,1,2,,2019-01-07 15:18:49.243 UTC,,2019-01-23 13:53:59.213 UTC,2019-01-23 13:53:59.213 UTC,,10115120,,10115120,1,0,command-line|ibm-cloud|ibm-watson,46
About the license for commercial use,52809726,About the license for commercial use,"<p>I would like to use Google Cloud Vision service in commercial purpose.</p>

<p>But I could not find any clear description about whether it is permitted to use that service for the commercial use without following any license condition.</p>

<p>Briefly my question is 2 points below.</p>

<ol>
<li>Is there any license which should be followed in order to commercial use?</li>
<li>the condition for commercial use</li>
</ol>

<p>If there is someone who have answer about them,
it would be highly appreciated that you could give me answers. </p>",,1,0,,2018-10-15 04:22:28.937 UTC,,2018-11-05 17:18:23.110 UTC,2018-10-15 04:24:10.843 UTC,,107625,,10505241,1,-1,google-cloud-vision,47
SurfaceView - Timestamp seems implausible relative to expectedPresent,52039178,SurfaceView - Timestamp seems implausible relative to expectedPresent,"<p>For my barcode scanner app, I am leveraging parts of the <a href=""https://github.com/googlesamples/android-vision/blob/master/visionSamples/FaceTracker/app/src/main/java/com/google/android/gms/samples/vision/face/facetracker/ui/camera/CameraSourcePreview.java"" rel=""nofollow noreferrer"">CameraSourcePreview class</a> found in the Google Vision sample code.</p>

<p>I then leverage this class in my Fragment using this XML snippet:</p>

<pre><code>&lt;com.mattdonders.android.barcodescanner.barcode.CameraSourcePreview
    android:id=""@+id/cameraSourcePreview""
    android:layout_width=""match_parent""
    android:layout_height=""320dp""
    android:layout_marginLeft=""16dp""
    android:layout_marginRight=""16dp""
    android:layout_marginBottom=""16dp"" /&gt;
</code></pre>

<p>I have a function in my Fragment that launches an instance of this CameraSourcePreview when a button is pressed.</p>

<pre><code>public void scanBarcode() {

    Log.i(TAG, ""Barcode scanner called."");

    // Check for the camera permission before accessing the camera.  If the
    // permission is not granted yet, request permission.
    int rc = ActivityCompat.checkSelfPermission(getActivity(), Manifest.permission.CAMERA);
    if (rc == PackageManager.PERMISSION_GRANTED) {
        createCameraSource();
    } else {
        requestCameraPermission();
    }

    if (ContextCompat.checkSelfPermission(getActivity(), Manifest.permission.CAMERA)
            != PackageManager.PERMISSION_GRANTED) {
        requestPermissions(new String[]{Manifest.permission.CAMERA}, RC_HANDLE_CAMERA_PERM);
    }

    // Show &amp; Hide CardViews for Barcode Scanner Function
    showField(cardViewBarcodeScanner);

    // Starting camera source
    startCameraSource();
}
</code></pre>

<p>When the view appears on screen and the camera starts, my Logcat starts getting spammed with these log items. They don't seem to affect performance, but I have no idea what it means and if it's something I should be worried about.</p>

<p>The only reference I can find to this error are from the AOSP framework code that exists on Github. Here is a <a href=""https://github.com/UnicornioSucio/android_frameworks_native/blob/56c2b8aa1d82f782ebe258259946e5ee0cd23bf2/services/surfaceflinger/BufferLayer.cpp"" rel=""nofollow noreferrer"">specific example</a> and here is a <a href=""https://github.com/search?q=seems%20implausible%20relative%20to%20expectedpresent&amp;type=Code"" rel=""nofollow noreferrer"">broad search with all the results</a> I found, none of which seem to indicate exactly what causes the error when this SurfaceView appears.</p>

<p>The only thing I noticed is that I don't see these errors when I launch the Barcode Scanner / CameraSourcePreview in its own Activity (with just that and a LinearLayout element).</p>

<pre><code>2018-08-26 22:10:24.608 1413-1413/? W/Layer: [SurfaceView - com.mattdonders.android.barcodescanner/com.mattdonders.android.barcodescanner.MainActivity#0] Timestamp 233137964798000 seems implausible relative to expectedPresent 99377402186342
2018-08-26 22:10:24.663 1413-1413/? W/Layer: [SurfaceView - com.mattdonders.android.barcodescanner/com.mattdonders.android.barcodescanner.MainActivity#0] Timestamp 233137996282000 seems implausible relative to expectedPresent 99377456284340
2018-08-26 22:10:24.687 1413-1413/? W/Layer: [SurfaceView - com.mattdonders.android.barcodescanner/com.mattdonders.android.barcodescanner.MainActivity#0] Timestamp 233138032277000 seems implausible relative to expectedPresent 99377474317006
2018-08-26 22:10:24.752 1413-1413/? W/Layer: [SurfaceView - com.mattdonders.android.barcodescanner/com.mattdonders.android.barcodescanner.MainActivity#0] Timestamp 233138117551000 seems implausible relative to expectedPresent 99377546447670
2018-08-26 22:10:24.846 1413-1413/? W/Layer: [SurfaceView - com.mattdonders.android.barcodescanner/com.mattdonders.android.barcodescanner.MainActivity#0] Timestamp 233138173687000 seems implausible relative to expectedPresent 99377636611000
2018-08-26 22:10:24.862 1413-1413/? W/Layer: [SurfaceView - com.mattdonders.android.barcodescanner/com.mattdonders.android.barcodescanner.MainActivity#0] Timestamp 233138205545000 seems implausible relative to expectedPresent 99377654643666
2018-08-26 22:10:24.898 1413-1413/? W/Layer: [SurfaceView - com.mattdonders.android.barcodescanner/com.mattdonders.android.barcodescanner.MainActivity#0] Timestamp 233138244564000 seems implausible relative to expectedPresent 99377690708998
2018-08-26 22:10:24.951 1413-1413/? W/Layer: [SurfaceView - com.mattdonders.android.barcodescanner/com.mattdonders.android.barcodescanner.MainActivity#0] Timestamp 233138311966000 seems implausible relative to expectedPresent 99377744806996
</code></pre>",,0,0,,2018-08-27 12:16:03.463 UTC,1,2018-08-27 13:12:31.663 UTC,2018-08-27 13:12:31.663 UTC,,2649012,,657693,1,4,java|android|android-fragments|surfaceview,258
AWS Kinesis Video Streaming with AWS Rekognition on Android App,55590797,AWS Kinesis Video Streaming with AWS Rekognition on Android App,"<p>I have been trying to integrate AWS Kinesis Video Stream with Rekognition in an Android app and haven't been able to get best tutorials for the same.</p>

<p>I want to implement Facial Recognition and I am stuck at the step of PutMedia. In the demo/documentation provided by Amazon, I found details related to <strong>Java Producer Library and SDK</strong> only and nothing related to Android Producer Library and SDK where I need to use Android app as Kinesis Producer and stream the video to the Rekognition service.</p>

<p>Is there any alternative of <strong>PutMedia</strong> for Android? If <strong>yes</strong>, what is it and how to implement it? And if <strong>no</strong>, how to implement PutMedia in an Android App with AWS Android Producer Library and SDK.</p>

<p>I have already referred the following links so far:
<a href=""https://docs.aws.amazon.com/rekognition/latest/dg/recognize-faces-in-a-video-stream.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/rekognition/latest/dg/recognize-faces-in-a-video-stream.html</a>
<a href=""https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/examples-putmedia.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/examples-putmedia.html</a>
<a href=""https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/producer-sdk-android.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/producer-sdk-android.html</a>
<a href=""https://github.com/awslabs/aws-sdk-android-samples/tree/master/AmazonKinesisVideoDemoApp"" rel=""nofollow noreferrer"">https://github.com/awslabs/aws-sdk-android-samples/tree/master/AmazonKinesisVideoDemoApp</a>
<a href=""https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/producersdk-android-downloadcode.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/producersdk-android-downloadcode.html</a></p>

<p>Required Complete flow is as below:</p>

<pre><code>1. Start video streaming on Android Device.
</code></pre>

<ol start=""2"">
<li>Detect the face from the streaming Video.</li>
</ol>

<pre><code>3. After detecting the face, match(Compare) it with already existing face from the list of images in the S3 bucket.
</code></pre>

<ol start=""4"">
<li>If the match of the face is found then return True otherwise False.</li>
</ol>

<pre><code>
Any help with the issue would be a great.

Thanks
</code></pre>",,0,0,,2019-04-09 10:34:58.387 UTC,,2019-04-09 10:46:12.117 UTC,2019-04-09 10:46:12.117 UTC,,8854738,,8854738,1,0,android|amazon-web-services|video-streaming|amazon-kinesis|amazon-rekognition,66
Face API DetectAsync Error,47885650,Face API DetectAsync Error,"<p>I wanted to create a simple program to detect faces using Microsoft Azure Face API and Visual Studio 2015. Following the guide from (<a href=""https://social.technet.microsoft.com/wiki/contents/articles/37893.c-face-detection-and-recognition-with-azure-face-api.aspx"" rel=""nofollow noreferrer"">https://social.technet.microsoft.com/wiki/contents/articles/37893.c-face-detection-and-recognition-with-azure-face-api.aspx</a>), whenever my program calls UploadAndDetectFaces:</p>

<pre><code>private async Task&lt;Face[]&gt; UploadAndDetectFaces(string imageFilePath)
{
    try
    {
        using (Stream imageFileStream = File.OpenRead(imageFilePath))
        {
            var faces = await faceServiceClient.DetectAsync(imageFileStream,
                true,
                 true,
                 new FaceAttributeType[] 
                 {
                     FaceAttributeType.Gender,
                     FaceAttributeType.Age,
                     FaceAttributeType.Emotion
                 });
            return faces.ToArray();
        }
    }
    catch (Exception ex)
    {
        MessageBox.Show(ex.Message);
        return new Face[0];
    }
}
</code></pre>

<p>I also declared the keys to the endpoint:</p>

<pre><code>private readonly IFaceServiceClient faceServiceClient = new FaceServiceClient(""MY_KEY_HERE"");
</code></pre>

<p>an error returns: </p>

<blockquote>
  <p>""Exception of type 'Microsoft.ProjectOxford.Face.FaceAPIException' was thrown.""</p>
</blockquote>

<p>Does anyone know what's wrong or any changes required to prevent the error?</p>",47897474,1,3,,2017-12-19 11:09:56.263 UTC,,2017-12-20 01:12:30.050 UTC,2017-12-19 11:19:27.193 UTC,,106866,user8824475,,1,0,c#|visual-studio|azure|face-api,452
Android Vision - Reduce bar code tracking window,36405717,Android Vision - Reduce bar code tracking window,"<p>I'm trying to implement Google Visions scanner into an app im working on. By default its a full screen activity and barcodes are tracked over the entire screen.</p>

<p>However, I need a fullscreen camera but with a limited scanning window. For example, the surface view for the camera needs to be fullscreen, it has 2 transparent overlays set to 35% of the screen height top and bottom leaving a 30% viewport in the center.</p>

<p>I have changed the graphic overlay so it will only display in the middle viewport but havent been able to work out how to limit the barcode tracker to the same area.</p>

<p>Any ideas?</p>",36428822,2,1,,2016-04-04 14:39:33.673 UTC,3,2018-04-16 07:52:22.823 UTC,,,,,4346218,1,8,android|barcode-scanner|google-vision,4598
Vision API topicality and score always the same,51273104,Vision API topicality and score always the same,"<p>When I look in label_annotions of the Google Vision API, the ""score"" and ""topicality"" field values are always the same. This is also for example the case <a href=""https://www.infoworld.com/article/3269367/apis/how-to-use-the-google-vision-api.html"" rel=""nofollow noreferrer"">here</a>. According to this <a href=""https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate"" rel=""nofollow noreferrer"">reference</a> topicality refers to ""the relevancy of the ICA (Image Content Annotation) label to the image"" whereas score has replaced ""confidence"". Though it's now not so clear to me what ""score"" actually means.</p>

<p>Are these supposed to be always the same? What does that mean?</p>",52862848,1,0,,2018-07-10 19:57:30.880 UTC,1,2018-10-17 20:09:08.127 UTC,,,,,3565255,1,0,google-cloud-platform|vision-api,238
Can text structure be retained using Google Cloud Vision TEXT_DETECTION?,35519689,Can text structure be retained using Google Cloud Vision TEXT_DETECTION?,"<p>Version 1 of the Google Cloud Vision API (beta) permits optical character recognition via TEXT_DETECTION requests. While recognition quality is good, characters are returned without any hint of the original layout. Structured text (e.g., tables, receipts, columnar data) are therefore sometimes incorrectly ordered.</p>

<p>Is it possible to preserve document structure with the Google Cloud Vision API? Similar questions have been asked of tesseract and hOCR. For example, [1] and [2]. There is currently no information about TEXT_DETECTION options in the documentation [3].</p>

<p>[1] <a href=""https://stackoverflow.com/questions/22609778/how-to-preserve-document-structure-in-tesseract"">How to preserve document structure in tesseract</a>
[2] <a href=""https://stackoverflow.com/questions/18089013/tesseract-ambiguity-in-space-and-tab"">Tesseract - ambiguity in space and tab</a>
[3] <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/</a></p>",,1,1,,2016-02-20 05:49:23.720 UTC,2,2016-02-20 16:33:05.320 UTC,2017-05-23 12:18:24.630 UTC,,-1,,3761401,1,4,google-cloud-vision,1188
How to run specific code after n seconds on a video,53827914,How to run specific code after n seconds on a video,"<p>I am working on python3 and using Microsoft azure face API function 'CF.face.detect' to detect faces in a video.
I want to detect faces after every 1 second in the video that means run CF.face.detect once/second on video frame.</p>

<p>Please tell how to do it </p>

<p>Thanks in advance </p>",,1,1,,2018-12-18 07:07:22.947 UTC,,2018-12-18 08:53:31.890 UTC,,,,,9781290,1,2,python|azure|opencv,48
Scale and Translate an ImageView Programatically,50722472,Scale and Translate an ImageView Programatically,"<p>I have a <code>ImageView</code> of <strong>Face</strong>. I used <code>Google Vision Library</code> to find <strong>Eyes Landmark</strong> <code>(cx,cy)</code>. Now I want to <code>Scale</code> the <code>ImageView</code> so that the <strong>Eyes</strong> will be in focus and <code>Translate</code> it accordigly. </p>

<p>What will be the easiest possible way to do this than using <code>zoom_animation.xml</code></p>

<p><strong>I tried this but not getting proper result</strong></p>

<pre><code>Matrix m = imageView.getImageMatrix();
float[] values = new float[9];
m.getValues(values);
float xxx = values[Matrix.MTRANS_X];
float yyy = values[Matrix.MTRANS_Y];
float www = values[Matrix.MSCALE_X] * imageView.getWidth();
float hhh = values[Matrix.MSCALE_Y] * imageView.getHeight();

//cx and cy are eye positions
imageView.setScale(1.5f, cx - www / 2, cy - yyy / 2, false);
</code></pre>

<p>I read the documentation and find <code>setScale()</code> accept these parameters: </p>

<p><code>setScale(float scale, float focalX, float focalY, boolean animate)</code></p>

<p><strong>I am wondering how should I implement</strong> <code>focalX and focalY</code> <strong>or is there any way so I can convert</strong> <code>cx, cy</code> <strong>into</strong> <code>focalX, focalY</code> <strong>??</strong></p>

<p><strong>Thank You In Advance</strong></p>",,0,0,,2018-06-06 14:06:13.743 UTC,,2018-06-09 05:55:01.367 UTC,2018-06-09 05:55:01.367 UTC,,8989121,,8989121,1,1,android|animation,53
How to use Amazon Rekognition in Unity3D?,45508302,How to use Amazon Rekognition in Unity3D?,"<p>I’ve faced such a problem:</p>

<p>In the documentations AWS have Rekognition SDK of Unity3D <a href=""https://aws.amazon.com/ru/documentation/sdk-for-unity/"" rel=""nofollow noreferrer"">https://aws.amazon.com/ru/documentation/sdk-for-unity/</a></p>

<p>By downloading AWS Mobile SDK for Unity3D the Rekognition Amazon is absent, although it contains some other different sdks (S3, Lambda and others).</p>

<p>Through the nuget I can’t get SDK in Unity3D, because it’s in conflict with Core.</p>

<p>Even if I download version for the net35 directly, the conflict with Core arises which goes to SDK for Unity3D.</p>

<p>SDK Rekognition for Unity3D exists?</p>

<p>Or there is other way to connect with (refer to) Amazon Rekognition service from Unity3D?</p>

<p>Thank you.</p>",,1,0,,2017-08-04 13:36:16.827 UTC,,2017-08-05 15:24:21.650 UTC,,,,,7332348,1,0,amazon-web-services|unity3d|sdk|amazon|amazon-rekognition,442
"Android camera2 app where barcode detection is not working, despite face detection working as it should",53104628,"Android camera2 app where barcode detection is not working, despite face detection working as it should","<p>I'm trying to work with barcode scanning on android using Camera2. There is already others who have done stuff similar to this and I'm trying to make them work for me (which is not going too well to be honest). </p>

<p>I started by using the barcode scanning sample (google vision) for for the regular camera source  and then tried to combine it with stuff made by Ezequiel Adrian Minniti (he made it work with both camera and camera2). Ezequil made the face detector work and I'm trying to extend that to work with the barcode detector. I built it up almost exactly the same, but it's not working. </p>

<p>I can switch between using barcodeDetector and previewFaceDetector by just commenting away the one i don't want to use when I build the Camera2Source at the end of the createCameraSource method. When I use the previewFaceDetector everything is working just fine. The Log.d for every new item in the GraphicFaceTracker prints out just as it should. When I use the barcodeDetector I get no response from the detector. The camera preview works just as normal, but no graphical overlay, no printing for the new items in the BarcodeGraphicTracker, no indication at all that anything has been found. And no error related to this either...</p>

<p>The barcodes I'm scanning have been tried with the google vision example (using just camera, not camera2) so I don't think there should be an issue there. 
Can anyone help with finding the issue here? Why does face tracking work but not barcode tracking?</p>

<pre><code>private void createCameraSource(){
    // A barcode detector is created to track barcodes.  An associated multi-processor instance
    // is set to receive the barcode detection results, track the barcodes, and maintain
    // graphics for each barcode on screen.  The factory is used by the multi-processor to
    // create a separate tracker instance for each barcode.


    BarcodeDetector barcodeDetector = new BarcodeDetector.Builder(context).build();
    BarcodeTrackerFactory barcodeFactory = new BarcodeTrackerFactory();

    if (!barcodeDetector.isOperational()) {
        // Note: The first time that an app using the barcode or face API is installed on a
        // device, GMS will download a native libraries to the device in order to do detection.
        // Usually this completes before the app is run for the first time.  But if that
        // download has not yet completed, then the above call will not detect any barcodes
        // and/or faces.
        //
        // isOperational() can be used to check if the required native libraries are currently
        // available.  The detectors will automatically become operational once the library
        // downloads complete on device.
        Log.w(TAG, ""Detector dependencies are not yet available."");

        // Check for low storage.  If there is low storage, the native library will not be
        // downloaded, so detection will not become operational.
        IntentFilter lowstorageFilter = new IntentFilter(Intent.ACTION_DEVICE_STORAGE_LOW);
        boolean hasLowStorage = registerReceiver(null, lowstorageFilter) != null;

        if (hasLowStorage) {
            Toast.makeText(context, R.string.low_storage_error, Toast.LENGTH_LONG).show();
            Log.w(TAG, getString(R.string.low_storage_error));
        }
    } else {
        barcodeDetector.setProcessor(new MultiProcessor.Builder&lt;&gt;(barcodeFactory).build());
    }


    FaceDetector previewFaceDetector = new FaceDetector.Builder(context)
            .setClassificationType(FaceDetector.ALL_CLASSIFICATIONS)
            .setLandmarkType(FaceDetector.ALL_LANDMARKS)
            .setMode(FaceDetector.FAST_MODE)
            .setProminentFaceOnly(true)
            .setTrackingEnabled(true)
            .build();

    if(previewFaceDetector.isOperational()) {
        previewFaceDetector.setProcessor(new MultiProcessor.Builder&lt;&gt;(new GraphicFaceTrackerFactory()).build());
    } else {
        Toast.makeText(context, ""FACE DETECTION NOT AVAILABLE"", Toast.LENGTH_SHORT).show();
    }


        mCameraSource = new Camera2Source.Builder(context, previewFaceDetector)
//        mCameraSource = new Camera2Source.Builder(context, barcodeDetector)
                .setFocusMode(Camera2Source.CAMERA_AF_CONTINUOUS_PICTURE)
                .setFlashMode(Camera2Source.CAMERA_FLASH_ON)
                .setFacing(Camera2Source.CAMERA_FACING_BACK)
                .build();

            startCameraSource();
}

private class GraphicFaceTrackerFactory implements MultiProcessor.Factory&lt;Face&gt; {
    @Override
    public Tracker&lt;Face&gt; create(Face face) {
        return new GraphicFaceTracker(mGraphicOverlay);
    }
}

private class BarcodeTrackerFactory implements MultiProcessor.Factory&lt;Barcode&gt; {
    @Override
    public Tracker&lt;Barcode&gt; create(Barcode barcode) {
        return new BarcodeGraphicTracker(mGraphicOverlay);
    }

}

private class GraphicFaceTracker extends Tracker&lt;Face&gt; {
    private GraphicOverlay mOverlay;

    GraphicFaceTracker(GraphicOverlay overlay) {
        mOverlay = overlay;
        mFaceGraphic = new FaceGraphic(overlay, context);
    }

    /**
     * Start tracking the detected face instance within the face overlay.
     */
    @Override
    public void onNewItem(int faceId, Face item) {
        mFaceGraphic.setId(faceId);
        Log.d(TAG, ""GraphicFaceTracker new item"");
    }

    /**
     * Update the position/characteristics of the face within the overlay.
     */
    @Override
    public void onUpdate(FaceDetector.Detections&lt;Face&gt; detectionResults, Face face) {
        mOverlay.add(mFaceGraphic);
        mFaceGraphic.updateFace(face);
    }

    /**
     * Hide the graphic when the corresponding face was not detected.  This can happen for
     * intermediate frames temporarily (e.g., if the face was momentarily blocked from
     * view).
     */
    @Override
    public void onMissing(FaceDetector.Detections&lt;Face&gt; detectionResults) {
        mFaceGraphic.goneFace();
        mOverlay.remove(mFaceGraphic);
    }

    /**
     * Called when the face is assumed to be gone for good. Remove the graphic annotation from
     * the overlay.
     */
    @Override
    public void onDone() {
        mFaceGraphic.goneFace();
        mOverlay.remove(mFaceGraphic);
    }
}

private class BarcodeGraphicTracker extends Tracker&lt;Barcode&gt; {

    private GraphicOverlay mOverlay;
    private BarcodeGraphic mGraphic;

    BarcodeGraphicTracker(GraphicOverlay overlay) {
        mOverlay = overlay;
        mGraphic = new BarcodeGraphic(overlay);
    }

    /**
     * Start tracking the detected item instance within the item overlay.
     */
    @Override
    public void onNewItem(int id, Barcode item) {
        mGraphic.setId(id);
        Log.d(TAG, ""BarcodeGraphicTracker new item"");
    }

    /**
     * Update the position/characteristics of the item within the overlay.
     */
    @Override
    public void onUpdate(Detector.Detections&lt;Barcode&gt; detectionResults, Barcode item) {
        mOverlay.add(mGraphic);
        mGraphic.updateItem(item);
    }

    /**
     * Hide the graphic when the corresponding object was not detected.  This can happen for
     * intermediate frames temporarily, for example if the object was momentarily blocked from
     * view.
     */
    @Override
    public void onMissing(Detector.Detections&lt;Barcode&gt; detectionResults) {
        mOverlay.remove(mGraphic);
    }

    /**
     * Called when the item is assumed to be gone for good. Remove the graphic annotation from
     * the overlay.
     */
    @Override
    public void onDone() {
        mOverlay.remove(mGraphic);
    }
}
</code></pre>",,0,2,,2018-11-01 15:43:52.577 UTC,,2018-11-01 15:43:52.577 UTC,,,,,2286979,1,0,android|android-camera|android-camera2,83
Pass HttpPostedFileBase object in asp.net Identity action method and validating with face api,47104465,Pass HttpPostedFileBase object in asp.net Identity action method and validating with face api,"<p>Hello guys i try to implement a registration functionality using asp.net identity.</p>

<p>One of my required properties is the new user's photo  which  i pass as an argument in a  action method where finally  i call <code>faceServiceClient.DetectAsync()</code> method of azure face api.</p>

<p>I grab the photo using a <code>&lt;input type='file'&gt;</code> in my view 
and then in account controller of identity in register action method i use a <code>HttpPostedFileBase object</code> to read it.</p>

<p>The problem is that <code>faceServiceClient.DetectAsync()</code> needs as first argument a stream object or a string(imagePath) but in my case i cant figure it out how i can give a stream object or the image path</p>

<p><code>HttpPostedFileBase</code> doesn't retrieve the image path but to get the image i need this type of object.</p>

<p>And then even if i try to pass <code>httpPostedFilebase object.InputStream</code> as argument i get null by break pointing the <code>await faceServiceClient.DetectAsync</code> command line</p>

<p>To be specific</p>

<p><strong>Account Controller</strong></p>

<pre><code>    [HttpPost]
    [AllowAnonymous]
    [ValidateAntiForgeryToken]
    public async Task&lt;ActionResult&gt; Register([Bind(Exclude = ""UserPhoto"")]RegisterViewModel model, HttpPostedFileBase userPhoto)
    {
        if ((userPhoto==null) || (userPhoto.ContentLength &lt;= 0))
        {
            ModelState.AddModelError(""error"", @""Please Select a profile picture"");
        }

        if (ModelState.IsValid)
         {
          var faceApiresult = await new FaceRecognitionController().GetDetectedFaces(userPhoto);

          if (!faceApiresult)
          {
               ModelState.AddModelError(""error"", @""Your picture does not include your face"");
                return RedirectToAction(""Index"", ""Home"");
            }



            var user = new ApplicationUser
                {
                    UserName = model.Email,
                    Email = model.Email,
                    UName = model.Username,
                    UserPhoto = model.UserPhoto
                };

                var result = await UserManager.CreateAsync(user, model.Password);
                if (result.Succeeded)
                {
                    await SignInManager.SignInAsync(user, isPersistent: false, rememberBrowser: false);

                // For more information on how to enable account confirmation and password reset please visit https://go.microsoft.com/fwlink/?LinkID=320771
                // Send an email with this link
                // string code = await UserManager.GenerateEmailConfirmationTokenAsync(user.Id);
                // var callbackUrl = Url.Action(""ConfirmEmail"", ""Account"", new { userId = user.Id, code = code }, protocol: Request.Url.Scheme);
                // await UserManager.SendEmailAsync(user.Id, ""Confirm your account"", ""Please confirm your account by clicking &lt;a href=\"""" + callbackUrl + ""\""&gt;here&lt;/a&gt;"");


                    return RedirectToAction(""Index"", ""Home"");
                }
                AddErrors(result);

        }


        // If we got this far, something failed, redisplay form
        return View(model);
    }
</code></pre>

<p>and my  <strong>FaceRecognitionController</strong></p>

<pre><code>public class FaceRecognitionController : Controller
{

    private static string ServiceKey = ConfigurationManager.AppSettings[""FaceServiceKey""];
    private static string EndPoint = ConfigurationManager.AppSettings[""FaceServiceEndPoint""];

            [HttpGet]
            public  async Task&lt;dynamic&gt; GetDetectedFaces(HttpPostedFile userPhoto)
            {
              var photo = new byte[userPhoto.ContentLength];

              if (userPhoto.ContentLength == 0) return false;
                try
                {
                    // Create Instance of Service Client by passing Servicekey as parameter in constructor 
                    var faceServiceClient = new FaceServiceClient(ServiceKey);
                    var faces = await faceServiceClient.DetectAsync(userPhoto.InputStream, true, true, new FaceAttributeType[] { FaceAttributeType.Gender, FaceAttributeType.Age, FaceAttributeType.Smile, FaceAttributeType.Glasses });
                    //check if find any faces
                    var results = faces.Length;


                    return results != 0;
                }
                catch (FaceAPIException)
                {
                    //do exception work
                }

                return false;
            }

           }
         }
</code></pre>

<p>As you can see i just check if validation api find any face and just return true to complete the registration</p>

<p>any thoughts on how i can overcome this??</p>",,1,0,,2017-11-03 21:11:24.963 UTC,,2017-11-03 21:57:56.827 UTC,,,,,4400301,1,0,c#|asp.net|azure|asp.net-identity,113
Testing Google Cloud Vision with Java in Eclipse,48690047,Testing Google Cloud Vision with Java in Eclipse,"<p>I am testing Google Cloud Vision with Java in Eclipse.</p>

<p>I have copied the java code from <a href=""https://cloud.google.com/vision/docs/reference/libraries#client-libraries-install-java"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/reference/libraries#client-libraries-install-java</a></p>

<pre><code>// Imports the Google Cloud client library

import com.google.cloud.vision.v1.AnnotateImageRequest;
import com.google.cloud.vision.v1.AnnotateImageResponse;
import com.google.cloud.vision.v1.BatchAnnotateImagesResponse;
import com.google.cloud.vision.v1.EntityAnnotation;
import com.google.cloud.vision.v1.Feature;
import com.google.cloud.vision.v1.Feature.Type;
import com.google.cloud.vision.v1.Image;
import com.google.cloud.vision.v1.ImageAnnotatorClient;
import com.google.protobuf.ByteString;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.List;

public class QuickstartSample {
  public static void main(String... args) throws Exception {
    // Instantiates a client
    try (ImageAnnotatorClient vision = ImageAnnotatorClient.create()) {

      // The path to the image file to annotate
      String fileName = ""./resources/wakeupcat.jpg"";

      // Reads the image file into memory
      Path path = Paths.get(fileName);
      byte[] data = Files.readAllBytes(path);
      ByteString imgBytes = ByteString.copyFrom(data);

      // Builds the image annotation request
      List&lt;AnnotateImageRequest&gt; requests = new ArrayList&lt;&gt;();
      Image img = Image.newBuilder().setContent(imgBytes).build();
      Feature feat = Feature.newBuilder().setType(Type.LABEL_DETECTION).build();
      AnnotateImageRequest request = AnnotateImageRequest.newBuilder()
          .addFeatures(feat)
          .setImage(img)
          .build();
      requests.add(request);

      // Performs label detection on the image file
      BatchAnnotateImagesResponse response = vision.batchAnnotateImages(requests);
      List&lt;AnnotateImageResponse&gt; responses = response.getResponsesList();

      for (AnnotateImageResponse res : responses) {
        if (res.hasError()) {
          System.out.printf(""Error: %s\n"", res.getError().getMessage());
          return;
        }

        for (EntityAnnotation annotation : res.getLabelAnnotationsList()) {
          annotation.getAllFields().forEach((k, v) -&gt;
              System.out.printf(""%s : %s\n"", k, v.toString()));
        }
      }
    }
  }
}
</code></pre>

<p>Where can I download the .JAR's from so that everything compiles properly?</p>

<p>Google themselves say (see the link above)</p>

<blockquote>
  <p>If you are using Maven, add this to your pom.xml file:   </p>
</blockquote>

<pre><code>    &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;  
    &lt;artifactId&gt;google-cloud-vision&lt;/artifactId&gt;  
    &lt;version&gt;1.14.0&lt;/version&gt; &lt;/dependency&gt;
</code></pre>

<p>However, I am not using Maven (nor Gradle or SBT which are their other suggestions). </p>

<p>So I thought to open a New Maven project in Eclipse, which would then download all the JAR's automatically, and then copy them across to my project.</p>

<p>So I did ""new Maven Project"" in Eclipse and then when it said ""enter a group id for the artifact"" I entered the details from Google as I pasted above, but it did not download anything.</p>

<p>Any ideas how I can get the JARs so that the code will compile?</p>

<p>The pom.xml file which was auto-generated by Eclipse is</p>

<pre><code>&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
  xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

  &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;
  &lt;artifactId&gt;google-cloud-vision&lt;/artifactId&gt;
  &lt;version&gt;1.14.0&lt;/version&gt;
  &lt;packaging&gt;jar&lt;/packaging&gt;

  &lt;name&gt;google-cloud-vision&lt;/name&gt;
  &lt;url&gt;http://maven.apache.org&lt;/url&gt;

  &lt;properties&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
  &lt;/properties&gt;

  &lt;dependencies&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;junit&lt;/groupId&gt;
      &lt;artifactId&gt;junit&lt;/artifactId&gt;
      &lt;version&gt;3.8.1&lt;/version&gt;
      &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
  &lt;/dependencies&gt;
&lt;/project&gt;
</code></pre>",48690952,1,4,,2018-02-08 16:17:40.297 UTC,,2018-02-08 17:08:15.777 UTC,2018-02-08 16:52:42.657 UTC,,5320906,,757661,1,0,java|eclipse|maven|google-cloud-platform,170
"Cannot find the gem ""Could not find aws-sigv4-1.0.0 in any of the sources""",51319671,"Cannot find the gem ""Could not find aws-sigv4-1.0.0 in any of the sources""","<pre><code>gem 'aws-sdk'
gem 'dotenv'
gem 'sinatra'
</code></pre>

<p>This is my gem file. But when i use bundle install i see this output.</p>

<pre><code>root@DESKTOP-ETLLRI1 C:\Users\root\Desktop\case 9\FaceRekognition-Demo
$ bundle install
Resolving dependencies...
Your Gemfile has no gem server sources. If you need gems that are not already on
your machine, add a line like this to your Gemfile:
source 'https://rubygems.org'
Could not find aws-sigv4-1.0.0 in any of the sources
</code></pre>",51319708,2,0,,2018-07-13 07:18:02.750 UTC,,2018-12-03 06:23:52.853 UTC,,,,,9594020,1,1,ruby-on-rails|amazon-web-services,184
Upload local file with fetch / content-type: octet-stream,51456202,Upload local file with fetch / content-type: octet-stream,"<p>I am currently developing a Sketch Plugin, where an image gets sent to the Microsoft Custom Vision API for object detection.</p>

<p>The Sketch Plugin itself is written in Javascript, a fs polyfill and a fetch polyfill is available. An API call with an image url works perfectly fine. However, I have trouble sending a local file from my computer as I am not 100% how I can access it. </p>

<pre><code>    var templateUrl = require('../assets/test.png').replace('file://', '');
    var file = fs.readFileSync(templateUrl);

    var request = postData('https://southcentralus.api.cognitive.microsoft.com/customvision/v2.0/Prediction/XXXXXXX/image');
    request.then(data =&gt; data.json()).then(data =&gt; myFunction(data));

    // post request with file
    function postData(url, data) {
    return fetch(url, {  
        method: 'POST',  
        headers: {  
          'Prediction-Key': 'XXXXXXXXXX',
          'Content-Type': 'application/octet-stream',
        },  
        body: file,
    })
}
</code></pre>

<p>Does anyone have experience with sending local files to the image recognition API? Any help would be very much appreciated!</p>

<p>Thanks in advance!</p>

<p>Best, C</p>",,0,3,,2018-07-21 12:44:27.060 UTC,1,2018-07-21 12:53:55.660 UTC,2018-07-21 12:53:55.660 UTC,,5216430,,5216430,1,1,javascript|fs|fetch-api|microsoft-cognitive|sketchapp,625
"dotnet, how to set Environment Variable 'GOOGLE_APPLICATION_CREDENTIALS'?",40856101,"dotnet, how to set Environment Variable 'GOOGLE_APPLICATION_CREDENTIALS'?","<p>I want to use OCR using Text Detection in Google Cloud Vision. So, I downloaded Text detection sample source and tested it. </p>

<p>In my local text, text detection works properly.
using local path : 
<code>my computer-advanced system setting-enviroment variable-add new 'GOOGLE_APPLICATION_CREDENTIALS' &amp; json path</code></p>

<p>So, I immigrate that code to my server. But, when I run OCR function, error message appeared.</p>

<blockquote>
  <p>Error reading credential file from location MY-JSON-FILEPATH . Please check the value of Environment Variable GOOGLE_APPLICATION_CREDENTIALS</p>
</blockquote>

<p>Local program can access my local json file, but in my server can't access that file. However, I don't know how to set environment variable in dotnet.</p>

<p>So. I want to need help. Thank you.</p>

<p>My programming language is .net using visual studio 2012 + IIS 6.0</p>

<p><a href=""https://i.stack.imgur.com/XTqVM.png"" rel=""nofollow noreferrer"">error message capture image</a></p>",,0,4,,2016-11-29 00:55:23.727 UTC,1,2016-11-29 02:16:12.963 UTC,2016-11-29 02:16:12.963 UTC,,1050927,,7222803,1,2,.net|json|ocr,357
iOS : Scanning barcode very slow,47532758,iOS : Scanning barcode very slow,"<p>I am developing scanning barcode application using swift4.</p>

<p>I have tried using 2 popular open source , those are <a href=""https://github.com/mikebuss/MTBBarcodeScanner"" rel=""nofollow noreferrer"" title=""click here"">MTBarcode</a> (using AVFoundation) and <a href=""https://github.com/googlesamples/ios-vision"" rel=""nofollow noreferrer"" title=""click here"">iOS Vision</a> (using GoogleVision Framework), but scanning speed is not fast as I expect, it takes about 2 or 3 seconds to detect real barcode.</p>

<p>Some people suggest me change sessionPreset to AVCaptureSessionPresetMedium but it is not effective.</p>

<p>Anyone can suggest me how to improve scanning speed. Should we update another camera configuration or use another open source ?</p>",,2,4,,2017-11-28 13:27:37.413 UTC,1,2019-05-05 18:22:46.143 UTC,,,,,1147147,1,0,ios|avfoundation|barcode|swift4|google-vision,839
Google Application Credentials set and not found,45727079,Google Application Credentials set and not found,"<p>I have an Amazon EC2 with Linux Instance set up and running for my Java Web Application to consume REST requests. The problem is that I am trying to use Google Cloud Vision in this application to recognize violence/nudity in users pictures.</p>

<p>Accessing the EC2 in my Terminal, I set the GOOGLE_APPLICATION_CREDENTIALS by the following command, which I found in the documentation:</p>

<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=&lt;my_json_path.json&gt;
</code></pre>

<p>Here comes my first problem: When I restart my server, and ran 'echo $GOOGLE_APPLICATION_CREDENTIALS' the variable is gone. Ok, I set it to the bash_profile and bashrc and now it is ok.</p>

<p>But, when I ran my application, consuming the above code, to get the adult and violence status of my picture, I got the following error:</p>

<pre><code>java.io.IOException: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.
</code></pre>

<p>My code is the following:</p>

<p><em>Controller</em>:</p>

<pre><code>        if(SafeSearchDetection.isSafe(user.getId())) {
            if(UserDB.updateUserProfile(user)==false){
                throw new SQLException(""Failed to Update"");
            }
        } else {
            throw new IOException(""Explicit Content"");
        }
</code></pre>

<p><em>SafeSearchDetection.isSafe(int idUser)</em>:</p>

<pre><code>    String path = IMAGES_PATH + idUser + "".jpg"";

    try {
        mAdultMedicalViolence = detectSafeSearch(path);
        if(mAdultMedicalViolence.get(0) &gt; 3)
            return false;
        else if(mAdultMedicalViolence.get(1) &gt; 3)
            return false;
        else if(mAdultMedicalViolence.get(2) &gt; 3)
            return false;

    } catch (IOException e) {
        e.printStackTrace();
    } catch (Exception e) {
        e.printStackTrace();
    }
    return true;
</code></pre>

<p><em>detectSafeSearch(String path)</em>:</p>

<pre><code>    List&lt;AnnotateImageRequest&gt; requests = new ArrayList&lt;AnnotateImageRequest&gt;();
    ArrayList&lt;Integer&gt; adultMedicalViolence = new ArrayList&lt;Integer&gt;();

    ByteString imgBytes = ByteString.readFrom(new FileInputStream(path));

    Image img = Image.newBuilder().setContent(imgBytes).build();
    Feature feat = Feature.newBuilder().setType(Type.SAFE_SEARCH_DETECTION).build();
    AnnotateImageRequest request = AnnotateImageRequest.newBuilder().addFeatures(feat).setImage(img).build();
    requests.add(request);

    ImageAnnotatorClient client = ImageAnnotatorClient.create();

    BatchAnnotateImagesResponse response = client.batchAnnotateImages(requests);
    List&lt;AnnotateImageResponse&gt; responses = response.getResponsesList();

    for (AnnotateImageResponse res : responses) {
        if (res.hasError()) {
            System.out.println(""Error: ""+res.getError().getMessage()+""\n"");
            return null;
        }

        SafeSearchAnnotation annotation = res.getSafeSearchAnnotation();
        adultMedicalViolence.add(annotation.getAdultValue());
        adultMedicalViolence.add(annotation.getMedicalValue());
        adultMedicalViolence.add(annotation.getViolenceValue());

    }
    for(int content : adultMedicalViolence)
        System.out.println(content + ""\n"");

    return adultMedicalViolence;
</code></pre>",,1,0,,2017-08-17 05:17:06.627 UTC,,2017-08-21 07:47:42.147 UTC,2017-08-21 07:47:42.147 UTC,,322020,,8475952,1,0,amazon-web-services|amazon-ec2|google-cloud-platform|google-authentication|google-cloud-vision,373
Take photo with bitmap in android camera,41713056,Take photo with bitmap in android camera,"<p>I want to make live filter in camera like Snapchat app. This app based on <a href=""https://github.com/googlesamples/android-vision/tree/master/visionSamples/FaceTracker"" rel=""nofollow noreferrer"">Google Vision Face Tracker</a>. </p>

<p>I have these following code in FaceGraphic.java:</p>

<pre><code>package com.google.android.gms.samples.vision.face.facetracker;

import android.graphics.Canvas;
import android.graphics.Color;
import android.graphics.Paint;
import android.graphics.Bitmap;
import android.graphics.BitmapFactory;

import com.google.android.gms.samples.vision.face.facetracker.ui.camera.GraphicOverlay;
import com.google.android.gms.vision.face.Face;
import com.google.android.gms.vision.face.Landmark;

/**
 * Graphic instance for rendering face position, orientation, and landmarks within an associated
* graphic overlay view.
*/
class FaceGraphic extends GraphicOverlay.Graphic {
    private static final float FACE_POSITION_RADIUS = 10.0f;
    private static final float ID_TEXT_SIZE = 40.0f;
    private static final float ID_Y_OFFSET = 50.0f;
    private static final float ID_X_OFFSET = -50.0f;
    private static final float BOX_STROKE_WIDTH = 5.0f;

    private static final int COLOR_CHOICES[] = {
        Color.BLUE,
        Color.CYAN,
        Color.GREEN,
        Color.MAGENTA,
        Color.RED,
        Color.WHITE,
        Color.YELLOW
    };
    private static int mCurrentColorIndex = 0;

    private Paint mFacePositionPaint;
    private Paint mIdPaint;
    private Paint mBoxPaint;

    private volatile Face mFace;
    private int mFaceId;
    private float mFaceHappiness;

    private Bitmap bitmap;
    private Bitmap sunglasses;

    FaceGraphic(GraphicOverlay overlay) {
        super(overlay);

        mCurrentColorIndex = (mCurrentColorIndex + 1) % COLOR_CHOICES.length;
        final int selectedColor = COLOR_CHOICES[mCurrentColorIndex];

        mFacePositionPaint = new Paint();
        mFacePositionPaint.setColor(selectedColor);

        mIdPaint = new Paint();
        mIdPaint.setColor(selectedColor);
        mIdPaint.setTextSize(ID_TEXT_SIZE);

        mBoxPaint = new Paint();
        mBoxPaint.setColor(selectedColor);
        mBoxPaint.setStyle(Paint.Style.STROKE);
        mBoxPaint.setStrokeWidth(BOX_STROKE_WIDTH);

        bitmap = BitmapFactory.decodeResource(getOverlay().getContext().getResources(), R.drawable.sunglasses);
        sunglasses = bitmap;

    }

    void setId(int id) {
        mFaceId = id;
    }


    /**
     * Updates the face instance from the detection of the most recent frame.  Invalidates the
     * relevant portions of the overlay to trigger a redraw.
     */
    void updateFace(Face face) {
        mFace = face;

        sunglasses = Bitmap.createScaledBitmap(bitmap, (int) scaleX(face.getWidth()),
                (int) scaleY(((bitmap.getHeight() * face.getWidth()) / bitmap.getWidth())), false);

        postInvalidate();
    }

    /**
     * Draws the face annotations for position on the supplied canvas.
     */
    @Override
    public void draw(Canvas canvas) {
        Face face = mFace;

        if (face == null) {
            return;
        }

        float x = translateX(face.getPosition().x + face.getWidth() / 2);
        float y = translateY(face.getPosition().y + face.getHeight() / 2);

        // Draws a bounding box around the face.
        float xOffset = scaleX(face.getWidth() / 2.0f);
        float yOffset = scaleY(face.getHeight() / 2.0f);
        float left = x - xOffset;
        float top = y - yOffset;
        float right = x + xOffset;
        float bottom = y + yOffset;
        canvas.drawRect(left, top, right, bottom, mBoxPaint);

        //Get the left eye to place the sunglasses over the eyes
        float eyeY = top + sunglasses.getHeight() / 2;
        for(Landmark l : face.getLandmarks()){
            if(l.getType() == Landmark.LEFT_EYE){
                eyeY = l.getPosition().y + sunglasses.getHeight() / 2;
            }
        }
        canvas.drawBitmap(sunglasses, left, eyeY, new Paint());

    }
}
</code></pre>

<p>I create function to take a photo in <a href=""https://gist.github.com/elraghifary/ae7f4ef221f2e0a7c2571889a8b1a07d"" rel=""nofollow noreferrer"">FaceTrackerActivity.java</a>:</p>

<pre><code>findViewById(R.id.capture).setOnClickListener(new View.OnClickListener() {
        @Override
        public void onClick(View v) {
            mCameraSource.takePicture(null, new CameraSource.PictureCallback() {

                @Override
                public void onPictureTaken(byte[] bytes) {
                    Log.d(TAG, ""onPictureTaken - jpeg"");
                    capturePic(bytes);
                }

                private void capturePic(byte[] bytes) {
                    try {
                        String mainpath = getExternalStorageDirectory() + separator + ""MaskIt"" + separator + ""images"" + separator;
                        File basePath = new File(mainpath);
                        if (!basePath.exists())
                            Log.d(""CAPTURE_BASE_PATH"", basePath.mkdirs() ? ""Success"": ""Failed"");
                        File captureFile = new File(mainpath + ""photo_"" + getPhotoTime() + "".jpg"");
                        if (!captureFile.exists())
                            Log.d(""CAPTURE_FILE_PATH"", captureFile.createNewFile() ? ""Success"": ""Failed"");
                        FileOutputStream stream = new FileOutputStream(captureFile);
                        stream.write(bytes);
                        stream.flush();
                        stream.close();
                    } catch (IOException e) {
                        e.printStackTrace();
                    }
                }

                private String getPhotoTime(){
                    SimpleDateFormat sdf=new SimpleDateFormat(""ddMMyy_hhmmss"");
                    return sdf.format(new Date());
                }
            });
        }
    });
</code></pre>

<p>First, open app and give permission to access camera, and app will detect faces and draw bitmap (sunglasses) to them. I create button ""Take a picture"" with id <em>capture</em>.</p>

<p>This is my main.xml:
    </p>

<pre><code>&lt;LinearLayout
    xmlns:android=""http://schemas.android.com/apk/res/android""
    xmlns:app=""http://schemas.android.com/apk/res-auto""
    xmlns:tools=""http://schemas.android.com/tools""
    android:id=""@+id/topLayout""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""
    android:keepScreenOn=""true""
    android:weightSum=""1""
    android:orientation=""vertical""&gt;

  &lt;LinearLayout
      android:orientation=""horizontal""
      android:layout_width=""match_parent""
      android:layout_height=""64dp""&gt;

    &lt;Button
        android:text=""Sunglasses""
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:id=""@+id/sun""
        android:layout_weight=""1""
        tools:ignore=""HardcodedText"" /&gt;

    &lt;Button
        android:text=""Helmet""
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:id=""@+id/helm""
        android:layout_weight=""1""
        tools:ignore=""HardcodedText"" /&gt;

    &lt;Button
        android:text=""Mustache""
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:id=""@+id/must""
        android:layout_weight=""1""
        tools:ignore=""HardcodedText"" /&gt;
  &lt;/LinearLayout&gt;
&lt;com.google.android.gms.samples.vision.face.facetracker.ui.camera.CameraSourcePreview
      android:id=""@+id/preview""
      android:layout_width=""match_parent""
      android:layout_height=""0dp""
      android:layout_weight=""0.99""
      android:weightSum=""1""&gt;
    &lt;com.google.android.gms.samples.vision.face.facetracker.ui.camera.GraphicOverlay
        android:id=""@+id/faceOverlay""
        android:layout_width=""match_parent""
        android:layout_height=""match_parent""
        android:layout_weight=""0.79"" /&gt;

&lt;/com.google.android.gms.samples.vision.face.facetracker.ui.camera.CameraSourcePreview&gt;

  &lt;Button
      android:id=""@+id/capture""
      android:layout_width=""match_parent""
      android:layout_height=""wrap_content""
      android:text=""Take a Picture""
      tools:ignore=""HardcodedText"" /&gt;
    &lt;/LinearLayout&gt;
</code></pre>

<p>How can I get bitmap that I draw in FaceGraphic? If I take a photo, I only get the default photo without bitmap. I want to take photo with user faces and bitmap and save into gallery. Sorry, I hope you understand my question. Thank you.</p>",,2,0,,2017-01-18 06:50:49.890 UTC,,2019-04-23 12:37:56.017 UTC,2017-01-18 21:02:36.153 UTC,,1000551,,4670082,1,1,android|bitmap|android-bitmap|google-vision,418
rekognition.detectModerationLabels not working,52326351,rekognition.detectModerationLabels not working,"<p>rekognition.detectModerationLabels in amazon rekognition Javascript sdk is not working. It throwing an error in cosole ""Uncaught TypeError: rekognition.detectModerationLabels is not a function"". Please help</p>",,0,0,,2018-09-14 06:49:16.260 UTC,,2018-09-14 06:49:16.260 UTC,,,,,4548886,1,0,amazon-rekognition,25
Unable to use custom classifier in Node-Red Watson Visual Recognition node,47982254,Unable to use custom classifier in Node-Red Watson Visual Recognition node,"<p>I have a flow which I use to get an image from the IBM Object storage and pass it to a Watson Visual Recognition node for classification using a custom classifier I had trained. A few weeks earlier it stopped working and the visual recognition node would throw an error saying ""Invalid JSON parameter received. Unable to parse."". I used ""change"" nodes to set the parameters of the message to be classified as shown here:<a href=""https://i.stack.imgur.com/F2FnW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F2FnW.png"" alt=""enter image description here""></a>
I noticed that if I delete the node in which I set the Classifier Id, then I get no error and the image is classified using the default classifier. I tried using a function node to set the parameters using the following code, but I got the same error:</p>

<pre><code>msg.params = {};
msg.params[""detect_mode""] = ""classify"";
msg.params[""classifier_ids""] = ""person_705615375"";
msg.params[""threshold""] = 0;
return msg;
</code></pre>

<p>In addition, if I set the classifier to ""Default"" the image should be classified using the default classifier according to the visual recognition node's info page. However I still get the same error. Here is an example of a message passed for classification:</p>

<p><a href=""https://i.stack.imgur.com/NZ9Xl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NZ9Xl.png"" alt=""enter image description here""></a></p>

<p>Some extra info from the visual recognition node's result:</p>

<pre><code>result: object
    error: object
        message: ""Invalid JSON parameter received. Unable to parse.""
        stack: ""Error: Invalid JSON parameter received. Unable to parse.↵ at Request._callback (/home/vcap/app/node_modules/node-red-node-watson/node_modules/watson-developer-cloud/lib/requestwrapper.js:85:15)↵ at Request.self.callback (/home/vcap/app/node_modules/node-red-node-watson/node_modules/watson-developer-cloud/node_modules/request/request.js:186:22)↵ at emitTwo (events.js:87:13)↵ at Request.emit (events.js:172:7)↵ at Request.&lt;anonymous&gt; (/home/vcap/app/node_modules/node-red-node-watson/node_modules/watson-developer-cloud/node_modules/request/request.js:1163:10)↵ at emitOne (events.js:77:13)↵ at Request.emit (events.js:169:7)↵ at IncomingMessage.&lt;anonymous&gt; (/home/vcap/app/node_modules/node-red-node-watson/node_modules/watson-developer-cloud/node_modules/request/request.js:1085:12)↵ at IncomingMessage.g (events.js:260:16)↵ at emitNone (events.js:72:20)↵ at IncomingMessage.emit (events.js:166:7)↵ at endReadableNT (_stream_readable.js:923:12)↵ at nextTickC...""
        code: 400
        images_processed: 0
        error: ""Invalid JSON parameter received. Unable to parse.""
        description: ""Invalid JSON parameter received. Unable to parse.""
        error_id: ""parameter_error""
</code></pre>",48001147,1,0,,2017-12-26 18:20:03.100 UTC,2,2019-02-08 06:47:01.233 UTC,2017-12-26 19:05:36.630 UTC,,504554,,2442119,1,1,node-red|watson|visual-recognition,373
Image size is incorrect from google cloud vision api (OCR) when using type: TEXT_DETECTION,48188923,Image size is incorrect from google cloud vision api (OCR) when using type: TEXT_DETECTION,"<p>I am facing the problem that in some cases the image size in json is incorrect, when I am using type: ""TEXT_DETECTION"". I have figured out, that it happens when the image size differs only some pixels from a common ratio. 
I have tested an image in size: </p>

<pre><code> ""width"": 5152,
  ""height"": 2896,
</code></pre>

<p>Google cloud vision api reports an image size of</p>

<pre><code> ""width"": 5152,
  ""height"": 2895,
</code></pre>

<p>The strange thing is, that it always differs one pixel. But only if the image ratio is close to a common ratio. (16:9 in my case)
If you use e.g. an image size 5152 x 2890 the values from the API are correct. </p>

<p>This problem does not appear when you use  ""type"": ""DOCUMENT_TEXT_DETECTION"".</p>

<p>I appreciate any idea how to fix this.</p>

<p>Thanks</p>",,0,0,,2018-01-10 13:50:32.043 UTC,,2018-01-10 13:50:32.043 UTC,,,,,9198705,1,1,ocr|google-cloud-vision,205
How to convert Image url to Byte Array in java?,54444114,How to convert Image url to Byte Array in java?,"<p>i'm trying to make a webscrapper with aws image recogntion api. So I have to convert the image to a byte array in order for the api to work. However, I'm getting some error saying <code>The method openStream() is undefined for the type String</code>. If i use a local image file, then it works perfectly fine.     Can someone please help me ? 
Thanks</p>

<pre><code>public class HelloAppEngine extends HttpServlet{

    /**
     * 
     * 
     * 
     * 
     * 
     * 
     * 
     */
         // where jsoup images are stored 

    static ArrayList&lt;String&gt; testImages = new ArrayList&lt;String&gt;();

    public static void getimages() {

        String photo =  testImages.get(0);



    ByteBuffer imageBytes = null;
    try (InputStream inputStream = photo.openStream())) {
        imageBytes = ByteBuffer.wrap(IOUtils.toByteArray(inputStream));
    }

    catch (IOException e1) {
        // TODO Auto-generated catch block
        e1.printStackTrace();
    }


    AmazonRekognition rekognitionClient = AmazonRekognitionClientBuilder.defaultClient();

    DetectLabelsRequest request = new DetectLabelsRequest()
            .withImage(new Image()
                    .withBytes(imageBytes))
            .withMaxLabels(10)
            .withMinConfidence(77F);

    try {

        DetectLabelsResult result = rekognitionClient.detectLabels(request);
        List &lt;Label&gt; labels = result.getLabels();

        System.out.println(""Detected labels for "" + photo);
        for (Label label: labels) {
           System.out.println(label.getName() + "": "" + label.getConfidence().toString());
        }

    } catch (AmazonRekognitionException e) {
        e.printStackTrace();
    }

}
</code></pre>",,1,1,,2019-01-30 15:32:43.740 UTC,,2019-01-30 15:38:11.167 UTC,2019-01-30 15:36:44.977 UTC,,5951915,,6830361,1,0,java,199
"AWS Rekognition ""response['FaceDetails']""",51192145,"AWS Rekognition ""response['FaceDetails']""","<p>I am running into an issue with AWS Rekognition python API. The interesting part is that the issue I'm encountering seems to only affect the <code>response = client.detect_faces</code>API as the problem doesn't occur with <code>response = client.detect_labels</code>. What I'm trying to do is to filter out only the information that I want to use in the next parts of my program. This works fine for the label detection with this code:</p>

<pre><code>labels1=response['Labels']
extractor=len(labels1)
for i in range(0,extractor):
    print(labels1[diction_counter])
    diction_counter += 1
diction_counter=0
</code></pre>

<p>A similar code defines <code>labels1</code> as a list which includes all FaceDetails:</p>

<pre><code>labels1=response['FaceDetails']
</code></pre>

<p>When printing that list it displays a huge block of information and that is what it should do at this moment. However, when requesting the length of the list,</p>

<pre><code>print(len(labels1))
</code></pre>

<p>The answer I am receiving is 1<br><br>
At this point, the problems I am encountering are starting. According to the response structure for FaceDetection found in the <a href=""https://boto3.readthedocs.io/en/latest/reference/services/rekognition.html#Rekognition.Client.detect_faces"" rel=""nofollow noreferrer"">AWS Boto 3 docs</a>, the response structure of <code>FaceDetails</code>is separated into 15 dictionaries which then contain the information I'm after. However, I am unable to separate the dictionaries using the method that worked for the DetectLables function:</p>

<pre><code>extractor=len(labels1)
for i in range(0,extractor):
    print(labels1[diction_counter])
    diction_counter += 1
</code></pre>

<p>The reason for this is that python sees the response as a list with the length 1 and therefore displays everything again. The output looks like this:</p>

<pre><code>[{'BoundingBox': {'Width': 0.23359374701976776, 'Height': 0.41527777910232544, 
'Left': 0.484375, 'Top': 0.125}, 'AgeRange': {'Low': 12, 'High': 22}, 'Smile': 
{'Value': False, 'Confidence': 99.90431213378906}, 'Eyeglasses': {'Value': 
False, 'Confidence': 99.99996185302734}, 'Sunglasses': {'Value': False, 
'Confidence': 98.4183578491211}, 'Gender': {'Value': 'Male', 'Confidence': 
99.9287338256836},...
</code></pre>

<p>I've tried several things such as converting all of this into a string and then into a list which is separated by commas but nothing has worked. If anybody has an idea on how I can extract certain information so that the output looks like this:</p>

<pre><code>'AgeRange': {'Low': 12, 'High': 22},
'Smile':{'Value': False, 'Confidence': 99.90431213378906},
'Eyeglasses': {'Value': False, 'Confidence': 99.99996185302734},
'...
</code></pre>

<p>I would greatly appreciate any tips and tricks on how to achieve this.</p>",,0,3,,2018-07-05 13:08:09.330 UTC,,2018-07-05 14:21:22.150 UTC,2018-07-05 14:21:22.150 UTC,,10020490,,10020490,1,0,python|list|amazon-web-services|dictionary|boto3,57
require_once(Net/URL2.php): failed to open stream: No such file or directory with Microsoft Face API,43520997,require_once(Net/URL2.php): failed to open stream: No such file or directory with Microsoft Face API,"<p>I have implemented microsoft face api, with the help of the library:</p>

<p><a href=""https://github.com/pear/HTTP_Request2"" rel=""nofollow noreferrer"">https://github.com/pear/HTTP_Request2</a> </p>

<p>The script is working perfectly on my local machine and I can identify the faces and store them on microsoft db. But when I upload it to my server, it is showing the below error and api call won't process:</p>

<blockquote>
  <p>_>require_once(Net/URL2.php): failed to open stream: No such file or directory</p>
</blockquote>

<p>I have then installed the corresponding files with composer and place the folder with the 'Net' having URL.php..but then it shows the following error.</p>

<blockquote>
  <p><b>Fatal error</b>:  Uncaught &lt;table style=&quot;border: 1px&quot;
  cellspacing=&quot;0&quot;&gt; &lt;tr&gt;&lt;td colspan=&quot;3&quot;
  style=&quot;background: #ff9999&quot;&gt;
  &lt;b&gt;HTTP_Request2_ConnectionException&lt;/b&gt;: Unable to
  connect to tls://westus.api.cognitive.microsoft.com:443. Error:
  stream_socket_client(): unable to connect to
  tls://westus.api.cognitive.microsoft.com:443 (Unknown error)
  stream_socket_client(): Failed to enable crypto
  stream_socket_client(): SSL operation failed with code 1. OpenSSL
  Error messages: error:14090086:SSL
  routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed in
  &lt;b&gt;/home/content/08/10141608/html/testing/faceapi/HTTP/Request2/Adapter/Socket.php&lt;/b&gt;
  on line &lt;b&gt;332&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td
  colspan=&quot;3&quot; style=&quot;background-color: #aaaaaa;
  text-align: center; font-weight: bold;&quot;&gt;Exception
  trace&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td style=&quot;text-align:
  center; background: #cccccc; width:20px; font-weight:
  bold;&quot;&gt;#&lt;/td&gt;&lt;td style=&quot;text-align: center;
  background: #cccccc; font-weight:
  bold;&quot;&gt;Function&lt;/td&gt;&lt;td style=&quot;text-align:
  center; background: #cccccc; font-weight: bold; in
  <b>/home/content/08/10141608/html/testing/faceapi/HTTP/Request2/SocketWrapper.php</b>
  on line <b>134</b><br /></p>
</blockquote>

<p>Let me know if you need any further information...</p>

<p>Thanks...</p>",,0,2,,2017-04-20 13:30:09.610 UTC,,2017-04-20 14:42:25.257 UTC,2017-04-20 14:42:25.257 UTC,,5860924,,5860924,1,1,php|httprequest|face-detection|face-api,279
Is there a way to get bounding boxes from the Microsoft's custom vision object detection model.pb file?,54048657,Is there a way to get bounding boxes from the Microsoft's custom vision object detection model.pb file?,"<p>Is there a way to get bounding boxes of a particular object detected via Microsoft custom vision model.pb file? I know we can get that via API calls to the azure custom vision service. 
Say for example, we can get the bounding boxes from the ssd frozen inference graph.pb file as there are tensors present. Can we do the same for custom vision's model.pb file? </p>

<p>This is the code that I am using the print out the operations for a tensorflow model and the output.</p>

<pre><code>detection_graph = tf.Graph()

with detection_graph.as_default():
    graph_def = tf.GraphDef()
    with tf.gfile.GFile('model.pb,'rb') as fid:
        serialized_graph = fid.read()
        graph_def.ParseFromString(serialized_graph)
        tf.import_graph_def(graph_def, name='')

with tf.Session(graph=detection_graph) as sess:
    ops = tf.get_default_graph().get_operations()
    for op in ops:
        for output in op.outputs:
            print(output.name)


Placeholder:0
layer1_conv/weights:0
layer1_conv/weights/read:0
layer1_conv/Conv2D:0
layer1_conv/biases:0
layer1_conv/biases/read:0
layer1_conv/BiasAdd:0
layer1_leaky/alpha:0
layer1_leaky/mul:0
layer1_leaky:0
pool1:0
layer2_conv/weights:0
layer2_conv/weights/read:0
layer2_conv/Conv2D:0
layer2_conv/biases:0
layer2_conv/biases/read:0
layer2_conv/BiasAdd:0
layer2_leaky/alpha:0
layer2_leaky/mul:0
layer2_leaky:0
pool2:0
layer3_conv/weights:0
layer3_conv/weights/read:0
layer3_conv/Conv2D:0
layer3_conv/biases:0
layer3_conv/biases/read:0
layer3_conv/BiasAdd:0
layer3_leaky/alpha:0
layer3_leaky/mul:0
layer3_leaky:0
pool3:0
layer4_conv/weights:0
layer4_conv/weights/read:0
layer4_conv/Conv2D:0
layer4_conv/biases:0
layer4_conv/biases/read:0
layer4_conv/BiasAdd:0
layer4_leaky/alpha:0
layer4_leaky/mul:0
layer4_leaky:0
pool4:0
layer5_conv/weights:0
layer5_conv/weights/read:0
layer5_conv/Conv2D:0
layer5_conv/biases:0
layer5_conv/biases/read:0
layer5_conv/BiasAdd:0
layer5_leaky/alpha:0
layer5_leaky/mul:0
layer5_leaky:0
pool5:0
layer6_conv/weights:0
layer6_conv/weights/read:0
layer6_conv/Conv2D:0
layer6_conv/biases:0
layer6_conv/biases/read:0
layer6_conv/BiasAdd:0
layer6_leaky/alpha:0
layer6_leaky/mul:0
layer6_leaky:0
pool6:0
layer7_conv/weights:0
layer7_conv/weights/read:0
layer7_conv/Conv2D:0
layer7_conv/biases:0
layer7_conv/biases/read:0
layer7_conv/BiasAdd:0
layer7_leaky/alpha:0
layer7_leaky/mul:0
layer7_leaky:0
layer8_conv/weights:0
layer8_conv/weights/read:0
layer8_conv/Conv2D:0
layer8_conv/biases:0
layer8_conv/biases/read:0
layer8_conv/BiasAdd:0
layer8_leaky/alpha:0
layer8_leaky/mul:0
layer8_leaky:0
m_outputs0/weights:0
m_outputs0/weights/read:0
m_outputs0/Conv2D:0
m_outputs0/biases:0
m_outputs0/biases/read:0
m_outputs0/BiasAdd:0
model_outputs:0
</code></pre>

<p>The <code>Placeholder:0</code> and <code>model_outputs:0</code> are the inputs and the outputs. The <code>Placeholder:0</code> takes a tensor of shape <code>(?,416,416,3)</code> and the <code>model_outputs:0</code> outputs a tensor of shape <code>(1, 13, 13, 30)</code>. If I am detecting just a single object, how do I get the bounding boxes from the <code>model_outputs:0</code> tensor. </p>

<p>Where am I going wrong? Any suggestions are welcome.</p>",,1,0,,2019-01-05 03:14:09.023 UTC,1,2019-02-26 13:32:45.593 UTC,,,,,3931830,1,2,tensorflow|deep-learning|object-detection|bounding-box|microsoft-custom-vision,222
Recognise fields from identity card,51590523,Recognise fields from identity card,"<p>The question has been <a href=""https://stackoverflow.com/questions/25684190/how-to-read-words-from-identity-card-using-tesseract-ocr"">asked</a> and the commercial solution from BlinkID is working well for me.</p>

<p>I am trying to extend the application to recognise more type of document. For the moment I am using Google Cloud Vision with a <a href=""https://en.wikipedia.org/wiki/Bigram"" rel=""nofollow noreferrer"">Bigram</a> algorithm to (heuristic) detect the keyword (<code>Nom</code>, <code>Prénom</code>, <code>Née</code> etc) but the result is not optimal: sometimes the fields are in the same paragraph, sometimes not; some keywords art too short (<code>à</code>) and not always visible etc.</p>

<p>Having no background in computer vision, I'm looking for a way to improve the current solution or a better one.</p>

<p><a href=""https://i.stack.imgur.com/QAwd6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QAwd6.jpg"" alt=""enter image description here""></a></p>",,0,2,,2018-07-30 09:10:33.207 UTC,,2019-01-29 11:26:48.733 UTC,,,,,1244262,1,3,ocr|google-cloud-vision,322
Camera API Text Recognising android Take picture failed Only Oreo and Pie Devices,55665445,Camera API Text Recognising android Take picture failed Only Oreo and Pie Devices,"<p>I am working on a live capture text detection using firebase google vision api.</p>

<blockquote>
  <p>Problem was when CameraSource preview start then get call back called.
  I got error for take picture failed in Oreo and pie Device.</p>
</blockquote>

<p>Am using my reference: 
<a href=""https://medium.com/@prakash_pun/text-recognition-for-android-using-google-mobile-vision-a8ffabe3f5d6"" rel=""nofollow noreferrer"">https://medium.com/@prakash_pun/text-recognition-for-android-using-google-mobile-vision-a8ffabe3f5d6</a>
The code to capture image is in face track fragment java file:-</p>

<pre><code>    textRecognizer.setProcessor(new Detector.Processor&lt;TextBlock&gt;() {
                    @Override
                    public void release() {
                    }

                    /*Detect all the text from camera using TextBlock and the values into a stringBuilder
                    which will then be process in conversion*/
                    @Override
                    public void receiveDetections(Detector.Detections&lt;TextBlock&gt; detections) {
                        Handler handler = new Handler(Looper.getMainLooper());
                        handler.post(new Runnable() {
                            @Override
                            public void run() {
                                try {
                                    AudioManager audio = (AudioManager) getActivity().getSystemService(Context.AUDIO_SERVICE);
                                    currentVolume = audio.getStreamVolume(AudioManager.STREAM_SYSTEM);
                                    audio.setStreamVolume(AudioManager.STREAM_SYSTEM, 0, AudioManager.FLAG_REMOVE_SOUND_AND_VIBRATE);
                                    if (cameraSource != null) {
                                        cameraSource.takePicture(null, new CameraSource.PictureCallback() {
                                            public void onPictureTaken(final byte[] bytes) {
                                                if (isVolumeChanged) {
                                                    audio.setStreamVolume(AudioManager.STREAM_SYSTEM, currentVolume, AudioManager.FLAG_REMOVE_SOUND_AND_VIBRATE);
                                                }
                                                orientation = Exif.getOrientation(bytes);
                                                if (bytes != null) {
                                                    bitmap = BitmapFactory.decodeByteArray(bytes, 0, bytes.length);
                                                    Bitmap rotateBitmap = getRotation(bitmap);
                                                    croppedBmp = cropImage(rotateBitmap);
                                                }
                                            }
                                        });
                                    }
                                } catch (Exception e) {

                                    e.printStackTrace();
                                }
                            }
                        });

                        if (croppedBmp != null) {
                            Frame frame = new Frame.Builder().setBitmap(croppedBmp).build();
                            final SparseArray&lt;TextBlock&gt; items = textRecognizer.detect(frame); 
}
</code></pre>

<blockquote>
  <p>Error: 15497-15497/com.hagglelens W/System.err:
  java.lang.RuntimeException: takePicture failed 2019-04-13 16:16:39.469
  15497-15497/com.hagglelens W/System.err:     at
  android.hardware.Camera.native_takePicture(Native Method) 2019-04-13
  16:16:39.469 15497-15497/com.hagglelens W/System.err:     at
  android.hardware.Camera.takePicture(Camera.java:1507) 2019-04-13
  16:16:39.469 15497-15497/com.hagglelens W/System.err:     at
  com.google.android.gms.vision.CameraSource.takePicture(Unknown
  Source:60) 2019-04-13 16:16:39.470 15497-15497/com.hagglelens
  W/System.err:     at
  com.hagglelens.view.fragment.livecapture.LiveCaptureFragment$3$1.run(LiveCaptureFragment.java:239)
  2019-04-13 16:16:39.470 15497-15497/com.hagglelens W/System.err:<br>
  at android.os.Handler.handleCallback(Handler.java:789) 2019-04-13
  16:16:39.470 15497-15497/com.hagglelens W/System.err:     at
  android.os.Handler.dispatchMessage(Handler.java:98) 2019-04-13
  16:16:39.470 15497-15497/com.hagglelens W/System.err:     at
  android.os.Looper.loop(Looper.java:164) 2019-04-13 16:16:39.471
  15497-15497/com.hagglelens W/System.err:     at
  android.app.ActivityThread.main(ActivityThread.java:6673) 2019-04-13
  16:16:39.471 15497-15497/com.hagglelens W/System.err:     at
  java.lang.reflect.Method.invoke(Native Method) 2019-04-13 16:16:39.471
  15497-15497/com.hagglelens W/System.err:     at
  com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)
  2019-04-13 16:16:39.471 15497-15497/com.hagglelens W/System.err:<br>
  at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:769)</p>
</blockquote>

<p><strong>Help me to solve this issues!</strong></p>",,0,0,,2019-04-13 12:29:35.560 UTC,1,2019-04-13 12:29:35.560 UTC,,,,,11065564,1,0,android|firebase|android-camera|surfaceview|camera-api,28
Android library behaves diferently when importing aar instead of module,56068100,Android library behaves diferently when importing aar instead of module,"<p>I made an Android library that I use locally via .aar</p>

<p>In the library prohect, it looks like this:</p>

<ul>
<li>AAR Demo project -
: app
: library (source code)</li>
</ul>

<p>The library consists of a QR scanner using <code>Google Play Services Vision</code></p>

<p>then, I export the .aar file and then I import into the other app using </p>

<p><code>Android Studio new / module / from aar/jar</code></p>

<ul>
<li>Other app project -
: otherApp
: library (aar)</li>
</ul>

<p>There are a couple of things that the library behaves diferently:</p>

<p>1- The QR scanner doesn't work AT ALL if used as a .aar but it works perfectly fine if imported via source code, I tried importing the source code as a module into <code>:otherApp</code> and it works fine, but using the .aar doesnt work, at all.</p>

<p>2- In order to use the library, I also have to include the <code>Google Play Services Vision</code> library into the <code>:otherApp</code> if I dont import it, I get a <code>ClassNotFoundException</code>, I tried defining Google Vision as a transitive dependency like this:</p>

<pre><code>implementation('com.google.android.gms:play-services-vision:17.0.2') {
   transitive = true
}
</code></pre>

<p>but the <code>:otherApp</code> project doesnt seem to read it, But i don't have to define it in the <code>:app</code> module inside the library source code project.</p>

<p>I want to know if there is a difference between using a library as .aar vs using the source code in a module (R8 / ProGuard is disabled in all projects) and if there is (because seems like that is the case), how can I make the .aar work the same as the library imported via source code?</p>",,0,0,,2019-05-09 21:53:40.917 UTC,,2019-05-09 21:53:40.917 UTC,,,,,4664987,1,0,android|android-library|aar,11
Express: 404 NotFoundError due to urlEncodedParser,51117724,Express: 404 NotFoundError due to urlEncodedParser,"<p>When I submit a file upload form on my website, why would I get the following 404 NotFoundError? I think the error is due to the urlencodedParser middleware not being installed (as that is in the last line of the traceback), but even after I installed and added the middleware to app.js, I still get the same error. </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>NotFoundError: Not Found
    at /Users/katiegu/jobfinderapp/Resume-to-Job/app.js:54:8
    at Layer.handle [as handle_request] (/Users/katiegu/jobfinderapp/Resume-to-Job/node_modules/express/lib/router/layer.js:95:5)
    at trim_prefix (/Users/katiegu/jobfinderapp/Resume-to-Job/node_modules/express/lib/router/index.js:317:13)
    at /Users/katiegu/jobfinderapp/Resume-to-Job/node_modules/express/lib/router/index.js:284:7
    at Function.process_params (/Users/katiegu/jobfinderapp/Resume-to-Job/node_modules/express/lib/router/index.js:335:12)
    at next (/Users/katiegu/jobfinderapp/Resume-to-Job/node_modules/express/lib/router/index.js:275:10)
    at /Users/katiegu/jobfinderapp/Resume-to-Job/node_modules/express/lib/router/index.js:635:15
    at next (/Users/katiegu/jobfinderapp/Resume-to-Job/node_modules/express/lib/router/index.js:260:14)
    at Function.handle (/Users/katiegu/jobfinderapp/Resume-to-Job/node_modules/express/lib/router/index.js:174:3)
    at router (/Users/katiegu/jobfinderapp/Resume-to-Job/node_modules/express/lib/router/index.js:47:12)
    at Layer.handle [as handle_request] (/Users/katiegu/jobfinderapp/Resume-to-Job/node_modules/express/lib/router/layer.js:95:5)
    at trim_prefix (/Users/katiegu/jobfinderapp/Resume-to-Job/node_modules/express/lib/router/index.js:317:13)
    at /Users/katiegu/jobfinderapp/Resume-to-Job/node_modules/express/lib/router/index.js:284:7
    at Function.process_params (/Users/katiegu/jobfinderapp/Resume-to-Job/node_modules/express/lib/router/index.js:335:12)
    at next (/Users/katiegu/jobfinderapp/Resume-to-Job/node_modules/express/lib/router/index.js:275:10)
    at urlencodedParser (/Users/katiegu/jobfinderapp/Resume-to-Job/node_modules/urlencoded-parser/dist/index.js:56:3)</code></pre>
</div>
</div>
</p>

<p>These are my routes (The uploadresume/post was having this error.): </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>var express = require('express');
var router = express.Router();

var home_controller = require('../controllers/homeController');
/* GET home page. */
router.get('/', home_controller.index);
router.get('/login', home_controller.login);
router.post('/login', home_controller.login_post);
router.get('/signup', home_controller.signup);
router.post('/signup', home_controller.signup_post);
router.get('/uploadresume', home_controller.upload_resume);
router.get('/uploadresume/post', home_controller.upload_resume_post);

module.exports = router;</code></pre>
</div>
</div>
</p>

<p>These are my controllers. The upload_resume_post controller takes the file input from the HTML form and sends it to the Google Vision API server. </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>exports.upload_resume_post = function(req, res, next) {
    const vision = require('@google-cloud/vision');
    const client = new vision.ImageAnnotatorClient();

    var reader = new FileReader();
    var file = document.getElementById('file').files;
    var b64url = reader.readAsDataURL(file[0]);
    client.documentTextDetection(b64url)
      .then(results =&gt; {
          const fullTextAnnotation = results[0].fullTextAnnotation;
          console.log(fullTextAnnotation.text);
      })
      .catch(err =&gt; {
          console.error('ERROR:', err);
      });
};</code></pre>
</div>
</div>
</p>

<p>This is the view where I have my form for uploading the file:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>doctype html
html(lang='en')
  head
    meta(charset='utf-8')
    meta(name='viewport', content='width=device-width, initial-scale=1')
    link(rel='stylesheet', href='https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css')
    script(src='https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js')
    script(src='https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js')
    link(rel='stylesheet', href='/stylesheets/upload_resume.css')

  body
    h1 Upload Resume
    form(method='post', action='/uploadresume/post', enctype='multipart/form-data')
      label(for='file') Choose File to Upload.
      br
      br
      input(type='file', id='file', accept='image/*')
      br
      br
      input(type='submit', id='submit', value='Submit')</code></pre>
</div>
</div>
</p>

<p>This is my app.js. Is the way I imported and added my bodyParser and urlencodedParser middleware correct?</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>var express = require('express');
var app = express();
var path = require('path');
var createError = require('http-errors');
var cookieParser = require('cookie-parser');
var logger = require('morgan');
var bodyParser = require('body-parser');
var urlencodedParser = require('urlencoded-parser');
var mongoose = require('mongoose');
var session = require('express-session');
var MongoStore = require('connect-mongo')(session);


var mongodb = 'mongodb:/katie_gu:database4project@ds163850.mlab.com:63850/jobfinderapp';
mongoose.connect(mongodb);
mongoose.Promise = global.Promise;
var db = mongoose.connection;

db.on('error', console.error.bind(console, 'connection error:'));
// view engine setup
app.set('views', path.join(__dirname, 'views'));
app.set('view engine', 'pug');

app.use(logger('dev'));
app.use(express.json());
app.use(express.urlencoded({ extended: true }));
app.use(cookieParser());
app.use(express.static(path.join(__dirname, 'public')));
app.use(session({
  secret: 'work hard',
  resave: true,
  saveUninitialized: false,
  store: new MongoStore({
    mongooseConnection: db
  })
}));
app.use(bodyParser.json());
app.use(bodyParser.urlencoded({ extended: true }));
app.use(urlencodedParser);

var indexRouter = require('./routes/index');
var catalogRouter = require('./routes/catalog');
var usersRouter = require('./routes/users');
var profileRouter = require('./routes/profile');


app.use('/', indexRouter);
app.use('/catalog', catalogRouter);
app.use('/users', usersRouter);
app.use('/profile', profileRouter);

// catch 404 and forward to error handler
app.use(function(req, res, next) {
  next(createError(404));
});

// error handler
app.use(function(err, req, res, next) {
  // set locals, only providing error in development
  res.locals.message = err.message;
  res.locals.error = req.app.get('env') === 'development' ? err : {};

  // render the error page
  res.status(err.status || 500);
  res.render('error');
});

module.exports = app;</code></pre>
</div>
</div>
</p>",,0,0,,2018-06-30 18:59:06.840 UTC,,2018-06-30 18:59:06.840 UTC,,,,,9987452,1,0,javascript|node.js|express,104
Cloud Vision returning Chinese name on a logo detection,52477690,Cloud Vision returning Chinese name on a logo detection,"<p>We are using Google vision for a while now. The Logo detection doesn't detect so many things but recently the Infinity car brand was detected as ""英菲尼迪"" logo.
We checked the translation and it means Infiniti! Which is the good point ;-)
Did anybody experience the same issue? And found a way to return the result in a specific language?
Thanks </p>",,0,1,,2018-09-24 10:49:44.587 UTC,1,2018-09-24 10:49:44.587 UTC,,,,,10407348,1,1,google-cloud-vision,47
Sending a request to Google Vision API,44556228,Sending a request to Google Vision API,"<p>I want to send a json object while by using the http POST method to the Google Vision API. I am using the following code:</p>

<pre><code>URL url = new URL(""https://vision.googleapis.com/v1/images:annotate?key=&lt;API-KEY&gt;""); 
HttpsURLConnection http = (HttpsURLConnection)url.openConnection();
http.setDoOutput(true);
http.setRequestMethod(""POST""); 
http.setRequestProperty(""Content-Type"", ""application/json""); 
http.connect();

DataOutputStream wr = new DataOutputStream(http.getOutputStream());
wr.writeBytes(request.toString());
Log.v(""JSON"",request.toString());
wr.flush();
wr.close();
</code></pre>

<p>I getting a bad request error. Need help with this. The format of my json object (request) is as follows:</p>

<pre><code>{""imageContext"":"""",
 ""requests"":""
    {""image"":
        {""content"":""...""},
    ""features"":
        {""type"":""WEB DETECTION""}
        {""maxResults"":10}
    }
}
</code></pre>",,2,1,,2017-06-14 23:30:15.840 UTC,,2017-06-16 16:37:47.510 UTC,2017-06-15 15:16:31.090 UTC,,5231007,,8162708,1,0,java|google-cloud-vision,769
Moving Firebase (GoogleService-Info.plist & API Key) to Production on iOS App Store?,54282675,Moving Firebase (GoogleService-Info.plist & API Key) to Production on iOS App Store?,"<p>My question revolves around how to transition the information in the GoogleService-Info.plist file to Production (i.e. deployed on the iOS App Store)?</p>

<p>My predicament is pretty simple: I am developing a native iOS app in Swift. I am using Firebase ML Kit (Google Cloud Vision API). When I first set up my project in Firebase, I was prompted to download the GoogleService-Info.plist file, which I did. It is on the Desktop on my local computer, and all my local development works fine.</p>

<p>But, now I am ready to deploy this app to the App Store. Obviously, the App Store app is not going to fetch the credentials from my local Desktop. I am confused on how to transition my credentials to Production. I have read all of the Google Cloud docs (Securing API Keys, Service Accounts, etc.) but I have not found a simple, well-explained solution on how to approach this. If you are able to explain this to me as opposed to link to one of these resources that would be incredibly helpful. Thank you very much.</p>",,1,1,,2019-01-21 01:58:56.010 UTC,1,2019-01-21 06:43:47.813 UTC,,,,,7601384,1,1,ios|firebase|google-cloud-platform|firebase-mlkit,144
async action never completes in asp.net,39865311,async action never completes in asp.net,"<p>I'm trying to use the Microsoft Face Api and I have stuck with this code for several hours now:</p>

<pre><code>public async Task&lt;List&lt;Rectangle&gt;&gt; ApiDetect(int Id)
{
    FaceServiceClient f = new FaceServiceClient(""the api key"");
    Microsoft.ProjectOxford.Face.Contract.Face[] faces = await  f.DetectAsync($""http://www.example.com/uphotos/raw/{Id}.jpg"");
    // here it stuck
    List&lt;Rectangle&gt; lst = faces.Select(r =&gt; new Rectangle
    {
        Height = r.FaceRectangle.Height,
        Width = r.FaceRectangle.Width,
        X = r.FaceRectangle.Left,
        Y = r.FaceRectangle.Top
    }).ToList();
    return lst;
}
</code></pre>

<p>What am I doing wrong here?</p>",39865788,2,2,,2016-10-05 04:10:52.283 UTC,0,2016-10-05 06:23:45.330 UTC,2016-10-05 06:23:45.330 UTC,,3096477,,1079221,1,0,c#|asp.net|async-await|face-api,339
Google vision api - 503 Deadline Exceeded,54536079,Google vision api - 503 Deadline Exceeded,"<p>Getting the below-mentioned error on calling Google Vision API</p>

<pre><code>Traceback (most recent call last):
  File ""/Users/kumarnitin/anaconda3/envs/chatty/lib/python3.6/site-packages/google/api_core/grpc_helpers.py"", line 57, in error_remapped_callable
    return callable_(*args, **kwargs)
  File ""/Users/kumarnitin/anaconda3/envs/chatty/lib/python3.6/site-packages/grpc/_channel.py"", line 550, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File ""/Users/kumarnitin/anaconda3/envs/chatty/lib/python3.6/site-packages/grpc/_channel.py"", line 467, in _end_unary_response_blocking
    raise _Rendezvous(state, None, None, deadline)
grpc._channel._Rendezvous: &lt;_Rendezvous of RPC that terminated with:
    status = StatusCode.UNAVAILABLE
    details = ""Deadline Exceeded""
    debug_error_string = ""{""created"":""@1549374414.480926000"",""description"":""Deadline Exceeded"",""file"":""src/core/ext/filters/deadline/deadline_filter.cc"",""file_line"":69,""grpc_status"":14}""


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/image_ocr/views.py"", line 22, in post
    succ, resp = OCRHandler.handle(data)
  File ""/image_ocr/handler.py"", line 22, in handle
    res = OCRExtraction(id_type, res).handler()
  File ""/image_ocr/extractor.py"", line 122, in handler
    response = VisionOCR.detect_document(self.img_path)
  File ""/image_ocr/extractor.py"", line 38, in detect_document
    image=image, timeout=30)
  File ""/Users/kumarnitin/anaconda3/envs/chatty/lib/python3.6/site-packages/google/cloud/vision_helpers/decorators.py"", line 101, in inner
    response = self.annotate_image(request, retry=retry, timeout=timeout)
  File ""/Users/kumarnitin/anaconda3/envs/chatty/lib/python3.6/site-packages/google/cloud/vision_helpers/__init__.py"", line 72, in annotate_image
    r = self.batch_annotate_images([request], retry=retry, timeout=timeout)
  File ""/Users/kumarnitin/anaconda3/envs/chatty/lib/python3.6/site-packages/google/cloud/vision_v1/gapic/image_annotator_client.py"", line 234, in batch_annotate_images
    request, retry=retry, timeout=timeout, metadata=metadata
  File ""/Users/kumarnitin/anaconda3/envs/chatty/lib/python3.6/site-packages/google/api_core/gapic_v1/method.py"", line 143, in __call__
    return wrapped_func(*args, **kwargs)
  File ""/Users/kumarnitin/anaconda3/envs/chatty/lib/python3.6/site-packages/google/api_core/timeout.py"", line 102, in func_with_timeout
    return func(*args, **kwargs)
  File ""/Users/kumarnitin/anaconda3/envs/chatty/lib/python3.6/site-packages/google/api_core/grpc_helpers.py"", line 59, in error_remapped_callable
    six.raise_from(exceptions.from_grpc_error(exc), exc)
  File ""&lt;string&gt;"", line 3, in raise_from
google.api_core.exceptions.ServiceUnavailable: 503 Deadline Exceeded
</code></pre>",,0,0,,2019-02-05 13:58:31.723 UTC,,2019-03-05 08:33:46.403 UTC,,,,,4575486,1,0,google-vision,75
Specific area from SurfaceView,53341184,Specific area from SurfaceView,"<p>I need a small help. I have added Google Vision API to detect text from image and for that, the image comes from camera, so it is dynamic. The camera is laid on SurfaceView and text, that is found on the surfaceView is captured. I want the feature where SurfaceView captures the whole image, but the text is taken only from specific area defined by me. To say in short, ""add limit in the camera (Like a rectangle) so that the data/text that the image processes is from that rectangle only""</p>

<pre><code>mCameraSource = new CameraSource.Builder(this, textRecognizer)
        .setFacing(CameraSource.CAMERA_FACING_BACK)
        .setRequestedPreviewSize(1280, 1024)
        .setAutoFocusEnabled(true)
        .setRequestedFps(2.0f)
        .build();

mSurfaceView.getHolder().addCallback(new SurfaceHolder.Callback() {
    @Override
    public void surfaceCreated(SurfaceHolder holder) {
        try {
            if (ActivityCompat.checkSelfPermission(RechargeActivity.this, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {

                ActivityCompat.requestPermissions(RechargeActivity.this,
                        new String[]{Manifest.permission.CAMERA},
                        RequestCameraPermissionID);
                return;
            }
            mCameraSource.start(mSurfaceView.getHolder());
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    @Override
    public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {

    }

    @Override
    public void surfaceDestroyed(SurfaceHolder holder) {
        mCameraSource.stop();
    }
});
</code></pre>",,0,0,,2018-11-16 15:47:42.020 UTC,1,2018-11-16 15:47:42.020 UTC,,,,,8237551,1,0,android,15
card.io custom border scanning for ionic/cordova,37085795,card.io custom border scanning for ionic/cordova,"<p>I want to use the border the feature that card.io has. 
I want the feature where in the user puts their camera over it and i get a perfect image so that i can scan it and implement Google cloud vision on it.
I tried using the cordova plugin also but it's not working and as the developer is not supporting it it's a little tough for me to create a whole system on top it.</p>",,0,4,,2016-05-07 07:30:02.537 UTC,,2016-05-07 07:30:02.537 UTC,,,,,6044934,1,0,android|angularjs|cordova|ionic-framework|card.io,268
Multi-threading python calls to Google Cloud Vision API,47428173,Multi-threading python calls to Google Cloud Vision API,"<p>I am running a server in a Docker container supported by a macOS machine, from which I need to send several images for processing by the Google Cloud Vision API.</p>

<p>It is imperative for me to be able to minimize the time spent uploading and processing the images.</p>

<p>I started by wrapping calls to GCV in a <code>Queue.Queue</code> and <code>Threading.Thread</code>, but this sporadically crashes my code (not trapped by a python <code>Exception</code>) thus:</p>

<pre><code>E1121 14:15:10.902211037   25448 sync_posix.c:38]            assertion failed: pthread_mutex_destroy(mu) == 0
</code></pre>

<p>According to several github threads this is a gRPC-inspired (or httplib?) bug/feature, but I can't find steps to an easy workaround - see e.g. <a href=""https://github.com/grpc/grpc/issues/11184"" rel=""nofollow noreferrer"">https://github.com/grpc/grpc/issues/11184</a> and <a href=""https://github.com/grpc/grpc/issues/10909"" rel=""nofollow noreferrer"">https://github.com/grpc/grpc/issues/10909</a></p>

<p>Given that this seems to be a commonly-experienced problem with no clear solution, what is the best way to mitigate it?</p>

<p>Is it as simple as using a single Batch call (but what about the overall speed?) to GCV? Is there no other way to safely thread the calls?</p>

<h2>Update:</h2>

<p>With fear of breakage, I embarked on a rollback of gRPC to v1.2.x a la</p>

<p><a href=""https://github.com/grpc/grpc/issues/10909#issuecomment-302581954"" rel=""nofollow noreferrer"">https://github.com/grpc/grpc/issues/10909#issuecomment-302581954</a></p>

<p>Except that I had to add <code>/usr/local/lib</code> to <code>LD_LIBRARY_PATH</code> in my Docker container in order to pick up <code>libprotobuf.so.12</code>.</p>

<p>I made no changes to any other python packages.</p>

<p>I then did:</p>

<pre><code>pip install grpcio==1.2.1
</code></pre>

<p>Obviously this gets expensive, but the crash rate is &lt;1/20 calls (and counting) compared to 1/3 calls before.</p>

<p><strong>Now:</strong> How can I test <em>conclusively</em> that this is fixed?</p>",47474119,1,0,,2017-11-22 06:29:40.273 UTC,,2017-11-24 13:14:18.840 UTC,2017-11-22 15:25:52.947 UTC,,1021819,,1021819,1,3,python|multithreading|google-cloud-vision,439
Can I receive a boudingPoly for LABEL_DETECTION results?,35737830,Can I receive a boudingPoly for LABEL_DETECTION results?,"<p>How can this be completed with the Google Vision-API please?</p>

<ol>
<li>send image to vision-api</li>
<li>request: 'features': [{': 'LABEL_DETECTION','maxResults': 10,}]           </li>
<li>receive the labels in particular the one I'm interest in is a ""clock""</li>
<li>receive the boundingPoly so that I know the exact location of the clock within the image</li>
<li>having received the boundingPoly I would want to use it to create a dynamic AR marker to be tracked by the AR library</li>
</ol>

<p>Currently it doesn't look like Google Vision-API supports a boudingPoly for LABELS hence the question if there is a way to solve it with the Vision-API.</p>",,2,0,,2016-03-02 03:15:35.063 UTC,4,2018-04-05 09:03:11 UTC,,,,,91799,1,4,google-cloud-platform|google-api-client|google-cloud-vision,441
How to handle efficiently Google Vision API Client Lib response?,52017315,How to handle efficiently Google Vision API Client Lib response?,"<p>I'm new to Google Vision API Client Lib</p>

<p>I'm using Vision API Client Lib for PHP to detect text in images, this is my code:</p>

<pre><code>&lt;?php
require 'vendor/autoload.php';

use Google\Cloud\Vision\VisionClient;
function object_to_array($object) {
    return (array) $object;
}
$vision = new VisionClient(
['keyFile' =&gt; json_decode(file_get_contents(""smartcity-credentials.json""), true)]
);
$img = file_get_contents('img2.jpg');
$image=$vision-&gt;image($img,['DOCUMENT_TEXT_DETECTION']);
$result=$vision-&gt;annotate($image);
$res=object_to_array($result);
var_dump($res);
?&gt;
</code></pre>

<p>All I need is using response as an array for handling, but $result returns something likes array of object/object ( sorry because I don't know much of OOP/Object)</p>

<p>Although i convert $result to array $res, but if I use foreach loop</p>

<pre><code>foreach ($res as $key=&gt;$value){
    echo $value;
    echo '&lt;br&gt;';
}
</code></pre>

<p>I get this</p>

<blockquote>
  <p>Catchable fatal error: Object of class Google\Cloud\Vision\Annotation\Document could not be converted to string</p>
</blockquote>

<p>How do we get value (text detected) in above response for using ?. </p>",,2,0,,2018-08-25 12:28:23.500 UTC,,2018-10-23 21:02:01.563 UTC,2018-08-26 14:25:05.433 UTC,,1016716,,8688875,1,1,php|json|google-cloud-platform|response|google-cloud-vision,340
Line by line data from Google cloud vision API OCR,50673749,Line by line data from Google cloud vision API OCR,"<p>I have scanned PDFs (image based) of bank statements.
Google vision API is able to detect the text pretty accurately but it returns blocks of text and I need line by line text (bank transactions).
Any idea how to go about it?</p>",50675950,2,2,,2018-06-04 05:13:38.857 UTC,3,2019-01-30 14:49:18.370 UTC,,,,,8009119,1,0,python|pdf|ocr|google-cloud-vision,857
When first open camera I don't have permissions,45546462,When first open camera I don't have permissions,"<p>I have problem with access to camera first time. I installed
 my app. Next go to scan to QR Code. I use to scan QR Code google vision. App show dialog, which show about the permissions, next click ""Allow"",but camera doesn't open. But I go back activity and go to activity which scan QR Code, camera open. </p>

<p>my AdnroidManifest.xml</p>

<p>
</p>

<p>my class </p>

<pre><code>public class ScanQrCodeActivity extends AppCompatActivity {

    protected void attachBaseContext(Context context) {
        super.attachBaseContext(CalligraphyContextWrapper.wrap(context));
    }

    private static final String TAG = ""Barcode-reader"";
    // intent request code to handle updating play services if needed.
    private static final int RC_HANDLE_GMS = 9001;
    // permission request codes need to be &lt; 256
    private static final int RC_HANDLE_CAMERA_PERM = 2;

    private DatabaseHandler helper;
    private TextView title_app;
    private CameraSource mCameraSource;
    private CameraSourcePreview mPreview;
    private GraphicOverlay&lt;BarcodeGraphic&gt; mGraphicOverlay;
    String passwordString, eightChars, barcodeString, decodedBarcodeValue, OTP;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_scan_qr_code);
        Toolbar toolbar = (Toolbar) findViewById(R.id.toolbar);
        setSupportActionBar(toolbar);
        toolbar.setNavigationIcon(R.drawable.ic_arrow_back);
        title_app = (TextView) findViewById(R.id.toolbar_title);


        helper = new DatabaseHandler(this);

        mPreview = (CameraSourcePreview) findViewById(R.id.preview);
        mGraphicOverlay = (GraphicOverlay&lt;BarcodeGraphic&gt;) findViewById(R.id.graphicOverlay);
        eightChars = getIntent().getStringExtra(""eightChars"");
        passwordString = getIntent().getStringExtra(""passwordString"");

        int rc = ActivityCompat.checkSelfPermission(this, Manifest.permission.CAMERA);
        if (rc == PackageManager.PERMISSION_GRANTED) {
            createCameraSource();
        } else {
            requestCameraPermission();
        }

        Snackbar snackbar = Snackbar.make(mGraphicOverlay, R.string.zeskanuj_wygenerowany_kod, Snackbar.LENGTH_INDEFINITE);
        View mView = snackbar.getView();
        TextView mTextView = (TextView) mView.findViewById(android.support.design.R.id.snackbar_text);

        if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.JELLY_BEAN_MR1)
            mTextView.setTextAlignment(View.TEXT_ALIGNMENT_CENTER);
        else
            mTextView.setGravity(Gravity.CENTER_HORIZONTAL);
        snackbar.show();

    }

    public boolean onCreateOptionsMenu(Menu menu) {
        MenuInflater inflater = getMenuInflater();
        inflater.inflate(R.menu.stage2, menu);
        return true;
    }

    @Override
    public boolean onOptionsItemSelected(MenuItem item) {
        if (item.getItemId() == android.R.id.home) {
            finish();
        }
        return super.onOptionsItemSelected(item);
    }

    private void requestCameraPermission() {
        Log.w(TAG, ""Camera permission is not granted. Requesting permission"");

        final String[] permissions = new String[]{Manifest.permission.CAMERA};

        if (!ActivityCompat.shouldShowRequestPermissionRationale(this,
                Manifest.permission.CAMERA)) {
            ActivityCompat.requestPermissions(this, permissions, RC_HANDLE_CAMERA_PERM);
            return;
        }

        final Activity thisActivity = this;

        View.OnClickListener listener = new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                ActivityCompat.requestPermissions(thisActivity, permissions,
                        RC_HANDLE_CAMERA_PERM);
            }
        };

        findViewById(R.id.topLayout).setOnClickListener(listener);
        Snackbar.make(mGraphicOverlay, ""Aby móc wykryć Qr Koda potrzebny jest dostęp do kamery."",
                Snackbar.LENGTH_INDEFINITE)
                .setAction(R.string.ok, listener)
                .show();

    }

    @SuppressLint(""InlinedApi"")
    private void createCameraSource() {

        boolean autoFocus = false;
        boolean useFlash = false;
        Context context = getApplicationContext();

        BarcodeDetector barcodeDetector = new BarcodeDetector.Builder(context).build();
        BarcodeTrackerFactory barcodeFactory = new BarcodeTrackerFactory(mGraphicOverlay);
        barcodeDetector.setProcessor(
                new MultiProcessor.Builder&lt;&gt;(barcodeFactory).build());

        barcodeFactory.setNewBarcodeListener(new BarcodeTrackerFactory.OnNewBarcodeListener() {
            @Override
            public void onNewItem(final Barcode item) {
                Log.d(""BarcodeFound"", ""Found new barcode! "" + item.rawValue);

                title_app.post(new Runnable() {
                    @Override
                    public void run() {


        if (!barcodeDetector.isOperational())

        {
            IntentFilter lowstorageFilter = new IntentFilter(Intent.ACTION_DEVICE_STORAGE_LOW);
            boolean hasLowStorage = registerReceiver(null, lowstorageFilter) != null;

            if (hasLowStorage) {
                Toast.makeText(this, R.string.low_storage_error, Toast.LENGTH_LONG).show();
                Log.w(TAG, getString(R.string.low_storage_error));
            }
        }

        CameraSource.Builder builder = new CameraSource.Builder(getApplicationContext(), barcodeDetector)
                .setFacing(CameraSource.CAMERA_FACING_BACK)
                .setRequestedPreviewSize(1600, 1024)
                .setRequestedFps(15.0f);


        if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.ICE_CREAM_SANDWICH)

        {
            builder = builder.setFocusMode(
                    autoFocus ? Camera.Parameters.FOCUS_MODE_CONTINUOUS_PICTURE : null);
        }

        mCameraSource = builder
                .setFlashMode(useFlash ? Camera.Parameters.FLASH_MODE_TORCH : null)
                .

                        build();
    }


    @Override
    protected void onResume() {
        super.onResume();
        startCameraSource();
    }

    @Override
    protected void onPause() {
        super.onPause();
        if (mPreview != null) {
            mPreview.stop();
        }
    }

    @Override
    protected void onDestroy() {
        super.onDestroy();
        if (mPreview != null) {
            mPreview.release();
        }
    }

    private void startCameraSource() throws SecurityException {
        // check that the device has play services available.
        int code = GoogleApiAvailability.getInstance().isGooglePlayServicesAvailable(
                getApplicationContext());
        if (code != ConnectionResult.SUCCESS) {
            Dialog dlg =
                    GoogleApiAvailability.getInstance().getErrorDialog(this, code, RC_HANDLE_GMS);
            dlg.show();
        }

        if (mCameraSource != null) {
            try {
                mPreview.start(mCameraSource, mGraphicOverlay);
            } catch (IOException e) {
                Log.e(TAG, ""Unable to start camera source."", e);
                mCameraSource.release();
                mCameraSource = null;
            }
        }
    }

}
</code></pre>",,2,5,,2017-08-07 12:03:35.800 UTC,,2017-08-07 13:03:25.890 UTC,,,,,8119193,1,0,android|android-recyclerview,229
Azure Face API returns diminishing confidence for images of the same person,50265754,Azure Face API returns diminishing confidence for images of the same person,"<p>For our application we are using Azure Face API to build training set for each person. We are using ""LargePersonGroup"" for our application. We have created ""Person"" inside group for all user profiles we have. Every time new face is added for a person we ""Train"" the whole group. This is how we have implemented.</p>

<p>Now the issue is when we call ""Identify"" to validate an uploaded image with a certain confidence threshold which is 0.95 (95%) in our case system is returning diminishing confidence even for the same user images. Out of 32 test uploads we got </p>

<p>Below 90:  2</p>

<p>90 -  94 : 12</p>

<p>95 - 100 : 18</p>

<p>Is there any way i can improve the results or is it an issue with Azure Face API. I checked with AWS and their results were far better.</p>

<p>Please help.</p>",,0,3,,2018-05-10 04:57:25.953 UTC,1,2018-05-10 08:08:25.260 UTC,2018-05-10 08:08:25.260 UTC,,8478436,,9768589,1,0,azure|face-recognition|microsoft-cognitive|face-api,138
How to get the result of an SQL query from Big Query in Airflow?,49619897,How to get the result of an SQL query from Big Query in Airflow?,"<p>Using Airflow I want to get the result of an SQL Query fomratted as a pandas DataFrame.</p>

<pre><code>def get_my_query(*args, **kwargs)
    bq_hook = BigQueryHook(bigquery_conn_id='my_connection_id', delegate_to=None)
    my_query = """""" 
                 SELECT col1, col2
                 FROM `my_bq_project.my_bq_dataset.my_table`
                """"""
    df = bq_hook.get_pandas_df(bql=my_query, dialect='standard')
    logging.info('df.head()\n{}'.format(df.head()))
</code></pre>

<p>Above is the python function that I want to execute in a <code>PythonOperator</code>. Here is the DAG:</p>

<pre><code>my_dag = DAG('my_dag',start_date=datetime.today())
start = DummyOperator(task_id='start', dag=my_dag)
end = DummyOperator(task_id='end', dag=my_dag)
work = PythonOperator(task_id='work',python_callable=get_my_query, dag=my_dag)
start &gt;&gt; work &gt;&gt; end
</code></pre>

<p>But, the work step is throwing an exception. Here is the log :</p>

<pre><code>[2018-04-02 20:25:50,506] {base_task_runner.py:98} INFO - Subtask: [2018-04-02 20:25:50,506] {gcp_api_base_hook.py:82} INFO - Getting connection using a JSON key file.
[2018-04-02 20:25:51,035] {base_task_runner.py:98} INFO - Subtask: [2018-04-02 20:25:51,035] {slack_operator.py:70} ERROR - Slack API call failed (%s)
[2018-04-02 20:25:51,070] {base_task_runner.py:98} INFO - Subtask: Traceback (most recent call last):
[2018-04-02 20:25:51,071] {base_task_runner.py:98} INFO - Subtask:   File ""/opt/conda/bin/airflow"", line 28, in &lt;module&gt;
[2018-04-02 20:25:51,072] {base_task_runner.py:98} INFO - Subtask:     args.func(args)
[2018-04-02 20:25:51,072] {base_task_runner.py:98} INFO - Subtask:   File ""/home/airflow/.local/lib/python2.7/site-packages/airflow/bin/cli.py"", line 392, in run
[2018-04-02 20:25:51,073] {base_task_runner.py:98} INFO - Subtask:     pool=args.pool,
[2018-04-02 20:25:51,074] {base_task_runner.py:98} INFO - Subtask:   File ""/home/airflow/.local/lib/python2.7/site-packages/airflow/utils/db.py"", line 50, in wrapper
[2018-04-02 20:25:51,075] {base_task_runner.py:98} INFO - Subtask:     result = func(*args, **kwargs)
[2018-04-02 20:25:51,075] {base_task_runner.py:98} INFO - Subtask:   File ""/home/airflow/.local/lib/python2.7/site-packages/airflow/models.py"", line 1493, in _run_raw_task
[2018-04-02 20:25:51,076] {base_task_runner.py:98} INFO - Subtask:     result = task_copy.execute(context=context)
[2018-04-02 20:25:51,077] {base_task_runner.py:98} INFO - Subtask:   File ""/home/airflow/.local/lib/python2.7/site-packages/airflow/operators/python_operator.py"", line 89, in execute
[2018-04-02 20:25:51,077] {base_task_runner.py:98} INFO - Subtask:     return_value = self.execute_callable()
[2018-04-02 20:25:51,078] {base_task_runner.py:98} INFO - Subtask:   File ""/home/airflow/.local/lib/python2.7/site-packages/airflow/operators/python_operator.py"", line 94, in execute_callable
[2018-04-02 20:25:51,079] {base_task_runner.py:98} INFO - Subtask:     return self.python_callable(*self.op_args, **self.op_kwargs)
[2018-04-02 20:25:51,080] {base_task_runner.py:98} INFO - Subtask:   File ""/home/airflow/.local/lib/python2.7/site-packages/processing/dags/my_dag.py"", line 37, in get_my_query
[2018-04-02 20:25:51,080] {base_task_runner.py:98} INFO - Subtask:     df = bq_hook.get_pandas_df(bql=my_query, dialect='standard')
[2018-04-02 20:25:51,081] {base_task_runner.py:98} INFO - Subtask:   File ""/home/airflow/.local/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 94, in get_pandas_df
[2018-04-02 20:25:51,081] {base_task_runner.py:98} INFO - Subtask:     schema, pages = connector.run_query(bql)
[2018-04-02 20:25:51,082] {base_task_runner.py:98} INFO - Subtask:   File ""/home/airflow/.local/lib/python2.7/site-packages/pandas_gbq/gbq.py"", line 502, in run_query
[2018-04-02 20:25:51,082] {base_task_runner.py:98} INFO - Subtask:     except self.http_error as ex:
[2018-04-02 20:25:51,082] {base_task_runner.py:98} INFO - Subtask: AttributeError: 'BigQueryPandasConnector' object has no attribute 'http_error'
</code></pre>

<p>This exception is due to this <a href=""https://issues.apache.org/jira/browse/AIRFLOW-2029"" rel=""nofollow noreferrer"">issue</a>, which accroding to the description </p>

<blockquote>
  <p>When BigQueryPandasConnector (in bigquery_hook.py) encounters a BQ job <strong>insertion error</strong>, the exception will be assigned to connector.http_error</p>
</blockquote>

<p>hides another exception, still strange because I'm not doing any insertion.</p>

<p>What am I doing wrong? Maybe there is a problem with <code>bigquery_conn_id</code> used in the <code>BigQueryHook</code>. Or, dataFrame is not the way to go in order to handle query results.</p>

<p>PS: result of <code>pip freeze</code></p>

<pre><code>alembic==0.8.10
amqp==2.2.2
apache-airflow==1.9.0
apache-beam==2.3.0
asn1crypto==0.24.0
avro==1.8.2
Babel==2.5.3
backports-abc==0.5
bcrypt==3.1.4
billiard==3.5.0.3
bleach==2.1.2
cachetools==2.0.1
celery==4.1.0
certifi==2018.1.18
cffi==1.11.4
chardet==3.0.4
click==6.7
configparser==3.5.0
crcmod==1.7
croniter==0.3.20
cryptography==2.1.4
dill==0.2.7.1
docutils==0.14
elasticsearch==1.4.0
enum34==1.1.6
fasteners==0.14.1
Flask==0.11.1
Flask-Admin==1.4.1
Flask-Bcrypt==0.7.1
Flask-Cache==0.13.1
Flask-Login==0.2.11
flask-swagger==0.2.13
Flask-WTF==0.14
flower==0.9.2
funcsigs==1.0.0
future==0.16.0
futures==3.2.0
gapic-google-cloud-datastore-v1==0.15.3
gapic-google-cloud-error-reporting-v1beta1==0.15.3
gapic-google-cloud-logging-v2==0.91.3
gapic-google-cloud-pubsub-v1==0.15.4
gapic-google-cloud-spanner-admin-database-v1==0.15.3
gapic-google-cloud-spanner-admin-instance-v1==0.15.3sta
gapic-google-cloud-spanner-v1==0.15.3
gitdb2==2.0.3
GitPython==2.1.8
google-api-core==1.1.0
google-api-python-client==1.6.5
google-apitools==0.5.20
google-auth==1.4.1
google-auth-oauthlib==0.2.0
google-cloud==0.27.0
google-cloud-bigquery==0.31.0
google-cloud-bigtable==0.26.0
google-cloud-core==0.28.1
google-cloud-dataflow==2.3.0
google-cloud-datastore==1.2.0
google-cloud-dns==0.26.0
google-cloud-error-reporting==0.26.0
google-cloud-language==0.27.0
google-cloud-logging==1.2.0
google-cloud-monitoring==0.26.0
google-cloud-pubsub==0.27.0
google-cloud-resource-manager==0.26.0
google-cloud-runtimeconfig==0.26.0
google-cloud-spanner==0.26.0
google-cloud-speech==0.28.0
google-cloud-storage==1.3.2
google-cloud-translate==1.1.0
google-cloud-videointelligence==0.25.0
google-cloud-vision==0.26.0
google-gax==0.15.16
google-resumable-media==0.3.1
googleads==4.5.1
googleapis-common-protos==1.5.3
googledatastore==7.0.1
grpc-google-iam-v1==0.11.4
grpcio==1.10.0
gunicorn==19.7.1
hdfs3==0.3.0
html5lib==1.0.1
httplib2==0.10.3
idna==2.6
ipaddress==1.0.19
itsdangerous==0.24
Jinja2==2.8.1
kombu==4.1.0
ldap3==2.4.1
lockfile==0.12.2
lxml==3.8.0
Mako==1.0.7
Markdown==2.6.11
MarkupSafe==1.0
mock==2.0.0
monotonic==1.4
mysqlclient==1.3.10
numpy==1.13.0
oauth2client==2.0.2
oauthlib==2.0.7
ordereddict==1.1
pandas==0.19.2
pandas-gbq==0.3.1
pbr==3.1.1
ply==3.8
proto-google-cloud-datastore-v1==0.90.4
proto-google-cloud-error-reporting-v1beta1==0.15.3
proto-google-cloud-logging-v2==0.91.3
proto-google-cloud-pubsub-v1==0.15.4
proto-google-cloud-spanner-admin-database-v1==0.15.3
proto-google-cloud-spanner-admin-instance-v1==0.15.3
proto-google-cloud-spanner-v1==0.15.3
protobuf==3.5.2
psutil==4.4.2
pyasn1==0.4.2
pyasn1-modules==0.2.1
pycosat==0.6.3
pycparser==2.18
Pygments==2.2.0
pyOpenSSL==17.5.0
PySocks==1.6.8
python-daemon==2.1.2
python-dateutil==2.7.0
python-editor==1.0.3
python-nvd3==0.14.2
python-slugify==1.1.4
pytz==2018.3
PyVCF==0.6.8
PyYAML==3.12
redis==2.10.6
requests==2.18.4
requests-oauthlib==0.8.0
rsa==3.4.2
setproctitle==1.1.10
setuptools-scm==1.15.0
singledispatch==3.4.0.3
six==1.11.0
slackclient==1.1.3
smmap2==2.0.3
SQLAlchemy==1.2.5
statsd==3.2.2
suds-jurko==0.6
tableauserverclient==0.5.1
tabulate==0.7.7
tenacity==4.9.0
thrift==0.11.0
tornado==5.0.1
typing==3.6.4
Unidecode==1.0.22
uritemplate==3.0.0
urllib3==1.22
vine==1.1.4
webencodings==0.5.1
websocket-client==0.47.0
Werkzeug==0.14.1
WTForms==2.1
xmltodict==0.11.0
zope.deprecation==4.3.0
</code></pre>",49624760,1,0,,2018-04-02 22:56:52.993 UTC,,2018-04-03 07:41:18.553 UTC,,,,,5715610,1,4,python|pandas|google-bigquery|airflow,1487
"Document Text Detection API respond error ""Bad image data"" since July 25 or later",51646568,"Document Text Detection API respond error ""Bad image data"" since July 25 or later","<p>The release of Google Cloud Vision API had been on July 24, but what else was there later?</p>

<p>There was no problem when using Document Text Detection of Vision API on July 25, but when I did the same thing on Augst 1, it became a error response ""Bad image data"".
Not all images causes this error.  Several images causes ""Bad image data"" response.
Using the not Document Text Detection but Text Detection API, I can get the correct response.</p>

<p>Does anyone else have the same problem?</p>

<p>My code is following.</p>

<pre><code>import io
import os

# Imports the Google Cloud client library
from google.cloud import vision
from google.cloud.vision import types

def detect_document(path):
    """"""Detects document features in an image.""""""
    client = vision.ImageAnnotatorClient()

    with io.open(path, 'rb') as image_file:
        content = image_file.read()

    image = types.Image(content=content)

    response = client.document_text_detection(image=image)
    document = response.full_text_annotation
    print response

    for page in document.pages:
        for block in page.blocks:
            block_words = []
            for paragraph in block.paragraphs:
                block_words.extend(paragraph.words)

            block_symbols = []
            for word in block_words:
                block_symbols.extend(word.symbols)

            block_text = ''
            for symbol in block_symbols:
                block_text = block_text + symbol.text

            print(block_text)
            print(block.bounding_box)

detect_document('test.jpg')
</code></pre>

<p>Response is following.</p>

<pre><code>error {
  code: 3
  message: ""Bad image data.""
}
</code></pre>",,0,9,,2018-08-02 06:07:19.973 UTC,,2018-08-02 06:07:19.973 UTC,,,,,10168813,1,1,python|google-cloud-vision,590
Severity Code Description Project File Line Suppression State Error The tag 'VideoResultControl' does not exist in XML,40258634,Severity Code Description Project File Line Suppression State Error The tag 'VideoResultControl' does not exist in XML,"<p>I am trying to implement Microsoft emotion api in C# using code available on github. I followed all the steps given in <a href=""https://www.microsoft.com/cognitive-services/en-us/Emotion-api/documentation/GetStarted"" rel=""nofollow"">https://www.microsoft.com/cognitive-services/en-us/Emotion-api/documentation/GetStarted</a>. 
I have 3 errors, some of them is: </p>

<p>Error : The tag 'VideoResultControl' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'. Line 28 Position 10. </p>

<p>Error:  The tag 'SampleScenarios' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'. Line 12 Position 10.      </p>

<p>In Solution Explorer, ""SampleUserControlLibrary (Load fail)"" appears: that means no user controls libraries are loaded.</p>

<p>Thanks in advance.</p>",,1,0,,2016-10-26 09:33:57.900 UTC,1,2016-12-15 16:57:04.557 UTC,,,,,5631474,1,0,c#|xml,576
OCR post processing changes order of text,51417666,OCR post processing changes order of text,"<p>I am using google vision OCR(offline).It changes the order of text on every time when it take image.How can I make it bound to generate same order of text.</p>

<p>Thanks</p>",,0,1,,2018-07-19 08:16:52.813 UTC,,2018-07-19 08:16:52.813 UTC,,,,,10005311,1,0,ocr|google-vision|post-processing,53
How to find query string of url doing POST,48056251,How to find query string of url doing POST,"<p>If you visit <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/</a> and scroll down a bit you will see the section </p>

<blockquote>
  <p>Try the API Drag image file here or browse from your computer</p>
</blockquote>

<p>If I drag and upload a Kannada (Indian language) <code>.png</code> file it reads it and shows text. </p>

<p>Can I know the underlying query string or URL so that I can pass the image files through my script?</p>

<p>PS: Google Cloud Vision API doesnt support Kannada language but it is working on this drag drop interface only. So I want to use it.</p>

<p>Any help would be highly appreciated.</p>",48060554,1,0,,2018-01-02 06:06:15.960 UTC,,2018-01-03 03:39:20.613 UTC,2018-01-03 03:39:20.613 UTC,,8852495,,8852495,1,0,google-cloud-platform|google-cloud-vision,50
Unity3D - OCR Number Recognition,40077320,Unity3D - OCR Number Recognition,"<p>Our initial use case called for writing an application in Unity3D (write solely in C# and deploy to both iOS and Android simultaneously) that allowed a mobile phone user to hold their camera up to the title of a magazine article, use OCR to read the title, and then we would process that title on the backend to get related stories. <a href=""http://www.vuforia.com/"" rel=""nofollow noreferrer"">Vuforia</a> was far and away the best for this use case because of its fast native character recognition.</p>

<p>After the initial application was demoed a bit, more potential uses came up. Any use case that needed solely A-z characters recognized was easy in Vuforia, but the second it called for number recognition we had to look elsewhere because Vuforia does not support number recognition (now or anywhere in the near future).</p>

<p>Attempted Workarounds:</p>

<ol>
<li><a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">Google Cloud Vision</a> - works great, but not native and camera images are sometime quite large, so not nearly as fast as we require. Even thought about using the <a href=""https://www.assetstore.unity3d.com/en/#!/content/21088"" rel=""nofollow noreferrer"">OpenCV</a> Unity asset to identify the numbers and then send multiple much smaller API calls, but still not native and one extra step.</li>
<li>Following instructions from <a href=""https://stackoverflow.com/questions/10947399/how-to-implement-and-do-ocr-in-a-c-sharp-project#answer-27385157"">SO</a> to use a .Net wrapper for Tesseract - would probably work great, but after building and trying to bring the external dlls into Unity I receive this error <code>.Net Assembly Not Found</code> (most likely an issue with the version of .Net the dlls were compiled in).</li>
<li>Install Tesseract from source on a server and then create our own API - honestly unclear why we tried this when Google's works so well and is actively maintained.</li>
</ol>

<p>Has anyone run into this same problem in Unity and ultimately found a good solution?</p>",,1,0,,2016-10-17 01:32:35.460 UTC,,2018-01-02 20:55:04.657 UTC,2017-05-23 12:06:46.463 UTC,,-1,,747723,1,1,opencv|unity3d|ocr|tesseract|vuforia,2931
Installing google-cloud-vision,43852275,Installing google-cloud-vision,"<p>I'm new to raspberry pi, google cloud, python, somewhat new to linux and would like a suggestion on how to fix/debug this problem.  I'm getting an error when I install the <a href=""https://cloud.google.com/vision/docs/reference/libraries"" rel=""nofollow noreferrer"">Cloud Vision API Client Libraries for python</a>. </p>

<p>It seems that this installation breaks pip and pip3 on raspian.  Here's how I reproduced the problem from a fresh install of raspian:</p>

<pre><code>pi@raspberrypi:~ $ pip --version
pip 1.5.6 from /usr/lib/python2.7/dist-packages (python 2.7)
pi@raspberrypi:~ $
pi@raspberrypi:~ $ sudo pip install --upgrade google-cloud-vision
...
... 
At the end of the log:

  Running setup.py install for dill
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""/usr/local/lib/python2.7/dist-packages/setuptools/__init__.py"", line 12, in &lt;module&gt;
        import setuptools.version
      File ""/usr/local/lib/python2.7/dist-packages/setuptools/version.py"", line 1, in &lt;module&gt;
        import pkg_resources
      File ""/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 70, in &lt;module&gt;
        import packaging.version
    ImportError: No module named packaging.version
    Complete output from command /usr/bin/python -c ""import setuptools, tokenize;__file__='/tmp/pip-build-1dbzT3/dill/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-lUJmAy-record/install-record.txt --single-version-externally-managed --compile:
    Traceback (most recent call last):

  File ""&lt;string&gt;"", line 1, in &lt;module&gt;

  File ""/usr/local/lib/python2.7/dist-packages/setuptools/__init__.py"", line 12, in &lt;module&gt;

    import setuptools.version

  File ""/usr/local/lib/python2.7/dist-packages/setuptools/version.py"", line 1, in &lt;module&gt;

    import pkg_resources

  File ""/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 70, in &lt;module&gt;

    import packaging.version

ImportError: No module named packaging.version
</code></pre>

<p>Afterwards, when I run pip, I get this:</p>

<pre><code>pi@raspberrypi:~ $ pip --version
Traceback (most recent call last):
  File ""/usr/bin/pip"", line 5, in &lt;module&gt;
    from pkg_resources import load_entry_point
  File ""/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 70, in &lt;module&gt;
    import packaging.version
ImportError: No module named packaging.version
</code></pre>

<p>I'm not sure how to go about fixing this.</p>",,1,1,,2017-05-08 16:02:34.963 UTC,,2017-05-08 18:19:43.623 UTC,,,,,7483930,1,0,raspberry-pi|pip|google-cloud-platform|raspbian|google-cloud-vision,386
Microsoft Emotion API for Video Operation Result Not Found - Rails,37152708,Microsoft Emotion API for Video Operation Result Not Found - Rails,"<p>I'm working with the Microsoft Emotion API for processing emotions in video in a Rails app. I was able to make the call to the API to submit an operation, but now I have to query another API to get the status of the operation and once it's done it will provide the emotions data. </p>

<p>My issue is that when I query the results API, the response is that my operation is not found. As in, it doesn't exist. </p>

<p>I first sent the below request through my controller, which worked great:</p>

<pre><code>#static controller
    uri = URI('https://api.projectoxford.ai/emotion/v1.0/recognizeinvideo')
    uri.query = URI.encode_www_form({})

    request = Net::HTTP::Post.new(uri.request_uri)
    request['Ocp-Apim-Subscription-Key'] = ENV['MEA_SubscriptionKey1']
    request['Content-Type'] = 'application/octet-stream'
    request.body = File.read(""./public/mark_zuck.mov"")

    response = Net::HTTP.start(uri.host, uri.port, :use_ssl =&gt; uri.scheme == 'https') do |http|
        http.request(request)
    end

    # Get response headers
    response.each_header do |key, value|
      p ""#{key} =&gt; #{value}""
    end

    # Get operation location and id of operation
    operation_location = response[""operation-location""]
    oid = operation_location.split(""/"")[6]
</code></pre>

<p>The response of this first call is:</p>

<pre><code>""operation-location =&gt; https://api.projectoxford.ai/emotion/v1.0/operations/e7ef2ee1-ce75-41e0-bb64-e33ce71b1668""
</code></pre>

<p>The protocol is for one to grab the end of the <code>""operation-location""</code> url, which is the operation id, and send it back to the results API url like below:</p>

<pre><code>    # parse operation ID from url and add it to results API url
    url = 'https://api.projectoxford.ai/emotion/v1.0/operations/' + oid
    uri = URI(url)
    uri.query = URI.encode_www_form({})

    request = Net::HTTP::Get.new(uri.request_uri)
    request['Ocp-Apim-Subscription-Key'] = ENV['MEA_SubscriptionKey1']
    response = Net::HTTP.start(uri.host, uri.port, :use_ssl =&gt; uri.scheme == 'https') do |http|
        http.request(request)
    end

    # Get response headers
    response.each_header do |key, value|
      p ""#{key} =&gt; #{value}""
    end
</code></pre>

<p>The result I get is:</p>

<pre><code>""{\""error\"":{\""code\"":\""Unspecified\"",\""message\"":\""Operation not found.\""}}""
</code></pre>

<p>I get the same result when I query the Microsoft online API console with the operation id of an operation created through my app. </p>

<p>Does anyone have any ideas or experience with this? I would greatly appreciate it.</p>",,1,1,,2016-05-11 03:40:34.190 UTC,,2016-05-19 09:53:37.137 UTC,,,,,5767942,1,0,ruby-on-rails|json|api|net-http|microsoft-cognitive,324
How to scan a dynamodb table with a where condition,51009080,How to scan a dynamodb table with a where condition,"<p>I am getting data in this get_item . how can I get this data in scan query where EventName='newevent'  and 'RekognitionId': {'S': match['Face']['FaceId']</p>

<pre><code> face = dynamodb.get_item(
                                    TableName='athlete_collection',
                                    Key={'RekognitionId': {'S': match['Face']['FaceId']}
                                        # ,'EventName': {'S' : 'celeb'}
                                         }
                                )
</code></pre>",,1,0,,2018-06-24 10:36:33.947 UTC,,2018-06-25 08:38:42.843 UTC,,,,,7958560,1,1,amazon-web-services|amazon-dynamodb,32
Google API Vision - java.lang.NoSuchMethodError,44119233,Google API Vision - java.lang.NoSuchMethodError,"<p>I'm trying to run the <a href=""https://cloud.google.com/vision/docs/reference/libraries"" rel=""noreferrer"">Google Api Vision sample code</a> but I'm getting this error:</p>

<p><strong>Exception in thread ""main"" java.lang.NoSuchMethodError: com.google.common.util.concurrent.MoreExecutors.directExecutor()Ljava/util/concurrent/Executor;</strong></p>

<p>These are the characteristics of my project:</p>

<p><strong>1-POM.xml file</strong></p>

<pre><code>&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
    xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;br.com.multiedro&lt;/groupId&gt;
    &lt;artifactId&gt;vision&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;API Vision&lt;/name&gt;
    &lt;description&gt;Estudando a api vision&lt;/description&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.api-client&lt;/groupId&gt;
            &lt;artifactId&gt;google-api-client&lt;/artifactId&gt;
            &lt;version&gt;1.22.0&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;google-cloud-vision&lt;/artifactId&gt;
            &lt;version&gt;0.18.0-beta&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.6.1&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;1.8&lt;/source&gt;
                    &lt;target&gt;1.8&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;
</code></pre>

<p><strong>2- Class</strong></p>

<pre><code>package br.com.visio;

// Imports the Google Cloud client library
import com.google.cloud.vision.spi.v1.ImageAnnotatorClient;
import com.google.cloud.vision.v1.AnnotateImageRequest;
import com.google.cloud.vision.v1.AnnotateImageResponse;
import com.google.cloud.vision.v1.BatchAnnotateImagesResponse;
import com.google.cloud.vision.v1.EntityAnnotation;
import com.google.cloud.vision.v1.Feature;
import com.google.cloud.vision.v1.Feature.Type;
import com.google.cloud.vision.v1.Image;
import com.google.protobuf.ByteString;

import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.List;

public class QuickstartSample {
    public static void main(String... args) throws Exception {
        // Instantiates a client
        ImageAnnotatorClient vision = ImageAnnotatorClient.create();

        // The path to the image file to annotate
        String fileName = ""/home/thiago/Imagens/demo-image.jpg"";


        // Reads the image file into memory
        Path path = Paths.get(fileName);
        byte[] data = Files.readAllBytes(path);
        ByteString imgBytes = ByteString.copyFrom(data);

        // Builds the image annotation request
        List&lt;AnnotateImageRequest&gt; requests = new ArrayList&lt;&gt;();
        Image img = Image.newBuilder().setContent(imgBytes).build();
        Feature feat = Feature.newBuilder().setType(Type.LABEL_DETECTION).build();

        AnnotateImageRequest request = AnnotateImageRequest.newBuilder().addFeatures(feat).setImage(img).build();
        requests.add(request);

        // Performs label detection on the image file
        BatchAnnotateImagesResponse response = vision.batchAnnotateImages(requests);
        List&lt;AnnotateImageResponse&gt; responses = response.getResponsesList();

        for (AnnotateImageResponse res : responses) {
            if (res.hasError()) {
                System.out.printf(""Error: %s\n"", res.getError().getMessage());
                return;
            }

            for (EntityAnnotation annotation : res.getLabelAnnotationsList()) {
                annotation.getAllFields().forEach((k, v) -&gt; System.out.printf(""%s : %s\n"", k, v.toString()));
            }
        }
    }
}
</code></pre>

<p><strong>3- Google Cloud SDK</strong>
I used the command: gcloud auth application-default login</p>

<p>Can anybody help me?</p>

<p>Thanks.</p>",,0,2,,2017-05-22 17:56:18.087 UTC,,2017-05-22 17:56:18.087 UTC,,,,,4801130,1,5,java|api|vision,235
AWS rekognition and s3 bucket region,49466041,AWS rekognition and s3 bucket region,"<p>I am getting the following error when trying to access my s3 bucket with aws rekognition:</p>

<pre><code>message: 'Unable to get object metadata from S3. Check object key, region and/or access permissions.',
</code></pre>

<p>My hunch is it has something to do with the region. </p>

<p>Here is the code:</p>

<pre><code>const config = require('./config.json');
const AWS = require('aws-sdk');
AWS.config.update({region:config.awsRegion});
const rekognition = new AWS.Rekognition();


var params = {
   ""CollectionId"": config.awsFaceCollection
}

rekognition.createCollection(params, function(err, data) {
  if (err) {
    console.log(err, err.stack);
  }
  else  {
    console.log('Collection created');           // successful response
  }

});
</code></pre>

<p>And here is my config file:</p>

<pre><code>{
  ""awsRegion"":""us-east-1"",
  ""s3Bucket"":""daveyman123"",
  ""lexVoiceId"":""justin"",
  ""awsFaceCollection"":""raspifacecollection6""
}
</code></pre>

<p>I have given almost all the permissions to the user I can think of. Also the region for the s3 bucket appears to be in a place that can work with rekognition. What can I do?</p>",,2,4,,2018-03-24 14:48:03.920 UTC,,2018-09-12 07:40:42.203 UTC,,,,,9493647,1,1,javascript|amazon-web-services|amazon-s3|aws-regions|facial-identification,543
PHP: AWS SDK Class not found on production server,54120224,PHP: AWS SDK Class not found on production server,"<p>I installed AWS SDK along with Facebook and Google SDKs. All of them are working with no problem on my local MacOs environment. But once I pushed to our server all AWS clients are not working. FB and Google still working on production. </p>

<pre><code>include_once(__DIR__ . ""/../../../vendor/autoload.php"");
use Aws\Rekognition\RekognitionClient;
use Aws\Sdk;
class RekognitionTest extends CI_Controller
{
private $client;

function __construct()
{
    $sharedConfig = [
        'version' =&gt; 'latest',
        'region' =&gt; 'us-east-1',
        'credentials' =&gt; [
            'key' =&gt; ""xxxxxxxxxxxxxxxxxxxxxxxxxxxxx"",
            'secret' =&gt; ""xxxxxxxxxxxxxxxxxxxxxxxxxxxxx""
        ]
    ];
    $sdk = new Sdk($sharedConfig);
    $this-&gt;client = $sdk-&gt;createRekognition();
 }
}
</code></pre>

<p>in the above code I am getting error: </p>

<blockquote>
  <p>PHP Fatal error:  Class 'Aws\Sdk' not found</p>
</blockquote>

<p>Also tried different ways to initiate the client using: </p>

<pre><code>$this-&gt;client = new RekognitionClient([
            'version' =&gt; 'latest',
            'region' =&gt; 'us-east-1',
            'credentials' =&gt; [
                'key' =&gt; ""xxxxxxxxxxxxxxxxxxxxxxxxxxxxx"",
                'secret' =&gt; ""xxxxxxxxxxxxxxxxxxxxxxxxxxxxx""
            ]
        ]);
</code></pre>

<p>With the second way I am getting :</p>

<blockquote>
  <p>PHP Fatal error:  Class 'Aws\Rekognition\RekognitionClient' not found</p>
</blockquote>

<p>SDK version: <strong>""aws/aws-sdk-php"": ""^3.82""</strong></p>

<p>I am not sure what I am doing wrong here. </p>",54138187,1,4,,2019-01-10 00:01:24.320 UTC,,2019-01-10 23:04:32.510 UTC,,,,,2259546,1,0,php|linux|amazon-web-services|sdk,76
AWS Step/Lambda - storing variable between runs,44792573,AWS Step/Lambda - storing variable between runs,"<p>In my first foray into any computing in the cloud, I was able to follow Mark West's <a href=""https://github.com/markwest1972/smart-security-camera"" rel=""nofollow noreferrer"">instructions</a> on how to use AWS Rekognition to process images from a security camera that are dumped into an S3 bucket and provide a notification if a person was detected. His code was setup for the Raspberry Pi camera but I was able to adapt it to my IP camera by having it FTP the triggered images to my Synology NAS and use CloudSync to mirror it to the S3 bucket. A step function calls Lambda functions per the below figure and I get an email within 15 seconds with a list of labels detected and the image attached.</p>

<p><a href=""https://i.stack.imgur.com/tw8tB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tw8tB.png"" alt=""Step functions""></a></p>

<p>The problem is the camera will upload one image per second as long the condition is triggered and if there is a lot of activity in front of the camera, I can quickly rack up a few hundred emails.</p>

<p>I'd like to insert a function between make-alert-decision and nodemailer-send-notification that would check to see if an email notification was sent within the last minute and if not, proceed to nodemailer-send-notification right away and if so, store the list of labels, and path to the attachment in an array and then send a single email with all of the attachments once 60 seconds had passed.</p>

<p>I know I have to store the data externally and came across <a href=""https://sc5.io/posts/amazon-aws-lambda-data-caching-solutions-compared/"" rel=""nofollow noreferrer"">this article</a> explaining the benefits of different methods of caching data and I also thought that I could examine the timestamps of the files uploaded to S3 to compare the time elapsed between the two most recent uploaded files to decide whether to proceed or batch the file for later.</p>

<p>Being completely new to AWS, I am looking for advice on which method makes the most sense from a complexity and cost perspective.  I can live with the lag involved in any of methods discussed in the article, just don't know how to proceed as I've never used or even heard of any of the services.</p>

<p>Thanks!</p>",,1,0,,2017-06-28 02:10:43.397 UTC,1,2017-06-30 07:24:07.587 UTC,2017-06-29 20:51:50.523 UTC,,2593745,,7612553,1,3,node.js|amazon-web-services|amazon-s3|aws-lambda,300
"Tracking eye pupil in camera preview, with most watched part of the screen",56302753,"Tracking eye pupil in camera preview, with most watched part of the screen","<p>I have live camera preview, now project requires eye pupil movement tracking (which is different from Eye is open/close). Means which part of the screen is being seen by the user this time and what is the most viewed part of screen at the end of Video stream. Although, I tried with OpenCV &amp; Google Vision api, but they are not able to perform the task perfectly.
Please suggest if you have any solution.</p>

<p><a href=""https://android.jlelse.eu/a-beginners-guide-to-setting-up-opencv-android-library-on-android-studio-19794e220f3c"" rel=""nofollow noreferrer"">https://android.jlelse.eu/a-beginners-guide-to-setting-up-opencv-android-library-on-android-studio-19794e220f3c</a></p>

<p><a href=""https://developers.google.com/vision/"" rel=""nofollow noreferrer"">https://developers.google.com/vision/</a></p>",,0,0,,2019-05-25 07:30:47.297 UTC,2,2019-05-25 07:30:47.297 UTC,,,,,8602105,1,1,android|video-streaming|android-camera|http-live-streaming,51
Wait for the Google Vision OCR promise in node.js/express and return it,51611493,Wait for the Google Vision OCR promise in node.js/express and return it,"<p>I am having problems to return a promise from the Google Vision OCR. Here is the sample code from Google:  </p>

<pre><code> const vision = require('@google-cloud/vision');

    // Creates a client
    const client = new vision.ImageAnnotatorClient();

    /**
     * TODO(developer): Uncomment the following line before running the sample.
     */
    // const fileName = 'Local image file, e.g. /path/to/image.png';

    // Performs text detection on the local file
    client
      .textDetection(fileName)
      .then(results =&gt; {
        const detections = results[0].fullTextAnnotation.text;
        console.log('Text:');
        console.log(detections);
      })
      .catch(err =&gt; {
        console.error('ERROR:', err);
      });
</code></pre>

<p>This will output the full text to the console. If I put the above code into a function and return the variable <em>detections</em> I get only <em>undefined</em> back. I assume the cause of the problem is that a promise is async.</p>

<p>How can I return <em>detections</em> in a route and wait for the promise to resolve so that I can return it via res.send?</p>

<p>This is the function:</p>

<pre><code>function ocrresults(imgpath) {
  console.debug(""OCR recognition started"");
  client
    .textDetection(imgpath)
    .then(results =&gt; {
      const detections = results[0].fullTextAnnotation.text;
      console.log(detections);
      return detections;
    })
    .catch(err =&gt; {
      var MyError = ('ERROR:' err);
      console.error('ERROR:', err);
      return MyError;
    });
}
</code></pre>

<p>This is the route:</p>

<pre><code>app.post('/getmytext', function (req, res) {
  var upload = multer({
    storage: storage
  }).single('userFile')
  upload(req, res, function (err) {
    res.end(ocrresults(imagepath));
  })
})
</code></pre>

<p>Thank you.</p>",,1,4,,2018-07-31 10:43:28.750 UTC,,2018-07-31 14:15:17.153 UTC,2018-07-31 11:08:32.630 UTC,,3623019,,3623019,1,-1,node.js|express|google-vision,137
Minimum faces required in a Person Group - Azure Face Api,56407381,Minimum faces required in a Person Group - Azure Face Api,"<p>I just came over Microsoft Azure Face-API cloud-based service for enabling face recognition in my python based application. But according to my previous experience in developing Face Recognition apps, my models used to require at least 3-4 persons to classify faces correctly(to some extent). </p>

<p>My question is that is there any such minimum required persons that are needed to be added in a personGroup so that model can be then trained to classify faces correctly. </p>

<p>I just wanted to know this before I make a hasty decision of opting the Azure Face API as my primary FR platform.</p>",,0,0,,2019-06-01 13:56:46.077 UTC,,2019-06-01 13:56:46.077 UTC,,,,,8446480,1,0,python|azure|face-api,6
Why result of google vision api difference with ocr from website?,51744886,Why result of google vision api difference with ocr from website?,"<p>I try OCR image by Google Vision API:
<a href=""https://i.stack.imgur.com/bOpBZ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bOpBZ.jpg"" alt=""enter image description here""></a></p>

<p>Result text return from API:</p>

<pre><code>""text"": ""ROBERT M. SCHARF. M.D\n1645 Dorchester Drive\nPlano, Texas 75075\nTELEPHONE (972) 508-3328\nDEA REG. #AS 6975342\nJohn Doe\nDATE 6/1/08\nSPHERICAL\nCYLINDRICAL\nAXIS\nPRISM\nBASE\nOD-3.00\nos3.75\nOD+2.50\nos +2.50\n+1.25\n+1.75\nbi or tri\nD.V\n125\nN.V\nREMARKS +2.75 Hoya GP\n""
</code></pre>

<p>Result text i get from website of google: <a href=""https://cloud.google.com/vision/docs/drag-and-drop"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/drag-and-drop</a></p>

<pre><code> ""text"": ""ROBERT M. SCHARF, M.D.\n1645 Dorchester Drive\nPlano, Texas 75075\nTELEPHONE (072) 596-3328\nDEA REG. #AS 6075342\nFOR John Doe\n- DATE_6/1/08\nPRISM\nBASE\nADDRESS\nB\nOD\nDV\nos\nS PHERICAL\n-3.00\n-3.75\n+2.50\n+2.50\nCYLINDRICAL\n+1.25\n+1.75\nbi or tri\nAXIS\n55\n125\nOS\nREMARKS +2.75 Hoya GP\nRobertas charl, mb.\n""
</code></pre>

<p>From website of google :
It can get correct value [<code>-3.75</code>], but from API , it only return [<code>3.75</code>]</p>

<p>Why result of google vision api difference with ocr from website?</p>",51758735,1,0,,2018-08-08 10:53:20.570 UTC,,2018-08-09 04:13:34.903 UTC,,,,,1497597,1,1,google-vision,31
AWS Recognition to Compare Face using IOS Swift,53954761,AWS Recognition to Compare Face using IOS Swift,"<p>I have an issue with my logic trying to invoke the <code>AWS</code> Recognition Compare Faces api using <code>IOS Swift</code>.  There isn't any documentation for <code>Swift</code> yet (as of this posting), but believe I may have the request set up correctly, just not invoking it correctly to receive the response object and confirm the results.  </p>

<p>Any advice?  </p>

<pre><code>let sourceImage = AWSRekognitionImage()
let sourceImageS3Object = AWSRekognitionS3Object()
sourceImageS3Object?.bucket = ""face-badges""
sourceImageS3Object?.name = ""me.jpg""
sourceImage?.s3Object = sourceImageS3Object

let targetImage = AWSRekognitionImage()
let targetImageS3Object = AWSRekognitionS3Object()
targetImageS3Object?.bucket = ""face-badges""
targetImageS3Object?.name = ""me2.jpg""
targetImage?.s3Object = targetImageS3Object

let request = AWSRekognitionCompareFacesRequest()
request?.similarityThreshold = 90
request?.sourceImage = sourceImage
request?.targetImage = targetImage

let key = ""testCompareFaces""
let credentialsProvider = AWSCognitoCredentialsProvider(regionType:.USEast1,
                                                        identityPoolId:""xxxxx"")
let configuration = AWSServiceConfiguration(region:.USEast1, credentialsProvider:credentialsProvider)

AWSRekognition.register(with: configuration!, forKey: key)
AWSRekognition(forKey: key).compareFaces(AWSRekognitionCompareFacesRequest()).continueWith(block: {(_ task: AWSTask) -&gt; Any in
    print(""completed"")

    return true;

}).waitUntilFinished()
</code></pre>",,1,3,,2018-12-28 06:52:29.097 UTC,0,2019-01-08 19:22:49.977 UTC,2018-12-28 08:27:27.133 UTC,,3060199,,3060199,1,0,ios|swift|amazon-web-services|amazon-rekognition,184
Can I use Google Mobile Vision API in IntelliJ IDEA or Atom IDE?,47942396,Can I use Google Mobile Vision API in IntelliJ IDEA or Atom IDE?,"<p>I am exploring the APIs provided by <code>Google</code>. Firstly, I was experimenting with <code>Google Cloud Vision API</code> with Python in PyCharm in order to try to perform Optical Character Recognition (OCR) with various texts. </p>

<p>So I wrote a basic program in Python in PyCharm which was calling this API, I gave to it as an input an image which included text e.g. the <code>image/photo</code> of an ice-cream bucket and then the output was the text written on this bucket.</p>

<p>Now I want to test the barcode scanner of <code>Google Mobile Vision API</code>. But<code>Google Mobile Vision API</code> is supported by Java and not Python so ideally I would like to call the <code>Google Mobile Vision API</code> in a Java program which calls this API, give as an input an <code>image/photo</code> of a barcode and take as an output the details saved in this barcode.</p>

<p>My question is if this can be (easily) done with <code>IntelliJ IDEA</code> or <code>Atom IDE</code> or if I should download Android Studio to do this simple task?</p>

<p>In other words, can I call easily call a mobile API in an IDE which is not specifically for mobile app development like <code>Android Studio</code> but it is for general java applications like <code>IntelliJ IDEA</code> or <code>Atom IDE</code>?</p>",47942433,1,2,,2017-12-22 13:31:22.140 UTC,,2017-12-22 15:44:20.997 UTC,,,,,9024698,1,2,java|android|intellij-idea|atom-editor,123
AWS Rekognition with Android Studio - Cannot resolve method withEndpoint,42536697,AWS Rekognition with Android Studio - Cannot resolve method withEndpoint,"<p>Working through AWS Rekognition Exercise 2: Detect Faces (API) but having a problem at the following line. From some reason withEndpoint won't resolve?</p>

<pre><code>AmazonRekognitionClient rekognitionClient = new AmazonRekognitionClient(credentials).withEndpoint(""rekognition.us-east-1.amazonaws.com"");
</code></pre>

<p>As best I can tell I've included everything necessary as build.gradle has</p>

<pre><code>compile 'com.amazonaws:aws-android-sdk-core:2.3.9'
compile 'com.amazonaws:aws-android-sdk-s3:2.3.9'
compile 'com.amazonaws:aws-android-sdk-ddb:2.3.9'
compile 'com.amazonaws:aws-android-sdk-rekognition:2.3.9'
compile 'com.amazonaws:aws-java-sdk:+' 
</code></pre>

<p>Has anyone had success with the examples in Android Studio? I found 2 related questions but one didn't include a completion solution and the other used Maven with IntelliJ. Thanks</p>",42536988,2,0,,2017-03-01 16:13:35.833 UTC,,2018-08-03 13:18:37.760 UTC,,,,,2279829,1,0,java|android|amazon-web-services|android-studio|amazon-rekognition,1616
BitmapFactory: Unable to decode stream. No such file or directory,51443537,BitmapFactory: Unable to decode stream. No such file or directory,"<p>I am currently developing / experimenting ""Analzye Image Application"" with Camera 2 API and Microsoft Cognitive - Computer Vision.</p>

<p>Instead of using a normal camera, I used API to capture image and let the bitmap be analyzed by the Computer Vision. What I did here is that I fetch the File Path of the captured image and directly converted it to Bitmap using BitmapFactory. But I always got the error of:</p>

<blockquote>
  <p>E/BitmapFactory: Unable to decode stream: java.io.FileNotFoundException: /storage/emulated/0/IMG_20Jul2018_8112.jpg: open failed: ENOENT (No such file or directory)</p>
</blockquote>

<p>I can see the image inside my phone storage but the Bitmap returns null.</p>

<p>Here's my code:</p>

<p>Inside the <code>onCreate</code>, touchListener (Doubletap to capture the image)</p>

<pre><code>     textureView.setOnTouchListener(new View.OnTouchListener() {

        private GestureDetector gestureDetector = new GestureDetector(Camera.this, new GestureDetector.SimpleOnGestureListener() {
            @Override
            public boolean onDoubleTap(MotionEvent e) {
                Snackbar.make(findViewById(R.id.textureView), ""Capturing..."", Snackbar.LENGTH_SHORT).show();
                takePicture();

                //if(mBitmap == null) {
                //    mBitmap = BitmapFactory.decodeFile(file.getAbsolutePath());
                //}
                // START OF COMPUTER VISION
                onActivityResult();
                // END OF COMPUTER VISION
                return super.onDoubleTap(e);
            }
            // implement here other callback methods like onFling, onScroll as necessary
        });

        @Override
        public boolean onTouch(View v, MotionEvent event) {
            gestureDetector.onTouchEvent(event);
            return true;
        }
    });
</code></pre>

<p>Inside the <code>takePicture()</code> function (inserted after //Check orientation base on device):</p>

<pre><code>        Date c = Calendar.getInstance().getTime();
        System.out.println(""Current time =&gt; "" + c);

        SimpleDateFormat df = new SimpleDateFormat(""ddMMMyyyy"");

        // Generate random number
        Random r = new Random();
        final int currentNumber = r.nextInt((9999 - 1) + 1) + 1;

        String fileName = ""IMG_"" + df.format(c) + ""_"" + currentNumber + "".jpg"";
        file = new File(Environment.getExternalStorageDirectory()+""/""+fileName);

        //Convert Bitmap to stream
        try {
            Bitmap bitmap = null;
            File f= new File(pathUpload);
            BitmapFactory.Options options = new BitmapFactory.Options();
            options.inPreferredConfig = Bitmap.Config.ARGB_8888;

            bitmap = BitmapFactory.decodeStream(new FileInputStream(f), null, options);

            // Put path into bitmap
            mBitmap = BitmapFactory.decodeFile(file.getAbsolutePath());
            //image.setImageBitmap(bitmap);

            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
            bitmap.compress(Bitmap.CompressFormat.JPEG,100,outputStream);
            ByteArrayInputStream inputStream = new ByteArrayInputStream(outputStream.toByteArray());

        } catch (Exception e) {
            e.printStackTrace();
        }
</code></pre>

<p>Based on the error, it deals something with <code>mBitmap = BitmapFactory.decodeFile(file.getAbsolutePath());</code></p>

<p>What might be the error?</p>

<p>Please base the codes here:
<a href=""https://github.com/eddydn/AndroidCamera2API"" rel=""nofollow noreferrer"">Camera 2 API</a> and 
<a href=""https://github.com/Microsoft/Cognitive-Vision-Android/blob/master/Sample/app/src/main/java/com/microsoft/projectoxford/visionsample/AnalyzeActivity.java"" rel=""nofollow noreferrer"">Microsoft Computer Vision</a></p>

<p>Thank you in advance guys!</p>

<p>EDIT: Additional Info</p>

<p>I have set user permission to use both camera and access storage.</p>

<pre><code>&lt;uses-permission android:name=""android.permission.WRITE_EXTERNAL_STORAGE""/&gt;
&lt;uses-permission android:name=""android.permission.READ_EXTERNAL_STORAGE"" /&gt;
</code></pre>

<p>Also, I requested permission at my runtime. Please refer <a href=""https://stackoverflow.com/questions/41673282/how-to-display-image-on-imageview-selected-from-internal-storage"">here</a>.</p>",,2,8,,2018-07-20 13:23:23.417 UTC,,2018-07-20 14:58:04.137 UTC,2018-07-20 14:58:04.137 UTC,,8109510,,8109510,1,0,android|microsoft-cognitive,772
react-native-camera Cannot read property 'Constants' of undefined on android,56302400,react-native-camera Cannot read property 'Constants' of undefined on android,"<p>I'm aware of <a href=""https://github.com/react-native-community/react-native-video/issues/354"" rel=""nofollow noreferrer"">this</a> issue.
I have installed and configured the react-native-camera this way:</p>

<pre><code>yarn add react-native-camera --save
react-native link react-native-camera 
</code></pre>

<p>added these to <strong>AndroidManifest.xml</strong>:</p>

<pre><code>  &lt;uses-permission android:name=""android.permission.CAMERA"" /&gt;
  &lt;uses-permission android:name=""android.permission.RECORD_AUDIO""/&gt;
  &lt;uses-permission android:name=""android.permission.READ_EXTERNAL_STORAGE"" /&gt;
  &lt;uses-permission android:name=""android.permission.WRITE_EXTERNAL_STORAGE"" /&gt;
</code></pre>

<p>I'm using <a href=""https://github.com/react-native-community/react-native-camera/blob/master/docs/RNCamera.md"" rel=""nofollow noreferrer"">this</a> basic code:</p>

<pre><code> &lt;RNCamera
          ref={ref =&gt; {
            this.camera = ref;
          }}
          style={styles.preview}
          type={RNCamera.Constants.Type.back}
          flashMode={RNCamera.Constants.FlashMode.on}
          androidCameraPermissionOptions={{
            title: 'Permission to use camera',
            message: 'We need your permission to use your camera',
            buttonPositive: 'Ok',
            buttonNegative: 'Cancel',
          }}
          androidRecordAudioPermissionOptions={{
            title: 'Permission to use audio recording',
            message: 'We need your permission to use your audio',
            buttonPositive: 'Ok',
            buttonNegative: 'Cancel',
          }}
          onGoogleVisionBarcodesDetected={({ barcodes }) =&gt; {
            console.log(barcodes);
          }}
        /&gt;
</code></pre>

<p>And get this error:</p>

<blockquote>
  <p>ExceptionsManager.js:74 TypeError: TypeError: Cannot read property 'Constants' of undefined</p>
</blockquote>",,1,2,,2019-05-25 06:38:24.510 UTC,,2019-05-25 07:20:13.243 UTC,,,,,11542791,1,0,android|reactjs|react-native|react-native-camera,40
Android: Minimum API level requirement to use Google Vision API?,43302771,Android: Minimum API level requirement to use Google Vision API?,"<p>I'm working on an Andriod Studio project and I'm trying to use the Google Cloud Vision API. I've been trying to figure out if I can use it since my target sdk is level 15-25, but I can't find the minimum required sdk level anywhere in the documentation. </p>

<p>The only information relevant to this that I found was the only sample application on their website and it says under prerequisites ""</p>

<blockquote>
  <p>An Android device running Android 5.0 or higher <a href=""https://github.com/GoogleCloudPlatform/cloud-vision/blob/master/android/README.md"" rel=""nofollow noreferrer"">1</a></p>
</blockquote>

<p>That doesn't necessarily mean it doesn't work for lower API levels. Does anyone know what's the minimum requirement?  </p>",43302890,1,0,,2017-04-09 03:40:12.237 UTC,1,2017-04-09 04:01:22.673 UTC,,,,,3557965,1,2,android,1537
Json 3 dimensional array to PHP,43639575,Json 3 dimensional array to PHP,"<p>i'm using Microsoft Emotion Api and it return a result as a json. so i want to access emotion values and assign that values to php variables. i used json_decode function but it can't do it. 
result like below </p>

<pre><code>[  
  {
    ""faceRectangle"": {
      ""left"": 63,
      ""top"": 94,
      ""width"": 66,
      ""height"": 97
    },
    ""scores"": {
      ""anger"": 0.00300731952,
      ""contempt"": 2.18678448E-08,
      ""disgust"": 9.284124E-06,
      ""fear"": 0.0001912825,
      ""happiness"": 0.9874571,
      ""neutral"": 0.0009631537,
      ""sadness"": 1.887755E-05,
      ""surprise"": 0.008223598
    }
  }
]
</code></pre>",43639627,1,0,,2017-04-26 16:16:22.950 UTC,,2017-04-26 16:22:19.943 UTC,2017-04-26 16:20:23.027 UTC,,1491895,,7879609,1,0,php|json|microsoft-cognitive,225
Google Cloud Vision API Testing,44613751,Google Cloud Vision API Testing,"<p>I'm using the following <a href=""https://cloud.google.com/blog/big-data/2017/06/training-an-object-detector-using-cloud-machine-learning-engine"" rel=""nofollow noreferrer"">link</a>, to start working on Google Cloud Vision platform. I tried the following steps:</p>

<p>1) Created the project</p>

<p>2) Created the bucket</p>

<p>3) I also ran the following code:</p>

<p>4) Ran each code given in the tutorial line-by-line</p>

<p>When I'm running this part of the code, it's throwing the following error.</p>

<pre><code>gcloud ml-engine jobs submit training `whoami`_object_detection_`date +%s` \
--job-dir=gs://$BUCKET_NAME/data \
--packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz \
--module-name object_detection.train \
--region us-central1 \
--config object_detection/samples/cloud/cloud.yml \
-- \
--train_dir=gs://$BUCKET_NAME/data \
--pipeline_config_path=${BUCKET_NAME}/data/faster_rcnn_resnet101_pets.config
</code></pre>

<p>Error:</p>

<blockquote>
  <p>ERROR: gcloud crashed (IOError): [Errno 2] No such file or directory: 'slim/dist/slim-0.1.tar.gz'</p>
  
  <p>If you would like to report this issue, please run the following
  command:   gcloud feedback</p>
  
  <p>To check gcloud for common problems, please run the following command:
  gcloud info --run-diagnostics</p>
</blockquote>

<p>I'm configuring the pipeline as following:</p>

<pre><code>sed -i ""s|gcloudstuff/visionex/models|""${BUCKET_NAME}""/data|g"" object_detection/samples/configs/faster_rcnn_resnet101_pets.config
</code></pre>",,0,4,,2017-06-18 09:50:06.450 UTC,,2017-06-20 00:16:09.790 UTC,2017-06-20 00:16:09.790 UTC,,322020,,697363,1,0,python|google-cloud-vision,357
ajax request to Microsoft Emotion API in javascript,33687536,ajax request to Microsoft Emotion API in javascript,"<p>I am trying to try the <a href=""https://dev.projectoxford.ai/docs/services/5639d931ca73072154c1ce89/operations/563b31ea778daf121cc3a5fa"" rel=""nofollow"">microsoft emotion api</a>. I am running a simple python web server with CORS enabled. Below is my server python file with which I start the server:</p>

<p><strong>python-server.py</strong></p>

<pre><code>#! /usr/bin/env python2
from SimpleHTTPServer import SimpleHTTPRequestHandler
import BaseHTTPServer

class CORSRequestHandler (SimpleHTTPRequestHandler):
    def end_headers (self):
        self.send_header('Access-Control-Allow-Origin', '*')
        SimpleHTTPRequestHandler.end_headers(self)

if __name__ == '__main__':
    BaseHTTPServer.test(CORSRequestHandler, BaseHTTPServer.HTTPServer)
</code></pre>

<p>I have an index.html file in which I am sending the http request:</p>

<p><strong>index.html</strong></p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;JSSample&lt;/title&gt;
    &lt;script src=""http://ajax.googleapis.com/ajax/libs/jquery/1.9.0/jquery.min.js""&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;

&lt;script type=""text/javascript""&gt;
    $(function() {
        $.ajax({
            url: ""https://api.projectoxford.ai/emotion/v1.0/recognize"",
            beforeSend: function(xhrObj){
                // Request headers
                xhrObj.setRequestHeader(""Content-Type"",""application/json"");
                xhrObj.setRequestHeader(""Ocp-Apim-Subscription-Key"",JSON.stringify({""my-key""}));
            },
            type: ""POST"",
            // Request body
            data: JSON.stringify({""url"": ""http://tinyurl.com/2g9mqh""}),
        })
        .done(function(data) {
            alert(""success"");
        })
        .fail(function() {
            alert(""error"");
        });
    });
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>After about 30 seconds I get the connection refused response. The http request code was taken from the emotion api's page I linked earlier. I wonder whether I need a real server or is there a mistake in the code? Thanks.</p>",33799841,3,4,,2015-11-13 07:11:24.443 UTC,,2016-05-14 19:30:07.887 UTC,,,,,1354513,1,0,javascript|ajax|windows|microsoft-cognitive,1049
ImportError: No module named 'google.cloud.proto.vision',47754119,ImportError: No module named 'google.cloud.proto.vision',"<p><strong>Code :</strong></p>

<pre><code>from google.cloud import vision
import google.cloud.proto.vision.v1.image_annotator_pb2 as pvv 
import io

client = vision.ImageAnnotatorClient()

def mkrl(imageStr):
    im_obj = pvv.Image(content = imageStr)
    return pvv.AnnotateImageRequest(image = im_obj, features = [{""type"": ""TEXT_DETECTION""}])

def getR(imageList):
    req = map(mkrl,imageList)
    response = client.batch_annotate_images(req)
    return response
</code></pre>

<p>I'm trying to extract text from an image using the google vision api. I need to send a batch of images - things were working fine but now there is this error:</p>

<blockquote>
  <p>ImportError: No module named 'google.cloud.proto.vision'</p>
</blockquote>",,1,0,,2017-12-11 13:32:58.753 UTC,,2017-12-11 17:06:28.400 UTC,2017-12-11 13:56:59.293 UTC,,6805800,,4416197,1,-1,python|google-cloud-vision,308
Setting environment variables for Google Cloud Text to Speech in Raspbian doesn't work,51899558,Setting environment variables for Google Cloud Text to Speech in Raspbian doesn't work,"<p>I want to implement a text-to-speech function for my application using Python. However, I got this error after following a tutorial here <a href=""https://cloud.google.com/text-to-speech/docs/reference/libraries"" rel=""nofollow noreferrer"">https://cloud.google.com/text-to-speech/docs/reference/libraries</a>.</p>

<pre><code>Traceback (most recent call last):
  File ""tts.py"", line 3, in &lt;module&gt;
    client = texttospeech.TextToSpeechClient()
  File ""/usr/local/lib/python2.7/dist-packages/google/cloud/texttospeech_v1/gapic/text_to_speech_client.py"", line 84, in __init__
    scopes=self._DEFAULT_SCOPES,
  File ""/usr/local/lib/python2.7/dist-packages/google/api_core/grpc_helpers.py"", line 170, in create_channel
    credentials, _ = google.auth.default(scopes=scopes)
  File ""/usr/local/lib/python2.7/dist-packages/google/auth/_default.py"", line 306, in default
    raise exceptions.DefaultCredentialsError(_HELP_MESSAGE)
google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. For more information, please see https://developers.google.com/accounts/docs/application-default-credentials.
</code></pre>

<p>I've set the environment variable (GOOGLE_APPLICATION_CREDENTIALS) using this command in Terminal and I am positive that my credentials are working as I've tested on other Google Cloud services.</p>

<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=""/home/pi/labs/test/google_credentials.json""
</code></pre>

<p>Here are some of the things that I've done but didn't manage to work still</p>

<ul>
<li>Reinstalled google-cloud-texttospeech</li>
<li><a href=""https://www.dexterindustries.com/howto/use-google-cloud-vision-on-the-raspberry-pi/"" rel=""nofollow noreferrer"">Making credentials permanent</a></li>
<li>Ensure that my environment variables are set by using env &amp; set | grep GOOGLE_APPLICATION_CREDENTIALS on the terminal to check</li>
<li>Restarted my raspberry pi</li>
</ul>",51900236,1,0,,2018-08-17 16:27:07.307 UTC,0,2018-08-17 17:20:46.547 UTC,,,,,9798258,1,0,python|raspberry-pi|google-cloud-platform|environment-variables|google-text-to-speech,270
Using Google Vision API with ARCore in Android,56031856,Using Google Vision API with ARCore in Android,"<p>I'm looking to build an app that detects certain objects and then overlays something using ARCore.  Is it possible to use Google's Vision API for real-time detection of objects? If not, is there another library that I could use that has object detection, landmark detection, and/or OCR?</p>",,0,0,,2019-05-07 23:58:20.360 UTC,,2019-05-08 04:13:40.373 UTC,2019-05-08 02:54:56.263 UTC,,15168,,5535448,1,1,android|augmented-reality|object-detection|arcore|google-vision,49
Google Vision API Samples: Get the CameraSource to Focus,32051973,Google Vision API Samples: Get the CameraSource to Focus,"<p>I have checkout out the latest Google Vision APIs from here:</p>

<p><a href=""https://github.com/googlesamples/android-vision"" rel=""noreferrer"">https://github.com/googlesamples/android-vision</a></p>

<p>And I am running it on a LG G2 device with KitKat. The only change I have made is to the minSdkVerion in the Gradle file:</p>

<pre><code>...
defaultConfig {
    applicationId ""com.google.android.gms.samples.vision.face.multitracker""
    minSdkVersion 19
...
</code></pre>

<p>However it does not focus. How do I make it focus?</p>",32051974,3,0,,2015-08-17 13:41:50.487 UTC,2,2018-06-08 09:57:34.613 UTC,2015-09-28 09:31:22.003 UTC,,379726,,932052,1,12,android|camera|google-play-services|google-vision|android-vision,16461
Font recommendations for text recognition,40189866,Font recommendations for text recognition,"<p>I'm using google vision API for detecting names and numbers on running bibs. See below for a typical image. Any pointers of font or layout that would give the best detection result?</p>

<p><a href=""https://i.stack.imgur.com/25K7S.jpg"" rel=""nofollow""><img src=""https://i.stack.imgur.com/25K7S.jpg"" alt=""enter image description here""></a></p>",,0,1,,2016-10-22 08:16:40.300 UTC,1,2016-10-22 08:21:54.510 UTC,2016-10-22 08:21:54.510 UTC,,127401,,127401,1,1,google-cloud-platform|google-vision,94
Google Vision raises a Invalid JWT Signature,42229158,Google Vision raises a Invalid JWT Signature,"<p>I am trying to run the quick start demo by <a href=""https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/vision/cloud-client/quickstart.py"" rel=""nofollow noreferrer"">Google Vision APIs</a> on MacOS Sierra.</p>

<pre><code>def run_quickstart():
    # [START vision_quickstart]
    import io
    import os

    # Imports the Google Cloud client library
    from google.cloud import vision

    # Instantiates a client
    vision_client = vision.Client()

    # The name of the image file to annotate
    file_name = os.path.join(
        os.path.dirname(__file__),
        'resources/wakeupcat.jpg')

    # Loads the image into memory
    with io.open(file_name, 'rb') as image_file:
        content = image_file.read()
        image = vision_client.image(
            content=content)

    # Performs label detection on the image file
    labels = image.detect_labels()

    print('Labels:')
    for label in labels:
        print(label.description)
    # [END vision_quickstart]


if __name__ == '__main__':
    run_quickstart()
</code></pre>

<p>Script looks as above. I am using Service Account key file to authenticate. As document suggested I have installed google-vision dependencies via pip and set up an environment variable with, </p>

<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=/my_credentials.json
</code></pre>

<p>Environment variable is correctly set. Still script raises,</p>

<blockquote>
  <p>oauth2client.client.HttpAccessTokenRefreshError: invalid_grant: Invalid JWT Signature.</p>
</blockquote>

<p>There are similar questions asked when using API keys, when using Service account file was not mentioned.</p>",,0,2,,2017-02-14 14:49:48.460 UTC,,2017-02-15 16:25:46.713 UTC,2017-02-15 16:25:46.713 UTC,,5231007,,3248168,1,2,python|oauth-2.0|google-oauth|google-cloud-vision,534
Microsoft Cognitive Face API,45190421,Microsoft Cognitive Face API,<p>I have a need to consume Azure FACE API. But it is still not available in India. However If I host my application in some US servers like AWS. Will I be able to consume the API? </p>,,1,0,,2017-07-19 12:20:31.010 UTC,1,2017-07-19 15:59:25.450 UTC,,,,,6502726,1,0,face-detection|microsoft-cognitive|face-api,171
use an api recognition vision for android,44449053,use an api recognition vision for android,"<p>I want to create an application android that films on the road as long as it is open, and if it detects an accident it sends a request to a database, I directly thought to google vision, but unfortunately it paid and so, I found watson's vision, how i can use it for android studio</p>",,2,0,,2017-06-09 03:57:11.337 UTC,,2017-07-27 14:49:28.720 UTC,,,,,7814982,1,1,android|api|ibm-watson|image-recognition,103
Detect the number of physical objects in an image (Image processing),40037830,Detect the number of physical objects in an image (Image processing),"<p>I am developing a Ruby on Rails application where I want to detect the number of physical objects (bottles and food packets) in an image.</p>

<p>I just explored Google Vision API (<a href=""https://cloud.google.com/vision/"" rel=""nofollow"">https://cloud.google.com/vision/</a>) to check whether this is possible or not. I uploaded a photo which has some cool drink bottles and got the below response.</p>

<pre><code>{
  ""responses"" : [
    {
      ""labelAnnotations"" : [
        {
          ""mid"" : ""\/m\/01jwgf"",
          ""score"" : 0.77698487,
          ""description"" : ""product""
        },
        {
          ""mid"" : ""\/m\/0271t"",
          ""score"" : 0.72027034,
          ""description"" : ""drink""
        },
        {
          ""mid"" : ""\/m\/02jnhm"",
          ""score"" : 0.51373237,
          ""description"" : ""tin can""
        }
      ]
    }
  ]
}
</code></pre>

<p>My concern here is, it is not giving the number of cool drink bottles available in the image, rather it returning type of objects available in the photo.</p>

<p>Is this possible in Google Vision API or any other solution available for this?</p>

<p>Any help would be much appreciated.</p>",,2,3,,2016-10-14 07:52:58.397 UTC,,2016-10-15 04:05:36.663 UTC,2016-10-14 10:30:22.567 UTC,,1537791,,1537791,1,0,ruby-on-rails|ruby|image-processing|google-vision,313
How to identify multiple speakers and their text from an audio input?,41959043,How to identify multiple speakers and their text from an audio input?,"<p>I am using Microsoft's cognitive services. I have an audio input and need to identify multiple speakers and their individual text. </p>

<p>As per my understanding, Speaker Rekognition API can identify different individuals and Bing Speech API can convert speech to text. However, to do both at the same time, I need to manually split audio file into pieces (based on pause/silence)  and then send the audio stream to individual services. Is there a better way to do it? Any other ecosystem that I should switch to like AWS Lex/Polly or Google's offerings?</p>",41985285,1,0,,2017-01-31 13:39:41.763 UTC,3,2019-01-15 07:17:18.443 UTC,,,,,5501520,1,4,speech-recognition|ibm-watson|microsoft-cognitive|google-speech-api|dialogflow,4760
AWS Rekognition with Lambda – what is the correct ARN ressource name for the policy?,44572133,AWS Rekognition with Lambda – what is the correct ARN ressource name for the policy?,"<p>I want to use the Amazon Lambda blueprint (Python) for S3/Rekognition. I made sure all my ressources are in eu-west-1 which is one of the three regions where Rekognition is available.</p>

<p>While trying to add an inline policy to the role I use and get stuck at the ARN field. I tried the S3 ARN as well as the ARN of the Lambda function itself to no avail. </p>

<p>I always get this error:</p>

<blockquote>
  <p>Resource field is not valid. You must enter a valid ARN</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/YXbYU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YXbYU.png"" alt=""enter image description here""></a></p>

<p>What is the correct ARN that I have to enter?</p>",,1,0,,2017-06-15 16:03:25.277 UTC,,2018-06-01 11:25:36.163 UTC,2018-06-01 11:25:36.163 UTC,,174777,,7117003,1,0,amazon-web-services|amazon-s3|aws-lambda|amazon-rekognition,265
Google Vision API returns PERMISSION_DENIED,35672845,Google Vision API returns PERMISSION_DENIED,"<p>I'm trying to use the Google Vision API. I'm following <a href=""https://cloud.google.com/vision/docs/getting-started"" rel=""nofollow"">getting started guide</a>:</p>

<ol>
<li>I have enabled the Cloud Vision API</li>
<li>I have enabled billing</li>
<li>I have set up an API key</li>
<li>Made base64-encoded data from my image</li>
<li><p>Made JSON file with settings:</p>

<pre><code>{
  ""requests"":[
    {
      ""image"":{
        ""content"":""my base64-encoded data""
      },
      ""features"":[
        {
          ""type"":""LABEL_DETECTION"",
          ""maxResults"":5
        }
      ]
    }
  ]
}
</code></pre></li>
<li><p>Sent request with <code>curl</code>:</p>

<pre><code>$ curl -v -k -s -H ""Content-Type: application/json"" https://vision.googleapis.com/v1/images:annotate?key=my_browser_key --data-binary @path_to_file.json
</code></pre></li>
</ol>

<p>After that I got response:</p>

<pre><code>    {
      ""error"": {
        ""code"": 403,
        ""message"": ""Requests from referer \u003cempty\u003e are blocked."",
        ""status"": ""PERMISSION_DENIED"",
        ""details"": [
          {
            ""@type"": ""type.googleapis.com/google.rpc.Help"",
            ""links"": [
              {
                ""description"": ""Google developer console API key"",
                ""url"": ""https://console.developers.google.com/project/***********/apiui/credential""
              }
            ]
          }
        ]
      }
    }
</code></pre>

<p><em>\u003cempty\u003e means &lt;empty&gt;</em></p>

<p>Any ideas? Somebody have the same problem?</p>",35672902,1,0,,2016-02-27 17:17:58.910 UTC,1,2016-04-12 02:56:06.130 UTC,,,,,3725402,1,2,google-cloud-vision,3654
Rotating UIImage for Google ML Vision framework on Swift 4,51145859,Rotating UIImage for Google ML Vision framework on Swift 4,"<p>When an image gets captured it defaults to left orientation. So when you feed it into the <code>textDetector</code> inside the <code>Google Vision framework</code>, it comes all jumbled, unless you take the photo oriented left (home button on the right). I want my app to support both orientations.</p>

<p><a href=""https://i.stack.imgur.com/Jizhv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jizhv.png"" alt=""enter image description here""></a></p>

<pre><code>let visionImage = VisionImage(image: image)

self.textDetector?.detect(in: visionImage, completion: { (features, error) in
    //2
    guard error == nil, let features = features, !features.isEmpty else {
        print(""Could not recognize any text"")
        self.dismiss(animated: true, completion: nil)
        return
    }

    //3
    print(""Detected Text Has \(features.count) Blocks:\n\n"")
    for block in features {
    //4
        print(""\(block.text)\n\n"")
    }
})
</code></pre>

<p>I have tried to recreate the Image with a new orientation and that won't change it.</p>

<p>Does anyone know what to do?</p>

<p>I have tried all of these suggestions
<a href=""https://stackoverflow.com/questions/40882487/how-to-rotate-image-in-swift"">How to rotate image in Swift?</a></p>",51146032,1,0,,2018-07-03 02:27:36.070 UTC,,2018-08-31 16:06:39.713 UTC,2018-08-31 16:06:39.713 UTC,,6599590,,4521884,1,0,ios|swift|apple-vision,128
Microsoft Cognitive Services: Uploading image,37900554,Microsoft Cognitive Services: Uploading image,"<p>I am trying to upload an image to the Microsoft Computer Vision API from a mobile device, but I am constantly receiving a 400 Bad Request for Invalid File Format ""Input data is not a valid image"". The documentation states that I can send the data as application/octet-stream in the following form:</p>

<blockquote>
  <p>[Binary image data]</p>
</blockquote>

<p>I have the data of the image in terms of base64 encoding (""/9j/4AAQSkZJ..........""), and I also have the image as a FILE_URI, but I can't seem to figure out the format in which to send the data. Here is a sample code:</p>

<pre><code>$(function() {
    $.ajax({
        url: ""https://api.projectoxford.ai/vision/v1.0/describe"",
        beforeSend: function (xhrObj) {
            // Request headers
            xhrObj.setRequestHeader(""Content-Type"", ""application/octet-stream"");
            xhrObj.setRequestHeader(""Ocp-Apim-Subscription-Key"", computerVisionKey);
        },
        type: ""POST"",
        // Request body
        data: base64image,
        processData: false        
    })
    .done(function(data) {
      alert(""success"");
    })
    .fail(function(error) {
      alert(""fail"");
    });
});
</code></pre>

<p>I've tried the following:</p>

<ul>
<li>[base64image]</li>
<li>{base64image} </li>
<li>""data:image/jpeg;base64,"" + base64image</li>
<li>""image/jpeg;base64,"" + base64image</li>
</ul>

<p>and more.</p>

<p>I did tested these on the Computer Vision API console. Is it because base64 encoded binary isn't an acceptable format? Or am I sending it in the incorrect format completely?</p>

<p>Note: The operation works when sending a URL as application/json.</p>",37902210,2,0,,2016-06-18 18:55:06.803 UTC,,2017-04-26 12:12:22.550 UTC,,,,,4488647,1,2,image|mobile|microsoft-cognitive,3214
How to send a variable from view to controller,50546373,How to send a variable from view to controller,"<p>I'm trying to use IBM Watson visual recognition in a web application. I want to send the path of the photo uploaded by the client to a function or a controller so I can use it to build and get a result from visual recognition(build an object).
I managed to get the path like this(in internet explorer):</p>

<pre><code>var input = document.getElementById(""file"");
var filepath1 = input.value;
</code></pre>

<p>I want to know how can i send the path to a controller or to a function in c#.</p>

<p>I also tried to build a form and add an action to the controller but the controller name didn't show up.</p>

<pre><code>&lt;form action="""" method=""post"" enctype=""multipart/form-data""&gt;

    &lt;label for=""file""&gt;Filename:&lt;/label&gt;
    &lt;input accept=""image/*"" title=""Choose an image to upload"" type=""file"" name=""file"" id=""file"" /&gt;

    &lt;input type=""submit"" /&gt;
&lt;/form&gt; 
</code></pre>",,1,0,,2018-05-26 19:32:51.693 UTC,,2018-05-27 04:21:34.837 UTC,2018-05-26 19:55:10.400 UTC,,2141621,,9535823,1,-2,c#|asp.net-core,53
MICROSOFT computer vision API,53799577,MICROSOFT computer vision API,"<p>I've been using microsoft computer vision cognitive services API as trial version. I'm trying to read text from image.Now, the question is why am I facing the difference in results when I use online API's and integrate those API's with my python code?</p>

<p>Is this the issue as I'm using trial version?</p>

<p>Any help would be appreciated.</p>",,0,2,,2018-12-16 05:04:17.107 UTC,,2018-12-16 05:04:17.107 UTC,,,,,7751100,1,0,python|ocr,55
Does Google Cloud Vision contain (the possibility of) post-OCR text correction?,51961697,Does Google Cloud Vision contain (the possibility of) post-OCR text correction?,"<p>I am searching for the answer on this question on the internet, but can't find it. 
I mean something like auto-correction, or no correction but suggestions for more obvious words. Is this feature part of Google cloud vision, or should i use an external program for this?</p>

<p>I know that Google cloud vision also tells you something about the likeliness of discussing a certain topic (medical, violence, etc). Doe it has a built-in feature that automatically uses a 'medical dictionary' when analyzing a medical document? For example, when the word 'miniscule' is being found in an medical text, does it change (or propose to change) it to 'meniscus'? So is domain specific knowledge being used?</p>

<p>And does anybody know how about for Microsoft Cognitive Services?</p>",,1,0,,2018-08-22 07:22:27.083 UTC,,2018-08-22 19:25:08.250 UTC,2018-08-22 07:34:35.450 UTC,,10063312,,10063312,1,0,microsoft-cognitive|google-cloud-vision,58
How do I use Google Cloud Vision API to return the image with the highest confidence for a particular label?,42850135,How do I use Google Cloud Vision API to return the image with the highest confidence for a particular label?,"<p>I have a list of external URLs (.jpg or .png images) and want to send those  as requests to the Google Cloud Vision API for label detection. I want the image with the highest confidence for a particular label(s) returned first. Basically I would like to sort images in descending order of confidence for a label (such as car).</p>

<p>So far I've figured out how to annotate images stored locally but am trying to figure out how I can feed it a list of external image URLs and sort them by confidence for 'car'. </p>",42856402,2,0,,2017-03-17 05:49:41.680 UTC,1,2017-04-07 17:57:10.450 UTC,2017-03-19 20:48:08.950 UTC,,5231007,,7682662,1,2,python|google-cloud-platform|google-cloud-vision,595
Adding a local path to Microsoft Face API by Python,40714481,Adding a local path to Microsoft Face API by Python,"<p>I'm using Microsoft Face API for a small project and I was trying to detect a face inside a .jpg file in the local system (say, stored in a directory <em>D:\Image\abc.jpg</em>)</p>

<p>The example code, as shown in their <a href=""https://dev.projectoxford.ai/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236"" rel=""nofollow noreferrer"">documentation</a>, works very well on url from online sources, but it does not seem to work for local path address. I have tried to do the following:</p>

<pre><code>dict = ""{ 'url' : 'file:///D:/Image/abc.jpg'}""
</code></pre>

<p>But it does not seem to work. It seems that there is a method for Java (using <a href=""https://stackoverflow.com/questions/39541634/how-to-send-a-local-image-instead-of-url-to-microsoft-cognitive-face-api-using-j"">FileEntity</a>). I'm wondering if there is a method for Python. I'm new to coding. I really hope someone can help me with this. I'm using Python3.</p>",,1,1,,2016-11-21 06:58:50.140 UTC,2,2018-02-07 06:04:00.660 UTC,2018-02-07 06:04:00.660 UTC,,2698552,,7187987,1,2,python|microsoft-cognitive,1318
Microsoft Face API error in method identify(),43598191,Microsoft Face API error in method identify(),"<p>I am using project oxford for Microsoft Face API in JavaScript, when I use the function ""identify"", I receive ""Invalid request body.""</p>

<pre><code>                    client.face.identify({
                       faces: arrayFaceId,
                       personGroupId: ""groupId"",
                       maxNumOfCandidatesReturned: 1,
                       confidenceThreshold: 0.8
                    }).then(function(response){
                       console.log('Response ' + JSON.stringify(response.personId));
                    }, function(error){
                     console.log(""Error2""+JSON.stringify(error));
                   });
</code></pre>

<p>Anyone knows how I could fix it?</p>",43602132,1,6,,2017-04-24 21:42:02.497 UTC,,2019-03-17 18:58:40.177 UTC,2019-03-17 18:58:40.177 UTC,,5909603,,7138022,1,0,javascript|microsoft-cognitive|azure-cognitive-services|face-api,152
What is the current limit for Google Cloud Vision API image file size,50331196,What is the current limit for Google Cloud Vision API image file size,"<p>In my Java project I'm using Google Cloud Vision API to extract text from images. For text extraction I'm using the following <a href=""https://cloud.google.com/vision/docs/detecting-fulltext#vision-fulltext-detection-gcs-java"" rel=""nofollow noreferrer"">piece of code</a>.</p>

<p>Today, I've found that Google has changed the limits for maximum file size. Previously it was 4 MB. </p>

<p>Now, based on <a href=""https://cloud.google.com/vision/quotas"" rel=""nofollow noreferrer"">Quotas and Limits</a> and <a href=""https://cloud.google.com/vision/docs/supported-files"" rel=""nofollow noreferrer"">Supported Images</a>, the maximum image file size should be <strong>20 MB</strong> for images hosted on Cloud Storage or at a publicly-accessible URL. Also there is maximum JSON request object size (10 MB). </p>

<p>I'm using option with images hosted on Cloud Storage. For images larger than ~7.95 MB (12000 x 6500) I'm getting an error message:</p>

<pre><code>responses {
  error {
    code: 3
    message: ""Invalid image contents: gs://... .""
  }
}
</code></pre>

<p>For images with lower size I'm getting correct response. I know that there is a recommended size 1024 x 768 for TEXT_DETECTION and DOCUMENT_TEXT_DETECTION feature but, according to the following note, higher size should not be problem:</p>

<blockquote>
  <p>Note: generally, the Vision API requires images to be a sufficient size so that important features within the request can be easily distinguished. Sizes smaller or larger than these recommended sizes may work. However, smaller sizes may result in lower accuracy, while larger sizes may increase processing time and bandwidth usage without providing comparable benefits in accuracy.</p>
</blockquote>

<p>Is there something I did not notice? </p>

<p>Note: I'm getting the same error when calling the Vision API directly (see <a href=""https://cloud.google.com/vision/docs/auth?authuser=0"" rel=""nofollow noreferrer"">Bearer tokens section</a>).</p>",,2,2,,2018-05-14 13:13:05.117 UTC,1,2018-06-27 19:36:34.893 UTC,2018-05-14 18:31:44.370 UTC,,2598453,,2598453,1,1,google-cloud-vision,944
Set the camera resolution with google vision android,43999829,Set the camera resolution with google vision android,"<p>Currently working on a barcode scanner on xamarin android. I am using the google vision API. </p>

<pre><code>    cameraSource = new CameraSource
        .Builder(this, barcodeDetector)
        .SetRequestedPreviewSize(1920, 1080)
        .Build();
</code></pre>

<p>This is the code that i'm using to build the camera view.
If i understand correctly, SetRequestedPreviewSize is used to display the camera view on the phone.
How can i change the resolution that the camera of the phone is using? I couldn't find any answer sadly.</p>",44017455,1,0,,2017-05-16 11:10:03.173 UTC,,2017-05-17 06:45:00.407 UTC,,,,,7734801,1,0,android|xamarin|google-vision,747
How to upload an image to AWS Rekognition using command line tools (AWS CLI),46995404,How to upload an image to AWS Rekognition using command line tools (AWS CLI),"<p>I am trying to upload JPG or PNG images stored in the local file system to Amazon Rekognition on the command line using <strong>aws-cli/1.11.175</strong>. Images stored in S3 work perfectly fine, but I can't figure out how the CLI call should look like, if the file is stored locally:</p>

<pre><code># aws rekognition detect-labels --image '???'
</code></pre>

<p>The documentation suggests <code>--image ""Bytes='...'""</code> and I also understand, that the image should be base64 encoded. However, whatever I try, I end up with the following error message.</p>

<pre><code>An error occurred (InvalidImageFormatException) when calling the DetectLabels
operation: Invalid image encoding
</code></pre>

<p>I tried things like this:</p>

<pre><code># IMAGE=$(base64 --wrap=0 image.jpg)
# aws rekognition detect-labels --image ""Bytes='${IMAGE}'""

# base64 --wrap=0 image.png &gt; image.png.b64
# aws rekognition detect-labels --image ""Bytes='file:///image.png.b64'""
</code></pre>

<p>Can someone provide an example, how to pass an image stored in the file system to Rekognition, without the need to copy it to an S3 bucket first? How should the <code>--image</code> option look like?</p>",,1,3,,2017-10-28 23:12:04.980 UTC,1,2018-05-02 11:01:01.470 UTC,2017-10-29 02:53:11.347 UTC,user8633839,,,7895627,1,3,amazon-web-services|aws-cli|amazon-rekognition,499
Google Cloud Vision isn't returning underscore characters,46764797,Google Cloud Vision isn't returning underscore characters,"<p>I'm testing out <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">Google Cloud Vision</a> and noticing it's replacing underscores with spaces.</p>

<p>Does the API provide a feature to ensure these underscores are not omitted?</p>

<p>Code example:</p>

<pre><code>require ""google/cloud/vision""

vision = Google::Cloud::Vision.new
image  = vision.image ""example_image.png""

puts image.text
</code></pre>

<p>Here is the example_image.png that I'm using:<br>
<a href=""https://i.stack.imgur.com/O6Jcu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O6Jcu.png"" alt=""enter image description here""></a></p>

<p>Which produces the output: <code>Tom1c TH</code> (no underscore).</p>",,0,2,,2017-10-16 07:14:31.400 UTC,1,2017-10-16 07:14:31.400 UTC,,,,,869936,1,2,ruby|google-cloud-vision,50
Google vision numbers recognition,49915680,Google vision numbers recognition,"<p>I have to create a sudoku solver, so I create with google vision, a number recognition to retrieve numbers from the grid.This numbers recognition trim the grid to analyse each cell but the recognition doesn't work.. I think the problem comes from TextRecognizer who has trouble recognizing a single character.</p>

<p>Can you help me please?</p>

<p>Thanks.</p>

<pre><code>btnProcess.setOnClickListener(new View.OnClickListener() {
        @Override
        public void onClick(final View v) {
            new Thread(new Runnable() {
                public void run() {
                    final StringBuilder stringBuilder = new StringBuilder();
                    TextRecognizer textRecognizer=new TextRecognizer.Builder(getApplicationContext()).build();
                    if(!textRecognizer.isOperational()){
                        Log.e(""Error"",""Detector not available"");
                    }
                    else {
                        int largeur = (bitmap.getWidth()) / 9;
                        int hauteur = (bitmap.getHeight()) / 9;
                        Bitmap cellule = null;
                        for (int y = 0; y&lt; 9; y++) {
                            for (int x = 0; x &lt; 9; x++) {
                                cellule = Bitmap.createBitmap(bitmap,x*largeur,y*hauteur,largeur,hauteur);
                                Frame frame = new Frame.Builder().setBitmap(cellule).build();
                                cellule.recycle();
                                SparseArray&lt;TextBlock&gt; items = textRecognizer.detect(frame);
                                if (items.size()==0){
                                    stringBuilder.append(""0"" + "" "");
                                }
                                else{
                                    TextBlock item=items.valueAt(0);
                                    stringBuilder.append(item.getValue() + "" "");
                                }

                            }
                            stringBuilder.append(""\n"");
                        }
                    }
                    runOnUiThread(new Runnable() {
                        public void run() {
                            txtResult.setText(stringBuilder.toString());
                        }
                    });
                }
            }).start();

        };
    });
</code></pre>",,0,0,,2018-04-19 07:56:16.417 UTC,,2018-04-19 07:56:16.417 UTC,,,,,9582793,1,1,android|android-studio|ocr|google-vision,42
Flutter + AWS: It always gives broken pipe error,53675809,Flutter + AWS: It always gives broken pipe error,"<p>I'm trying to compare faces with AWS rekognition API. but somehow I'm getting ""broken pipe"" error all the time. There is no problem on aws keys and photos. I'm trying to get more info from http.post but It just says ""broken pipe"", it doesn't give any detail, unfortunately.</p>

<p>Scenario;</p>

<ul>
<li>User takes 2 photos (working)</li>
<li>on second taken, I will parse images to bytes (working)</li>
<li>send bytes with standard request to aws API (doesn't work)</li>
</ul>

<p>I changed the image quality to the lowest as well, but It didn't help.</p>

<hr>

<p>Main.dart code</p>

<pre><code>import 'dart:async';
import 'dart:io';
import 'package:flutter/material.dart';
import 'package:camera/camera.dart';
import 'testa.dart';
import 'package:path_provider/path_provider.dart';

List&lt;CameraDescription&gt; cameras;

Future&lt;void&gt; main() async {
  cameras = await availableCameras();
  runApp(MyApp());
}

class MyApp extends StatelessWidget {
  // This widget is the root of your application.
  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'Flutter Demo',
      theme: ThemeData(
        // This is the theme of your application.
        //
        // Try running your application with ""flutter run"". You'll see the
        // application has a blue toolbar. Then, without quitting the app, try
        // changing the primarySwatch below to Colors.green and then invoke
        // ""hot reload"" (press ""r"" in the console where you ran ""flutter run"",
        // or simply save your changes to ""hot reload"" in a Flutter IDE).
        // Notice that the counter didn't reset back to zero; the application
        // is not restarted.
        primarySwatch: Colors.blue,
      ),
      home: MyHomePage(title: 'Flutter Demo Home Page'),
    );
  }
}

class MyHomePage extends StatefulWidget {
  MyHomePage({Key key, this.title}) : super(key: key);

  // This widget is the home page of your application. It is stateful, meaning
  // that it has a State object (defined below) that contains fields that affect
  // how it looks.

  // This class is the configuration for the state. It holds the values (in this
  // case the title) provided by the parent (in this case the App widget) and
  // used by the build method of the State. Fields in a Widget subclass are
  // always marked ""final"".

  final String title;

  @override
  _MyHomePageState createState() =&gt; _MyHomePageState();
}

class _MyHomePageState extends State&lt;MyHomePage&gt; {
  int _counter = 0;
  CameraController controller;

  @override
  void initState() {
    super.initState();
    controller = CameraController(cameras[0], ResolutionPreset.low);
    controller.initialize().then((_) {
      if (!mounted) {
        return;
      }
      setState(() {});
    });
  }

  void _incrementCounter() {
    setState(() {
      // This call to setState tells the Flutter framework that something has
      // changed in this State, which causes it to rerun the build method below
      // so that the display can reflect the updated values. If we changed
      // _counter without calling setState(), then the build method would not be
      // called again, and so nothing would appear to happen.
      _counter++;
    });
  }

  Future&lt;String&gt; _checkImage(String filePath, String secondPath) async {
    File sourceImagefile, targetImagefile; //load source and target images in those File objects
    String accessKey, secretKey, region ; //load your aws account info in those variables

    print(filePath);
    print(secondPath);

    targetImagefile = File(filePath);
    sourceImagefile = File(secondPath);

    print(targetImagefile.existsSync());
    print(sourceImagefile.existsSync());

    accessKey = '';
    secretKey = '';
    region = 'eu-west-1';

    RekognitionHandler rekognition = new RekognitionHandler(accessKey, secretKey, region);
    String labelsArray = await rekognition.compareFaces(sourceImagefile, targetImagefile);
    return labelsArray;
  }

  Widget cameraPart() {
    if (!controller.value.isInitialized) {
      return Container();
    }
    return AspectRatio(
        aspectRatio:
        controller.value.aspectRatio,
        child: CameraPreview(controller));
  }

  @override
  Widget build(BuildContext context) {
    // This method is rerun every time setState is called, for instance as done
    // by the _incrementCounter method above.
    //
    // The Flutter framework has been optimized to make rerunning build methods
    // fast, so that you can just rebuild anything that needs updating rather
    // than having to individually change instances of widgets.
    return Scaffold(
      appBar: AppBar(
        // Here we take the value from the MyHomePage object that was created by
        // the App.build method, and use it to set our appbar title.
        title: Text(widget.title),
      ),
      body: cameraPart(),
      floatingActionButton: FloatingActionButton(
        onPressed: takePhoto,
        tooltip: 'Increment',
        child: Icon(Icons.add),
      ), // This trailing comma makes auto-formatting nicer for build methods.
    );
  }

  Future&lt;String&gt; get360PhotoFolder() async {
    final Directory appFolder = await getAppFolder();
    final String dirPath = '${appFolder.path}/photos360';
    await Directory(dirPath).create(recursive: true);

    return dirPath;
  }

  String firstPath = '';
  String secondPath = '';

  Future&lt;bool&gt; takePhoto() async {
    final String dirPath = await get360PhotoFolder();
    final String filePath = '$dirPath/${timestamp()}_test.jpg';

    try {
      debugPrint('photo taken - $filePath');
      await controller.takePicture(filePath);

      setState(() {
        if (firstPath == '') {
          print('a');
          firstPath = filePath;
        }else if (secondPath == '') {
          print('b');
          secondPath = filePath;

          _checkImage(firstPath, secondPath).then((value) {
            print(value);
          }).catchError((error) {
            print(error);
          });
          firstPath = '';
          secondPath = '';
        }
      });
    } on CameraException catch (e) {
      print([e.code, e.description]);
      return true;
    }

    return false;
  }

  String timestamp() =&gt; DateTime.now().millisecondsSinceEpoch.toString();

  Future&lt;Directory&gt; getAppFolder() async =&gt;
      await getApplicationDocumentsDirectory();
}
</code></pre>

<p>AWS rekognition code</p>

<pre><code>import 'dart:async';
import 'dart:convert';
import 'dart:io';

import 'package:intl/intl.dart';

import 'testb.dart';

class RekognitionHandler {
  final String _accessKey, _secretKey, _region;

  RekognitionHandler(this._accessKey, this._secretKey, this._region);

  Future&lt;String&gt; _rekognitionHttp(String amzTarget, String body) async {
    String endpoint = ""https://rekognition.$_region.amazonaws.com/"";
    String host = ""rekognition.$_region.amazonaws.com"";
    String httpMethod = ""POST"";
    String service = ""rekognition"";

    var now = new DateTime.now().toUtc();
    var amzFormatter = new DateFormat(""yyyyMMdd'T'HHmmss'Z'"");
    String amzDate =
    amzFormatter.format(now); // format should be '20170104T233405Z""

    var dateFormatter = new DateFormat('yyyyMMdd');
    String dateStamp = dateFormatter.format(
        now); // Date w/o time, used in credential scope. format should be ""20170104""

    int bodyLength = body.length;

    String queryStringParamters = """";
    Map&lt;String, String&gt; headerParamters = {
      ""content-length"": bodyLength.toString(),
      ""content-type"": ""application/x-amz-json-1.1"",
      ""host"": host,
      ""x-amz-date"": amzDate,
      ""x-amz-target"": amzTarget
    };

    String signature = Signature.generateSignature(
        endpoint,
        service,
        _region,
        _secretKey,
        httpMethod,
        now,
        queryStringParamters,
        headerParamters,
        body);

    String authorization =
        ""AWS4-HMAC-SHA256 Credential=$_accessKey/$dateStamp/$_region/$service/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date;x-amz-target, Signature=$signature"";
    headerParamters.putIfAbsent('Authorization', () =&gt; authorization);

    //String labelsArray = """";
    StringBuffer builder = new StringBuffer();
    try {
      HttpClient httpClient = new HttpClient();
      httpClient.connectionTimeout = Duration(minutes: 10);
      HttpClientRequest request = await httpClient.postUrl(Uri.parse(endpoint));

      request.headers.set('content-length', headerParamters['content-length']);
      request.headers.set('content-type', headerParamters['content-type']);
      request.headers.set('host', headerParamters['host']);
      request.headers.set('x-amz-date', headerParamters['x-amz-date']);
      request.headers.set('x-amz-target', headerParamters['x-amz-target']);
      request.headers.set('Authorization', headerParamters['Authorization']);

      request.write(body);

      HttpClientResponse response = await request.close();

      await for (String a in response.transform(utf8.decoder)) {
        builder.write(a);
      }
    } catch (e) {
      print(e);
    }

    return Future.value(builder.toString());
  }

  Future&lt;String&gt; compareFaces(
      File sourceImagefile, File targetImagefile) async {
    try {
      List&lt;int&gt; sourceImageBytes = sourceImagefile.readAsBytesSync();
      String base64SourceImage = base64Encode(sourceImageBytes);
      List&lt;int&gt; targetImageBytes = targetImagefile.readAsBytesSync();
      String base64TargetImage = base64Encode(targetImageBytes);
      String body =
          '{""SourceImage"":{""Bytes"": ""$base64SourceImage""},""TargetImage"":{""Bytes"": ""$base64TargetImage""}}';
      String amzTarget = ""RekognitionService.CompareFaces"";

      String response = await _rekognitionHttp(amzTarget, body);
      return response;
    } catch (e) {
      print(e);
      return ""{}"";
    }
  }
}
</code></pre>",,0,0,,2018-12-07 19:34:52.530 UTC,2,2018-12-07 19:34:52.530 UTC,,,,,2093240,1,1,amazon-web-services|dart|flutter,93
Google Cloud Vision - Numbers and Numerals OCR,39540741,Google Cloud Vision - Numbers and Numerals OCR,"<p>I've been trying to implement an OCR program with Python that reads numbers with a specific format, XXX-XXX. I used Google's Cloud Vision API Text Recognition, but the results were unreliable. Out of 30 high-contrast 1280 x 1024 bmp images, only a handful resulted in the correct output, or at least included the correct output in the results. The program tends to omit some numbers, output in non-English languages or sneak in a few special characters.</p>

<p>The goal is to at least output the correct numbers consecutively, doesn't matter if the results are sprinkled with other junk. Is there a way to help the program recognize numbers better, for example limit the results to a specific format, or to numbers only?</p>",39680724,2,0,,2016-09-16 22:06:03.330 UTC,,2019-04-11 06:47:41.867 UTC,2016-09-19 13:17:59.667 UTC,,6841211,,6841211,1,15,python|ocr|google-cloud-platform|google-cloud-vision|text-recognition,3047
How to set Google Vision QR Scanner to detect only one value?,54683291,How to set Google Vision QR Scanner to detect only one value?,"<p>I have implemented a QR scanner(QRScanner class) using Google Vision API. Once a value is detected it is passed to another activity(Info class) using Intents. The problem is that once a QR code is scanned the Info class gets opened several times.I want to limit the QRScanner class to get only one QR value and Info classed to be opened only once.</p>

<p><strong>QRScanner Class</strong></p>

<pre><code>@Override
protected void onCreate(Bundle savedInstanceState) {
super.onCreate(savedInstanceState);
setContentView(R.layout.activity_qr_scanner);

cameraPreview = (SurfaceView) findViewById(R.id.camera_surface);
qrResult = (TextView) findViewById(R.id.scannerResult);
setupCamera();
}

private void setupCamera() {

BarcodeDetector barcodeDetector = new BarcodeDetector.Builder(this).setBarcodeFormats(Barcode.QR_CODE).build();
final CameraSource cameraSource = new CameraSource.Builder(this, barcodeDetector)
        .setAutoFocusEnabled(true)
        .setRequestedPreviewSize(1600, 1024)
        .build();

cameraPreview.getHolder().addCallback(new SurfaceHolder.Callback() {
    @Override
    public void surfaceCreated(SurfaceHolder holder) {

        if (ActivityCompat.checkSelfPermission(QrScanner.this, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {
            // TODO: Consider calling
            //    ActivityCompat#requestPermissions
            // here to request the missing permissions, and then overriding
            //   public void onRequestPermissionsResult(int requestCode, String[] permissions,
            //                                          int[] grantResults)
            // to handle the case where the user grants the permission. See the documentation
            // for ActivityCompat#requestPermissions for more details.
            return;
        }
        try {
            cameraSource.start(cameraPreview.getHolder());
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    @Override
    public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {

    }

    @Override
    public void surfaceDestroyed(SurfaceHolder holder) {
        cameraSource.stop();
    }
});


barcodeDetector.setProcessor(new Detector.Processor&lt;Barcode&gt;() {
    @Override
    public void release() {

    }

    @Override
    public void receiveDetections(Detector.Detections&lt;Barcode&gt; detections) {

        final SparseArray&lt;Barcode&gt; qrCodes = detections.getDetectedItems();

        if(qrCodes.size()&gt;0)
        {
            qrResult.post(new Runnable() {
                @Override
                public void run() {

                    qrResult.setText(qrCodes.valueAt(0).displayValue);
                    Intent intent = new Intent(QrScanner.this,Info.class);
                    intent.putExtra(QR_CODE,qrCodes.valueAt(0).displayValue);
                    startActivity(intent);
                }
            });
        }
    }
});
}
</code></pre>

<p><strong>Info Class</strong></p>

<pre><code>Intent intent = getIntent();
QRCODE = (String) intent.getStringExtra(QrScanner.QR_CODE);

DB = FirebaseDatabase.getInstance();
ref = DB.getReference().child(""Animals"").child(QRCODE);
ref.addValueEventListener(new ValueEventListener() {
    @Override
    public void onDataChange(@NonNull DataSnapshot dataSnapshot) {

        String DBAnimalClass = dataSnapshot.child(""class"").getValue().toString();
        String DBAnimalFamily = dataSnapshot.child(""family"").getValue().toString();
        String DBAnimalOrder = dataSnapshot.child(""order"").getValue().toString();

    }

    @Override
    public void onCancelled(@NonNull DatabaseError databaseError) {

    }
});
</code></pre>

<p>Currently once a QR is detected the Info class gets called several times. I want the QRScanner to get only one value and Info class to get called only once.</p>",54921090,4,2,,2019-02-14 04:36:52.377 UTC,,2019-02-28 08:13:08.997 UTC,,,,,6737536,1,0,java|android|firebase|qr-code|google-vision,135
Setting the amount of exposure using RNcamera while taking pictures in React-Native?,55921253,Setting the amount of exposure using RNcamera while taking pictures in React-Native?,"<p>i am using react-native-camera module to take pictures but i want to set the camera brightness via a slider, does this module support brightness settings like the one in samsung's native camera app</p>

<p><a href=""https://i.stack.imgur.com/pFW3F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pFW3F.png"" alt=""enter image description here""></a></p>

<p>my current configurations are:</p>

<pre><code>              &lt;RNCamera
                    ref={ref =&gt; {
                        this.camera = ref;
                    }}
                    style={styles.preview}
                    type={RNCamera.Constants.Type.back}
                    flashMode={RNCamera.Constants.FlashMode.on}
                    androidCameraPermissionOptions={{
                        title: 'Permission to use camera',
                        message: 'We need your permission to use your camera',
                        buttonPositive: 'Ok',
                        buttonNegative: 'Cancel',
                    }}
                    androidRecordAudioPermissionOptions={{
                        title: 'Permission to use audio recording',
                        message: 'We need your permission to use your audio',
                        buttonPositive: 'Ok',
                        buttonNegative: 'Cancel',
                    }}
                    onGoogleVisionBarcodesDetected={({ barcodes }) =&gt; {
                        console.log(barcodes);
                    }}
                /&gt;
</code></pre>",,1,0,,2019-04-30 12:44:34.350 UTC,,2019-04-30 13:34:54.020 UTC,,,,,7845429,1,1,react-native|react-native-camera,44
It is possible to get the location of classified content in Watson Visual Recognition?,54353716,It is possible to get the location of classified content in Watson Visual Recognition?,"<p>I'm testing IBM's Watson Visual Recognition using Node-RED, I've trained it to identify some elements in the image, but I wonder if it's possible to get the exact position of these elements.</p>",54366995,1,0,,2019-01-24 19:06:32.267 UTC,,2019-01-25 14:12:35.660 UTC,,,,,10229463,1,1,ibm-watson|node-red|visual-recognition,47
How to test Rekognition from AWS locally without using S3 in detecting texts from images,54672488,How to test Rekognition from AWS locally without using S3 in detecting texts from images,"<p>I'm trying to scan for texts from images but I couldn't find source codes without using an S3 bucket. This is the only source code I found but it uses an S3. I'm using python for this project.</p>

<p><a href=""https://docs.aws.amazon.com/rekognition/latest/dg/text-detecting-text-procedure.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/rekognition/latest/dg/text-detecting-text-procedure.html</a></p>

<pre><code>import boto3

if __name__ == ""__main__"":

bucket='bucket'
photo='text.png'

client=boto3.client('rekognition')


response=client.detect_text(Image={'S3Object':{'Bucket':bucket,'Name':photo}})

textDetections=response['TextDetections']
print ('Detected text')
for text in textDetections:
        print ('Detected text:' + text['DetectedText'])
        print ('Confidence: ' + ""{:.2f}"".format(text['Confidence']) + ""%"")
        print ('Id: {}'.format(text['Id']))
        if 'ParentId' in text:
            print ('Parent Id: {}'.format(text['ParentId']))
        print ('Type:' + text['Type'])
        print
</code></pre>

<p>Found one here <a href=""https://stackoverflow.com/questions/51034435/can-i-use-amazon-rekognition-without-an-s3-bucket"">Can I use Amazon Rekognition without an S3 bucket?</a> and ran it's different from what I need because it detects labels only.</p>",54672780,1,3,,2019-02-13 14:24:16.727 UTC,,2019-02-13 14:46:50.443 UTC,2019-02-13 14:32:03.523 UTC,,7151999,,7151999,1,0,python|amazon-web-services|amazon-s3|amazon-rekognition,91
Using CROP_HINTS of google vision,47532783,Using CROP_HINTS of google vision,"<p>I am using google vision API to detect the face and crop the image accordingly.
this is my code to get the crop coordinates.but its returns the max size of bitmap image I have.</p>

<pre><code>                Vision.Builder builder = new Vision.Builder(httpTransport, jsonFactory, null);
                builder.setVisionRequestInitializer(requestInitializer);

                Vision vision = builder.build();

                BatchAnnotateImagesRequest batchAnnotateImagesRequest =
                        new BatchAnnotateImagesRequest();
                batchAnnotateImagesRequest.setRequests(new ArrayList&lt;AnnotateImageRequest&gt;() {{
                    AnnotateImageRequest annotateImageRequest = new AnnotateImageRequest();

                    Image base64EncodedImage = new Image();
                    ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();
                    bitmap.compress(Bitmap.CompressFormat.JPEG, 90, byteArrayOutputStream);
                    byte[] imageBytes = byteArrayOutputStream.toByteArray();

                    base64EncodedImage.encodeContent(imageBytes);
                    annotateImageRequest.setImage(base64EncodedImage);

                    ArrayList&lt;Feature&gt; features = new ArrayList&lt;&gt;();
                      Feature cropHints = new Feature();
                    cropHints.setType(""CROP_HINTS"");
                    features.add(cropHints);
                    annotateImageRequest.setFeatures(features);


                    add(annotateImageRequest);
                }});

                Vision.Images.Annotate annotateRequest =
                        vision.images().annotate(batchAnnotateImagesRequest);

                BatchAnnotateImagesResponse response = annotateRequest.execute();
Sting vertices=response.getResponses().get(0).getCropHintsAnnotation().getCropHints().get(0).getBoundingPoly().getVertices().toString())

            } 
</code></pre>

<p>the result of vertices</p>

<pre><code> [{}, {""x"":841}, {""x"":841,""y"":1499}, {""y"":1499}]
</code></pre>",,1,0,,2017-11-28 13:28:54.947 UTC,,2017-11-30 18:26:27.250 UTC,2017-11-30 18:26:27.250 UTC,,4482491,user8561650,,1,0,android|image|google-cloud-platform|google-cloud-vision,381
can't send/request base64 to azure face api with python,56405595,can't send/request base64 to azure face api with python,"<p>I want to send the image up on azure face api but it is straightforward to convert the image file to base64 and cannot send requests.</p>

<p>this is code request azure face api,running python but code show error</p>

<pre class=""lang-py prettyprint-override""><code>import requests
import json
import base64

subscription_key = 'my_key'
assert subscription_key

face_api_url = 'https://southeastasia.api.cognitive.microsoft.com/face/v1.0/detect'


headers = { 'Ocp-Apim-Subscription-Key': subscription_key }

#data = 'test.jpg'

with open(""test.jpg"", ""rb"") as image_file:
    encoded_string = base64.b64encode(image_file.read())
    print(encoded_string)

params = {
    'returnFaceId': 'true',
    'returnFaceLandmarks': 'false',
    'returnFaceAttributes': 'age,gender,headPose,smile,facialHair,glasses,emotion,hair,makeup,occlusion,accessories,blur,exposure,noise',
}

#body={
#    ""url"": 'data:image/jpeg;base64' + str(encoded_string)
#}
#data={
#    ""data"": encoded_string,
#    ""contentType"": ""application/octet-stream"",
#}

response = requests.post(face_api_url, params=params, headers=headers, json={""data"": ""image/JPEG;base64,/""+str(encoded_string)})
print(json.dumps(response.json()))
</code></pre>

<p>this is error ""{""error"": {""code"": ""InvalidURL"", ""message"": ""Invalid image URL.""}}""</p>

<p>this is correct [{""faceId"": ""f9fd11a4-8855-4304-af98-f200afcae843"", ""faceRectangle"": {""top"": 621, ""left"": 616, ""width"": 195, ""height"": 195}, ""faceAttributes"": {""smile"": 0.0, ""headPose"": {""pitch"": -11.4, ""roll"": 7.7, ""yaw"": 5.3}, ""gender"": ""male"", ""age"": 29.0, ""facialHair"": {""moustache"": 0.4, ""beard"": 0.4, ""sideburns"": 0.1}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.0, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.0, ""neutral"": 0.999, ""sadness"": 0.001, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.89}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.51}, ""noise"": {""noiseLevel"": ""medium"", ""value"": 0.59}, ""makeup"": {""eyeMakeup"": true, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.04, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.98}, {""color"": ""brown"", ""confidence"": 0.87}, {""color"": ""gray"", ""confidence"": 0.85}, {""color"": ""other"", ""confidence"": 0.25}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.02}]}}}, {""faceId"": ""6c83b2c8-2cdc-43ea-994c-840932601b1d"", ""faceRectangle"": {""top"": 693, ""left"": 1503, ""width"": 180, ""height"": 180}, ""faceAttributes"": {""smile"": 0.003, ""headPose"": {""pitch"": -9.0, ""roll"": -0.5, ""yaw"": -1.5}, ""gender"": ""female"", ""age"": 58.0, ""facialHair"": {""moustache"": 0.0, ""beard"": 0.0, ""sideburns"": 0.0}, ""glasses"": ""NoGlasses"", ""emotion"": {""anger"": 0.0, ""contempt"": 0.001, ""disgust"": 0.0, ""fear"": 0.0, ""happiness"": 0.003, ""neutral"": 0.984, ""sadness"": 0.011, ""surprise"": 0.0}, ""blur"": {""blurLevel"": ""high"", ""value"": 0.83}, ""exposure"": {""exposureLevel"": ""goodExposure"", ""value"": 0.41}, ""noise"": {""noiseLevel"": ""high"", ""value"": 0.76}, ""makeup"": {""eyeMakeup"": false, ""lipMakeup"": false}, ""accessories"": [], ""occlusion"": {""foreheadOccluded"": false, ""eyeOccluded"": false, ""mouthOccluded"": false}, ""hair"": {""bald"": 0.06, ""invisible"": false, ""hairColor"": [{""color"": ""black"", ""confidence"": 0.99}, {""color"": ""gray"", ""confidence"": 0.89}, {""color"": ""other"", ""confidence"": 0.64}, {""color"": ""brown"", ""confidence"": 0.34}, {""color"": ""blond"", ""confidence"": 0.07}, {""color"": ""red"", ""confidence"": 0.03}]}}}]</p>",,0,1,,2019-06-01 09:47:52.320 UTC,,2019-06-01 14:46:13.480 UTC,2019-06-01 14:46:13.480 UTC,,11586365,,11586365,1,-1,python|python-requests|face-recognition|azure-api-apps,10
RestSharp Azure Web API call fails when adding more parameters,56040881,RestSharp Azure Web API call fails when adding more parameters,"<p>I am trying to call an Azure Computer Vision API, specifically [POST] Batch Read File, using RestSharp. Everything is working fine in the code below:</p>

<pre class=""lang-cs prettyprint-override""><code>private void MakeBatchReadRequest(string imageFilePath)
{
    try
    {
        RestClient client = new RestClient(""https://southeastasia.api.cognitive.microsoft.com/"");
        client.AddDefaultHeader(""Ocp-Apim-Subscription-Key"", subscriptionKey);
        RestRequest request = new RestRequest(""vision/v2.0/read/core/asyncBatchAnalyze"", Method.POST);
        request.AddHeader(""Content-Type"", ""application/octet-stream"");

        byte[] byteData = GetImageAsByteArray(imageFilePath);
        request.AddParameter(""application/octet-stream"", byteData, ParameterType.RequestBody);

        RestResponse response = client.Execute(request);
        operationLocation = response.Headers.Where(x =&gt; x.Name == ""Operation-Location"").First.Value;
    }
    catch (Exception ex)
    {
        MessageBox.Show(ex.Message);
    }
}
</code></pre>

<p>I didn't have to include the parameter <code>mode</code> since according to the API documentation seen <a href=""https://westus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/2afb498089f74080d7ef85eb"" rel=""nofollow noreferrer"">here</a>, it was optional and the default value was <code>Printed</code> which was what I already wanted. However if I add the parameter <code>mode</code> (just in case I change my mind and switch to something else) in the request as shown below:</p>

<pre class=""lang-cs prettyprint-override""><code>private void MakeBatchReadRequest(string imageFilePath)
{
    try
    {
        RestClient client = new RestClient(""https://southeastasia.api.cognitive.microsoft.com/"");
        client.AddDefaultHeader(""Ocp-Apim-Subscription-Key"", subscriptionKey);
        RestRequest request = new RestRequest(""vision/v2.0/read/core/asyncBatchAnalyze"", Method.POST);
        request.AddHeader(""Content-Type"", ""application/octet-stream"");

        byte[] byteData = GetImageAsByteArray(imageFilePath);
        request.AddParameter(""application/octet-stream"", byteData, ParameterType.RequestBody);

        request.AddParameter(""mode"", ""Printed"") // This will cause the web API to return an error response.

        RestResponse response = client.Execute(request);
        operationLocation = response.Headers.Where(x =&gt; x.Name == ""Operation-Location"").First.Value;
    }
    catch (Exception ex)
    {
        MessageBox.Show(ex.Message);
    }
}
</code></pre>

<p>The API returns a response status code <code>415</code> and status description <code>Unsupported Media Type</code>. The whole JSON response is below:</p>

<pre><code>{
    ""error"": {
        ""code"": ""BadArgument"",
        ""message"": ""Unsupported media type.""
    }
}
</code></pre>

<p>I'm not really sure how adding a simple parameter to the request could trigger an error response from the API. Also I'm not sure why the error response is <code>Unsupported Media Type</code> since I am using a <code>JPG</code> image file that is supported and set its content type as <code>application/octet-stream</code> in the request.</p>

<p>Any help will be greatly appreciated.</p>",,1,3,,2019-05-08 12:32:12.967 UTC,,2019-05-09 12:17:46.803 UTC,,,,,2043436,1,0,c#|json|rest|restsharp|azure-cognitive-services,38
Watson Developer Cloud showing error: Microsoft Visual C++ 14.0 is required,49648719,Watson Developer Cloud showing error: Microsoft Visual C++ 14.0 is required,"<p>I am trying Watson visual recognition with Python, following this: <a href=""https://www.ibm.com/watson/developercloud/visual-recognition/api/v3/python.html?python#introduction"" rel=""nofollow noreferrer"">https://www.ibm.com/watson/developercloud/visual-recognition/api/v3/python.html?python#introduction</a></p>

<p>while tried to install the library:</p>

<pre><code>pip install --upgrade ""watson-developer-cloud&gt;=1.2.1""
</code></pre>

<p>I am getting following error even after installing ""Microsoft Visual C++ 14.0"", I have uninstalled other versions of MSVC++ too.</p>

<pre><code>Collecting watson-developer-cloud&gt;=1.2.1
  Using cached watson-developer-cloud-1.2.1.tar.gz
Requirement already up-to-date: requests&lt;3.0,&gt;=2.0 in c:\python36\lib\site-packages (from watson-developer-cloud&gt;=1.2.1)
Requirement already up-to-date: python_dateutil&gt;=2.5.3 in c:\python36\lib\site-packages (from watson-developer-cloud&gt;=1.2.1)
Requirement already up-to-date: autobahn&gt;=0.10.9 in c:\python36\lib\site-packages (from watson-developer-cloud&gt;=1.2.1)
Collecting Twisted&gt;=13.2.0 (from watson-developer-cloud&gt;=1.2.1)
  Using cached Twisted-17.9.0.tar.bz2
Collecting pyOpenSSL&gt;=16.2.0 (from watson-developer-cloud&gt;=1.2.1)
  Using cached pyOpenSSL-17.5.0-py2.py3-none-any.whl
Collecting service-identity&gt;=17.0.0 (from watson-developer-cloud&gt;=1.2.1)
  Using cached service_identity-17.0.0-py2.py3-none-any.whl
Requirement already up-to-date: idna&lt;2.7,&gt;=2.5 in c:\python36\lib\site-packages (from requests&lt;3.0,&gt;=2.0-&gt;watson-developer-cloud&gt;=1.2.1)
Requirement already up-to-date: certifi&gt;=2017.4.17 in c:\python36\lib\site-packages (from requests&lt;3.0,&gt;=2.0-&gt;watson-developer-cloud&gt;=1.2.1)
Requirement already up-to-date: chardet&lt;3.1.0,&gt;=3.0.2 in c:\python36\lib\site-packages (from requests&lt;3.0,&gt;=2.0-&gt;watson-developer-cloud&gt;=1.2.1)
Requirement already up-to-date: urllib3&lt;1.23,&gt;=1.21.1 in c:\python36\lib\site-packages (from requests&lt;3.0,&gt;=2.0-&gt;watson-developer-cloud&gt;=1.2.1)
Requirement already up-to-date: six&gt;=1.5 in c:\python36\lib\site-packages (from python_dateutil&gt;=2.5.3-&gt;watson-developer-cloud&gt;=1.2.1)
Requirement already up-to-date: txaio&gt;=2.7.0 in c:\python36\lib\site-packages (from autobahn&gt;=0.10.9-&gt;watson-developer-cloud&gt;=1.2.1)
Requirement already up-to-date: zope.interface&gt;=4.0.2 in c:\python36\lib\site-packages (from Twisted&gt;=13.2.0-&gt;watson-developer-cloud&gt;=1.2.1)
Requirement already up-to-date: constantly&gt;=15.1 in c:\python36\lib\site-packages (from Twisted&gt;=13.2.0-&gt;watson-developer-cloud&gt;=1.2.1)
Requirement already up-to-date: incremental&gt;=16.10.1 in c:\python36\lib\site-packages (from Twisted&gt;=13.2.0-&gt;watson-developer-cloud&gt;=1.2.1)
Requirement already up-to-date: Automat&gt;=0.3.0 in c:\python36\lib\site-packages (from Twisted&gt;=13.2.0-&gt;watson-developer-cloud&gt;=1.2.1)
Requirement already up-to-date: hyperlink&gt;=17.1.1 in c:\python36\lib\site-packages (from Twisted&gt;=13.2.0-&gt;watson-developer-cloud&gt;=1.2.1)
Collecting cryptography&gt;=2.1.4 (from pyOpenSSL&gt;=16.2.0-&gt;watson-developer-cloud&gt;=1.2.1)
  Using cached cryptography-2.2.2-cp36-cp36m-win_amd64.whl
Collecting pyasn1 (from service-identity&gt;=17.0.0-&gt;watson-developer-cloud&gt;=1.2.1)
  Using cached pyasn1-0.4.2-py2.py3-none-any.whl
Collecting pyasn1-modules (from service-identity&gt;=17.0.0-&gt;watson-developer-cloud&gt;=1.2.1)
  Using cached pyasn1_modules-0.2.1-py2.py3-none-any.whl
Requirement already up-to-date: attrs in c:\python36\lib\site-packages (from service-identity&gt;=17.0.0-&gt;watson-developer-cloud&gt;=1.2.1)
Requirement already up-to-date: setuptools in c:\python36\lib\site-packages (from zope.interface&gt;=4.0.2-&gt;Twisted&gt;=13.2.0-&gt;watson-developer-cloud&gt;=1.2.1)
Collecting cffi&gt;=1.7; platform_python_implementation != ""PyPy"" (from cryptography&gt;=2.1.4-&gt;pyOpenSSL&gt;=16.2.0-&gt;watson-developer-cloud&gt;=1.2.1)
  Using cached cffi-1.11.5-cp36-cp36m-win_amd64.whl
Collecting asn1crypto&gt;=0.21.0 (from cryptography&gt;=2.1.4-&gt;pyOpenSSL&gt;=16.2.0-&gt;watson-developer-cloud&gt;=1.2.1)
  Using cached asn1crypto-0.24.0-py2.py3-none-any.whl
Collecting pycparser (from cffi&gt;=1.7; platform_python_implementation != ""PyPy""-&gt;cryptography&gt;=2.1.4-&gt;pyOpenSSL&gt;=16.2.0-&gt;watson-developer-cloud&gt;=1.2.1)
  Using cached pycparser-2.18.tar.gz
Installing collected packages: Twisted, pycparser, cffi, asn1crypto, cryptography, pyOpenSSL, pyasn1, pyasn1-modules, service-identity, watson-developer-cloud
  Running setup.py install for Twisted: started
    Running setup.py install for Twisted: finished with status 'error'
    Complete output from command c:\python36\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\IPC_AD~1\\AppData\\Local\\Temp\\pip-build-ahl2cx7d\\Twisted\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\IPC_AD~1\AppData\Local\Temp\pip-tqzdkhaj-record\install-record.txt --single-version-externally-managed --compile:
    running install
    running build
    running build_py
    creating build
    creating build\lib.win-amd64-3.6
    creating build\lib.win-amd64-3.6\twisted
    copying src\twisted\copyright.py -&gt; build\lib.win-amd64-3.6\twisted
    copying src\twisted\plugin.py -&gt; build\lib.win-amd64-3.6\twisted
    copying src\twisted\_version.py -&gt; build\lib.win-amd64-3.6\twisted
    copying src\twisted\__init__.py -&gt; build\lib.win-amd64-3.6\twisted
    . 
    . 
    . 
    . 
    copying src\twisted\words\xish\xpathparser.g -&gt; build\lib.win-amd64-3.6\twisted\words\xish
    running build_ext
    building 'twisted.test.raiser' extension
    error: Microsoft Visual C++ 14.0 is required. Get it with ""Microsoft Visual C++ Build Tools"": http://landinghub.visualstudio.com/visual-cpp-build-tools

    ----------------------------------------
</code></pre>

<p>I have tried to install twisted by from .wl as</p>

<pre><code>pip install Twisted-17.9.0-cp37-cp37m-win_amd64.whl
</code></pre>

<p>and</p>

<pre><code>pip install Twisted-17.9.0-cp27-cp27m-win32.whl
</code></pre>

<p>both failed with error:</p>

<pre><code>Twisted-17.9.0-cp37-cp37m-win_amd64.whl is not a supported wheel on this platform.
</code></pre>

<p>I am using Windows 7 64 </p>",49648847,3,0,,2018-04-04 10:28:18.323 UTC,,2018-08-30 10:06:16.587 UTC,,,,,7467556,1,0,python|pip|twisted|ibm-watson,465
How do I connect to Google Vision using client libraries? (Node.js),51580768,How do I connect to Google Vision using client libraries? (Node.js),"<p>I am trying to send an image to the Google Vision API using Node.js by following this tutorial: <a href=""https://cloud.google.com/vision/docs/libraries"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/libraries</a></p>

<p>I have installed the client libraries.  Then I created a Service Account key and explicitly set the value of the GOOGLE_APPLICATION_CREDENTIALS environment variable to the JSON file that was downloaded on creation of the Service Account key.</p>

<p>However when I run the following Node.js code:</p>

<pre><code>// Imports the Google Cloud client library
const vision = require('@google-cloud/vision');

// Creates a client
const client = new vision.ImageAnnotatorClient();

// Performs label detection on the image file
client
  .labelDetection('./Driver_license.jpg')
  .then(results =&gt; {
    const labels = results[0].labelAnnotations;

    console.log('Labels:');
    labels.forEach(label =&gt; console.log(label.description));
  })
  .catch(err =&gt; {
    console.error('ERROR:', err);
  });
</code></pre>

<p>I receive the following 2 errors in the console: </p>

<blockquote>
  <p>(node:22585) DeprecationWarning: grpc.load: Use the @grpc/proto-loader module with grpc.loadPackageDefinition instead</p>
  
  <p>Auth error:Error: self signed certificate in certificate chain</p>
</blockquote>

<p>The second error about the Auth error continues to print out over and over again until I terminate execution.</p>

<p>I have followed the Google Cloud tutorials closely so I'm not sure why this isn't working.  Have I missed a step in Authentication?  </p>",,0,2,,2018-07-29 13:27:29.210 UTC,1,2018-07-29 13:27:29.210 UTC,,,,,2989759,1,0,node.js|authentication|google-cloud-platform|gcloud|google-vision,213
Microsoft Face API see database?,54236946,Microsoft Face API see database?,"<p>I'm using Microsoft Face API and I have many photos of persons there. I know that in azure database its saved only geometry of the face, not the whole photo. Now I want to see that data. I know that I can see part of this data, as I`m making requests, like to list all large person groups or to list all persons in the current large group. But I want to see all my data of persons, personId's, groups and photos geometry which is saved in azure's database from azure portal or somewhere else. 
And my question is:</p>

<p><strong>Can I see all my data which is saved in azure's database?</strong> </p>",54251248,1,0,,2019-01-17 13:25:55.523 UTC,,2019-01-24 02:53:29.397 UTC,2019-01-18 10:13:33.213 UTC,,3136339,,7429425,1,0,microsoft-cognitive|face-api,94
Google vision api integration in java (error),39212656,Google vision api integration in java (error),"<p>I am trying to implement and add google vision services to my project using the below github sample code link. 
<a href=""https://github.com/GoogleCloudPlatform/java-docs-samples/tree/master/vision/text/"" rel=""nofollow"">https://github.com/GoogleCloudPlatform/java-docs-samples/tree/master/vision/text/</a>
Running into this error in ImageText and Word java classes where </p>

<pre><code>public static Builder builder() {
return new AutoValue_ImageText.Builder();
  }
</code></pre>

<p>where AutoValue_ImageText type can not be resolved and</p>

<pre><code> public static Builder builder() {
return new AutoValue_Word.Builder();
  }
</code></pre>

<p>where AutoValue_Word type cannot be resolved.
please help! i can not even fix these syntax errors to see if this code even complies properly.
thank you in advance</p>",,1,0,,2016-08-29 18:20:20.793 UTC,,2016-09-07 12:57:43.500 UTC,,,,,3761701,1,0,java|api|typeerror|google-vision,281
Does Azure face api record person's face?,44203114,Does Azure face api record person's face?,"<p>In azure face api we create a person and add face to person and we can identify same person.</p>

<p>My question is  when we add face to person does azure upload and save image on azure or just it stores mathematical attributes of person'a face. </p>",,2,0,,2017-05-26 13:40:15.553 UTC,,2018-06-05 05:57:50.270 UTC,2017-05-26 16:48:02.880 UTC,,5330659,,5330659,1,1,azure|face-api,458
Google Vision split document into boxes,54131993,Google Vision split document into boxes,"<p>Let's say I have the following document:
<a href=""https://i.stack.imgur.com/47F0d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/47F0d.png"" alt=""enter image description here""></a></p>

<p>I am sending it to Google Vision. What I get back, is the words and their boundingPoly. What I would like is if I could somehow group the result by the rectangles shown on the image. Is there a way to detect those boxes? Can  <a href=""https://cloud.google.com/vision/docs/crop-hints"" rel=""nofollow noreferrer"">Crop Hints</a> be used somehow?</p>",,0,0,,2019-01-10 15:32:37.390 UTC,,2019-01-10 15:32:37.390 UTC,,,,,5364135,1,0,javascript|ocr|google-vision|text-recognition|straight-line-detection,11
Unused Screen Overlay Detected permission Dialog,39078750,Unused Screen Overlay Detected permission Dialog,"<p>I am developing an app that allows a user to connect to a specific wifi network using the google vision api to scan a QR code containing an ssid. To scan for wifi networks I need to request the user to grant the location permission. This all works fine if i go to the settings and manually enable the Location permission but when requesting the permission at run-time, the user taps enable then a second dialog pops up, <code>Screen Overlay Detected</code> which requires a context switch to enable the <code>Screen Overlay</code> permission. I am not drawing over other apps and when i manually enable the location permission there is no problems. So why is this permission dialog popping up? </p>

<p>As I am not sure why Android is asking for this permission i am  not sure what code to post. i have included the request for the 3 required permission for the change network request.</p>

<pre><code>final int permissionCheck = ContextCompat.checkSelfPermission(mActivity, Manifest.permission.CAMERA);
final int locationPermissionCheck = ContextCompat.checkSelfPermission(mActivity, Manifest.permission.ACCESS_COARSE_LOCATION);
final int wifiPermissionCheck = ContextCompat.checkSelfPermission(mActivity, Manifest.permission.CHANGE_WIFI_STATE);
final int granted = PackageManager.PERMISSION_GRANTED;
if ((permissionCheck != granted) || (locationPermissionCheck != granted) || (wifiPermissionCheck != granted)) {
    ActivityCompat.requestPermissions(mActivity, new String[]{
            Manifest.permission.CAMERA,
            Manifest.permission.ACCESS_COARSE_LOCATION,
            Manifest.permission.CHANGE_WIFI_STATE

    }, 0);

} else {
    cameraSource.start(cameraView.getHolder());
}
</code></pre>

<p><strong>Update</strong></p>

<p>Turns out its not just the location permission that trips it up. Any permission request is experiencing the same behaviour.</p>",,1,0,,2016-08-22 11:48:16.090 UTC,,2016-08-22 13:16:16.793 UTC,2016-08-22 13:16:16.793 UTC,,1685748,,1685748,1,0,android|permissions|sony-xperia|runtime-permissions,491
Why is AWS Rekognition faster than Google Cloud Vision?,47297666,Why is AWS Rekognition faster than Google Cloud Vision?,"<p>I'm using a Raspberry Pi 2 to upload a test image to both AWS and Google Cloud. Google takes 3 seconds to return a label response. Amazon takes 1 second.</p>

<p><strong>Amazon Rekognition Results:</strong></p>

<pre><code>pi@raspberrypi:~ $ python amazon-detect.py
&gt; People 
&gt; 1.18470716476
</code></pre>

<p><strong>Google Cloud Vision Results:</strong></p>

<pre><code>pi@raspberrypi:~ $ python glabel.py
&gt; people
&gt; 3.31247806549
</code></pre>

<p>Here's the 2 python files that I'm using:</p>

<p><strong>amazon-detect.py</strong></p>

<pre><code>#!/usr/bin/env python
import boto3
import json
import time


rekonize = boto3.client(
    'rekognition',
    aws_access_key_id='xx',
    aws_secret_access_key='xx',
    region_name='us-east-1',
)

start = time.time()

with open(""/home/pi/group.jpg"", ""rb"") as f:
    data = f.read()
    response = rekonize.detect_labels(Image={'Bytes': data,},MaxLabels=1,MinConfidence=80,)

array = json.dumps(response)
a = json.loads(array)
print a['Labels'][0][""Name""]

timer = time.time() - start
print timer
</code></pre>

<p><strong>glabel.py</strong></p>

<pre><code>#!/usr/bin/env python
import os
import time
import json

# Imports the Google Cloud client library
from google.cloud import vision

# Instantiates a client
client = vision.Client()

start = time.time() 

# Loads the image into memory
with open(""/home/pi/group.jpg"", ""rb"") as image_file:
    content = image_file.read()
    image = client.image(content=content)
    labels = image.detect_labels(limit=1)

for label in labels:
    print label.description

timer = time.time() - start
print timer
</code></pre>

<p>How can I improve the speed of Google Cloud Vision's response? Thank you in advance for any tips!</p>

<h2>edit 1:</h2>

<p>I'm now using the cURL api for Cloud Vision. Results are much better!</p>

<pre><code>pi@raspberrypi:~ $ python glabel-curl.py
people
1.2390229702
</code></pre>

<h2>edit 2:</h2>

<p>For reference, I moved the timer code to start at the beginning each python file. This gives me a better idea of the response speed:</p>

<pre><code>pi@raspberrypi:~ $ python glabel-curl.py
people
1.27051591873
pi@raspberrypi:~ $ python glabel.py
people
7.16125893593
pi@raspberrypi:~ $ python amazon-detect.py
People
1.57734918594
</code></pre>",,0,5,,2017-11-15 00:46:19.223 UTC,1,2017-11-15 02:31:25.537 UTC,2017-11-15 02:31:25.537 UTC,,1722056,,1722056,1,0,python|amazon-web-services|google-app-engine|aws-sdk|google-vision,432
Get Data from Image in Shiny,52154222,Get Data from Image in Shiny,"<p>I am trying to get the details of the image using google vision api.  </p>

<p>everything works fine,<br>
but when i use this line of code <code>dta &lt;- getGoogleVisionResponse(input_image_file(),feature = ""LANDMARK_DETECTION"")</code>
then it doesn't find the image,<br>
but if i use like this <code>dta &lt;- getGoogleVisionResponse(""images/abc.jpg"",feature = ""LANDMARK_DETECTION"")</code> then this works fine and it classifies the image.</p>

<p>If you see i have also used the same method to plot the image in <code>renderImage</code>,and for plotting image this datapath works fine, then why it is not working for this <code>dta &lt;- getGoogleVisionResponse(input_image_file(),feature = ""LANDMARK_DETECTION"")</code>   </p>

<pre><code>library(shiny)
library(shinydashboard)
library(RoogleVision)
library(EBImage)
library(tidyverse)
library(leaflet)
library(jsonlite)


ui &lt;-  dashboardPage(

  dashboardHeader(title = ""Image recognition""),
     dashboardSidebar(
       sidebarMenu(
         menuItem(""File Upload"",tabName = ""imagefiledata""),
         menuItem(""Image Classification"", tabName = ""imageclassification"")
       )
     ),  

  dashboardBody(

      tabItems(
         tabItem(tabName = ""imagefiledata"",
                 fileInput(""imagefile"", 
                           ""Image File Upload:"",
                           accept = c('image/png', 'image/jpeg','image/jpg')
                           )
         ),

         tabItem(tabName = ""imageclassification"",
                 fluidRow(
                   box(width = 3,
                     actionButton(""imageclassifybutton"",""Classify Image"")
                   )
                 ),   
                 fluidRow(
                   tabBox(width = 12,
                          tabPanel(""Image"",
                                   imageOutput(""uploadedimage"",height=""500px"")

                          ),
                          tabPanel(""Landmark"",
                                   box(width = 12,  
                                       uiOutput(""detectlandmark"")
                                   )
                          )

                     ) 
                 )
            )

      )

    )

  )

server &lt;- function(input,output) {


  #image plotting starts
  plotimagefile &lt;- eventReactive(input$imageclassifybutton,{
    if(is.null(input$imagefile)){
      return()
    }
    paste(input$imagefile$datapath)
  })

  output$uploadedimage &lt;- renderImage({
      list(src = plotimagefile())
    })
  #image plotting ends




  input_image_file &lt;- reactive({
    if (is.null(input$imagefile)) {
      return("""")
    }
    paste(input$imagefile$datapath)
  })


  imagedata &lt;- eventReactive(input$imageclassifybutton,{
    if(is.null(input$imagefile)){
      return()
    }

    dta &lt;- getGoogleVisionResponse(input_image_file(),feature = ""LANDMARK_DETECTION"")

  })


  output$detectlandmark &lt;- renderUI({
    box(imagedata()$description[1])
  })


}





shinyApp(ui = ui, server = server)
</code></pre>",,0,1,,2018-09-03 17:37:49.093 UTC,,2018-09-03 17:37:49.093 UTC,,,,,10310461,1,1,r|shiny|shinydashboard|google-vision,24
Where is the ocr translated text saved in the google cloud console,56050457,Where is the ocr translated text saved in the google cloud console,"<p>Im following this tutorial to set up the google vision ocr: <a href=""https://cloud.google.com/functions/docs/tutorials/ocr"" rel=""nofollow noreferrer"">https://cloud.google.com/functions/docs/tutorials/ocr</a>. In the tutorial it says that translated text from images is saved in your google cloud storage. Ive created a bucket to save the translations but when I try to upload an image in the command prompt with this command: gsutil cp PATH_TO_IMAGE gs://YOUR_IMAGE_BUCKET_NAME. It succesfully adds the image to my image bucket, but I don`t know where it puts the text translation.</p>",,1,0,,2019-05-08 23:58:20.860 UTC,,2019-05-10 02:52:02.363 UTC,,,,,9394904,1,0,google-cloud-platform|google-vision,19
AWS Rekognition image encode issue,50866887,AWS Rekognition image encode issue,"<p>I have a project to be finished and whenever the image is encoded into base64 as what AWS Rekognition docs told to do in order to obtain metadata.</p>

<p><code>.getScreenshot()</code> return base64 of image that has captured.</p>

<p>Below is my code so far:</p>

<pre><code>import React, { Component } from ""react"";
import Webcam from ""react-webcam"";
import AWS from ""aws-sdk"";
import { creds } from ""./secret"";
import { Grid, Container, Button, Input } from ""semantic-ui-react"";

AWS.config.update({
  accessKeyId: creds.accessKeyId,
  secretAccessKey: creds.secretAccessKey,
  region: ""us-east-1""
});

const rekognition = new AWS.Rekognition({ apiVersion: ""2016-06-27"" });

class CameraComponent extends Component {
  constructor() {
    super();

    this.state = {
      verificationFailed: """",
      verificationSucceed: """",
      faceId: """",
      fileName: """"
    };

    this.changeToFileName = this.changeToFileName.bind(this);
  }

  setRef = webcam =&gt; {
    this.webcam = webcam;
  };

  capture = () =&gt; {
    const image = this.webcam.getScreenshot().slice(23);
    console.log(image);

    const params = {
      SourceImage: {
        Bytes: image
      },
      TargetImage: {
        S3Object: {
          Bucket: ""facerdb"",
          Name: ""jayprofile.jpg""
        }
      },
      SimilarityThreshold: 0.0
    };

    rekognition.compareFaces(params, (err, data) =&gt; {
      if (err) console.log(err);

      console.log(data);
    });
  };
</code></pre>

<p>Thanks in advance!</p>",,0,2,,2018-06-14 22:39:15.060 UTC,,2018-06-14 22:54:18.670 UTC,2018-06-14 22:54:18.670 UTC,,1953718,,9922660,1,0,reactjs|amazon-web-services|webcam|encode|amazon-rekognition,96
All GoogleVision label possibilities?,45313874,All GoogleVision label possibilities?,"<p>I'm searching for a list of all the possible image labels that the Google Cloud Vision API can return?
I believe they used the same labels the following project: <a href=""https://github.com/openimages/dataset"" rel=""nofollow noreferrer"">https://github.com/openimages/dataset</a></p>

<p>I thought of two possible methods of getting these labels:</p>

<ol>
<li>Sending thousands of different images to the API and recording the returned labels (I would automate this)</li>
<li>Going through all the Google Open Image data (which I linked above), and recording the labels.</li>
</ol>

<p>I'm not sure how I could do option 2, and was hoping that someone had already done one of these options.
Please let me know if there already exists a list like the one I am describing, or there is a better method of obtaining it (than the two which I thought of).</p>

<p>Thanks a lot for any help!</p>",45489263,1,0,,2017-07-25 21:33:35.983 UTC,,2017-08-03 15:58:23.383 UTC,2017-07-27 05:55:53.720 UTC,,5231007,,2251258,1,0,google-cloud-platform|google-cloud-vision,101
React Native component for Google Cloud Vision API - Text Detection,40103531,React Native component for Google Cloud Vision API - Text Detection,<p>I am using React Native's Image Picker component to capture images on my app. Before showing the picture I want to parse it using Google Cloud Vision's Text Detection API. I've been searching on components in React Native but no result. Does anybody know if there is something around or if it can be done within React Native?</p>,,1,0,,2016-10-18 08:43:10.310 UTC,1,2016-12-28 18:19:46.553 UTC,,,,,7035365,1,7,react-native|google-cloud-vision,3259
Google cloud vision detect_pdf_gcs ruby,51500118,Google cloud vision detect_pdf_gcs ruby,"<p>I keep receiving an error message when trying to use the detect_pdf ruby code sample provided by the google cloud platform</p>

<p>Link to the code samples : <a href=""https://github.com/GoogleCloudPlatform/ruby-docs-samples/tree/master/vision"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/ruby-docs-samples/tree/master/vision</a></p>

<p>Steps to reproduce the errors</p>

<ul>
<li>rails new test_app</li>
<li>cd test_app</li>
<li>add gem 'google-cloud-vision' &amp;&amp; gem 'google-cloud-storage' in gemfile</li>
<li>bundle install</li>
<li>add config.autoload_paths += %W(#{config.root}/lib) in application.rb</li>
<li>add file ocr_google_test.rb in lib</li>
<li>in file ocr_google_test.rb : class OcrGoogleTest + copy paste the provided sample code : <a href=""https://github.com/GoogleCloudPlatform/ruby-docs-samples/blob/master/vision/detect_pdf.rb"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/ruby-docs-samples/blob/master/vision/detect_pdf.rb</a></li>
<li>change line def detect_pdf_gcs with def self.detect_pdf_gcs</li>
<li>create a free key api and generate your key_file on the google platform <a href=""https://cloud.google.com/iam/docs/creating-managing-service-account-keys"" rel=""nofollow noreferrer"">https://cloud.google.com/iam/docs/creating-managing-service-account-keys</a></li>
<li>create a bucket on google cloud storage to serve as source and destination uris</li>
<li>add arguments for gcs_source_uri:, gcs_destination_uri:, project_id: and also the key file when launching Google::Cloud::Vision.new</li>
<li>go in console rails console</li>
<li><p>launch OcrGoogleTest.detect_pdf_gcs</p></li>
<li><p>RESULT : ArgumentError (wrong number of arguments (given 6, expected 0))</p></li>
</ul>

<p>If anyone could help me understand this, it would be greatly appreciated :)
Thanks a lot and have a great day</p>",,0,0,,2018-07-24 13:39:05.657 UTC,,2018-07-24 13:39:05.657 UTC,,,,,5581439,1,0,ruby-on-rails|ruby|google-cloud-vision,79
Kotlin AWS Rekognition Conversion,51547437,Kotlin AWS Rekognition Conversion,"<p>I am currently porting over an application built in Swift for iOS. In swift one makes a rekognition call the following way:</p>

<p>First initialize the client after importing the package to your podfile:</p>

<pre><code>rekognitionClient = AWSRekognition.default()
</code></pre>

<p>Then create a 'faceRequest' to call the service and see if a face in your collection has matched the image you sent:</p>

<pre><code>guard let FaceRequest = AWSRekognitionSearchFacesByImageRequest() else
       {
           puts(""Unable to initialize AWSRekognitionSearchfacerequest."")
           return
       }
       FaceRequest.collectionId = ""MY_COLLECTION_NAME""
       FaceRequest.faceMatchThreshold = 75
       FaceRequest.maxFaces = 2
       let FacesourceImage = capturedImage
       let Faceimage = AWSRekognitionImage()
       Faceimage!.bytes = UIImageJPEGRepresentation(FacesourceImage!, 0.7)
       FaceRequest.image = Faceimage
       rekognitionClient.searchFaces(byImage:FaceRequest) { (response:AWSRekognitionSearchFacesByImageResponse?, error:Error?) in
           if error == nil
           {
               //print(response!)

               for faceMatch in (response?.faceMatches)! {
                     //do something
               }
           }
        }
</code></pre>

<p>I am looking to convert this to Kotlin and am having issues with syntax and making the request. I have an image in a bitmap format that is ready to be sent to the service.</p>

<p>Here is one sample of what I have been trying:</p>

<pre><code>fun doRekognitionRequest(bitmap: Bitmap){

    //this says AmazonRekognitionClient has been deprecated
    val rekognitionClient = AmazonRekognitionClient()

    //unresolved reference
    val facesImageRequest = facesByImageRequest()
}
</code></pre>

<p>Here are my imports:</p>

<pre><code>import android.Manifest
import android.app.Activity
import android.content.Context
import android.content.ContextWrapper
import android.content.Intent
import android.graphics.Bitmap
import android.graphics.Canvas
import android.graphics.SurfaceTexture
import android.graphics.drawable.BitmapDrawable
import android.hardware.camera2.*
import android.net.Uri
import android.os.*
import android.util.Log
import android.view.*
import android.widget.Toast
import androidx.core.content.ContextCompat
import androidx.fragment.app.Fragment
import androidx.fragment.app.FragmentTransaction
import kotlinx.android.synthetic.main.camera_layout.*
import pub.devrel.easypermissions.AfterPermissionGranted
import pub.devrel.easypermissions.EasyPermissions
import java.io.*
import java.io.File
import java.util.*
import android.provider.MediaStore
import androidx.core.content.FileProvider
import com.amazonaws.services.rekognition.AmazonRekognition;
import com.amazonaws.services.rekognition.AmazonRekognitionClient
import com.amazonaws.services.rekognition.model.FaceMatch;
import com.amazonaws.services.rekognition.model.Image;
import com.amazonaws.services.rekognition.model.S3Object;
import com.amazonaws.services.rekognition.model.SearchFacesByImageRequest;
import com.amazonaws.services.rekognition.model.SearchFacesByImageResult;
</code></pre>

<p>I am trying to build off of what this guy has done: <a href=""https://github.com/awslabs/serverless-photo-recognition/blob/master/src/main/kotlin/com/budilov/rekognition/RekognitionService.kt"" rel=""nofollow noreferrer"">https://github.com/awslabs/serverless-photo-recognition/blob/master/src/main/kotlin/com/budilov/rekognition/RekognitionService.kt</a></p>

<p>As well as trying to convert the java sample code from rekognition  documentation to Kotlin, with no luck.</p>

<p><a href=""https://docs.aws.amazon.com/rekognition/latest/dg/search-face-with-image-procedure.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/rekognition/latest/dg/search-face-with-image-procedure.html</a></p>

<pre><code>   AmazonRekognition rekognitionClient = AmazonRekognitionClientBuilder.defaultClient();

  ObjectMapper objectMapper = new ObjectMapper();

   // Get an image object from S3 bucket.
  Image image=new Image()
          .withS3Object(new S3Object()
                  .withBucket(bucket)
                  .withName(photo));

  // Search collection for faces similar to the largest face in the image.
  SearchFacesByImageRequest searchFacesByImageRequest = new SearchFacesByImageRequest()
          .withCollectionId(collectionId)
          .withImage(image)
          .withFaceMatchThreshold(70F)
          .withMaxFaces(2);

   SearchFacesByImageResult searchFacesByImageResult = 
           rekognitionClient.searchFacesByImage(searchFacesByImageRequest);

   System.out.println(""Faces matching largest face in image from"" + photo);
  List &lt; FaceMatch &gt; faceImageMatches = searchFacesByImageResult.getFaceMatches();
  for (FaceMatch face: faceImageMatches) {
      System.out.println(objectMapper.writerWithDefaultPrettyPrinter()
              .writeValueAsString(face));
     System.out.println();
  }
</code></pre>

<p>Kotlin and Android are all very new to me, I come from  C# and Swift background so any help would be greatly appreciated. 
Cheers!</p>

<p><strong>Edit</strong>:</p>

<p>Managed to get the searchfacesbyImageRequest constructor to get seen by the compiler. Now stuck on converting a bitmap to an Image.</p>

<pre><code>    val facesImageRequest = SearchFacesByImageRequest()
    facesImageRequest.collectionId = ""MY_COLLECTION_NAME""
    facesImageRequest.maxFaces = 2
    facesImageRequest.faceMatchThreshold = 75.0F
    facesImageRequest.image = Image(bitmap)
</code></pre>",51552185,1,0,,2018-07-26 21:01:34.143 UTC,,2018-07-27 06:45:21.353 UTC,2018-07-26 21:32:02.720 UTC,,6575837,,6575837,1,0,java|android|amazon-web-services|kotlin|amazon-rekognition,120
Similar images training set generator,45654972,Similar images training set generator,"<p>I want to train my machine learning (Watson visual recognition) to detect sun doodle (black and white).<br>
The problem is that I have to train at least 700 images but I have just something like 30.</p>

<p><img src=""https://i.stack.imgur.com/G2oSQ.jpg"" alt=""A screen-shot of my gallery:""></p>

<p>I thought about a generator that takes my images and changes them and creates a lot of similar images using pixel games. 
Do you know a generator like this? Or do you have a good idea for me?</p>

<p>Thanks.</p>",,1,4,,2017-08-12 21:10:15.983 UTC,,2017-08-13 04:13:58.090 UTC,2017-08-13 01:52:31.373 UTC,,1205871,,1623454,1,-3,opencv|image-processing|machine-learning|deep-learning|generator,81
Calling Rekognition using AWS CLI,43660043,Calling Rekognition using AWS CLI,"<p>I have the AWS CLI installed on Windows and am using the Windows command prompt. </p>

<p>I am trying to use Rekognition but I cannot seem to get any commands working. The closest I have gotten is with: </p>

<pre><code>aws rekognition detect-faces --image S3Object=\{Bucket=innovation-bucket,Name=image.jpg,Version=1\} --attributes ""ALL"" --region us-east-1
</code></pre>

<p>This results in: </p>

<blockquote>
  <p>Error parsing parameter '--image': Expected: ',', received: '}' for input: S3Object={Bucket=innovation-bucket,Name=image.jpg,Version=1}</p>
</blockquote>

<p>Why is it expecting a comma? </p>

<p><strong>EDIT:</strong> </p>

<p>When I try the format from the documentation I also get errors: </p>

<pre><code>aws rekognition detect-faces --image '{""S3Object"":{""Bucket"":""innovation-bucket"",""Name"":""image.jpg""}}' --attributes ""ALL"" --region us-east-1
</code></pre>

<blockquote>
  <p>Error parsing parameter '--image': Expected: '=', received ''' for input: '{""S3Object"":{""Bucket"":""innovation-bucket"",""Name"":""image.jpg‌​""}}'</p>
</blockquote>",43660949,1,9,,2017-04-27 14:00:28.853 UTC,,2017-04-27 14:41:13.137 UTC,2017-04-27 14:15:47.060 UTC,,3435610,,3435610,1,3,amazon-web-services|amazon-s3|aws-cli|amazon-rekognition,950
Vision API - Force API to analyze a image not perceived as a single text line,38869448,Vision API - Force API to analyze a image not perceived as a single text line,"<p>I've been around <strong>Google Vision API</strong> but I have a problem I can't really 
solve. This is the image I'm dealing with:</p>

<p><a href=""https://i.stack.imgur.com/IymQv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IymQv.png"" alt=""enter image description here""></a></p>

<p>In the image above, <strong>Google Vision API</strong> (also happens with <strong>IBM (Watson)</strong> and 
<strong>Microsft (Cognitive Services)</strong>) does not understand that 2,99€ is something to read because it is not treated as a single line, so 
the output is all but what I expect him to do (understand the price of 
the label). </p>

<p>If I was using Tesseract, I would solve this by using the <code>-psm 7</code> option in order to force it to read it as a single text line, but I can't really find documentation for this situation using Google Vision API.</p>

<p>Has anyone done something similar before? I cannot figure out how to solve this problem...</p>",,1,0,,2016-08-10 09:31:33.727 UTC,2,2016-10-17 09:01:04.593 UTC,2016-08-12 09:25:44.163 UTC,,6306927,,6306927,1,1,ocr|tesseract|google-vision,185
How to use Firebase Storage image with Google Cloud Vision API?,41434746,How to use Firebase Storage image with Google Cloud Vision API?,"<p>I have a webapp where users are authenticated anonymously with Firebase Auth. I store images from users in Firebase Storage, which behind the scenes is backed by a Google Cloud Storage bucket. When I try to use the Google Cloud Vision API from client-side javascript to get the image properties I get a permission error. </p>

<pre><code>image-annotator::User lacks permission.: Can not open file: gs://MYAPP.appspot.com/photos/IMG_12345.jpg
</code></pre>

<p>If I make the image public, everything works. But this is user data and can't be public. How can I solve this?</p>

<p>My code for calling the vision api:</p>

<pre><code>gapi.client.init({
    'apiKey': MY_API_KEY,
    'discoveryDocs': ['https://vision.googleapis.com/$discovery/rest'],
    // clientId and scope are optional if auth is not required.
    'clientId': MY_APP_ID + '.apps.googleusercontent.com',
    'scope': 'profile',
  }).then(function() {
    // 3. Initialize and make the API request.
    return gapi.client.vision.images.annotate({
      ""requests"":
      [
        {
          ""features"":
          [
            {
              ""type"": ""LABEL_DETECTION""
            },
            {
              ""type"": ""IMAGE_PROPERTIES""
            }
          ],
          ""image"":
          {
            ""source"":
            {
              ""gcsImageUri"": ""gs://MY_APP.appspot.com/photos/"" + ""IMG_12345.jpg""
            }
          }
        }
      ]
    });
  }).then(function(response) {
    console.log(response.result);
  }, function(reason) {
    console.log('Error: ' + reason.result.error.message);
  });
</code></pre>

<p>My code for uploading images to storage:</p>

<pre><code>var file = e.target.files[0];
var metadata = {
  contentType: file.type
};
console.log(file);

var uploadTask = photosStorageRef.child(file.name).put(file, metadata);
</code></pre>",41447444,1,0,,2017-01-03 00:00:34.113 UTC,0,2017-01-03 16:00:07.453 UTC,2017-01-03 01:42:24.983 UTC,,4625829,,1004331,1,4,javascript|firebase|firebase-authentication|firebase-storage|google-cloud-vision,1152
Amazon Rekognition for text dertection text in image,52103546,Amazon Rekognition for text dertection text in image,"<p>I have an error about getting text in image using <code>Amazon Rekognition</code> service</p>

<pre><code>Process: com.example.myapplication, PID: 32656
    java.lang.NoSuchFieldError: org.apache.http.conn.ssl.AllowAllHostnameVerifier.INSTANCE
        at org.apache.http.conn.ssl.SSLConnectionSocketFactory.&lt;clinit&gt;(SSLConnectionSocketFactory.java:146)
        at com.amazonaws.http.apache.client.impl.ApacheConnectionManagerFactory.getPreferredSocketFactory(ApacheConnectionManagerFactory.java:86)
        at com.amazonaws.http.apache.client.impl.ApacheConnectionManagerFactory.create(ApacheConnectionManagerFactory.java:63)
        at com.amazonaws.http.apache.client.impl.ApacheConnectionManagerFactory.create(ApacheConnectionManagerFactory.java:56)
        at com.amazonaws.http.apache.client.impl.ApacheHttpClientFactory.create(ApacheHttpClientFactory.java:50)
        at com.amazonaws.http.apache.client.impl.ApacheHttpClientFactory.create(ApacheHttpClientFactory.java:38)
        at com.amazonaws.http.AmazonHttpClient.&lt;init&gt;(AmazonHttpClient.java:317)
        at com.amazonaws.http.AmazonHttpClient.&lt;init&gt;(AmazonHttpClient.java:301)
        at com.amazonaws.AmazonWebServiceClient.&lt;init&gt;(AmazonWebServiceClient.java:194)
        at com.amazonaws.services.rekognition.AmazonRekognitionClient.&lt;init&gt;(AmazonRekognitionClient.java:290)
        at com.amazonaws.services.rekognition.AmazonRekognitionClientBuilder.build(AmazonRekognitionClientBuilder.java:61)
        at com.amazonaws.services.rekognition.AmazonRekognitionClientBuilder.build(AmazonRekognitionClientBuilder.java:27)
        at com.amazonaws.client.builder.AwsSyncClientBuilder.build(AwsSyncClientBuilder.java:46)
        at com.amazonaws.services.rekognition.AmazonRekognitionClientBuilder.defaultClient(AmazonRekognitionClientBuilder.java:45)
        at com.example.myapplication.MainActivity.onCreate(MainActivity.java:59)
        at android.app.Activity.performCreate(Activity.java:5343)
        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1088)
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2340)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2438)
        at android.app.ActivityThread.access$800(ActivityThread.java:160)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1351)
        at android.os.Handler.dispatchMessage(Handler.java:110)
        at android.os.Looper.loop(Looper.java:193)
        at android.app.ActivityThread.main(ActivityThread.java:5368)
        at java.lang.reflect.Method.invokeNative(Native Method)
        at java.lang.reflect.Method.invoke(Method.java:515)
        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:828)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:644)
        at dalvik.system.NativeStart.main(Native Method)
</code></pre>

<p>and this is my code </p>

<pre><code>String photo = Environment.getExternalStorageDirectory().getAbsolutePath()+""/test/testImage.png"";

    ByteBuffer imageBytes;
        try  {

            InputStream inputStream = new FileInputStream(new File(photo));
            imageBytes = ByteBuffer.wrap(IOUtils.toByteArray(inputStream));
            Logger.getLogger(""org.apache.http"").setLevel(Level.FINEST);
            Logger.getLogger(""com.amazonaws"").setLevel(Level.FINEST);

            ImageView imageView = findViewById(R.id.image);
            Glide.with(this)
                    .load(photo)
                    .into(imageView);

                  AmazonRekognition rekognitionClient = AmazonRekognitionClientBuilder.defaultClient();


            DetectTextRequest  request = new DetectTextRequest ()
                    .withImage(new Image()
                            .withBytes(imageBytes));



            DetectTextResult  result = rekognitionClient.detectText(request);
            List&lt;TextDetection&gt; textDetections = result.getTextDetections();

            Log.i(TAG,""Detected lines and words for "" + photo);
                for (TextDetection  text: textDetections) {

                    Log.i(TAG,""Detected: "" + text.getDetectedText());
                    Log.i(TAG,""Confidence: "" + text.getConfidence().toString());


                }
        }
        catch (Exception e)
        {
            Log.e(TAG,e.toString());
        }
</code></pre>

<p>I tried using library 'org.apache.httpcomponents:httpmime:4.3.6' and 'org.apache.httpcomponents:httpclient-android:4.3.6' but for httpclient 'gradle' can't resolve its dependencies.</p>

<p>Did I write something wrong ? also can i use Text in image to get image from local storage ? and does it support Arabic or not ?</p>",,1,0,,2018-08-30 18:46:45.190 UTC,,2018-09-06 21:25:48.923 UTC,2018-09-05 14:42:16.823 UTC,,10151603,,10151603,1,0,android|apache|android-studio|amazon-rekognition,160
Python imp not finding modules,51540658,Python imp not finding modules,"<p>I'm trying to get the Google cloud modules for python working, but am having difficulty in imp seeing the modules correctly for verification purposes. I've even tried a few variants in spelling, but no joy.</p>

<p>I can import the module successfully, but imp cannot verify it:</p>

<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; pip.get_installed_distributions()       
[
    urllib3 1.23 (/home/user/.local/lib/python2.7/site-packages),
    six 1.11.0 (/home/user/.local/lib/python2.7/site-packages),
    setuptools 40.0.0 (/home/user/.local/lib/python2.7/site-packages),
    rsa 3.4.2 (/home/user/.local/lib/python2.7/site-packages),
    requests 2.19.1 (/home/user/.local/lib/python2.7/site-packages),
    pytz 2018.5 (/home/user/.local/lib/python2.7/site-packages),
    pysnmp 4.4.4 (/home/user/.local/lib/python2.7/site-packages),
    pysmi 0.3.1 (/home/user/.local/lib/python2.7/site-packages),
    pycryptodomex 3.6.4 (/home/user/.local/lib/python2.7/site-packages),
    pyasn1 0.4.4 (/home/user/.local/lib/python2.7/site-packages),
    pyasn1-modules 0.2.2 (/home/user/.local/lib/python2.7/site-packages),
    psutil 5.4.6 (/home/user/.local/lib/python2.7/site-packages),
    protobuf 3.6.0 (/home/user/.local/lib/python2.7/site-packages),
    proto-google-cloud-logging-v2 0.91.3 (/home/user/.local/lib/python2.7/site-packages),
    proto-google-cloud-error-reporting-v1beta1 0.15.3 (/home/user/.local/lib/python2.7/site-packages),
    proto-google-cloud-datastore-v1 0.90.4 (/home/user/.local/lib/python2.7/site-packages),
    ply 3.11 (/home/user/.local/lib/python2.7/site-packages),
    oauth2client 3.0.0 (/home/user/.local/lib/python2.7/site-packages),
    idna 2.7 (/home/user/.local/lib/python2.7/site-packages),
    httplib2 0.11.3 (/home/user/.local/lib/python2.7/site-packages),
    grpcio 1.13.0 (/home/user/.local/lib/python2.7/site-packages),
    grpc-google-iam-v1 0.11.4 (/home/user/.local/lib/python2.7/site-packages),
    googleapis-common-protos 1.5.3 (/home/user/.local/lib/python2.7/site-packages),
    google 2.0.1 (/home/user/.local/lib/python2.7/site-packages),
    google-resumable-media 0.3.1 (/home/user/.local/lib/python2.7/site-packages),
    google-gax 0.15.16 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud 0.32.0 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-vision 0.29.0 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-videointelligence 1.0.1 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-translate 1.3.1 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-trace 0.17.0 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-storage 1.10.0 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-speech 0.30.0 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-spanner 0.29.0 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-runtimeconfig 0.28.1 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-resource-manager 0.28.1 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-pubsub 0.30.1 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-monitoring 0.28.1 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-logging 1.4.0 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-language 1.0.2 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-firestore 0.28.0 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-error-reporting 0.28.0 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-dns 0.28.0 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-datastore 1.4.0 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-core 0.28.1 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-container 0.1.1 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-bigtable 0.28.1 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-bigquery 0.28.0 (/home/user/.local/lib/python2.7/site-packages),
    google-cloud-bigquery-datatransfer 0.1.1 (/home/user/.local/lib/python2.7/site-packages),
    google-auth 1.5.0 (/home/user/.local/lib/python2.7/site-packages),
    google-api-core 1.2.1 (/home/user/.local/lib/python2.7/site-packages),
    gapic-google-cloud-logging-v2 0.91.3 (/home/user/.local/lib/python2.7/site-packages),
    gapic-google-cloud-error-reporting-v1beta1 0.15.3 (/home/user/.local/lib/python2.7/site-packages),
    gapic-google-cloud-datastore-v1 0.15.3 (/home/user/.local/lib/python2.7/site-packages),
    futures 3.2.0 (/home/user/.local/lib/python2.7/site-packages),
    future 0.16.0 (/home/user/.local/lib/python2.7/site-packages),
    enum34 1.1.6 (/home/user/.local/lib/python2.7/site-packages),
    dill 0.2.8.2 (/home/user/.local/lib/python2.7/site-packages),
    chardet 3.0.4 (/home/user/.local/lib/python2.7/site-packages),
    certifi 2018.4.16 (/home/user/.local/lib/python2.7/site-packages),
    cachetools 2.1.0 (/home/user/.local/lib/python2.7/site-packages),
    beautifulsoup4 4.6.0 (/home/user/.local/lib/python2.7/site-packages)
]
&gt;&gt;&gt; imp.find_module('google-cloud')  
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named google-cloud
&gt;&gt;&gt; imp.find_module('google')      
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named google
&gt;&gt;&gt; import google.cloud
&gt;&gt;&gt; imp.find_module('google')
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named google
&gt;&gt;&gt; imp.find_module('google.cloud')
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named google.cloud
&gt;&gt;&gt; imp.find_module('google-cloud')
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named google-cloud
</code></pre>

<p>I have solved my problem by using another verification method, but I would like to understand what went wrong. Most, but not all, other modules can be verified correctly.</p>

<p>What is required before <code>imp</code> can <code>find_module</code>?</p>",,1,1,,2018-07-26 13:58:46.150 UTC,,2018-09-24 10:23:16.423 UTC,2018-09-24 10:23:16.423 UTC,,8831875,,2135406,1,1,python|import|pip|google-cloud-platform,69
"Field names are zzktl, zzktm, zzktn in Watches",47427567,"Field names are zzktl, zzktm, zzktn in Watches","<p>I am trying to test <a href=""https://code.tutsplus.com/tutorials/reading-qr-codes-using-the-mobile-vision-api--cms-24680"" rel=""nofollow noreferrer"">this sample code</a> which uses Google's Vision API. In <code>receiveDetections</code>, I tried to view what fields the argument has. The problem is that field names are cryptic. Why are the field names cryptic?</p>

<p><a href=""https://i.stack.imgur.com/pwnkz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pwnkz.png"" alt=""enter image description here""></a></p>

<pre><code>dependencies {
   ....
    compile 'com.google.android.gms:play-services-vision:11.6.0'
}
</code></pre>",,0,2,,2017-11-22 05:42:15.047 UTC,,2017-11-22 05:55:17.640 UTC,2017-11-22 05:55:17.640 UTC,,455796,,455796,1,1,android|google-play-services,15
"Get Lines and Paragraphs, not symbols from Google Vision API OCR on PDF",51972479,"Get Lines and Paragraphs, not symbols from Google Vision API OCR on PDF","<p>I am attempting to use the now supported PDF/TIFF Document Text Detection from the Google Cloud Vision API. Using their example code I am able to submit a PDF and receive back a JSON object with the extracted text.  My issue is that the JSON file that is saved to GCS only contains bounding boxes and text for ""symbols"", i.e. each character in each word.  This makes the JSON object quite unwieldy and very difficult to use.  I'd like to be able to get the text and bounding boxes for ""LINES"", ""PARAGRAPHS"" and ""BLOCKS"", but I can't seem to find a way to do it via the <code>AsyncAnnotateFileRequest()</code> method.  </p>

<p>The sample code is as follows:</p>

<pre><code>def async_detect_document(gcs_source_uri, gcs_destination_uri):
    """"""OCR with PDF/TIFF as source files on GCS""""""
    # Supported mime_types are: 'application/pdf' and 'image/tiff'
    mime_type = 'application/pdf'

    # How many pages should be grouped into each json output file.
    batch_size = 2

    client = vision.ImageAnnotatorClient()

    feature = vision.types.Feature(
        type=vision.enums.Feature.Type.DOCUMENT_TEXT_DETECTION)

    gcs_source = vision.types.GcsSource(uri=gcs_source_uri)
    input_config = vision.types.InputConfig(
        gcs_source=gcs_source, mime_type=mime_type)

    gcs_destination = vision.types.GcsDestination(uri=gcs_destination_uri)
    output_config = vision.types.OutputConfig(
        gcs_destination=gcs_destination, batch_size=batch_size)

    async_request = vision.types.AsyncAnnotateFileRequest(
        features=[feature], input_config=input_config,
        output_config=output_config)

    operation = client.async_batch_annotate_files(
        requests=[async_request])

    print('Waiting for the operation to finish.')
    operation.result(timeout=180)

    # Once the request has completed and the output has been
    # written to GCS, we can list all the output files.
    storage_client = storage.Client()

    match = re.match(r'gs://([^/]+)/(.+)', gcs_destination_uri)
    bucket_name = match.group(1)
    prefix = match.group(2)

    bucket = storage_client.get_bucket(bucket_name=bucket_name)

    # List objects with the given prefix.
    blob_list = list(bucket.list_blobs(prefix=prefix))
    print('Output files:')
    for blob in blob_list:
        print(blob.name)

    # Process the first output file from GCS.
    # Since we specified batch_size=2, the first response contains
    # the first two pages of the input file.
    output = blob_list[0]

    json_string = output.download_as_string()
    response = json_format.Parse(
        json_string, vision.types.AnnotateFileResponse())

    # The actual response for the first page of the input file.
    first_page_response = response.responses[0]
    annotation = first_page_response.full_text_annotation

    # Here we print the full text from the first page.
    # The response contains more information:
    # annotation/pages/blocks/paragraphs/words/symbols
    # including confidence scores and bounding boxes
    print(u'Full text:\n{}'.format(
        annotation.text))
</code></pre>",52086299,1,1,,2018-08-22 17:46:33.610 UTC,0,2018-09-02 00:01:06.887 UTC,2018-09-02 00:01:06.887 UTC,,328036,,1887261,1,16,python|google-cloud-platform|google-cloud-vision,1934
Limit to number of requests to Microsoft Face API,38967064,Limit to number of requests to Microsoft Face API,"<p>Does anyone know if the ""Out of call volume quota"" is exclusively for free trial user and if we subscribe to the monthly plan, there will be no limit to the number of calls to Microsoft Face API?
  I would also like to know since the API can take 10 requests per second from a paid key, does that mean by requesting with different processes simultaneously, the total process time can be shortened?</p>

<p>Thank you </p>",,1,0,,2016-08-16 05:14:10.917 UTC,,2016-08-16 16:36:38.273 UTC,,,,,6720262,1,0,microsoft-cognitive,853
Send images which are not publicly accessible on the internet for Azure Face API Training,53493720,Send images which are not publicly accessible on the internet for Azure Face API Training,"<p>I am using <a href=""https://docs.microsoft.com/en-us/azure/cognitive-services/face/apireference"" rel=""nofollow noreferrer"">Azure Face API</a> for a face recognition project. I can only send images using URL. But since images are stored on our on-premise servers, authorisation is required to access those images using the URL. How can I pass images using the <a href=""https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f3039523b"" rel=""nofollow noreferrer"">REST API</a></p>",,0,2,,2018-11-27 06:10:49.670 UTC,,2018-11-27 06:10:49.670 UTC,,,,,1820828,1,0,azure|azure-cognitive-services,31
Parsing rekognition get_face_search results,50498421,Parsing rekognition get_face_search results,"<p>I am trying to parse out Face Matches from the results of the get_face_search() AWS Rekognition API. It outputs an array of Persons, within that array is another array of FaceMatches for a given person and timestamp. I want to take information from the FaceMatches array and be able to loop through the array of Face Matches. </p>

<p>I have done something similar before for single arrays and looped successfully, but I am missing something trivial here perhaps.</p>

<p>Here is output from API:</p>

<pre><code>    Response:
{
  ""JobStatus"": ""SUCCEEDED"",
  ""NextToken"": ""U5EdbZ+86xseDBfDlQ2u8QhSVzbdodDOmX/gSbwIgeO90l2BKWvJEscjUDmA6GFDCSSfpKA4"",
  ""VideoMetadata"": {
    ""Codec"": ""h264"",
    ""DurationMillis"": 6761,
    ""Format"": ""QuickTime / MOV"",
    ""FrameRate"": 30.022184371948242,
    ""FrameHeight"": 568,
    ""FrameWidth"": 320
  },
  ""Persons"": [
    {
      ""Timestamp"": 0,
      ""Person"": {
        ""Index"": 0,
        ""BoundingBox"": {
          ""Width"": 0.987500011920929,
          ""Height"": 0.7764084339141846,
          ""Left"": 0.0031250000465661287,
          ""Top"": 0.2042253464460373
        },
        ""Face"": {
          ""BoundingBox"": {
            ""Width"": 0.6778846383094788,
            ""Height"": 0.3819068372249603,
            ""Left"": 0.10096154361963272,
            ""Top"": 0.2654387652873993
          },
          ""Landmarks"": [
            {
              ""Type"": ""eyeLeft"",
              ""X"": 0.33232420682907104,
              ""Y"": 0.4194057583808899
            },
            {
              ""Type"": ""eyeRight"",
              ""X"": 0.5422032475471497,
              ""Y"": 0.41616082191467285
            },
            {
              ""Type"": ""nose"",
              ""X"": 0.45633792877197266,
              ""Y"": 0.4843473732471466
            },
            {
              ""Type"": ""mouthLeft"",
              ""X"": 0.37002310156822205,
              ""Y"": 0.567118763923645
            },
            {
              ""Type"": ""mouthRight"",
              ""X"": 0.5330674052238464,
              ""Y"": 0.5631639361381531
            }
          ],
          ""Pose"": {
            ""Roll"": -2.2475271224975586,
            ""Yaw"": 4.371307373046875,
            ""Pitch"": 6.83940315246582
          },
          ""Quality"": {
            ""Brightness"": 40.40004348754883,
            ""Sharpness"": 99.95819854736328
          },
          ""Confidence"": 99.87971496582031
        }
      },
      ""FaceMatches"": [
        {
          ""Similarity"": 99.81229400634766,
          ""Face"": {
            ""FaceId"": ""4699a1eb-9f6e-415d-8716-eef141d23433a"",
            ""BoundingBox"": {
              ""Width"": 0.6262923432480737,
              ""Height"": 0.46972032423490747,
              ""Left"": 0.130435005324523403604,
              ""Top"": 0.13354002343240603
            },
            ""ImageId"": ""1ac790eb-615a-111f-44aa-4017c3c315ad"",
            ""Confidence"": 99.19400024414062
          }
        }
      ]
    },
    {
      ""Timestamp"": 66,
      ""Person"": {
        ""Index"": 0,
        ""BoundingBox"": {
          ""Width"": 0.981249988079071,
          ""Height"": 0.7764084339141846,
          ""Left"": 0.0062500000931322575,
          ""Top"": 0.2042253464460373
        }
      }
    },
    {
      ""Timestamp"": 133,
      ""Person"": {
        ""Index"": 0,
        ""BoundingBox"": {
          ""Width"": 0.9781249761581421,
          ""Height"": 0.783450722694397,
          ""Left"": 0.0062500000931322575,
          ""Top"": 0.19894365966320038
        }
      }
    },
    {
      ""Timestamp"": 199,
      ""Person"": {
        ""Index"": 0,
        ""BoundingBox"": {
          ""Width"": 0.981249988079071,
          ""Height"": 0.783450722694397,
          ""Left"": 0.0031250000465661287,
          ""Top"": 0.19894365966320038
        },
        ""Face"": {
          ""BoundingBox"": {
            ""Width"": 0.6706730723381042,
            ""Height"": 0.3778440058231354,
            ""Left"": 0.10817307233810425,
            ""Top"": 0.26679307222366333
          },
          ""Landmarks"": [
            {
              ""Type"": ""eyeLeft"",
              ""X"": 0.33244985342025757,
              ""Y"": 0.41591548919677734
            },
            {
              ""Type"": ""eyeRight"",
              ""X"": 0.5446155667304993,
              ""Y"": 0.41204410791397095
            },
            {
              ""Type"": ""nose"",
              ""X"": 0.4586191177368164,
              ""Y"": 0.479543000459671
            },
            {
              ""Type"": ""mouthLeft"",
              ""X"": 0.37614554166793823,
              ""Y"": 0.5639738440513611
            },
            {
              ""Type"": ""mouthRight"",
              ""X"": 0.5334802865982056,
              ""Y"": 0.5592300891876221
            }
          ],
          ""Pose"": {
            ""Roll"": -2.4899401664733887,
            ""Yaw"": 3.7596628665924072,
            ""Pitch"": 6.3544135093688965
          },
          ""Quality"": {
            ""Brightness"": 40.46360778808594,
            ""Sharpness"": 99.95819854736328
          },
          ""Confidence"": 99.89802551269531
        }
      },
      ""FaceMatches"": [
        {
          ""Similarity"": 99.80543518066406,
          ""Face"": {
            ""FaceId"": ""4699a1eb-9f6e-415d-8716-eef141d9223a"",
            ""BoundingBox"": {
              ""Width"": 0.626294234234737,
              ""Height"": 0.469234234890747,
              ""Left"": 0.130435002334234604,
              ""Top"": 0.13354023423449180603
            },
            ""ImageId"": ""1ac790eb-615a-111f-44aa-4017c3c315ad"",
            ""Confidence"": 99.19400024414062
          }
        }
      ]
    },
    {
      ""Timestamp"": 266,
      ""Person"": {
        ""Index"": 0,
        ""BoundingBox"": {
          ""Width"": 0.984375,
          ""Height"": 0.7852112650871277,
          ""Left"": 0,
          ""Top"": 0.19718310236930847
        }
      }
    }
  ],
</code></pre>

<p>I have isolated the timestamps (just testing my approach) using the following:</p>

<pre><code>timestamps = [m['Timestamp'] for m in response['Persons']]
Output is this, as expected - [0, 66, 133, 199, 266]
</code></pre>

<p>However, when I try the same thing with FaceMatches, I get an error.</p>

<pre><code>[0, 66, 133, 199, 266]
list indices must be integers or slices, not str: TypeError
Traceback (most recent call last):
  File ""/var/task/lambda_function.py"", line 40, in lambda_handler
    matches = [m['FaceMatches']['Face']['FaceId'] for m in response['Persons']]
  File ""/var/task/lambda_function.py"", line 40, in &lt;listcomp&gt;
    matches = [m['FaceMatches']['Face']['FaceId'] for m in response['Persons']]
TypeError: list indices must be integers or slices, not str
</code></pre>

<p>What I need to end up with is for each face that is matched:</p>

<pre><code>Timestamp
FaceID
Similarity
</code></pre>

<p>Can anybody shed some light on this for me?</p>",50527504,1,0,,2018-05-23 22:35:44.863 UTC,,2018-05-25 10:49:53.253 UTC,2018-05-23 23:11:45.563 UTC,,9147327,,9147327,1,0,python|arrays|python-3.x|amazon-rekognition,156
drawing points on face based on landmarks,47353294,drawing points on face based on landmarks,"<p>I am trying to draw points on face based on land marks i am using google vision api to detect face and landmarks, <strong>My question is how i calculate the value of <code>scale</code> variable</strong>  </p>

<pre><code> for (Landmark landmark : face.getLandmarks()) {
        int cx = (int) (landmark.getPosition().x * scale);
        int cy = (int) (landmark.getPosition().y * scale);
        canvas.drawCircle(cx, cy, 10, paint);
    }
</code></pre>

<p>I find this solution on <a href=""http://forum.yourwebapp.mobi/an-introduction-to-face-detection-on-android/"" rel=""nofollow noreferrer"">this</a> website but this solution not work for me because i trying to use live camera preview (e.g. real time) not try to detect from image</p>

<pre><code> double scale = Math.min( viewWidth / imageWidth, viewHeight / imageHeight );
</code></pre>",,0,0,,2017-11-17 14:45:59.720 UTC,1,2017-11-17 15:13:03.140 UTC,2017-11-17 15:13:03.140 UTC,,2649012,,5727285,1,2,java|android|face-detection|google-vision,313
OCR on noisy image,45629810,OCR on noisy image,"<p>I have a set of images which have one digit on them and a noisy background. I want to identify the digit on the image. I tried these things:</p>

<p>-Google Vision API</p>

<p>-Python tesseract</p>

<p>-OpenCV's Example here: <a href=""http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_ml/py_knn/py_knn_opencv/py_knn_opencv.html#ocr-of-hand-written-digits"" rel=""nofollow noreferrer"">http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_ml/py_knn/py_knn_opencv/py_knn_opencv.html#ocr-of-hand-written-digits</a>
but I could not make it work. The vision API was not able to identify the digit itself and tesseract work good for simple/plain backgrounds. My backgrounds are very noisy.</p>

<p>Please suggest some solution. Is this at all possible?</p>

<p>Any links for a head start are also fine, i'll do the exploration.</p>

<p>BTW The image looks like this:<a href=""https://i.stack.imgur.com/XBfLD.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XBfLD.jpg"" alt=""enter image description here""></a></p>

<p>I can share the failed results if required </p>",,0,0,,2017-08-11 07:53:43.480 UTC,1,2017-08-11 07:53:43.480 UTC,,,,,6399494,1,1,python|opencv|ocr|tesseract|python-tesseract,301
Does anyone knows how to use AWSRekognition SDK in order to detect faces in images in swift 4?,51209404,Does anyone knows how to use AWSRekognition SDK in order to detect faces in images in swift 4?,"<p>I already integrated the AWSRekognitionthem into my project. and I make sure I'm  connected to AWS, by this code </p>

<pre><code>import AWSCore
      //. . .
AWSDDLog.add(AWSDDTTYLogger.sharedInstance)
AWSDDLog.sharedInstance.logLevel = .info
</code></pre>

<p>and the respnse is </p>

<blockquote>
  <p>Welcome to AWS! You are connected successfully</p>
</blockquote>

<p>but when I write this line it give me an error </p>

<pre><code>let rekognitionClient = AWSRekognition.defaultRekognition()
</code></pre>",,1,0,,2018-07-06 11:27:04.657 UTC,,2018-07-20 02:59:33.293 UTC,,,,,10041748,1,-1,ios|xcode|swift4,34
AWS - Face comparison command line mistake,53718791,AWS - Face comparison command line mistake,"<p>I am new to 'AWS'.. I trying to compare two faces using command line.. I use this code for face comparison</p>

<pre><code>aws rekognition compare-faces \
--source-image '{""S3Object"":{""Bucket"":""trailface"",""Name"":""image4.jpg""}}' \
--target-image '{""S3Object"":{""Bucket"":""trailface"",""Name"":""image5.jpg""}}'
</code></pre>

<p>After running this code I got <code>Unknown options: \, \</code></p>

<p><a href=""https://i.stack.imgur.com/SrR90.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SrR90.png"" alt=""enter image description here""></a></p>

<p>After I removed the \ I got another problem
<a href=""https://i.stack.imgur.com/4tYuG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4tYuG.png"" alt=""enter image description here""></a></p>

<p>Help me to solve this...</p>",54001794,2,2,,2018-12-11 06:46:55 UTC,,2019-01-02 05:48:18.977 UTC,2018-12-11 08:31:32.430 UTC,,9265440,,9265440,1,1,amazon-web-services,44
How to get a text based on its boundingPoly in Google Cloud Vision API,55223343,How to get a text based on its boundingPoly in Google Cloud Vision API,"<p>how to get a text based on its boundingPoly in Google Cloud Vision API?</p>

<p>I run below to get its texts</p>

<pre><code>{
      ""requests"": [
        {
          ""image"": {
            ""content"": 
                        {
              ""imageUri"": ""http://example.com/myimage_contains_text.jpg""
            }
          },
          ""features"": [
            {
              ""type"": ""DOCUMENT_TEXT_DETECTION""
            }
          ]
        }
      ]
    }
</code></pre>

<p>and get response, :</p>

<pre><code>....
{
          ""description"": ""12345678990123456"",
          ""boundingPoly"": {
            ""vertices"": [
              {
                ""x"": 121,
                ""y"": 71
              },
              {
                ""x"": 304,
                ""y"": 67
              },
              {
                ""x"": 304,
                ""y"": 85
              },
              {
                ""x"": 121,
                ""y"": 89
              }
            ]
          }
        },
       ....
</code></pre>

<p>I just want to get ""12345678990123456"",  is there any way to get it by passing its boundingPoly in Google Cloud Vision API?</p>

<p>please help</p>

<p>many thanks in advance</p>

<p>Don</p>",,0,0,,2019-03-18 14:11:59.327 UTC,,2019-03-18 14:11:59.327 UTC,,,,,6448745,1,0,google-cloud-platform,15
Google cloud vision api:: AttributeError: 'WebDetection' object has no attribute 'best_guess_labels',51116095,Google cloud vision api:: AttributeError: 'WebDetection' object has no attribute 'best_guess_labels',"<p>I am trying to call the function ""detect web"" from Google Cloud Vision API using python. However I am not able to call one of its method named ""best_guess_labels"". When I tried to call the method, it throws out an error as 
""AttributeError: 'WebDetection' object has no attribute 'best_guess_labels':</p>

<p>WebDetection is a json file that was created using this link and stored into a local folder ==> <a href=""https://cloud.google.com/docs/authentication/getting-started"" rel=""nofollow noreferrer"">https://cloud.google.com/docs/authentication/getting-started</a></p>

<pre><code>import os
os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]=""WebDetection.json""
</code></pre>

<p>The function of ""detect web"" is taken from this link --> <a href=""https://cloud.google.com/vision/docs/detecting-web"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/detecting-web</a></p>

<p>Here is the function copied from the above link for your ready reference. </p>

<pre><code>def detect_web(path):
    """"""Detects web annotations given an image.""""""
    client = vision.ImageAnnotatorClient()

with io.open(path, 'rb') as image_file:
    content = image_file.read()

image = vision.types.Image(content=content)

response = client.web_detection(image=image)
annotations = response.web_detection

if annotations.best_guess_labels:
    for label in annotations.best_guess_labels:
        print('\nBest guess label: {}'.format(label.label))

if annotations.pages_with_matching_images:
    print('\n{} Pages with matching images found:'.format(
        len(annotations.pages_with_matching_images)))

    for page in annotations.pages_with_matching_images:
        print('\n\tPage url   : {}'.format(page.url))

        if page.full_matching_images:
            print('\t{} Full Matches found: '.format(
                   len(page.full_matching_images)))

            for image in page.full_matching_images:
                print('\t\tImage url  : {}'.format(image.url))

        if page.partial_matching_images:
            print('\t{} Partial Matches found: '.format(
                   len(page.partial_matching_images)))

            for image in page.partial_matching_images:
                print('\t\tImage url  : {}'.format(image.url))

if annotations.web_entities:
    print('\n{} Web entities found: '.format(
        len(annotations.web_entities)))

    for entity in annotations.web_entities:
        print('\n\tScore      : {}'.format(entity.score))
        print(u'\tDescription: {}'.format(entity.description))

if annotations.visually_similar_images:
    print('\n{} visually similar images found:\n'.format(
        len(annotations.visually_similar_images)))

    for image in annotations.visually_similar_images:
        print('\tImage url    : {}'.format(image.url))
</code></pre>

<p>However, When i execute the above function using this code</p>

<pre><code>detect_web(""download.jpg"")
</code></pre>

<p>I am getting the below error:</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-71-6d38dd9b3a76&gt; in &lt;module&gt;()
----&gt; 1 detect_web(""download.jpg"")

&lt;ipython-input-70-c127dc709a32&gt; in detect_web(path)
     13     annotations = response.web_detection
     14 
---&gt; 15     if annotations.best_guess_labels:
     16         for label in annotations.best_guess_labels:
     17             print('\nBest guess label: {}'.format(label.label))

AttributeError: 'WebDetection' object has no attribute 'best_guess_labels'
</code></pre>

<p>I tried to debugging and found that the ""best_guess_labels"" is not part of the Json file. I am not sure whether the json file got corrupted, but i tried to redo the exercise, but i still getting the same error. </p>

<p>What might have caused the issue?</p>",,1,4,,2018-06-30 15:08:56.727 UTC,1,2018-10-12 14:51:15.380 UTC,,,,,8241998,1,0,python-3.x|google-cloud-platform|google-cloud-vision,344
Rekognition Facial Recognition parsing JSON with PHP,13617087,Rekognition Facial Recognition parsing JSON with PHP,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://stackoverflow.com/questions/4035742/parse-json-in-php"">Parse Json in php</a>  </p>
</blockquote>



<p>Im using an API to CURL a submitted user image to a remote server, the response is in JSON and I'm not sure how to parse it.</p>

<p>Here is my php CURL file</p>

<pre><code>&lt;?php
$api_key = 'xxxxxx';
$api_secret = 'xxxxx';
$Dirty_uid = $_GET['uid'];
$uid = htmlentities($Dirty_uid);
$Dirty_img = $_GET['img'];
$img = htmlentities($Dirty_img);

$query2 = 'http://rekognition.com/func/api/?api_key='.$api_key.'&amp;api_secret='.$api_secret.'&amp;jobs=face_recognize&amp;urls='.$img.'&amp;name_space=xxx&amp;user_id=xxx';


$ch2 = curl_init();
curl_setopt($ch2, CURLOPT_RETURNTRANSFER, true);
curl_setopt($ch2, CURLOPT_URL, $query2);    // get the url contents

$data2 = curl_exec($ch2); // execute curl request
curl_close($ch2);

$json2 = json_decode($data2);
print_r($json2)
?&gt;
</code></pre>

<p>and here is a sample of the JSON data that comes back (Generic)</p>

<p>Code:   </p>

<pre><code>{
url: ""http://farm3.static.flickr.com/2566/3896283279_0209be7a67.jpg"",
face_detection: [
  {
      boundingbox:
      {
        tl: {
          x: 50,
          y: 118
         },
         size:{
          width:232;
          height:232;
         }
      }
      name: ""mona_lisa:0.92,barack_obama:0.02,mark_zuckerberg:0.01,""
  }
  ]

usage: {
status: ""success"",
quota: 999,
api_id: ""xxx""
}
}
</code></pre>

<p>Where the json string says ""name"" I need my script to only print the username if the number (after the : is higher then a threshold (lets say .70 for now).</p>

<p>How do I do this? I've worked with XML api's before and returning the data was simple with a
Code:   </p>

<pre><code>$xml = simplexml_load_string($data);
</code></pre>

<p>type thing.</p>",,1,0,,2012-11-29 00:56:35.540 UTC,,2013-10-21 20:20:09.527 UTC,2017-05-23 11:48:56.767 UTC,,-1,,1861726,1,2,php|json|api|parsing|pattern-recognition,445
Android Studio how to import non-project library?,34785457,Android Studio how to import non-project library?,"<p>I'm using Android Studio</p>

<p>My project needs import google vision api</p>

<p>following</p>

<p><a href=""https://i.stack.imgur.com/c9IFo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c9IFo.png"" alt=""1""></a>
    com/google/vision/v1a1pha1</p>

<p>how can i do to import java files on my project?</p>

<p><a href=""https://i.stack.imgur.com/D4GbM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D4GbM.png"" alt=""2""></a></p>",34785559,1,0,,2016-01-14 09:08:44.587 UTC,,2016-01-14 09:13:54.307 UTC,2016-01-14 09:12:13 UTC,,1957401,,2873792,1,0,android|import|google-api|android-gradle|pom.xml,156
Google Cloud Vision API from Google Apps Script?,48721858,Google Cloud Vision API from Google Apps Script?,"<p>I've been accessing the Google Cloud Vision API from a script in a Google Sheet using UrlFetch and the REST API. </p>

<p>Until I got ""UrlFetch failed because too much upload bandwidth was used"" I didn't even know there was a quota on UrlFetch!</p>

<p>Is the a way to access Google Cloud APIs from a Google Apps Script so I can dodge the quota?</p>",,1,0,,2018-02-10 14:35:26.713 UTC,,2018-02-19 11:43:34.070 UTC,2018-02-11 07:45:34.650 UTC,,322020,,9342719,1,0,google-apps-script|google-cloud-vision,246
How to Authorize an API call c#,37796580,How to Authorize an API call c#,"<p>I am trying to use <em>Microsoft face API</em> software in <code>c#</code>. The first step says I need to authorize the API read below for instructions:</p>

<p>Every call to the Face API requires a subscription key. This key needs to be either passed through a query string parameter, or specified in the request header. To pass the subscription key through query string, please refer to the request URL for the Face - Detect as an example:</p>

<pre><code>https://api.projectoxford.ai/face/v1.0/detect[?returnFaceId][&amp;returnFaceLandmarks][&amp;returnFaceAttributes]&amp;subscription-key=&lt;Your subscription key&gt;
</code></pre>

<p>As an alternative, the subscription key can also be specified in the HTTP request header: <code>ocp-apim-subscription-key: &lt;Your subscription key&gt;</code> When using a client library, the subscription key is passed in through the constructor of the <code>FaceServiceClient</code> class. For example:</p>

<pre><code>faceServiceClient = new FaceServiceClient(""Your subscription key"");   
</code></pre>

<p>The subscription key can be obtained from the Marketplace page of your Azure management portal. See Subscriptions.</p>

<p>Now I am confused at how I would go about this. So first I tried using the query string way of doing it .</p>

<pre><code>sing System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using System.Net;
using Microsoft.ProjectOxford.Face;
using Microsoft.ProjectOxford;
using System.Web;
using System.Web.UI;

namespace faceAPI
{
    class Program
    {
        string fullname1 = Request.QueryString[""https://api.projectoxford.ai/face/v1.0/detect[?returnFaceId][&amp;returnFaceLandmarks][&amp;returnFaceAttributes]&amp;subscription-key=&lt;3ac0d7ef76274d59837a6c4ce60ce309&gt;""];
        string fullname2 = Request[""https://api.projectoxford.ai/face/v1.0/detect[?returnFaceId][&amp;returnFaceLandmarks][&amp;returnFaceAttributes]&amp;subscription-key=&lt;3ac0d7ef76274d59837a6c4ce60ce309&gt;""];

        static void Main(string[] args)
        {

        }
    }
}
</code></pre>

<p>But after i do this i get an error saying : </p>

<blockquote>
  <p>Error 3 The name 'Request' does not exist in the current context  </p>
</blockquote>",,0,6,,2016-06-13 18:16:09.257 UTC,,2016-06-13 20:30:15.763 UTC,2016-06-13 20:30:15.763 UTC,,1145114,,6461009,1,1,c#|request|http-headers|httprequest|query-string,337
java.lang.VerifyError exception while authenticating for Google Vision Api,48262231,java.lang.VerifyError exception while authenticating for Google Vision Api,"<p>The code for vision api works fine when using a java application however when using spring a java.lang.VerifyError exception is thrown on the following line.</p>

<pre><code>Labels app = new Labels(label.getVisionService());
</code></pre>

<p>It was certain that i had specified the json credentials for the system and not the web app so i have included the following bean in my root-context.xml:-</p>

<pre><code>&lt;bean id=""googleVisionApiAuthenticator"" class=""com.something.mypackage.Labels""&gt;
&lt;property name=""APPLICATION_NAME"" value=""myCompany-VisionLabelSample/1.0"" /&gt;
&lt;property name=""MAX_LABELS"" value=""3"" /&gt;
&lt;/bean&gt;
</code></pre>

<p>After inclusion of these lines in the root-context its gives page not found.</p>",48341143,1,0,,2018-01-15 11:37:51.407 UTC,,2018-01-19 12:28:28.677 UTC,,,,,8669515,1,0,spring-mvc|google-vision,21
How to read results of 4 dimensional array in C# from tensorflow model output with image object detection,54580554,How to read results of 4 dimensional array in C# from tensorflow model output with image object detection,"<p>I'm trying to process an image in C# with a pre-trained tensorflow model exported from Azure Custom Vision utilizing Object Detection. How do I parse the results of the graph, which come back as a 4 dimensional array. I am looking to get tag, prediction %, and bounding box.</p>

<p>I have been working with tensorflow for a few weeks now so this is all pretty new to me. I have had my code working with a similar model exported from Azure Custom Vision, but utilizing Image Classification and the results were much easier to read. Now switching to a hopefully more accurate model I'm unsure how to read the results. I've been able to parse results into the 4 dimensional array, but can't make sense of it after that. I was able to verify the the model output node using tensorboard and it is a DT_FLOAT.</p>

<p>I am using the TensowflowSharp library at version 1.12.0</p>

<p>Here is an example I have been somewhat following, but I think the model used in this example was trained differently than the one provided by Azure. Which yields different results to read from the graph. I get undefined when trying the same fetch operations in the example from the link below. This is also confirmed by using graph.getEnumeration() on both my model and the model from the example.
<a href=""https://github.com/migueldeicaza/TensorFlowSharp/tree/master/Examples/ExampleObjectDetection"" rel=""nofollow noreferrer"">C# Tensorflow Object Detection Example</a></p>

<p>Model and Label (tag) declarations:</p>

<pre><code>private static readonly byte[] model = File.ReadAllBytes(Path.Combine(""./Assets/model.pb""));
private static readonly string[] labels = File.ReadAllLines(Path.Combine(""./Assets/labels.txt""));
</code></pre>

<p>Input param for image:</p>

<pre><code>[Blob(""{data.url}"", FileAccess.Read)] Stream input,
</code></pre>

<p>Processing:</p>

<pre><code>using (var graph = new TFGraph()) {                          
     graph.Import(new TFBuffer(model));


     using (var session = new TFSession(graph)) {                                
          var tensor = TensorFlowHelper.CreateTensorFromImageFile(input);
          var runner = session.GetRunner();
          runner.AddInput(graph[""Placeholder""][0], tensor)
                .Fetch(graph[""model_outputs""][0]);
          var output = runner.Run();
          var result = output[0];
          var probabilities = ((float[][][][])result.GetValue(jagged: true))[0];
          Console.WriteLine(probabilities);
      }
}
</code></pre>

<p>TensorFlowHelper.CreateTensorFromImageFile:</p>

<pre><code>var tensor = TFTensor.CreateString(contents);
TFOutput input, output;
// Construct a graph to normalize the image
using (var graph = ConstructGraphToNormalizeImage(out input, out output, destinationDataType)) {
// Execute that graph to normalize this one image
    using (var session = new TFSession(graph)) {
        var normalized = session.Run(
               inputs: new[] { input },
               inputValues: new[] { tensor },                                            
               outputs: new[] { output });
        return normalized[0];
    }
 }
</code></pre>

<p>ConstructGraphToNormalizeImage:</p>

<pre><code>const int W = 224;
const int H = 224;
const float Mean = 117;
const float Scale = 1;

var graph = new TFGraph();
input = graph.Placeholder(TFDataType.String);
output = graph.Cast(graph.Div(
            x: graph.Sub(
              x: graph.ResizeBilinear(
                images: graph.ExpandDims(
                      input: graph.Cast(
                          graph.DecodeJpeg(contents: input, channels: 3), DstT: TFDataType.Float),
                      dim: graph.Const(0, ""make_batch"")),
                size: graph.Const(new int[] { W, H }, ""size"")),
              y: graph.Const(Mean, ""mean"")),
            y: graph.Const(Scale, ""scale"")), destinationDataType);

return graph;
</code></pre>

<p>I would expect to be able to parse out similar results to Azure's prediction API endpoint. Similar to results found in their documentation.
<a href=""https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/csharp-tutorial-od#use-the-prediction-endpoint"" rel=""nofollow noreferrer"">Microsoft Custom Vision Image Prediction API Documentation</a></p>",,0,0,,2019-02-07 19:08:02.567 UTC,2,2019-02-07 19:08:02.567 UTC,,,,,11029449,1,2,c#|tensorflow|azure-functions|object-detection,92
Passing HTML input file as the parameter of Firebase Cloud Function,50182544,Passing HTML input file as the parameter of Firebase Cloud Function,"<p>I am pretty new this area and I started firebase cloud function 2 days ago.</p>

<p>Sorry, I am still a student so I might not understand clearly some documentation.</p>

<p>I tried to figure out how the parameter is passed from my client-side javascript to firebase cloud function.</p>

<p>my cloud function</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>exports.OCR = functions.https.onCall((req) =&gt; {
  const vision = require('@google-cloud/vision');
  // Creates a client
  const client = new vision.ImageAnnotatorClient();
  console.log(req);
  // Performs label detection on the image file
  client
    .documentTextDetection(req)
    .then((results) =&gt; {
      console.log(""Entered"");
      console.log(req);
      const fullTextAnnotation = results[0].fullTextAnnotation;
      console.log(fullTextAnnotation.text);
      return results[0].fullTextAnnotation.text;
    })
    .catch(err =&gt; {
      console.error('ERROR:', err);
      return ""error"";
    });
})</code></pre>
</div>
</div>
</p>

<p>I am using firebase cloud function and Google Vision API. </p>

<p>actually I tried to pass the parameter like this</p>

<p>My client side coe</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>document.getElementById(""fileInput"").click();
    var file = document.getElementById(""fileInput"");
    var fileInput = document.getElementById('fileInput');
    fileInput.addEventListener('change', function (e) {
        var file = e.target.files[0];
        // Do something with the image file.
        var tmppath = URL.createObjectURL(file);
        console.log(file);
        console.log(tmppath);
        //var url = ""https://firebasestorage.googleapis.com/v0/b/recette-f3ef5.appspot.com/o/FB1.gif?alt=media&amp;token=28727220-181c-440e-87ae-4808b5c9ba28"";
        OCR(file)
        .then(function(result) {
            console.log(result);
        }).catch(function(err) {
            console.log(err);
        });
    });</code></pre>
</div>
</div>
</p>

<p>and it did not work. I always got null return when I trigger the function.</p>

<p>So, my question is that how can I pass the file (HTML INPUT TAG) to my cloud function?</p>

<p>p.s: when I tried the code with node the_code.js it works.</p>",,1,2,,2018-05-04 20:20:35.003 UTC,,2018-06-03 11:15:58.657 UTC,2018-05-04 21:40:36.100 UTC,,8129555,,8129555,1,1,javascript|html|firebase|google-cloud-platform|google-cloud-functions,180
Incomplete coordinate values for Google Vision OCR,39378862,Incomplete coordinate values for Google Vision OCR,"<p>I have a script that is iterating through images of different forms. When parsing the Google Vision Text detection response, I use the XY coordinates in the 'boundingPoly' for each text item to specifically look for data in different parts of the form. </p>

<p>The problem I'm having is that some of the responses come back with only an X coordinate. Example:</p>

<pre><code>{u'description': u'sometext', u'boundingPoly': {u'vertices': [{u'x': 5595}, {u'x': 5717}, {u'y': 122, u'x': 5717}, {u'y': 122, u'x': 5595}
</code></pre>

<p>I've set a try/except (using python 2.7) to catch this issue, but it's always the same issue: <code>KeyError: 'y'</code>. I'm iterating through thousands of forms; so far it has happened to 10 rows out of 1000. </p>

<p>Has anyone had this issue before? Is there a fix other than attempting to re-submit the request if it reaches this error? </p>",39378944,1,0,,2016-09-07 20:55:59.290 UTC,,2016-09-07 21:46:20.790 UTC,,,,,5915157,1,6,python|ocr|google-cloud-vision,835
Android Build failed (transformClassesWithDesugar) with Visa QRParser-2.2.0 and minSdkVersion 19,50513107,Android Build failed (transformClassesWithDesugar) with Visa QRParser-2.2.0 and minSdkVersion 19,"<p>I need to parse data from a Visa Payment QRCode with <a href=""https://developer.visa.com/images2/products/visa_direct/sdk/android/QRParser-2.2.0.zip"" rel=""nofollow noreferrer"">this</a> library from Visa</p>

<p>But gradle build failed with minSdkVersion &lt; 21 and throw transformClassesWithDesugar bellow</p>

<pre><code>16:09:05.083 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.
16:09:05.083 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 
16:09:05.083 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:
16:09:05.083 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Execution failed for task ':app:transformClassesWithDesugarForMockDebug'.
</code></pre>

<p>I tried <code>./gradlew build --stacktrace --debug</code></p>

<p>If I remove the Visa QRParser-2.2.0 dependency it builds fine with minSdkVersion 19 and above. Also, this is a standalone Java library for parsing QR value (not packaging zxling library for QR reading for example. I used Google Vision outside Visa parser for QR reading) so minSdkVersion shouldn't interferes with this dependency.</p>",,2,0,,2018-05-24 15:29:44.907 UTC,,2018-06-09 07:05:15.600 UTC,2018-06-09 07:05:15.600 UTC,,5018575,,5018575,1,3,android|gradle|java-8|android-gradle,183
AWS Rekognition Error - InvalidparameterException,48115069,AWS Rekognition Error - InvalidparameterException,"<pre><code>const AWS = require('aws-sdk');
AWS.config.loadFromPath('./config.json');
const rekognition = new AWS.Rekognition({apiVersion: '2016-06-27'});
const constants = require('./constants');
const s3BucketName = constants.s3BucketName;
const s3BucketKeyName = constants.s3FacebookBucketKey;

const params = {
    Image: {
        S3Object: {
            Bucket: ""mastekinnoations3learning"",
            Name: ""1527119837382460.jpeg""
        }
    }
};

rekognition.detectFaces(params, function(err, data) {
if (err)
console.log(err, err.stack); // an error occurred
else     {
    console.log(data);           // successful response
}
});
</code></pre>

<p>I am trying to execute the above code which was running successfully last month but it has stopped running suddenly giving error ""InvalidParameterException"". Any help no what I am missing will be of great help!!</p>

<p>The image that I am using is this</p>

<p><a href=""https://s3-us-west-2.amazonaws.com/mastekinnoations3learning/1527119837382460.jpeg"" rel=""nofollow noreferrer"">https://s3-us-west-2.amazonaws.com/mastekinnoations3learning/1527119837382460.jpeg</a>  </p>

<p>Detailed Error:</p>

<pre><code>{ InvalidParameterException: Request has Invalid Parameters
at Request.extractError (D:\Saurabh jain\Personal\nodejsprojects\ImageUpload\node_modules\aws-sdk\lib\protocol\json.js:48:27)
at Request.callListeners (D:\Saurabh jain\Personal\nodejsprojects\ImageUpload\node_modules\aws-sdk\lib\sequential_executor.js:106:20)
at Request.emit (D:\Saurabh jain\Personal\nodejsprojects\ImageUpload\node_modules\aws-sdk\lib\sequential_executor.js:77:10)
at Request.emit (D:\Saurabh jain\Personal\nodejsprojects\ImageUpload\node_modules\aws-sdk\lib\request.js:683:14)
at Request.transition (D:\Saurabh jain\Personal\nodejsprojects\ImageUpload\node_modules\aws-sdk\lib\request.js:22:10)
at AcceptorStateMachine.runTo (D:\Saurabh jain\Personal\nodejsprojects\ImageUpload\node_modules\aws-sdk\lib\state_machine.js:14:12)
at D:\Saurabh jain\Personal\nodejsprojects\ImageUpload\node_modules\aws-sdk\lib\state_machine.js:26:10
at Request.&lt;anonymous&gt; (D:\Saurabh jain\Personal\nodejsprojects\ImageUpload\node_modules\aws-sdk\lib\request.js:38:9)
at Request.&lt;anonymous&gt; (D:\Saurabh jain\Personal\nodejsprojects\ImageUpload\node_modules\aws-sdk\lib\request.js:685:12)
at Request.callListeners (D:\Saurabh jain\Personal\nodejsprojects\ImageUpload\node_modules\aws-sdk\lib\sequential_executor.js:116:18)
</code></pre>",,3,0,,2018-01-05 13:56:10.063 UTC,,2018-01-11 10:55:10.927 UTC,,,,,4894395,1,0,node.js|amazon-web-services|aws-sdk|amazon-rekognition,597
Limit Detection Area in Vision API,35489632,Limit Detection Area in Vision API,"<p>It seems I've found myself in the deep weeds of the Google Vision API for barcode scanning. Perhaps my mind is a bit fried after looking at all sorts of alternative libraries (ZBar, ZXing, and even some for-cost third party implementations), but I'm having some difficulty finding any information on where I can implement some sort of scan region limiting.</p>

<p>The use case is a pretty simple one: if I'm a user pointing my phone at a box with multiple barcodes of the same type (think shipping labels here), I want to explicitly point some little viewfinder or alignment straight-edge on the screen at exactly the thing I'm trying to capture, without having to worry about anything outside that area of interest giving me some scan results I don't want.</p>

<p>The above case is handled in most other Android libraries I've seen, taking in either a Rect with relative or absolute coordinates, and this is also a part of iOS' AVCapture metadata results system (it uses a relative CGRect, but really the same concept).</p>

<p>I've dug pretty deep into the sample app for the barcode-reader
<a href=""https://github.com/googlesamples/android-vision/tree/master/visionSamples/barcode-reader"" rel=""noreferrer"">here</a>, but the implementation is a tad opaque to get anything but the high level implementation details down.</p>

<p>It seems an ugly patch to, on successful detection of a barcode anywhere within the camera's preview frame, to simple no-op on barcodes outside of an area of interest, since the device is still working hard to compute those frames.</p>

<p>Am I missing something very simple and obvious on this one? Any ideas on a way to implement this cleanly, otherwise?</p>

<p>Many thanks for your time in reading through this!</p>",35509532,1,0,,2016-02-18 18:31:45.580 UTC,5,2016-02-19 16:00:46.637 UTC,,,,,3543729,1,11,android|google-vision|android-vision,6548
Is 50 Images a minimum requirement to recognize a face of a person using Watson visual recognitionservice?,46189464,Is 50 Images a minimum requirement to recognize a face of a person using Watson visual recognitionservice?,"<p>I have question on Watson Visual recognition Service of bluemix?</p>

<p>Is 50 Images a minimum requirement to recognize a face of a person?</p>

<p>What would happen if we train with less than 50 images? What would be the consistency of the output in terms of facial recognition?</p>

<p>Requirement is, Retrieve the employee id of an employee by his facial(visual) recognition.</p>

<p>Is it achievable with Watson visual recognition Service?</p>

<p>In real time, it may be little hard to have 50 images of an employee or a person.
?</p>

<p>Thanks,</p>

<p>Priyanka</p>",,1,2,,2017-09-13 05:26:59.617 UTC,,2017-09-13 12:37:44.677 UTC,2017-09-13 12:37:44.677 UTC,,7133482,,5847302,1,0,ibm-cloud|ibm-watson,143
IBM watson api of visual recognition add image issue in collectio using curl,42227768,IBM watson api of visual recognition add image issue in collectio using curl,"<p>i am using IBM watson visual recognition api when i add an imahe to collection but 
i receive follwoing error all the time</p>

<pre><code>string(59) ""{ ""error"": ""Missing multipart/form-data"", ""code"": 400 }"" bool(true)
</code></pre>

<p>here is my code</p>

<pre><code> &lt;?php
 if ( isset($_FILES['uploadedfile']) &amp;&amp; $_POST!="""" ) {
 $targetPath = 'uploads/'.basename($_FILES['uploadedfile']['name']);
 $url = 'https://gateway-a.watsonplatform.net/visual-recognition/api/v3/collections/searchItems_c5c677/images?api_key=655e4118jgfd8e967ce58ee0b67behjfh3ebfad22e38a34e&amp;version=2016-05-20';
 $fileData = $_FILES['uploadedfile']['name'];
 $post_data = array(
        'image_file' =&gt; $fileData
 );
 $ch = curl_init();
 curl_setopt($ch, CURLOPT_URL, $url);
 curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);
 curl_setopt($ch, CURLOPT_POST, 1);
 curl_setopt($ch, CURLOPT_POSTFIELDS, $post_data);
 $headers = array();
 $headers[] = ""Content-Type: multipart/form-data"";
 $headers[] = ""Accept: application/json"";
 curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
 $result = curl_exec($ch);
 if (curl_errno($ch)) {
    echo 'Error:' . curl_error($ch);die;
 }
   var_dump($result, true);die;
 }
?&gt;
&lt;form enctype=""multipart/form-data"" method='post' action=""index.php""&gt;
  &lt;input name=""uploadedfile"" type=""file"" value=""choose""&gt;
  &lt;input type=""submit"" value=""Upload""&gt;
&lt;/form&gt;
</code></pre>",,1,0,,2017-02-14 13:44:29.760 UTC,,2017-06-09 13:58:09.330 UTC,,,,,5219790,1,0,php|android|html|ios|curl,259
Handshake error while connecting to Google Vision API using Python,55606918,Handshake error while connecting to Google Vision API using Python,"<p>I am using below code to connect to Google Vision API. I have JSON from Google Vision.</p>

<p>The code is giving me below error . Not Sure why.. <code>&lt; TSI_PROTOCOL_FAILURE &gt;</code> 
Please suggest.. It is working fine on Windows Server Machine but not on my Windows 7 machine.</p>

<p>Below is code and Error Details.</p>

<pre><code>enter code here

Error details below.
E0410 12:18:28.854000000 17616 src/core/tsi/ssl_transport_security.cc:1239] Handshake failed with fatal error SSL_ERROR_SSL: error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED.



grpc._channel._Rendezvous: &lt;_Rendezvous of RPC that terminated with:
    status = StatusCode.UNAVAILABLE
    details = ""Connect Failed""
    debug_error_string = ""{""created"":""@1554878916.008000000"",""description"":""Failed to create subchannel"",""file"":""src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":2267,""referenced_errors"":[{""created"":""@1554878916.008000000"",""description"":""Pick Cancelled"",""file"":""src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":242,""referenced_errors"":[{""created"":""@1554878916.008000000"",""description"":""Connect Failed"",""file"":""src/core/ext/filters/client_channel/subchannel.cc"",""file_line"":962,""grpc_status"":14,""referenced_errors"":[{""created"":""@1554878916.008000000"",""description"":""Handshake failed"",""file"":""src/core/lib/security/transport/security_handshaker.cc"",""file_line"":291,""tsi_code"":10,""tsi_error"":""TSI_PROTOCOL_FAILURE""}]}]}]}""
</code></pre>",,1,0,,2019-04-10 07:24:32.573 UTC,,2019-04-12 20:44:48.047 UTC,2019-04-10 08:49:43.730 UTC,,704848,,11338671,1,0,python-3.x|google-vision|sslerrorhandler,46
module google.cloud.vision_v1.types doesn't have 'image'/'label_detection'/etc as member,54546886,module google.cloud.vision_v1.types doesn't have 'image'/'label_detection'/etc as member,"<p>i am tring to connect google cloud vision to python so that i want to perform OCR on image  .i wrote the code but when i tried to run the code it shows me""please set google_vision_credential ""but i already provided my key as set install GOOGLE_VISION_CREDENTIALS = path of the key
i took the code from ""<a href=""https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/vision/cloud-client/detect/detect.py"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/vision/cloud-client/detect/detect.py</a>""
and it shows an error at
 'image = vision.types.Image()' it says that google.cloud.vision_v1.types"" has no 'Image' member
same like that there are 64 more error of containing no such members the error is mostly at the keyword=""vision"" and ""client"" .</p>

<p>the sample code is as follows:</p>

<p>import io
from google.cloud import vision</p>

<p>vision_client = vision.ImageAnnotatorClient()
file_name = 'text.png'</p>

<p>with io.open(file_name,'rb') as image_file:
    content = image_file.read()</p>

<p>image = vision.types.Image(content=content) # here's the issue pointing at vision which says Instance of 'ImageAnnotatorClient' has no 'types' member</p>

<p>labels = image.detect_labels()</p>

<p>for label in labels:<br>
    print(label.description)</p>",,0,1,,2019-02-06 05:01:38.403 UTC,,2019-02-06 09:04:08.843 UTC,2019-02-06 09:04:08.843 UTC,,11018012,,11018012,1,0,python-3.6,98
How to get database data from another activity to main activity?,56329632,How to get database data from another activity to main activity?,"<p>Im creating SQLite database in android studio... In database, i have 1 table with 3 rows (<code>int id, varchar nama_obat, varchar deskripsi_obat</code>). I want to try to get data in specific row (deskripsi_obat) with another row (<code>nama_obat</code>) as search string. I cant use id because it needs to get data from string as search...</p>

<p>Problem is i dont know why my apps force closed when i try to run function <code>getFromDB()</code> any help or possible wrong code?</p>

<p>(ps : im trying to post part of code regarding database SQLite, my entire code include Google Vision API, Google Custom Search API, etc)</p>

<p>code for <code>MainActivity.java</code></p>

<pre><code>public class MainActivity extends AppCompatActivity {
    DBHelper mydb;
    TextView t1;
    TextView t2;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);
        t1 = (TextView)findViewById(R.id.textView2);
        t2 = (TextView)findViewById(R.id.textView3);

    }

public void getFromDB(){
        final String searchString = t1.getText().toString();
        mydb = new DBHelper(this);
        Cursor rs = mydb.getData(searchString);

        if(rs != null){
            rs.moveToFirst();
            String result = rs.getString(rs.getColumnIndex(DBHelper.OBAT_COLUMN_DESC));
            t2.setText(result);
        }
        else{
            Toast.makeText(this, ""Ga dapat database!"", Toast.LENGTH_SHORT).show();
        }

    }
</code></pre>

<p>code for DBHelper.java</p>

<pre><code>package com.example.asus.googleocr;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.Hashtable;
import android.content.ContentValues;
import android.content.Context;
import android.database.Cursor;
import android.database.DatabaseUtils;
import android.database.sqlite.SQLiteOpenHelper;
import android.database.sqlite.SQLiteDatabase;
import android.widget.Toast;

public class DBHelper extends SQLiteOpenHelper {

    public static final String DATABASE_NAME = ""db_obat"";
    public static final String OBAT_TABLE_NAME = ""tb_obat"";
    public static final String OBAT_COLUMN_ID = ""_id"";
    public static final String OBAT_COLUMN_NAME = ""nama_obat"";
    public static final String OBAT_COLUMN_DESC = ""deskripsi_obat"";

    public DBHelper(Context context) {
        super(context, DATABASE_NAME , null, 1);
    }

    @Override
    public void onCreate(SQLiteDatabase db) {
        // TODO Auto-generated method stub
        db.execSQL(""create table IF NOT EXISTS ""+OBAT_TABLE_NAME+ ""(_id integer primary key AUTOINCREMENT, nama_obat VARCHAR (255),deskripsi_obat VARCHAR (255))"");
        db.execSQL(""INSERT INTO "" +OBAT_TABLE_NAME+ ""(nama_obat, deskripsi_obat) VALUES ('Procold Flu', 'Obat untuk batuk dan pilek')"");
        db.execSQL(""INSERT INTO "" +OBAT_TABLE_NAME+ ""(nama_obat, deskripsi_obat) VALUES ('BETADINE', 'Obat untuk luka pada kulit')"");
    }

    @Override
    public void onUpgrade(SQLiteDatabase db, int oldVersion, int newVersion) {
        // TODO Auto-generated method stub
        db.execSQL(""DROP TABLE IF EXISTS tb_obat"");
        onCreate(db);
    }

    public Cursor getData(String name) {
        SQLiteDatabase db = this.getReadableDatabase();
        Cursor res =  db.rawQuery( ""select * from"" + OBAT_TABLE_NAME + ""where nama_obat=""+name+"""", null );
        return res;
    }
}

</code></pre>",,0,1,,2019-05-27 16:30:45.920 UTC,,2019-05-27 17:33:15.880 UTC,2019-05-27 17:33:15.880 UTC,,6296561,,11563257,1,-1,android,31
How to crop images using bounding box,45211033,How to crop images using bounding box,"<p>How do i crop an image while rendering to  browser using the bounding box returned by aws rekognition indexFaces api? below is the bounding box example </p>

<pre><code> Face: {
   BoundingBox: {
    Height: 0.16555599868297577, 
    Left: 0.30963000655174255, 
    Top: 0.7066670060157776, 
    Width: 0.22074100375175476
   }, 
   Confidence: 100, 
   FaceId: ""29a75abe-397b-5101-ba4f-706783b2246c"", 
   ImageId: ""147fdf82-7a71-52cf-819b-e786c7b9746e""
  }, 
  Similarity: 97.04154968261719 }
</code></pre>",,1,2,,2017-07-20 09:42:06.067 UTC,,2018-07-03 17:53:34.240 UTC,2017-07-20 09:52:24.373 UTC,,2499914,,2499914,1,1,javascript|css|crop|amazon-rekognition,380
"""Error: Unauthorized: Access is denied due to invalid credentials."" Watson visual recognition",40281166,"""Error: Unauthorized: Access is denied due to invalid credentials."" Watson visual recognition","<p>Im getting the error above while trying to create a new classification using the IBM watson visual recognition system.</p>

<pre><code>        var VisualRecognitionV3 = require('watson-developer-cloud/visualrecognition/v3');

        var visual_recognition = new VisualRecognitionV3({
           api_key: &lt;api-key&gt;,
           version_date: '2016-05-19'
        });
</code></pre>

<p>This is how I am fetching the credentials, from the examples that the documentation provided. Is there something wrong?</p>",,1,1,,2016-10-27 09:38:20.180 UTC,,2017-08-23 22:54:22.007 UTC,,,,,4453376,1,0,node.js|visual-recognition|watson,368
Google Vision Python 2.7 TypeError: construct_settings() got an unexpected keyword argument 'metrics_headers',43771382,Google Vision Python 2.7 TypeError: construct_settings() got an unexpected keyword argument 'metrics_headers',"<p>After installing the required packages using pip, downloading a Json key and setting the enviroment variable in the cmd window with: set GOOGLE_APPLICATION_CREDENTIALS = 'C:\Users\ xxx .json' and following the instructions to use the Google Vision API on <a href=""https://googlecloudplatform.github.io/google-cloud-python/stable/vision-usage.html#authentication-and-configuration"" rel=""nofollow noreferrer"">https://googlecloudplatform.github.io/google-cloud-python/stable/vision-usage.html#authentication-and-configuration</a></p>

<p>I tried the following and got the following error without any idea how to solve the error, so all suggestions are much appreciated</p>

<pre><code>&gt;&gt;&gt; from google.cloud import vision
&gt;&gt;&gt; client =vision.Client()
&gt;&gt;&gt; print client
&lt;google.cloud.vision.client.Client object at 0x08D414F0&gt;
&gt;&gt;&gt; image =  client.image(filename='test2.jpg')
&gt;&gt;&gt; print image
&lt;google.cloud.vision.image.Image object at 0x0CBF68F0&gt;
&gt;&gt;&gt; text = image.detect_text()
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Python27\lib\site-packages\google\cloud\vision\image.py"", line 289, in detect_text
    annotations = self.detect(features)
  File ""C:\Python27\lib\site-packages\google\cloud\vision\image.py"", line 143, in detect
    return self._detect_annotation(images)
  File ""C:\Python27\lib\site-packages\google\cloud\vision\image.py"", line 117, in _detect_annotation
    return self.client._vision_api.annotate(images)
  File ""C:\Python27\lib\site-packages\google\cloud\vision\client.py"", line 114, in _vision_api
    self._vision_api_internal = _GAPICVisionAPI(self)
  File ""C:\Python27\lib\site-packages\google\cloud\vision\_gax.py"", line 34, in __init__
    lib_version=__version__)
  File ""C:\Python27\lib\site-packages\google\cloud\gapic\vision\v1\image_annotator_client.py"", line 140, in __init__
    metrics_headers=metrics_headers, )
TypeError: construct_settings() got an unexpected keyword argument 'metrics_headers'
</code></pre>",,0,3,,2017-05-03 23:08:05.137 UTC,,2017-05-03 23:08:05.137 UTC,,,,,7116366,1,1,python-2.7|google-cloud-platform|vision,179
"Google Clous Vision API returns empty on object_localization call, while label_detection detects many",56093211,"Google Clous Vision API returns empty on object_localization call, while label_detection detects many","<p>I have encountered several cases that a call to object_localization, on Google Vision API, returns empty, whereas a cell to label_detection with the same image returns many object with hight level of certainty.</p>

<p>Example:</p>

<p>This image from Google Street View</p>

<p><a href=""https://i.stack.imgur.com/t8aYz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t8aYz.png"" alt=""enter image description here""></a></p>

<p>The API can identify the billboard with high certainty</p>

<pre class=""lang-py prettyprint-override""><code>from google.cloud import vision

path = '***'
credentials = '****'
client = vision.ImageAnnotatorClient(credentials=credentials)
image_file = io.open(path, 'rb')
content = image_file.read()
image = types.Image(content=content)

# Label detection API call
response = client.label_detection(image=image)
labels = response.label_annotations
for label in labels:
    print(""%s, %.4f"" % (label.description, label.score))
</code></pre>

<pre><code>Billboard, 0.9892
Advertising, 0.9786
Display advertising, 0.7132
Signage, 0.6700
Banner, 0.5565
Road, 0.5271
Asphalt, 0.5006
</code></pre>

<p>But then I try it with object localization and... nothing</p>

<pre class=""lang-py prettyprint-override""><code># Object localization API call
localized_objects = client.object_localization(image=image)
localized_objects.localized_object_annotations
</code></pre>

<pre><code>[]
</code></pre>

<p>When I try it with other images I get a partial localization, like in this image:
<a href=""https://i.stack.imgur.com/VdBjy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VdBjy.png"" alt=""enter image description here""></a></p>

<p>I do get both labels and object localization annotations:</p>

<p>Labels:</p>

<pre><code>Residential area, 0.9777
Road, 0.9677
Lane, 0.9456
Asphalt, 0.9204
Thoroughfare, 0.8924
Neighbourhood, 0.8511
Town, 0.8481
Transport, 0.8351
Suburb, 0.8245
Infrastructure, 0.8193
Street, 0.7811
Road surface, 0.7508
Sky, 0.7326
Overhead power line, 0.7230
Tree, 0.6869
Highway, 0.6551
Architecture, 0.6545
House, 0.6313
Compact car, 0.6117
Nonbuilding structure, 0.5980
Urban design, 0.5613
Building, 0.5337
Sidewalk, 0.5259
Public utility, 0.5242
City, 0.5135
</code></pre>

<p>Object localization annotations:</p>

<pre class=""lang-py prettyprint-override""><code>localized_objects[0]
</code></pre>

<pre><code>mid: ""/m/0k4j""
name: ""Car""
score: 0.6203218698501587
bounding_poly {
  normalized_vertices {
    x: 0.8491114974021912
    y: 0.6019535660743713
  }
  normalized_vertices {
    x: 0.9929937124252319
    y: 0.6019535660743713
  }
  normalized_vertices {
    x: 0.9929937124252319
    y: 0.7152476906776428
  }
  normalized_vertices {
    x: 0.8491114974021912
    y: 0.7152476906776428
  }
}
</code></pre>

<p><a href=""https://i.stack.imgur.com/NEvOG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NEvOG.png"" alt=""enter image description here""></a></p>

<p>So, it appears that there's no fundamental problem with my API configuration, but is there anything I can do different to make it work?</p>",,0,0,,2019-05-11 18:33:38.620 UTC,,2019-05-11 18:33:38.620 UTC,,,,,2633360,1,0,python|google-cloud-platform|computer-vision|google-cloud-vision,15
"Lambda exits before completing, but works locally",43517506,"Lambda exits before completing, but works locally","<p>I'm writing a simple Lambda function for a data processing pipeline, which gets called by Kinesis Firehose. This function downloads images from a URL, uploads them to S3, checks whether they have a face in them using OpenCV, and if they do, loads the image into Rekognition.</p>

<p>When I run it locally, I have no issues. Everything downloads, uploads and is processed with no errors. However, if I run it on Lambda, it gives me an error about exiting before completing.</p>

<pre><code>'use strict';

const AWS = require('aws-sdk');
const request = require('request');
const cv = require('opencv');

const s3 = new AWS.S3();
const rek = new AWS.Rekognition();

const uploadImage = data =&gt; {
  return new Promise((resolve, reject) =&gt; {
    request({
      url: data.images.standard,
      encoding: null // creates a buffer
    }, function(err, res, body) {
      if (err) return reject({ type: 'err', err: err });
      if (res.statusCode != 200) return reject({ type: 'fail', code: res.statusCode });

      console.log(`Fetched ${data._id}`)

      // Upload to s3
      s3.putObject({
        Bucket: 'mybucket',
        Key: `${data._id}.jpg`,
        ContentType: res.headers['content-type'],
        ContentLength: res.headers['content-length'],
        Body: body // buffer
      }, (err) =&gt; {
        if (err) return reject({ type: err, err: err });
        resolve({ record: data, buffer: body });
      });
    })
  })
}

const indexFacesLocal = data =&gt; {
  return new Promise((resolve, reject) =&gt; {
    cv.readImage(data.buffer, (err, image) =&gt; {
      if (err) return reject({ type: 'err', err: err });
      image.detectObject(cv.FACE_CASCADE, {}, (err, faces) =&gt; {
        if (err) return reject({ type: 'err', err: err });

        // Set default if undefined
        if (!faces) faces = [];

        console.log(`Completed OpenCV ${data.record._id}, should process = ${!!faces.length}`)

        data._jackal = !!faces.length;
        resolve(data);
      })
    })
  })
}

const indexFacesJackal = data =&gt; {
  return new Promise((resolve, reject) =&gt; {
    if (!data._jackal) return resolve(data.record);

    // Discard buffer and other data
    data = data.record;

    let params = {
      CollectionId: process.env.REK_COLLECTION,
      Image: {
        S3Object: {
          Bucket: `mybucket`,
          Name: `${data._id}.jpg`,
        }
      },
      DetectionAttributes: ['ALL'],
      ExternalImageId: data._id
    }

    rek.indexFaces(params, (err, faces) =&gt; {
      if (err) return reject({ type: 'err', err: err });

      console.log(`Indexed on Rek ${data._id}`)

      // Check if data is present
      if (!faces.FaceRecords.length) {
        return resolve(data);
      }

      // Do some data manipulation stuff here, nothing big
      // just used to reformat AWS response

      console.log(`Mapped ${data._id}`)

      return resolve(data);
    })
  })
}

exports.handler = function(event, ctx, callback) {
  /* Process the list of records and transform them */
  Promise.all(event.records.map(record =&gt; {
    return uploadImage(record.data)
      .then(indexFacesLocal)
      .then(indexFacesJackal)
      .then(data =&gt; {
        return {
          recordId: record.recordId,
          result: 'Ok',
          data: data,
        }
      }).catch(res =&gt; {

        if (res.type == 'fail') {
          // Unable to fetch media from Instagram
          console.log(`[${res.code}] - ${record.recordId}`);
          return {
            recordId: record.recordId,
            result: 'Dropped'
          }
        }

        console.log(`Processing failed for ${record.recordId}`);
        console.log(res.err.stack);

        return {
          recordId: record.recordId,
          result: 'ProcessingFailed'
        }
      })
  })).then(output =&gt; {
    console.log(`Processing completed, handled ${output.length} items`)
    callback(null, { records: output })
  })
};
</code></pre>

<p>When run locally, my output is:</p>

<pre><code>Fetched 1392753031552166622
Fetched 1379923796962022364
Fetched 1392750801239391628
Fetched 1392748163315653017
Completed OpenCV 1379923796962022364, should process = true
Completed OpenCV 1392748163315653017, should process = false
Completed OpenCV 1392750801239391628, should process = true
Completed OpenCV 1392753031552166622, should process = true
Indexed on Rek 1379923796962022364
Mapped 1379923796962022364
Indexed on Rek 1392750801239391628
Mapped 1392750801239391628
Indexed on Rek 1392753031552166622
Mapped 1392753031552166622
Processing completed, handled 4 items
{ records: 
   [ { recordId: '1379923796962022364', result: 'Ok', data: [Object] },
     { recordId: '1392748163315653017', result: 'Ok', data: [Object] },
     { recordId: '1392750801239391628', result: 'Ok', data: [Object] },
     { recordId: '1392753031552166622', result: 'Ok', data: [Object] } ] }
</code></pre>

<p>When run on AWS, I get:</p>

<pre><code>START Version: $LATEST

Fetched 1392753031552166622
Fetched 1392748163315653017
Fetched 1392750801239391628
Fetched 1379923796962022364
Completed OpenCV 1379923796962022364, should process = true
Completed OpenCV 1392748163315653017, should process = false
Completed OpenCV 1392750801239391628, should process = true

END
Process exited before completing request
</code></pre>

<p>I've checked memory allocation, it's only using ~130MB of 512. There are no other errors thrown. It successfully imports the OpenCV build for Amazon Linux, so it's not a problem with the binary either. I've also checked timeout, which is set to 5 minutes (it runs for ~8 seconds each time).</p>",,0,4,,2017-04-20 10:53:37.080 UTC,,2017-04-20 10:53:37.080 UTC,,,,,2078412,1,1,javascript|node.js|lambda,115
Creating/Reloading dynamic express folders in node.js,44999740,Creating/Reloading dynamic express folders in node.js,"<p>I'm trying to create files and place them into a folder, then send a link to the Microsoft Emotion API.</p>

<pre><code>app.use(bodyParser.urlencoded({ extended: false }));
app.use(bodyParser.json({limit: '4mb'}));

app.use('/pics', express.static(path.join(__dirname, 'pics')))
app.post('/face', function(req, res){
    var base64Data = req.body.img.replace(/^data:image\/png;base64,/, """");
    var id;
    fs.readdir(""./pics"", (err, files) =&gt; {
        id = files.length;
        fs.writeFile(`./pics/${id}.png`, base64Data, 'base64', function(err) {
            console.log(err);
            reload(http, app);
            setTimeout(function(){
                fetch(`https://westus.api.cognitive.microsoft.com/emotion/v1.0/recognize`, {
                    method: ""POST"",
                    data: `{""url"": ""${""https://{ngrok-id}.ngrok.io/pics/""+id+"".png""}""}`,
                    headers: {
                        'Content-Type': 'application/json',
                        'Ocp-Apim-Subscription-Key': '{api-key}'
                    }
                }).then(function(data) {
                    return data.json();
                }).then(function(data){
                    console.log(`{""url"": ""${""https://{ngrok-id}.ngrok.io/pics/""+id+"".png""}""}`);
                    res.json(data);
                });
            }, 2000);
        });
    });
});
</code></pre>

<p>Is there a way to make express folders dynamic or reload static folders?</p>",,2,0,,2017-07-09 18:36:34.347 UTC,,2017-07-09 19:00:27.200 UTC,,,,,6669145,1,0,node.js|express,19
Why is Google Cloud Vision api unable to detect text in a particular pdf file although it works fine on a very similar pdf?,56219425,Why is Google Cloud Vision api unable to detect text in a particular pdf file although it works fine on a very similar pdf?,"<p>The google cloud vision api works fine on one pdf <a href=""https://drive.google.com/open?id=1lBajk7rSUq_NSj3x9AZEJh7ZD6EVgcGu"" rel=""nofollow noreferrer"">pdf1</a> but returns absolutely nothing on the other pdf <a href=""https://drive.google.com/open?id=1BGN_QwMqwLZQY59DlpdC9EA56H5QmYoa"" rel=""nofollow noreferrer"">pdf2</a>. I'm unable to make sense of this behavior as both the pdfs are very similar and have almost the same font.Please help.</p>

<p>I'm using the code given in their examples section by uploading these files in a google cloud bucket.</p>

<pre><code>def async_detect_document(gcs_source_uri, gcs_destination_uri):
    """"""OCR with PDF/TIFF as source files on GCS""""""
    from google.cloud import vision
    from google.cloud import storage
    from google.protobuf import json_format
    # Supported mime_types are: 'application/pdf' and 'image/tiff'
    mime_type = 'application/pdf'

    # How many pages should be grouped into each json output file.
    batch_size = 2

    client = vision.ImageAnnotatorClient()

    feature = vision.types.Feature(
        type=vision.enums.Feature.Type.DOCUMENT_TEXT_DETECTION)

    gcs_source = vision.types.GcsSource(uri=gcs_source_uri)
    input_config = vision.types.InputConfig(
        gcs_source=gcs_source, mime_type=mime_type)

    gcs_destination = vision.types.GcsDestination(uri=gcs_destination_uri)
    output_config = vision.types.OutputConfig(
        gcs_destination=gcs_destination, batch_size=batch_size)

    async_request = vision.types.AsyncAnnotateFileRequest(
        features=[feature], input_config=input_config,
        output_config=output_config)

    operation = client.async_batch_annotate_files(
        requests=[async_request])

    print('Waiting for the operation to finish.')
    operation.result(timeout=180)

    # Once the request has completed and the output has been
    # written to GCS, we can list all the output files.
    storage_client = storage.Client()

    match = re.match(r'gs://([^/]+)/(.+)', gcs_destination_uri)
    bucket_name = match.group(1)
    prefix = match.group(2)

    bucket = storage_client.get_bucket(bucket_name=bucket_name)

    # List objects with the given prefix.
    blob_list = list(bucket.list_blobs(prefix=prefix))
    print('Output files:')
    for blob in blob_list:
        print(blob.name)

    # Process the first output file from GCS.
    # Since we specified batch_size=2, the first response contains
    # the first two pages of the input file.
    output = blob_list[0]

    json_string = output.download_as_string()
    response = json_format.Parse(
        json_string, vision.types.AnnotateFileResponse())

    # The actual response for the first page of the input file.
    first_page_response = response.responses[0]
    annotation = first_page_response.full_text_annotation

    # Here we print the full text from the first page.
    # The response contains more information:
    # annotation/pages/blocks/paragraphs/words/symbols
    # including confidence scores and bounding boxes
    print(u'Full text:\n{}'.format(
        annotation.text))
</code></pre>",,0,0,,2019-05-20 10:52:33.537 UTC,,2019-05-20 11:46:19.117 UTC,2019-05-20 11:46:19.117 UTC,,730537,,6418944,1,1,python|python-3.x|google-cloud-platform|google-cloud-vision,29
Python3.7 Vision API output to dictionary,52143046,Python3.7 Vision API output to dictionary,"<p>I am really new to python and i am using Google vision API to detect some text from image,In the API output there two thing which are of interest to me text and text.vertices both of them i have stored in two variables p &amp; c  however i dont know how to create a single dictionary which i can then use to do some post processing(sorting,dicing etc.) on that data below is code i am using, could anyone please suggest any solutions</p>

<pre><code>    for text in texts:
    if len(text.description) &lt; 100:  
    p=text.description
    vertices = (['{}'.format(vertex.y)
                for vertex in text.bounding_poly.vertices])
    c= vertices[0]  
    rpd={c:p}
</code></pre>

<p>However this creates seperate lines for each c &amp; p i want everyting into a single dictionary so that i then sort it in ascending/descending </p>

<pre><code>Actual output :{'562': 'MANUFACTURE'}
{'605': 'Net'}
{'604': 'Contents200m'}

desired output {'562': 'MANUFACTURE','605': 'Net','604': 'Contents200m')
</code></pre>",,2,0,,2018-09-03 04:10:53.290 UTC,,2018-09-04 06:22:17.077 UTC,2018-09-03 04:37:20.827 UTC,,2988730,,10298667,1,0,python|python-3.x,32
Google Cloud Vision API DOCUMENT_TEXT_DETECTION returning incorrect bounding box,52440902,Google Cloud Vision API DOCUMENT_TEXT_DETECTION returning incorrect bounding box,"<p>I'm using the ""DOCUMENT_TEXT_DETECTION"" option from the Google Cloud Vision API.</p>

<p>It seems that it's returning correct text value, but incorrect coordinates bounding box.</p>

<p>Why this problem occurred?</p>

<p>Thank you.</p>

<h2>raw picture</h2>

<p><a href=""https://i.stack.imgur.com/mS1VK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mS1VK.jpg"" alt=""enter image description here""></a></p>

<h2>draw bounding box picture</h2>

<p><a href=""https://i.stack.imgur.com/p2YFr.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p2YFr.jpg"" alt=""enter image description here""></a></p>

<h2>returning <a href=""https://gist.githubusercontent.com/namagon/03559df48688822e891c2f7e74220f0f/raw/e96eb45916a76fbb2ecc132735cfd7563ea7246c/gistfile1.txt"" rel=""nofollow noreferrer"">json</a></h2>

<hr>

<h1>appendix</h1>

<h2>draw bounding box words and overall</h2>

<p><a href=""https://i.stack.imgur.com/ItXPU.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ItXPU.jpg"" alt=""enter image description here""></a></p>",,1,2,,2018-09-21 09:42:33.593 UTC,,2018-11-19 18:13:52.160 UTC,2018-09-27 03:55:10.947 UTC,,10395827,,10395827,1,1,ocr|google-cloud-vision,282
App Engine default credentials stopped working on local dev server,45720587,App Engine default credentials stopped working on local dev server,"<p>I'm running app engine locally, with the Google vision API.  I'm using the application default credentials for OAuth and building the API client to do label detection.  Whenever I create the object it will refresh the credentials</p>

<pre><code>DISCOVERY_URL = 'https://{api}.googleapis.com/$discovery/rest?version={apiVersion}'

class VisionApi(object):
    def __init__(self):
        self.vision = self._create_client()

    def _create_client(self):
        credentials = GoogleCredentials.get_application_default()
        return discovery.build(
            'vision', 'v1', credentials=credentials,
            discoveryServiceUrl=DISCOVERY_URL)
</code></pre>

<p>This worked just fine yesterday, and I haven't changed it.  When I try to use it today, it attempts to get the credentials twice, and then I get the following 401 error:</p>

<pre><code>INFO     2017-08-16 18:12:14,228 discovery.py:863] URL being requested: POST https://vision.googleapis.com/v1/images:annotate?alt=json
INFO     2017-08-16 18:12:14,228 transport.py:157] Attempting refresh to obtain initial access_token
WARNING  2017-08-16 18:12:14,237 urlfetch_stub.py:504] Stripped prohibited headers from URLFetch request: ['content-length']
INFO     2017-08-16 18:12:14,542 transport.py:185] Refreshing due to a 401 (attempt 1/2)
WARNING  2017-08-16 18:12:14,544 urlfetch_stub.py:504] Stripped prohibited headers from URLFetch request: ['content-length']
INFO     2017-08-16 18:12:14,823 transport.py:185] Refreshing due to a 401 (attempt 2/2)
WARNING  2017-08-16 18:12:14,826 urlfetch_stub.py:504] Stripped prohibited headers from URLFetch request: ['content-length']
</code></pre>

<p><code>HttpError: &lt;HttpError 401 when requesting https://vision.googleapis.com/v1/images:annotate?alt=json returned ""Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https://developers.google.com/identity/sign-in/web/devconsole-project.""&gt;</code></p>

<p>Why was this working yesterday but isn't working today?  I understand that I can go the route of using server-to-server credentials, but it worked without that and I would prefer to continue using the API without that.</p>",,0,1,,2017-08-16 18:25:08.040 UTC,,2017-08-17 14:40:51.167 UTC,,,,,1578715,1,1,python|google-app-engine|oauth|google-cloud-vision,125
"Google Vision API is throwing 'Permission Denied' exception, when the billing & API both are enabled",55408126,"Google Vision API is throwing 'Permission Denied' exception, when the billing & API both are enabled","<p>I am trying to integrate the Google Vision API. I have enabled billing and on the console it shows enabled, but when I try to query it throws an exception:</p>

<blockquote>
  <p>google.api_core.exceptions.PermissionDenied: 403 This API method requires billing to be enabled. Please enable billing on project #435979098757 by visiting <a href=""https://console.developers.google.com/billing/enable?project=435979098757"" rel=""nofollow noreferrer"">https://console.developers.google.com/billing/enable?project=435979098757</a> then retry.</p>
</blockquote>",,0,0,,2019-03-28 23:01:16 UTC,,2019-03-29 08:29:58.360 UTC,2019-03-29 08:29:58.360 UTC,,6343018,,11274857,1,0,permission-denied|vision-api,23
Working in the DoInBackground or in the PostExecute of an Asynctask,55570088,Working in the DoInBackground or in the PostExecute of an Asynctask,"<p>I'm working in an Android application that is using Microsoft Azure Face Api to get some information from an image. After analizing all the people in the image I get the results in the postExecute() call correctly, but now I need to do some changes if I detect an specific person (all the work is different if this specific person is detected).</p>

<p>I correctly detect this person but I want to know if I can do my work in the DoInBackground() so that I don't need to wait for the result (because if I detected this person I need to send a socket message and the result is not valid).</p>

<p>I actually receive a full list of people in the onResult, then I look through all this list to find the specific person and send the socket message. I want to know if I can send this message as soon as I detect this person in the DoInBackground() and cancel the rest of the execution.</p>",55571112,2,0,,2019-04-08 09:22:07.700 UTC,,2019-04-08 10:56:04.937 UTC,2019-04-08 10:56:04.937 UTC,,2649012,,9364229,1,-2,android|android-asynctask,42
Format OCR text annotation from Cloud Vision API in Python,48266531,Format OCR text annotation from Cloud Vision API in Python,"<p>I am using the Google Cloud Vision API for Python on a small program I'm using. The function is working and I get the OCR results, but I need to format these before being able to work with them.</p>

<p>This is the function:</p>

<pre><code># Call to OCR API
def detect_text_uri(uri):
    """"""Detects text in the file located in Google Cloud Storage or on the Web.
    """"""
    client = vision.ImageAnnotatorClient()
    image = types.Image()
    image.source.image_uri = uri

    response = client.text_detection(image=image)
    texts = response.text_annotations

    for text in texts:
        textdescription = (""    ""+ text.description )
        return textdescription
</code></pre>

<p>I specifically need to slice the text line by line and add four spaces in the beginning and a line break in the end, but at this moment this is only working for the first line, and the rest is returned as a single line blob.</p>

<p>I've been checking the official documentation but didn't really find out about the format of the response of the API.</p>",48267374,1,0,,2018-01-15 15:59:31.633 UTC,4,2018-05-09 22:35:17.887 UTC,,,,,9176789,1,4,python|google-cloud-platform|google-cloud-vision,1245
Google vision api OCR not working for Hindi language,49635343,Google vision api OCR not working for Hindi language,"<p>I am having an issue with detecting Hindi fonts using Google Vision API OCR service. The documentatio says that Hindi is supported.</p>

<p>When I drag and drop an image on their demo page (<a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/</a>), it works flawlessly. However, when I make an API call for the same image, it gives me Korean characters.</p>

<p>I am using their PHP client from github to make the API callls.</p>

<p>I have tested some other foreign languages like Japanese/Chinese. Those seem to be working fine. </p>

<p>Is there something I am missing here?</p>",,0,3,,2018-04-03 16:51:36.480 UTC,,2018-04-03 16:51:36.480 UTC,,,,,2837315,1,0,php|google-vision,312
Including an external library in an android library package,48272065,Including an external library in an android library package,"<p>I'm writing an android library to be used in my Unity android project in order to make use of an android library that doesn't have a good C# equivalent. The library itself is packaged and works fine, except when I run the application, I get an error:</p>

<pre><code>AndroidJavaException: java.lang.NoClassDefFoundError: Failed resolution of: Lcom/google/android/gms/vision/face/FaceDetector$Builder;
</code></pre>

<p>Opening up my AAR I've noticed that the Google Vision library I'm using isn't in there, and since Unity can't install android libraries it can't resolve it.</p>

<p>How can I package my application with the library installed?</p>

<p>Here's  my <code>build.gradle</code> dependencies:</p>



<pre><code>dependencies {
    provided fileTree(dir: 'libs', include: ['*.jar'])
    implementation 'com.android.support:appcompat-v7:26.1.0'
    implementation 'com.android.support.constraint:constraint-layout:1.0.2'
    testImplementation 'junit:junit:4.12'
    testCompile 'org.mockito:mockito-core:1.10.19'
    androidTestImplementation 'com.android.support.test:runner:1.0.1'
    androidTestImplementation 'com.android.support.test.espresso:espresso-core:3.0.1'
    provided files('/Applications/Unity/PlaybackEngines/AndroidPlayer/Variations/mono/Release/Classes/classes.jar')
    compile 'com.google.android.gms:play-services-vision:8.1.0'
    }
</code></pre>



<p>Thanks!</p>",,1,0,,2018-01-15 23:22:07.060 UTC,,2018-01-16 00:18:18.667 UTC,,,,,3119628,1,0,android|gradle,39
AWS Lambda w/ Google Vision API throwing PEM_read_bio:no start line or Errno::ENAMETOOLONG,40032758,AWS Lambda w/ Google Vision API throwing PEM_read_bio:no start line or Errno::ENAMETOOLONG,"<p><strong>The Goal:</strong> User uploads to S3, Lambda is triggered to take the file and send to Google Vision API for analysis, returning the results.</p>

<p>According to <a href=""https://googlecloudplatform.github.io/google-cloud-node/#/docs/google-cloud/0.41.2/guides/faq"" rel=""nofollow"">this</a>, <code>google-cloud</code> requires native libraries and must be compiled against the OS that lambda is running. Using <code>lambda-packager</code> threw an error but some internet searching turned up using an EC2 with Node and NPM to run the install instead. In the spirit of hacking through this, that's what I did to get it mostly working*. At least lambda stopped giving me ELF header errors.</p>

<p>My current problem is that there are 2 ways to call the Vision API, neither work and both return a different error (mostly).</p>

<hr>

<p><strong>The Common Code:</strong> This code is always the same, it's at the top of the function, and I'm separating it to keep the later code blocks focused on the issue.</p>

<pre><code>'use strict';

const AWS = require('aws-sdk');
const S3 = new AWS.S3();
const Bucket = 'my-awesome-bucket'; 

const gCloudConfig = {
    projectId: 'myCoolApp',
    credentials: {
        client_email: 'your.serviceapi@project.email.com',
        private_key: 'yourServiceApiPrivateKey'
    }
}
const gCloud = require('google-cloud')(gCloudConfig);
const gVision = gCloud.vision();
</code></pre>

<hr>

<p><strong>Using <code>detect()</code></strong>: This code always returns the error <code>Error: error:0906D06C:PEM routines:PEM_read_bio:no start line</code>. Theoretically it should work because the URL is public. From searching on the error, I considered it might be an HTTPS thing, so I've even tried a variation on this where I replaced HTTPS with HTTP but got the same error.</p>

<pre><code>exports.handler = (event, context, callback) =&gt; {
    const params = {
        Bucket,
        Key: event.Records[0].s3.object.key
    }
    const img = S3.getSignedUrl('getObject', params);
    gVision.detect(img, ['labels','text'], function(err, image){
        if(err){
            console.log('vision error', err);
        }
        console.log('vision result:', JSON.stringify(image, true, 2));
    });
}
</code></pre>

<hr>

<p><strong>Using <code>detectLabels()</code>:</strong> This code always returns <code>Error: ENAMETOOLONG: name too long, open ....[the image in base64]...</code>. On a suggestion, it was believed that the method shouldn't be passed the base64 image, but instead the public path; which would explain why it says the name is too long (a base64 image is quite the URL). Unfortunately, that gives the PEM error from above. I've also tried not doing the base64 encoding and pass the object buffer directly from aws but that resulted in a PEM error too.</p>

<pre><code>exports.handler = (event, context, callback) =&gt; {
    const params = {
        Bucket,
        Key: event.Records[0].s3.object.key
    }
    S3.getObject(params, function(err, data){
        const img = data.Body.toString('base64');
        gVision.detectLabels(img, function(err, labels){
            if(err){
                console.log('vision error', err);
            }
            console.log('vision result:', labels);
        });
    });
}
</code></pre>

<hr>

<p>According to <a href=""https://cloud.google.com/vision/docs/best-practices"" rel=""nofollow"">Best Practices</a>, the image should be base64 encoded.</p>

<p>From the API docs and examples and whatever else, it seems that I'm using these correctly. I feel like I've read all those docs a million times.</p>

<p>I'm not sure what to make of the NAMETOOLONG error if it's expecting base64 stuff. These images aren't more than 1MB.</p>

<p>*The PEM error seems to be related to credentials, and because my understanding of how all these credentials work and how the modules are being compiled on EC2 (which doesn't have any kind of PEM files), that might be my problem. Maybe I need to set up some credentials before running <code>npm install</code>, kind of in the same vein as needing to be installed on a linux box? This is starting to be outside my range of understanding so I'm hoping someone here knows.</p>

<p>Ideally, using <code>detect</code> would be better because I can specify what I want detected, but just getting any valid response from Google would be awesome. Any clues you all can provide would be greatly appreciated.</p>",40095605,1,1,,2016-10-13 23:42:39.107 UTC,0,2016-10-17 21:03:02.193 UTC,2016-10-14 16:58:47.990 UTC,,2800116,,2800116,1,0,javascript|amazon-ec2|lambda|google-cloud-vision,722
Google Vision Java Client Batch Request Response Identification,39478404,Google Vision Java Client Batch Request Response Identification,"<p>I'm trying to make a batch request to Google Vision Text Detection API. So far, I put the paths of the images into a list, make the batch request and get the responses. However, I cannot determine which result belongs to which image. To do this, I tried to put an ID into the request, and when I get the result back, I would compare the IDs. However, I cannot put any custom field into the request. Is there something wrong with my approach? How can I know which response belongs to which image?</p>

<p>Here is the code I use for these requests:</p>

<pre><code>private Vision vision;
private static final String APPLICATION_NAME = ""ProjectName"";

public static Vision getVisionService() throws IOException, GeneralSecurityException {

    GoogleCredential credential = GoogleCredential.fromStream
            (new FileInputStream(""/project-key.json""))
            .createScoped(VisionScopes.all());
    JsonFactory jsonFactory = JacksonFactory.getDefaultInstance();
    return new Vision.Builder(GoogleNetHttpTransport.newTrustedTransport(), jsonFactory, credential)
            .setApplicationName(APPLICATION_NAME)
            .build();
}
/**
 * Gets up to {@code maxResults} text annotations for images stored at {@code paths}.
 */
public List&lt;String&gt; detectText(List&lt;Path&gt; paths) {
    ImmutableList.Builder&lt;AnnotateImageRequest&gt; requests = ImmutableList.builder();

    try {
        for (Path path : paths) {
            byte[] data;
            data = Files.readAllBytes(path);
            requests.add(
                    new AnnotateImageRequest()
                    .setImage(new Image().encodeContent(data))
                    .setFeatures(ImmutableList.of(
                            new Feature()
                            .setType(""TEXT_DETECTION"")
                            .setMaxResults(10))));
        }

        Vision.Images.Annotate annotate =
                vision.images()
                .annotate(new BatchAnnotateImagesRequest().setRequests(requests.build()));
        // Due to a bug: requests to Vision API containing large images fail when GZipped.
        annotate.setDisableGZipContent(true);
        BatchAnnotateImagesResponse batchResponse = annotate.execute();
        assert batchResponse.getResponses().size() == paths.size();

        List&lt;String&gt; output = new ArrayList();

        for (int i = 0; i &lt; paths.size(); i++) {
            AnnotateImageResponse response = batchResponse.getResponses().get(i);
            if(response != null &amp;&amp; response.getTextAnnotations() != null){
                System.out.println(response.toString());
                String result = getDescriptionFromJson(response.getTextAnnotations().toString());
                System.out.println(response.get(""customField""));
                output.add(result);
            }
        }
        return output;
    } catch (IOException ex) {
        System.out.println(""Exception occured: "" + ex);
        return null;
    }
}

public String getDescriptionFromJson(String json){
    JSONArray results = new JSONArray(json);
    JSONObject result = (JSONObject) results.get(0);
    return result.getString(""description"");
}

public static void main(String[] args) {
    GoogleVisionQueryHelper g = new GoogleVisionQueryHelper();

    try {
        g.vision = getVisionService();
        List&lt;Path&gt; paths = new ArrayList&lt;&gt;();

        String directory = ""/images"";

        File[] files = new File(directory).listFiles();
        for(File file : files){
          if(file.isFile() &amp;&amp; !file.getName().contains(""DS_Store"") &amp;&amp; !file.getName().startsWith(""."")){
            System.out.println(file.getAbsolutePath());
            paths.add(Paths.get(file.getAbsolutePath()));
          }
        }           
        System.out.println(""Starting..."");

        for(String s: g.detectText(paths)){
            System.out.println(s);
        }
    } catch (IOException | GeneralSecurityException e) {
        // TODO Auto-generated catch block
        e.printStackTrace();
    }
}
</code></pre>",,1,1,,2016-09-13 20:21:49.247 UTC,,2016-12-25 13:44:52.490 UTC,2016-09-14 23:12:54.727 UTC,,5231007,,2604150,1,1,java|google-cloud-platform|bulk|google-cloud-vision,433
How do I use a different AWS endpoint with another AWS region?,51429447,How do I use a different AWS endpoint with another AWS region?,"<p>I am using AWS recognition on an S3 bucket of data that is currently located in the US-West-1 region. Unfortunately, AWS Rekognition is not supported in that region. I attempted to copy over my bucket into a US-West-2 region, but encountered difficulties in getting metadata. As such, my question is, how do I route my API call to another endpoint, specifically the endpoint '<a href=""https://rekognition.us-east-1.amazonaws.com"" rel=""nofollow noreferrer"">https://rekognition.us-east-1.amazonaws.com</a>' even though the bucket is based in another region. Any help or advice would be appreciated.</p>

<p>EDIT: I thought it may be relevant to mention, I am running this on Python.</p>",51429546,1,2,,2018-07-19 18:19:41.860 UTC,,2018-07-19 18:35:31.163 UTC,2018-07-19 18:28:29.263 UTC,,4065708,,9018369,1,1,python|amazon-web-services|amazon-s3|boto,48
How many labels does Google Cloud Vision have?,45075891,How many labels does Google Cloud Vision have?,"<p>I think Google Cloud Vision has awesome accuracy and by API, it is very easy to use. But I don't know how many teacher labels it has. Because of this, it can be sometimes difficult to efficiently use scores and compare with other outputs. </p>

<p>On the official document, I could't find the description about the teacher label list or something. 
Anyone knows how many teacher label it has and the rules about the labels?(I felt it has some hierarchical rules.)</p>",,0,2,,2017-07-13 08:55:49.557 UTC,,2017-07-13 08:55:49.557 UTC,,,,,6474040,1,4,google-cloud-vision,163
Python Google Vision python script syntax errors,44966573,Python Google Vision python script syntax errors,"<p>I am having some issues using the cloudvisreq python script for the Google Vision Python API. I get this error when I run the code:</p>

<pre><code>File ""newvisreq.py"", line 46
    api_key, *image_filenames = argv[1:]
             ^
SyntaxError: invalid syntax
</code></pre>

<p>I am running the script through Python2.7, as it told me to in the tutorial I'm using to set this up. I found when I ran it through Python3 it was slightly more successful, as it managed to write as it was supposed to, but it received no data. The code can be found <a href=""https://gist.github.com/dannguyen/a0b69c84ebc00c54c94d/raw/2850e03ea92190114973f1064840783bfe026736/cloudvisreq.py"" rel=""nofollow noreferrer"">here</a>, and the line the error complains about is about half way through the file (line 46).</p>

<p>Thanks in advance,</p>

<p>Connor</p>",44966683,1,1,,2017-07-07 08:53:46.170 UTC,,2017-07-07 16:45:06.767 UTC,2017-07-07 16:43:25.410 UTC,,7976758,,6506426,1,0,python,53
Getting an InvalidParameterException when using compareFaces on aws-sdk-js,50237770,Getting an InvalidParameterException when using compareFaces on aws-sdk-js,"<p>When using the compare faces function of the aws-sdk with nodeJS we are sporadically seeing this error: </p>

<pre><code>InvalidParameterException: Request has Invalid Parameters
 at Request.extractError (/app/node_modules/aws-sdk/lib/protocol/json.js:48:27)
 at Request.callListeners (/app/node_modules/aws-sdk/lib/sequential_executor.js:105:20)
 at Request.emit (/app/node_modules/aws-sdk/lib/sequential_executor.js:77:10)
 at Request.emit (/app/node_modules/aws-sdk/lib/request.js:683:14)
 at Request.transition (/app/node_modules/aws-sdk/lib/request.js:22:10)     at AcceptorStateMachine.runTo (/app/node_modules/aws-sdk/lib/state_machine.js:14:12)
 at /app/node_modules/aws-sdk/lib/state_machine.js:26:10
 at Request.&lt;anonymous&gt; (/app/node_modules/aws-sdk/lib/request.js:38:9)
 at Request.&lt;anonymous&gt; (/app/node_modules/aws-sdk/lib/request.js:685:12)
 at Request.callListeners (/app/node_modules/aws-sdk/lib/sequential_executor.js:115:18)
 at Request.emit (/app/node_modules/aws-sdk/lib/sequential_executor.js:77:10)
 at Request.emit (/app/node_modules/aws-sdk/lib/request.js:683:14)     at Request.transition (/app/node_modules/aws-sdk/lib/request.js:22:10)
 at AcceptorStateMachine.runTo (/app/node_modules/aws-sdk/lib/state_machine.js:14:12)
 at /app/node_modules/aws-sdk/lib/state_machine.js:26:10
 at Request.&lt;anonymous&gt; (/app/node_modules/aws-sdk/lib/request.js:38:9)
   message: 'Request has Invalid Parameters',
   code: 'InvalidParameterException',
   time: 2018-05-08T15:27:28.188Z,
   requestId: 'XXXXX',
   statusCode: 400,
   retryable: false,
   retryDelay: 52.72405778418885 }
</code></pre>

<p>The images are captured every time using an iPhone camera, are saved as JPEG's and do contain faces. The images are not corrupt and have been tested using jpeginfo. They are then converted to binary and send to rekognition via the sdk. We have ran the same images through the python library Boto and successfully receive a comparison result. </p>

<p>Are there an further diagnostic steps we can take on the node side to aid in debugging? Or any insight into the cause of the error? </p>

<p><strong>Update:</strong> </p>

<p>Image sizes: 
source: 1189 × 750
target: 360 × 480</p>",,1,0,,2018-05-08 15:56:22.053 UTC,,2018-05-22 12:48:26.470 UTC,2018-05-22 12:48:26.470 UTC,,4853611,,4853611,1,0,node.js|aws-sdk-js|amazon-rekognition,146
Google Vision API applied to identifying a javascript function,54855549,Google Vision API applied to identifying a javascript function,"<p>I am trying to create an application that read an image containing a simple Javascript code -- ex: a <code>const a = 123; console.log(a);</code> -- and execute this code afterwards.</p>

<p>For that, I am trying to use Google Vision API. My main problem so far is that some special characters (<code>=</code>, <code>=&gt;</code>, <code>;</code>) are not being recognized correctly.</p>

<p>Some examples:</p>

<p><code>const a = 123</code> is converted to <code>const a -123</code></p>

<p><code>const a = '123'</code> is converted to <code>const a 123</code></p>

<p>I saw this <a href=""https://stackoverflow.com/questions/52829583/special-characters-which-are-identified-as-individual-word-in-google-vision-ocr"">question</a>, and it seems that I can insert some special chars to be recognized, but I do not know where.</p>

<p>Is there a way to improve the recognition for this context?</p>",,0,3,,2019-02-24 19:15:13.870 UTC,,2019-02-24 19:15:13.870 UTC,,,,,4437011,1,1,javascript|google-api|ocr|google-vision,27
Google Cloud Vision Api Error,49425490,Google Cloud Vision Api Error,"<p>I have a problem with the Google Cloud Vision Api. When I'm trying to send a request to the api with this url:</p>

<p><a href=""https://vision.googleapis.com/v1/images:annotate?key=MyKey"" rel=""nofollow noreferrer"">https://vision.googleapis.com/v1/images:annotate?key=MyKey</a></p>

<p>The server replies (Error 404):</p>

<blockquote>
  <p>The requested URL /v1/images:annotate?key=MyKey was not found on this 
  server. That’s all we know.</p>
</blockquote>

<p>How can I fix this? I tried to do with an ajax request and via browser too.</p>

<p>Thanks in advance!</p>",,0,4,,2018-03-22 09:55:30.940 UTC,,2018-03-22 10:35:33.857 UTC,2018-03-22 10:35:33.857 UTC,,8259345,,8842948,1,0,api|cloud|vision|google-vision,354
Can Google Cloud Vision generate labels in Spanish via its API?,45260779,Can Google Cloud Vision generate labels in Spanish via its API?,<p>say that I have images and I want to generate labels for them in Spanish - does the Google Cloud Vision API allow to select which language to return the labels in?</p>,,1,0,,2017-07-23 01:30:10.287 UTC,,2017-08-01 18:07:06.370 UTC,2017-08-01 18:07:06.370 UTC,,322020,,7692297,1,2,google-cloud-vision,385
Ip camera not detected in opencv 3.2 and Java,53695553,Ip camera not detected in opencv 3.2 and Java,"<p>Have anyone able to use ip camera with open cv with Java. I used the below code which was working fine for the web camera. But when i Tried to use a ip cam it was not working</p>

<pre><code>import java.io.ByteArrayInputStream;
import java.io.InputStream;
import java.nio.ByteBuffer;
import javax.imageio.ImageIO;
import org.opencv.core.Core;
import org.opencv.core.Mat;
import org.opencv.core.MatOfByte;
import org.opencv.core.MatOfRect;
import org.opencv.core.Point;
import org.opencv.core.Rect;
import org.opencv.core.Scalar;
import org.opencv.imgcodecs.Imgcodecs;
import org.opencv.imgproc.Imgproc;
import org.opencv.objdetect.CascadeClassifier;
import org.opencv.videoio.VideoCapture;

import com.amazonaws.services.rekognition.model.Image;
import com.amazonaws.util.IOUtils;
import com.wso2telco.rnd.ui.DashBoard;
import com.wso2telco.rnd.ui.UI;

public class TestIPCamera {

private static final long SLEEP_TIME = 1000;
private static VideoCapture camera;
private static FaceComparer comparer;
private static UI ui;
private static DashBoard dashUi;

public static void main(String args[]) {

    comparer = new FaceComparer();

    System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
    //nu.pattern.OpenCV.loadLocally();
    System.loadLibrary(""opencv_java320"");

    camera = new VideoCapture();
    camera.open(""http://192.168.1.2:8080/video?video=x.mpjeg"");
    System.out.println(camera.isOpened());
    while(!camera.isOpened()) {
        try {
            Thread.sleep(100);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }
    if (!camera.isOpened()) {
        System.out.println(""Camera Error"");
    } else {
        System.out.println(""Camera Ready"");
        ui = new UI();
        dashUi = new DashBoard();

    }
    while(true){
        runImage();
    }
}
</code></pre>

<p>I tried many suggestions given in other threads but couldn't find a concrete answer for the issue.</p>",53721952,1,12,,2018-12-09 18:49:11.327 UTC,,2018-12-11 10:19:02.930 UTC,,,,,4994602,1,0,java|opencv,53
Determine Categorical Hierarchy Level of Freebase MID Value,43740356,Determine Categorical Hierarchy Level of Freebase MID Value,"<p>After using the Google Cloud Vision API, I received MID values in the format of <code>/m/XXXXXXX</code> (not necessarily 7 characters at the end though). What I would like to do is determine how specific one MID value is compared to the others. Essentially how broad vs. refined a term is. For example, the term <em>Vehicle</em> might be <strong>level 1</strong> while the term <em>Van</em> might be <strong>level 2</strong>.</p>

<p>I have tried to run the MID values through the Google Knowledge Graph API but unfortunately these MIDs are not in that database and return no information. For example, a few MIDs and descriptions I have are as follows:</p>

<pre><code>/m/07s6nbt = text
/m/03gq5hm = font
/m/01n5jq = poster
/m/067408 = album cover
</code></pre>

<p>My initial thought on why these MIDs return nothing in the Knowledge Graph API is that they were not carried over after the discontinuation of Freebase. I understand that Google provides an RDF dump of Freebase but I'm not sure how to read that data in Python and use it to determine the depth of a mid in the hierarchy.</p>

<p>If it's not possible to determine the category level of the MID value, the number of connections a term had would also be an appropriate proxy. Assuming broader terms have more connections to other terms than more refined terms. I found <a href=""https://disi.unitn.it/~lissandrini/notes/freebase-data-dump.html"" rel=""nofollow noreferrer"">an article</a> that discusses the amount of ""edges"" a MID has which I believe means the number of connections. However, they do some converting between MID values to Long Values and use various scripts that keep giving me numerous errors in Python. I was hoping for a simple table with MID values in one column and the number of connections in another but I'm lost in their code, converting values, and Python Errors.</p>

<p>If you have any suggestions for easily determining the amount of connections a MID has or its hierarchical level, it would be greatly appreciated. Thank you!</p>",43745689,1,0,,2017-05-02 14:24:16.547 UTC,,2017-05-02 19:23:14.930 UTC,,,,,7020773,1,1,python|freebase|google-cloud-vision|google-knowledge-graph,109
Is there a simple way to get a NodeJS object consumable by SQL Server and/or .net?,54560404,Is there a simple way to get a NodeJS object consumable by SQL Server and/or .net?,"<p>I have a SQL Server 2017 database and a series of .net APIs that consume the stored procedures exposed from the database. This works fine for posting data from a website/mobile app and reading that data back. However, one of my pages, a registration page, needs to do more. It takes the usual, name, email, telephone etc and this is all good. It also takes a document as an image. The image uploads fine and is stored in the database. I want to be able to ""read"" that image though and extract specific parts. </p>

<p>For example, let's assume the image is a drivers licence. I know that the image should contain a headshot and various other key bits of info such as name, issuing date, reference number etc.</p>

<p>I want to be able to read the information from the image. I know there are pitfalls to this, and extracting the photo might be a step too far, but I want the app to extract the text so that I can verify it's what I'm expecting. If it matches, it's a pass. If it matches, say 90%, it gets quarantined for manual verification, and anything less than this is rejected. I'm actually hoping to get two processes, the one described above and another process that checks a second image to ensure that it's appropriate content (i.e. not porn).</p>

<p>I have a developer working on a tool that connects to the AWS Rekognition service which seems to do exactly what I need, but he's writing this in NodeJS.</p>

<p>So, my questions are:
1) Can this be compiled through Visual Studio into something that the .net website and mobile apps can work with?
2) If so, I'm assuming this would be a dll, can I then use that dll in a SQL Server CLR so the database can consume it too?</p>

<p>If the answers to these are no, is there another solution? Could I, for example, publish the NodeJS app exposing an API so it can be consumed that way?</p>

<p>My ultimate aim is to be able to check and extract the data from the image, so if there is a better solution using a different approach, I'm all ears.</p>",,1,0,,2019-02-06 18:37:32.973 UTC,,2019-02-06 19:12:00.613 UTC,,,,,2342099,1,0,.net|node.js|sql-server|amazon-web-services|image-manipulation,20
How to use normalized coordinates in a rectangle for OpenCV,51747822,How to use normalized coordinates in a rectangle for OpenCV,"<p>I'm using the Microsoft Custom Vision service for object detection with the Python SDK. I'm able to make predictions and I'm trying to use the bounding box information that comes back from the prediction to overlay a rectangle on the image using OpenCV.</p>

<p>However, I'm not sure how to exactly calculate from the normalized coordinates that come back from the Custom Vision service to the point vertexes that the OpenCV <code>rectangle</code> function takes in.</p>

<p>Here's an example of what comes back from the service as bounding box:</p>

<pre><code>{'left': 0.146396145,
 'top': 0.0305180848,
 'width': 0.373975337,
 'height': 0.570280433}
</code></pre>

<p>Currently, I'm doing these calculations below. The <code>x</code> and <code>y</code> values look like they're being calculated correctly, but I'm not sure how to calculate the second vertex. The image shape was resized to <code>(400, 400)</code>.</p>

<pre><code>for pred in predictions:
    x = int(pred.bounding_box.left * img.shape[0])
    y = int(pred.bounding_box.top * img.shape[1])

    width = int(pred.bounding_box.width * img.shape[0])
    height = int(pred.bounding_box.height * img.shape[1])

    img = cv2.rectangle(img, (x,y), (width,height), (0,0,255), 2)
</code></pre>

<p>And here is the resulting image from the above code:
<a href=""https://i.stack.imgur.com/aXrOf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aXrOf.png"" alt=""enter image description here""></a></p>

<p>The first box looks like it's not going far enough, whereas the second box looks like it produced a rectangle going the opposite way of where it should.</p>

<p>Does anyone know how to calculate these correctly from normalized coordinates?</p>",51751867,1,0,,2018-08-08 13:21:36.603 UTC,,2018-08-08 16:52:54.077 UTC,,,,,186013,1,1,python|python-3.x|opencv|microsoft-cognitive,550
Application Default Credentials do not exist,38620455,Application Default Credentials do not exist,"<p>I've been trying to use the Google Cloud Vision API to label and classify images, but I've been having a lot of trouble with credentials. I've set up credentials in the SDK and on the API manager itself, and I have set the GOOGLE_APPLICATION_CREDENTIALS environment variable, but the IDE I am running the code on still outputs:</p>

<pre><code>ApplicationDefaultCredentialsError: File C:\Users\elden\Documents\Credentials\My First Project-6c0f3ccb6309 (pointed by GOOGLE_APPLICATION_CREDENTIALS environment variable) does not exist!
</code></pre>

<p>Here is the section of code that obtains the credentials:</p>

<pre><code>credentials = gc.get_application_default()
service = build('vision', 'v1', credentials=credentials,discoveryServiceUrl=DISCOVERY_URL)
</code></pre>

<p>And here are the imports:</p>

<pre><code>import argparse
import base64
import httplib2

from googleapiclient.discovery import build
from oauth2client.client import GoogleCredentials as gc
</code></pre>

<p>I'm running the code on Spyder 2.7.11 32-bit install on Windows 10.</p>

<p>The key is a generated JSON file.</p>",38624767,1,0,,2016-07-27 18:19:58.657 UTC,1,2016-07-27 23:26:11.327 UTC,,,,,6646025,1,1,python-2.7|google-app-engine,539
Face API Android SSL Exception,54685737,Face API Android SSL Exception,"<p>We are using Microsoft's Azure Face API for over 12 months now. But our low level API Android users (19) starting to get this error: </p>

<p>javax.net.ssl.SSLException: hostname in certificate didn't match:  != &lt;<em>.cognitiveservices.azure.com> OR &lt;</em>.cognitiveservices.azure.com> </p>

<p>It is working on level api 21+ </p>",,1,1,,2019-02-14 08:09:32.723 UTC,,2019-02-25 11:08:22.670 UTC,,,,,6023760,1,0,android|azure|face-api|sslexception,127
Google vision api request format,50145372,Google vision api request format,"<p>I'm pretty new to the google vision api and I am trying to make a <code>TEXT_DETECTION</code> request. </p>

<p>Currently I am reading from an image file, encoding it to <code>base64</code> and trying to pass it on to the request</p>

<pre><code>const endPoint =`https://vision.googleapis.com/v1/images:annotate?key=${config.api_key}`;

fetch(endPoint, {
  method: ""POST"",
  body: JSON.stringify({
    requests: [
      {
        image: {
          content: data
        },
        features: [
          {
            type: ""TEXT_DETECTION""
          }
        ],
        imageContext: {
          langaugeHints: [""en""]
        }
      }
    ]
  })
})
</code></pre>

<p>But I am getting a <code>400</code> error. Could anyone spot the error in the code?. </p>",,1,0,,2018-05-03 00:39:45.057 UTC,,2018-05-03 01:20:36.537 UTC,,,,,1819683,1,0,node.js|api|google-vision,177
"""FormsApp is a namespace but used like type""",46516494,"""FormsApp is a namespace but used like type""","<p>I know this is a frequently asked question, but I couldn't find an answer. I am trying out a Microsoft Emotion API (I've used the generic key here for the purpose of asking the question), and it keeps giving me the error, ""<code>WindowsFormsApp2</code> is a namespace but is used as a type"" even when I change the namespace. I changed the namespace to a more appropriate title, but I still received the build error that <code>WindowsFormsApp2</code> was inappropriately used as a type, despite the fact it was nowhere in the code. I don't know where else I'm using it that it is creating this issue. </p>

<p>Here is my code:</p>

<pre><code>using System.Windows.Forms;
using System.IO;
using System.Net.Http.Headers;
using System.Net.Http;

namespace WindowsFormsApp2
{
    public partial class Form1 : Form
    {
        public Form1()
        {
            InitializeComponent();
            {
                textBox1.Text = (""Enter the path to a JPEG image file:"");
                openFileDialog1.ShowDialog();
                string imageFilePath = openFileDialog1.FileName;

                MakeRequest(imageFilePath);

                textBox1.Text = (""\n\n\nWait for the result below, then hit ENTER to exit...\n\n\n"");
            }


            byte[] GetImageAsByteArray(string imageFilePath)
            {
                FileStream fileStream = new FileStream(imageFilePath, FileMode.Open, FileAccess.Read);
                BinaryReader binaryReader = new BinaryReader(fileStream);
                return binaryReader.ReadBytes((int)fileStream.Length);
            }

            async void MakeRequest(string imageFilePath)
            {
                var client = new HttpClient();

                // Request headers - replace this example key with your valid key.
                client.DefaultRequestHeaders.Add(""Ocp-Apim-Subscription-Key"", ""13hc77781f7e4b19b5fcdd72a8df7156"");

                // NOTE: You must use the same region in your REST call as you used to obtain your subscription keys.
                //   For example, if you obtained your subscription keys from westcentralus, replace ""westus"" in the 
                //   URI below with ""westcentralus"".
                string uri = ""https://westus.api.cognitive.microsoft.com/emotion/v1.0/recognize?"";
                HttpResponseMessage response;
                string responseContent;

                // Request body. Try this sample with a locally stored JPEG image.
                byte[] byteData = GetImageAsByteArray(imageFilePath);

                using (var content = new ByteArrayContent(byteData))
                {
                    // This example uses content type ""application/octet-stream"".
                    // The other content types you can use are ""application/json"" and ""multipart/form-data"".
                    content.Headers.ContentType = new MediaTypeHeaderValue(""application/octet-stream"");
                    response = await client.PostAsync(uri, content);
                    responseContent = response.Content.ReadAsStringAsync().Result;
                }

                //A peak at the JSON response.
                textBox1.Text = (responseContent);
            }



        }

    }
}
</code></pre>",46534849,2,4,,2017-10-01 20:16:14.043 UTC,,2017-10-02 22:55:31.927 UTC,2017-10-02 04:04:08.377 UTC,,8437099,,8185416,1,0,c#|namespaces|windows-forms-designer,258
Google Vision: Drawing mask on Face with animations,45141098,Google Vision: Drawing mask on Face with animations,"<p>I am using google vision library for face detection. Face detection is perfect and I get all the info like vertices, angles like eulerY, eulerZ.</p>

<p>I want to draw mask on face, drawing is ok but the face mask is not following the face position as it should, the position is not correct. Here is my edited code to draw face mask on googly eyes project.</p>

<p>Here is my source code:</p>

<pre><code>package com.google.android.gms.samples.vision.face.googlyeyes;

import android.content.Context;
import android.graphics.Canvas;
import android.graphics.Color;
import android.graphics.Paint;
import android.graphics.PointF;
import android.graphics.Rect;
import android.graphics.drawable.Drawable;
import android.util.Log;
import android.view.LayoutInflater;
import android.view.View;
import android.widget.FrameLayout;
import android.widget.ImageView;
import android.widget.LinearLayout;

import com.google.android.gms.samples.vision.face.googlyeyes.ui.camera.GraphicOverlay;
import com.google.android.gms.vision.face.Face;

import java.util.HashMap;

/**
 * Graphics class for rendering Googly Eyes on a graphic overlay given the current eye positions.
 */
class GooglyEyesGraphic extends GraphicOverlay.Graphic {

 private Paint mEyeWhitesPaint;
 private Paint mEyeIrisPaint;
 private Paint mEyeOutlinePaint;
 private Paint mEyeLidPaint;
 Paint mBoxPaint;


 Context mContext;
 private static final float BOX_STROKE_WIDTH = 20.0 f;
 FrameLayout frameLayout;
 ImageView imageView;


 //    Bitmap bmpOriginal;

 //==============================================================================================
 // Methods
 //==============================================================================================

 GooglyEyesGraphic(GraphicOverlay overlay, Context mContext) {
  super(overlay);

  this.mContext = mContext;
  mEyeWhitesPaint = new Paint();
  mEyeWhitesPaint.setColor(Color.WHITE);
  mEyeWhitesPaint.setStyle(Paint.Style.FILL);

  mEyeLidPaint = new Paint();
  mEyeLidPaint.setColor(Color.YELLOW);
  mEyeLidPaint.setStyle(Paint.Style.FILL);

  mEyeIrisPaint = new Paint();
  mEyeIrisPaint.setColor(Color.BLACK);
  mEyeIrisPaint.setStyle(Paint.Style.FILL);

  mEyeOutlinePaint = new Paint();
  mEyeOutlinePaint.setColor(Color.BLACK);
  mEyeOutlinePaint.setStyle(Paint.Style.STROKE);
  mEyeOutlinePaint.setStrokeWidth(5);


  mBoxPaint = new Paint();
  mBoxPaint.setColor(Color.MAGENTA);
  mBoxPaint.setStyle(Paint.Style.STROKE);
  mBoxPaint.setStrokeWidth(BOX_STROKE_WIDTH);
  mBoxPaint.setAlpha(40);

  LayoutInflater li = (LayoutInflater) mContext.getSystemService(Context.LAYOUT_INFLATER_SERVICE);
  View view = li.inflate(R.layout.mask_layout, null);
  imageView = (ImageView) view.findViewById(R.id.flMaskIV);
  frameLayout = (FrameLayout) view.findViewById(R.id.frameLayout);

 }

 private volatile Face mFace;

 /**
  * Updates the eye positions and state from the detection of the most recent frame.  Invalidates
  * the relevant portions of the overlay to trigger a redraw.
  */
 void updateEyes(PointF leftPosition, boolean leftOpen,
  PointF rightPosition, boolean rightOpen, Face mFace) {

  if (facesList.containsKey(mFace.getId())) {
   PointF pointF1 = facesList.get(mFace.getId()).getPosition();
   PointF pointF2 = mFace.getPosition();

   double x = Math.sqrt(Math.pow(pointF2.x - pointF1.x, 2) - Math.pow(pointF2.y - pointF1.y, 2));
   if (x &lt; 0)
    x = (-1 * x);
   if (x &lt; 10)
    return;
   Log.e(""face Called"", ""FaceCalled"");

  }
  this.mFace = mFace;
  facesList.put(mFace.getId(), mFace);



  postInvalidate();
 }

 public HashMap &lt; Integer, Face &gt; facesList = new HashMap &lt; &gt; ();

 /**
  * Draws the current eye state to the supplied canvas.  This will draw the eyes at the last
  * reported position from the tracker, and the iris positions according to the physics
  * simulations for each iris given motion and other forces.
  */
 @Override
 public void draw(Canvas canvas) {
  if (mFace == null)
   return;
  //        if (facesList.containsKey(mFace.getId())) {
  //            PointF pointF1 = facesList.get(mFace.getId()).getPosition();
  //            PointF pointF2 = mFace.getPosition();
  //
  //            double x = Math.sqrt(Math.pow(pointF2.x - pointF1.x, 2) - Math.pow(pointF2.y - pointF1.y, 2));
  //            if (x &lt; 0)
  //                x = (-1 * x);
  //            if (x &lt; 10)
  //                return;
  //            Log.e(""face Called"", ""FaceCalled"");
  //
  //        }
  //
  //        facesList.put(mFace.getId(), mFace);

  if (this.canvas == null)
   this.canvas = canvas;
  applyMask();
 }

 Drawable drawable;
 Canvas canvas;

 private void applyMask() {

  if (canvas == null)
   return;
  //        Log.e(""mFace.getEulerY()"", ""mFace.getEulerY()=&gt; "" + mFace.getEulerY());
  if (GooglyEyesActivity.maskImgView != null) {
   GooglyEyesActivity.maskImgView.setVisibility(View.GONE);
   GooglyEyesActivity.maskImgView.setImageResource(GooglyEyesActivity.currEmoticonID);
  }

  float x = translateX(mFace.getPosition().x + mFace.getWidth() / 2);
  float y = translateY(mFace.getPosition().y + mFace.getHeight() / 2);

  // Draws a bounding box around the face.
  float xOffset = scaleX(mFace.getWidth() / 2.0 f);
  float yOffset = scaleY(mFace.getHeight() / 2.0 f);
  float left = x - xOffset - 50;
  float top = y - (yOffset) - 50;
  float right = x + xOffset + 50;
  float bottom = y + (yOffset) + 50;


  //        canvas.drawRect((int) left, (int) top, (int) right, (int) bottom, mBoxPaint);


  drawable = GooglyEyesActivity.maskImgView.getDrawable();
  ///////////////////
  canvas.save();
  canvas.translate(left, top);
  //        frameLayout.setX(left);
  //        frameLayout.setY(top);
  Rect rect = new Rect((int) left, (int) top, (int) right, (int) bottom);
  frameLayout.measure(rect.width(), rect.height());
  frameLayout.setLayoutParams(new LinearLayout.LayoutParams(rect.width(), rect.height()));

  frameLayout.layout(0, 0, (int) right, (int) bottom);


  frameLayout.setClipBounds(rect);
  imageView.setLayoutParams(new FrameLayout.LayoutParams(rect.width(), rect.height()));
  imageView.setRotationY(mFace.getEulerY());
  imageView.setRotation(mFace.getEulerZ());
  imageView.setImageDrawable(drawable);
  frameLayout.draw(canvas);
  canvas.restore();

 }


}
</code></pre>

<p>Also i need to add animations so i tried using <code>dlib</code> library to get landmarks points and draw it using <code>opengl</code> but in <code>opengl</code> i dont have any function to populate the <code>vertice</code> array i am getting from <code>dlib</code>. As the dlib landmarks are in points but the array there is not in such a way. Any help will be appreciated for both scenarios.</p>

<p>Thank you in advance.</p>

<p>Thanks.</p>",,0,2,,2017-07-17 09:55:27.063 UTC,,2017-07-17 10:19:45.307 UTC,2017-07-17 10:19:45.307 UTC,,736036,,1484775,1,0,android|animation|face-detection|dlib|google-vision,589
PHP Fatal error: Class Uncaught Error: Class 'Aws\Rekognition\RekognitionClient' in Sandbox,52659767,PHP Fatal error: Class Uncaught Error: Class 'Aws\Rekognition\RekognitionClient' in Sandbox,"<p>I have to try AWS Rekognition API's. And am new for PHP. So, Now am using PHP code for SandBox.</p>

<p>Now, I got following Error,</p>

<pre><code>&lt;br /&gt;
&lt;b&gt;Fatal error&lt;/b&gt;:  Uncaught Error: Class 'Aws\Rekognition\RekognitionClient' not found in [...][...]:4
Stack trace:
#0 {main}
thrown in &lt;b&gt;[...][...]&lt;/b&gt; on line &lt;b&gt;4&lt;/b&gt;&lt;br /&gt;
</code></pre>

<p>Note: my sandbox test cases URL: <a href=""http://sandbox.onlinephpfunctions.com/code/6f532af13c65cff6375cdbb4058b1547f6059ec6"" rel=""nofollow noreferrer"">http://sandbox.onlinephpfunctions.com/code/6f532af13c65cff6375cdbb4058b1547f6059ec6</a></p>",52659822,1,0,,2018-10-05 06:45:14.987 UTC,1,2018-10-05 06:49:14.610 UTC,,,,,6065008,1,0,php|amazon-web-services|aws-sdk|amazon-rekognition,62
Google vision API Barcode Scanner remove camera view,52003791,Google vision API Barcode Scanner remove camera view,"<p>Currently I am developing a bar code reader android app with Google vision API. I need to start camera preview when button is clicked and until the button is clicked screen should be empty white color screen. When I Try to do this camera preview starts at the same time screen also appears how to solve this problem please help me. </p>

<p><a href=""https://i.stack.imgur.com/631uj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/631uj.png"" alt=""Screen before Start scanner""></a></p>

<p><a href=""https://i.stack.imgur.com/bOJcw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bOJcw.png"" alt=""Screen After start Scanner""></a></p>

<p>My MainActivity Class</p>

<pre><code>public class MainActivity extends AppCompatActivity implements BarcodeRetriever {

// use a compound button so either checkbox or switch widgets work.


private static final String TAG = ""BarcodeMain"";


BarcodeCapture barcodeCapture;

@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.activity_main);


    barcodeCapture = (BarcodeCapture) getSupportFragmentManager().findFragmentById(barcode);
    barcodeCapture.setRetrieval(this);


    findViewById(R.id.refresh).setOnClickListener(new View.OnClickListener() {
        @Override
        public void onClick(View view) {
            barcodeCapture.setShowDrawRect(true)
                    .setSupportMultipleScan(false)
                    .setTouchAsCallback(true)
                    .shouldAutoFocus(true)
                    .setShowFlash(true)
                    .setBarcodeFormat(Barcode.ALL_FORMATS)
                    .setCameraFacing(false ? CameraSource.CAMERA_FACING_FRONT : CameraSource.CAMERA_FACING_BACK)
                    .setShouldShowText(true);

            barcodeCapture.resume();
            barcodeCapture.refresh(true);
        }
    });

}


@Override
public void onRetrieved(final Barcode barcode) {
    Log.d(TAG, ""Barcode read: "" + barcode.displayValue);
    runOnUiThread(new Runnable() {
        @Override
        public void run() {

            AlertDialog.Builder builder = new AlertDialog.Builder(MainActivity.this)
                    .setTitle(""code retrieved"")
                    .setMessage(barcode.displayValue);
            builder.show();
        }
    });
    barcodeCapture.stopScanning();


}

@Override
public void onRetrievedMultiple(final Barcode closetToClick, final List&lt;BarcodeGraphic&gt; barcodeGraphics) {
    runOnUiThread(new Runnable() {
        @Override
        public void run() {
            String message = ""Code selected : "" + closetToClick.displayValue + ""\n\nother "" +
                    ""codes in frame include : \n"";
            for (int index = 0; index &lt; barcodeGraphics.size(); index++) {
                Barcode barcode = barcodeGraphics.get(index).getBarcode();
                message += (index + 1) + "". "" + barcode.displayValue + ""\n"";
            }
            AlertDialog.Builder builder = new AlertDialog.Builder(MainActivity.this)
                    .setTitle(""code retrieved"")
                    .setMessage(message);
            builder.show();
        }
    });

}

@Override
public void onBitmapScanned(SparseArray&lt;Barcode&gt; sparseArray) {
    for (int i = 0; i &lt; sparseArray.size(); i++) {
        Barcode barcode = sparseArray.valueAt(i);
        Log.e(""value"", barcode.displayValue);
    }

}

@Override
public void onRetrievedFailed(String reason) {

}

@Override
public void onPermissionRequestDenied() {

}
</code></pre>

<p>}</p>

<p>My activity_main.xml file</p>

<pre><code>   &lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;RelativeLayout xmlns:android=""http://schemas.android.com/apk/res/android""
    xmlns:app=""http://schemas.android.com/apk/res-auto""
    xmlns:tools=""http://schemas.android.com/tools""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""
    tools:context="".MainActivity""&gt;

    &lt;RelativeLayout
        android:layout_width=""match_parent""
        android:layout_height=""match_parent""
        android:layout_above=""@+id/actions""
        android:layout_weight=""1""
        android:gravity=""center""&gt;

        &lt;fragment
            android:id=""@+id/barcode""
       android:name=""com.google.android.gms.samples.vision.barcodereader.BarcodeCapture""
            android:layout_width=""fill_parent""
            android:layout_height=""fill_parent""
            app:gvb_auto_focus=""true""
            app:gvb_code_format=""code_39|aztec""
            app:gvb_flash=""false""
            app:gvb_rect_colors=""@array/rect_color"" /&gt;
    &lt;/RelativeLayout&gt;

    &lt;LinearLayout
        android:id=""@+id/actions""
        android:layout_width=""match_parent""
        android:layout_height=""wrap_content""
        android:layout_alignParentBottom=""true""
        android:orientation=""vertical""
        android:paddingLeft=""16dp""
        android:layout_gravity=""center_vertical""
        android:gravity=""center_vertical""
        android:padding=""20dp""
        android:paddingRight=""16dp""&gt;

        &lt;RelativeLayout
            android:id=""@+id/refresh""
            android:layout_width=""match_parent""
            android:layout_height=""60dp""
            android:layout_marginLeft=""40dp""
            android:layout_marginRight=""40dp""
            android:layout_gravity=""center_vertical""
            android:gravity=""center_vertical""
            android:background=""@drawable/circle_layout_for_start_scan""&gt;

        &lt;/RelativeLayout&gt;
    &lt;/LinearLayout&gt;
&lt;/RelativeLayout&gt;
</code></pre>",52072359,2,5,,2018-08-24 11:46:26.110 UTC,,2018-08-29 07:57:04.650 UTC,2018-08-24 12:32:18.010 UTC,,9763253,,7018008,1,0,android|barcode-scanner|google-vision,368
Google Vision API Requests from IP address ... are blocked,43409733,Google Vision API Requests from IP address ... are blocked,"<p>I using Google Vision OCR to get text from image with PHP language.
When i used in a long time. I get error below:</p>

<pre><code>array(1) {
  [""error""]=&gt;
  array(4) {
    [""code""]=&gt;
    int(403)
    [""message""]=&gt;
    string(53) ""Requests from IP address ..... are blocked.""
    [""status""]=&gt;
    string(17) ""PERMISSION_DENIED""
    [""details""]=&gt;
    array(1) {
      [0]=&gt;
      array(2) {
        [""@type""]=&gt;
        string(35) ""type.googleapis.com/google.rpc.Help""
        [""links""]=&gt;
        array(1) {
          [0]=&gt;
          array(2) {
            [""description""]=&gt;
            string(32) ""Google developer console API key""
            [""url""]=&gt;
            string(75) ""https://console.developers.google.com/project/191619933224/apiui/credential""
          }
        }
      }
    }
  }
}
</code></pre>

<p>Please show me reason and how to fix it.
Thank all so much.</p>",,1,2,,2017-04-14 10:27:24.077 UTC,,2017-04-14 19:23:43.497 UTC,,,,,5217578,1,0,php|ocr|google-vision,509
Function not returning anything when using switch case,51189901,Function not returning anything when using switch case,"<p>If I return an <code>array</code> before <code>SWITCH</code> condition it works fine but if try to return something after <code>SWITCH</code> condition (even hardcoded array) it does not return anything. Also it does not go in any <code>CASE</code>, not even <code>DEFAULT</code>. Even print or echo does not work.</p>

<p>My <code>$e-&gt;getAwsErrorCode()</code> function is returning <code>InvalidSignatureException</code> but it is not entering into related switch case.</p>

<p>I checked error log and there is nothing there, no error or warning printed on the page.</p>

<pre><code>private function rekognition_error_catch($e)
    {
        $arr_error = array();
        /*return [
            'error_code' =&gt; 34,
            'error_message' =&gt; 'Error'
        ];*/
        switch ($e-&gt;getAwsErrorCode()) {
            case 'InvalidParameterException':
                $arr_error['error_code'] = 71;
                $arr_error['error_message'] = $e-&gt;getAwsErrorMessage();
                break;
            case 'InvalidS3ObjectException':
                $arr_error['error_code'] = 72;
                $arr_error['error_message'] = $e-&gt;getAwsErrorMessage();
                break;
            case 'ImageTooLargeException':
                $arr_error['error_code'] = 73;
                $arr_error['error_message'] = $e-&gt;getAwsErrorMessage();
                break;
            case 'AccessDeniedException':
                $arr_error['error_code'] = 74;
                $arr_error['error_message'] = $e-&gt;getAwsErrorMessage();
                break;
            case 'InternalServerError':
                $arr_error['error_code'] = 75;
                $arr_error['error_message'] = $e-&gt;getAwsErrorMessage();
                break;
            case 'ThrottlingException':
                $arr_error['error_code'] = 76;
                $arr_error['error_message'] = $e-&gt;getAwsErrorMessage();
                break;
            case 'ProvisionedThroughputExceededException':
                $arr_error['error_code'] = 77;
                $arr_error['error_message'] = $e-&gt;getAwsErrorMessage();
                break;
            case 'InvalidImageFormatException':
                $arr_error['error_code'] = 78;
                $arr_error['error_message'] = $e-&gt;getAwsErrorMessage();
                break;
            case 'InvalidSignatureException': 
                $arr_error['error_code'] = 79;
                $arr_error['error_message'] = $e-&gt;getAwsErrorMessage();
                echo '1';
                print_r($arr_error);
                break;
            default:
                //throw new Exception($e-&gt;getAwsErrorMessage(),80);
                $arr_error['error_code'] = 80;
                $arr_error['error_message'] = $e-&gt;getAwsErrorMessage();
        }
        echo '2';
        print_r($arr_error);
        return [
          'error_code' =&gt; 34,
          'error_message' =&gt; 'Error'
        ];
    }
</code></pre>",,2,3,,2018-07-05 11:14:18.710 UTC,,2018-07-05 11:39:05.940 UTC,2018-07-05 11:16:09.560 UTC,,1533709,,1533709,1,0,php|exception|exception-handling|aws-sdk|amazon-rekognition,48
Failed to connect to camera service while using google vision qr code reader,52361997,Failed to connect to camera service while using google vision qr code reader,"<p>I am using google vision QR code scanner in my app, but I am getting the below error    </p>

<pre><code>09-17 11:02:22.419 5022-5022/com.gms E/AndroidRuntime: FATAL EXCEPTION: main
Process: com.gms, PID: 5022
java.lang.RuntimeException: Fail to connect to camera service
    at android.hardware.Camera.&lt;init&gt;(Camera.java:519)
    at android.hardware.Camera.open(Camera.java:364)
    at com.google.android.gms.vision.CameraSource.zza(Unknown Source)
    at com.google.android.gms.vision.CameraSource.start(Unknown Source)
    at com.easym.webrtc.GoogleQrCodeActivity$1.surfaceCreated(GoogleQrCodeActivity.java:98)
    at android.view.SurfaceView.updateWindow(SurfaceView.java:656)
    at android.view.SurfaceView$3.onPreDraw(SurfaceView.java:172)
    at android.view.ViewTreeObserver.dispatchOnPreDraw(ViewTreeObserver.java:1013)
    at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2542)
    at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1537)
    at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:7183)
    at android.view.Choreographer$CallbackRecord.run(Choreographer.java:959)
    at android.view.Choreographer.doCallbacks(Choreographer.java:734)
    at android.view.Choreographer.doFrame(Choreographer.java:670)
    at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:945)
    at android.os.Handler.handleCallback(Handler.java:751)
    at android.os.Handler.dispatchMessage(Handler.java:95)
    at android.os.Looper.loop(Looper.java:154)
    at android.app.ActivityThread.main(ActivityThread.java:6776)
    at java.lang.reflect.Method.invoke(Native Method)
    at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1496)
    at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1386)
</code></pre>

<p>Below is my QrcodeActivity that does all the qr code scanning related stuff</p>

<pre><code>public class GoogleQrCodeActivity extends AppCompatActivity {


private BarcodeDetector barcodeDetector;
private CameraSource cameraSource;

Button btnAction;
String intentData = """";
boolean isEmail = false;

@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.activity_google_qr_code);

    surfaceView = findViewById(R.id.surfaceView_qr_code_scanner);

}

private void initialiseDetectorsAndSources() {

    Toast.makeText(getApplicationContext(), ""Barcode scanner started"", Toast.LENGTH_SHORT).show();

    barcodeDetector = new BarcodeDetector.Builder(this)
            .setBarcodeFormats(Barcode.QR_CODE)
            .build();

    cameraSource = new CameraSource.Builder(this, barcodeDetector)
            .setRequestedPreviewSize(1920, 1080)
            .setAutoFocusEnabled(true) //you should add this feature
            .build();

    surfaceView.getHolder().addCallback(new SurfaceHolder.Callback() {
        @Override
        public void surfaceCreated(SurfaceHolder holder) {
            try {
                if (ActivityCompat.checkSelfPermission(GoogleQrCodeActivity.this, android.Manifest.permission.CAMERA) == PackageManager.PERMISSION_GRANTED) {
                        cameraSource.start(surfaceView.getHolder());
                }
                else {
                    ActivityCompat.requestPermissions(GoogleQrCodeActivity.this, new
                            String[]{Manifest.permission.CAMERA}, ConstantsApp.REQUEST_CAMERA_PERMISSION);
                }

            } catch (IOException e) {
                e.printStackTrace();
            }


        }

        @Override
        public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {
        }

        @Override
        public void surfaceDestroyed(SurfaceHolder holder) {
            cameraSource.stop();
        }
    });


    barcodeDetector.setProcessor(new Detector.Processor&lt;Barcode&gt;() {
        @Override
        public void release() {
  Toast.makeText(getApplicationContext(), ""To prevent memory leaks barcode scanner has been stopped"", Toast.LENGTH_SHORT).show();
        }

        @Override
        public void receiveDetections(Detector.Detections&lt;Barcode&gt; detections) {
            final SparseArray&lt;Barcode&gt; barcodes = detections.getDetectedItems();
            if (barcodes.size() != 0) {


                Intent mIntent =new Intent();
                mIntent.putExtra(""barcode"",barcodes.valueAt(0));
                setResult(RESULT_OK,mIntent);
                finish();

            }
        }
    });
}


@Override
protected void onPause() {
    super.onPause();
    cameraSource.release();
}

@Override
protected void onResume() {
    super.onResume();
    initialiseDetectorsAndSources();
}
</code></pre>

<p>}</p>

<p>I have set the permissions in manifest as below</p>

<pre><code>&lt;uses-permission android:name=""android.permission.CAMERA""  /&gt;
&lt;uses-feature
    android:name=""android.hardware.camera""
    android:required=""true"" /&gt;
&lt;uses-feature
    android:name=""android.hardware.camera.autofocus""
    android:required=""false"" /&gt; 
</code></pre>

<p>I am mainly getting this error when scanning in Samsung devices.
I am able to scan the QR code for the first time,getting the above error from second onwards</p>

<p>I couldn't figure out what is really wrong with my code</p>",,0,0,,2018-09-17 06:37:38.320 UTC,,2018-09-17 06:37:38.320 UTC,,,,,8850788,1,0,android|qr-code|google-vision|android-vision,112
Is it possible to get the count of objects using Google's Vision API or Amazon's Rekognition?,50815200,Is it possible to get the count of objects using Google's Vision API or Amazon's Rekognition?,"<p>I have been exploring to get the count of the objects in an image / video using AWS Rekognition &amp; Google's Vision, but haven't been able to find a way out. Though at <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">Google's Vision</a> site, they do have a section 'Insight from the Images' where apparently it seems like that the quantity has been captured. </p>

<p>Attached is a snapshot from that URL.<a href=""https://i.stack.imgur.com/fR3Ew.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fR3Ew.png"" alt=""Vision""></a> </p>

<p>Can someone please suggest if it is possible with Google's Vision or any other API which can help in getting the count of objects in an image. Thanks</p>

<p>Edit:</p>

<p>For example - For the image shown below, the count returned should be 10 cars.  As Torry Yang suggested in his answer, the label Annotations count can give the required number but it does not seem to be the case as the count for label annotations is 18. The returned object is somewhat like this.</p>

<p><code>""labelAnnotations"": [
    {
      ""mid"": ""/m/0k4j"",
      ""description"": ""car"",
      ""score"": 0.98658943,
      ""topicality"": 0.98658943
    },
    {
      ""mid"": ""/m/012f08"",
      ""description"": ""motor vehicle"",
      ""score"": 0.9631113,
      ""topicality"": 0.9631113
    },
    {
      ""mid"": ""/m/07yv9"",
      ""description"": ""vehicle"",
      ""score"": 0.9223521,
      ""topicality"": 0.9223521
    },
    {
      ""mid"": ""/m/01w71f"",
      ""description"": ""personal luxury car"",
      ""score"": 0.8976857,
      ""topicality"": 0.8976857
    },
    {
      ""mid"": ""/m/068mqj"",
      ""description"": ""automotive design"",
      ""score"": 0.8736646,
      ""topicality"": 0.8736646
    },
    {
      ""mid"": ""/m/012mq4"",
      ""description"": ""sports car"",
      ""score"": 0.8418799,
      ""topicality"": 0.8418799
    },
    {
      ""mid"": ""/m/01lcwm"",
      ""description"": ""luxury vehicle"",
      ""score"": 0.7761523,
      ""topicality"": 0.7761523
    },
    {
      ""mid"": ""/m/06j11d"",
      ""description"": ""performance car"",
      ""score"": 0.76816446,
      ""topicality"": 0.76816446
    },
    {
      ""mid"": ""/m/03vnt4"",
      ""description"": ""mid size car"",
      ""score"": 0.75732976,
      ""topicality"": 0.75732976
    },
    {
      ""mid"": ""/m/03vntj"",
      ""description"": ""full size car"",
      ""score"": 0.6855145,
      ""topicality"": 0.6855145
    },
    {
      ""mid"": ""/m/0h8ls87"",
      ""description"": ""automotive exterior"",
      ""score"": 0.66056395,
      ""topicality"": 0.66056395
    },
    {
      ""mid"": ""/m/014f__"",
      ""description"": ""supercar"",
      ""score"": 0.592226,
      ""topicality"": 0.592226
    },
    {
      ""mid"": ""/m/02swz_"",
      ""description"": ""compact car"",
      ""score"": 0.5807265,
      ""topicality"": 0.5807265
    },
    {
      ""mid"": ""/m/0h6dlrc"",
      ""description"": ""bmw"",
      ""score"": 0.5801241,
      ""topicality"": 0.5801241
    },
    {
      ""mid"": ""/m/01h80k"",
      ""description"": ""muscle car"",
      ""score"": 0.55745816,
      ""topicality"": 0.55745816
    },
    {
      ""mid"": ""/m/021mp2"",
      ""description"": ""sedan"",
      ""score"": 0.5522745,
      ""topicality"": 0.5522745
    },
    {
      ""mid"": ""/m/0369ss"",
      ""description"": ""city car"",
      ""score"": 0.52938646,
      ""topicality"": 0.52938646
    },
    {
      ""mid"": ""/m/01d1dj"",
      ""description"": ""coupé"",
      ""score"": 0.50642073,
      ""topicality"": 0.50642073
    }
  ]</code> </p>

<p><a href=""https://i.stack.imgur.com/eN8lS.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eN8lS.jpg"" alt=""image""></a></p>",,2,0,,2018-06-12 10:40:51.170 UTC,,2018-06-26 12:40:21.023 UTC,2018-06-17 05:59:28.443 UTC,,1758386,,1758386,1,3,face-recognition|google-vision|vision|amazon-rekognition,785
Google Cloud Vision / PHP - Make single request with Label and Safe Search detection,54683853,Google Cloud Vision / PHP - Make single request with Label and Safe Search detection,"<p>On google cloud vision you get charged per request. If you do a ""Label Detection"" you get a free ""Safe Search"" but it has to be rolled into the same request. I have working code for both the Label Detection and the Safe Search detection but I am not sure how to combine the two into one request. </p>

<p>Someone had answered this question in Python but not sure how to translate it in PHP.</p>

<p><a href=""https://stackoverflow.com/questions/53195930/how-to-call-for-label-detection-and-safe-search-detection-at-a-time-on-googl"">How to call for &quot;Label Detection&quot; and &quot;Safe Search Detection&quot; at a time on Google Cloud Vision API</a></p>

<p>Does anyone know how I could call them in PHP? Any insight would be appreciated. Thanks.</p>

<pre><code>
# imports the Google Cloud client library
use Google\Cloud\Vision\V1\ImageAnnotatorClient;

# instantiates a client
$imageAnnotator = new ImageAnnotatorClient();

# the name of the image file to annotate
$fileName = 'images/d4aed5533322946.jpg';

# prepare the image to be annotated
$image = file_get_contents($fileName);

# performs label detection on the image file
$response = $imageAnnotator-&gt;labelDetection($image);
$labels = $response-&gt;getLabelAnnotations();

if ($labels) {
    echo(""Labels:"" . PHP_EOL);
    foreach ($labels as $label) {
        echo($label-&gt;getDescription() . PHP_EOL);
    }
} 
</code></pre>

######### Safe Search would look as follows

<pre><code>function detect_safe_search($path)
{
    $imageAnnotator = new ImageAnnotatorClient();

    # annotate the image
    $image = file_get_contents($path);
    $response = $imageAnnotator-&gt;safeSearchDetection($image);
    $safe = $response-&gt;getSafeSearchAnnotation();

    $adult = $safe-&gt;getAdult();
    $medical = $safe-&gt;getMedical();
    $spoof = $safe-&gt;getSpoof();
    $violence = $safe-&gt;getViolence();
    $racy = $safe-&gt;getRacy();

    # names of likelihood from google.cloud.vision.enums
    $likelihoodName = ['UNKNOWN', 'VERY_UNLIKELY', 'UNLIKELY',
    'POSSIBLE','LIKELY', 'VERY_LIKELY'];

echo ""Adult $adult\n"";
    printf(""Adult: %s"" . PHP_EOL, $likelihoodName[$adult]);
    printf(""Medical: %s"" . PHP_EOL, $likelihoodName[$medical]);
    printf(""Spoof: %s"" . PHP_EOL, $likelihoodName[$spoof]);
    printf(""Violence: %s"" . PHP_EOL, $likelihoodName[$violence]);
    printf(""Racy: %s"" . PHP_EOL, $likelihoodName[$racy]);

    $imageAnnotator-&gt;close();
}

$path = 'images/d4aed5533322946.jpg';
detect_safe_search($path);

echo ""\n"";
$path = 'images/5.jpg.6f23b929dcc008f3bc394b0b6b0c6e5e.jpg';
detect_safe_search($path);
</code></pre>",54693089,1,0,,2019-02-14 05:38:25.323 UTC,,2019-02-14 14:47:05.420 UTC,,,,,11060185,1,0,php|google-api|google-cloud-platform|google-cloud-vision,83
How to Get chinese and japanese text from camera?,52984076,How to Get chinese and japanese text from camera?,"<p>I am working on augmented reality where i have to 
Recognise chinese &amp; japanese text
I used google vision and tessarct but still i am not getting text  when camera on text . </p>",,0,0,,2018-10-25 07:51:48.980 UTC,,2018-10-25 10:42:30.700 UTC,2018-10-25 10:42:30.700 UTC,,8881042,,8881042,1,0,android,18
Google Vision api to extract text from a Bitmap,40921512,Google Vision api to extract text from a Bitmap,"<p>I'm trying to develop an application that extracts text from a screenshot and with these data (numbers and texts) I do something. It works but not as I expected, it isn't accurate at all. The strange thing is that the same screenshot at the same resolution is recognized in a different way by my application and the ""try api"" on <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">google vision api site</a> </p>

<blockquote>
  <p>for example: a screenshot with ""410"" as text was recognized as ""A10"" by my app. Otherwise google api site properly recognizes 410</p>
</blockquote>

<p>I noticed that google Keep OCR work better than my app, it use the same api? what can i do for improve the text recognition in my app as google Keep or google vison api site? </p>

<p>here is my code:</p>

<blockquote>
  <p>Ocr class</p>
</blockquote>

<pre><code>public class Ocr extends AppCompatActivity {

private static int RESULT_LOAD_IMAGE = 1;

String TAG = ""MAIN ACTIVITY"";


@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.activity_main);

    Button buttonLoadImage = (Button) findViewById(R.id.upload_btn);

    buttonLoadImage.setOnClickListener(new View.OnClickListener() {

        /**
         * on click method of the upload button
         * @param arg0
         */
        @Override
        public void onClick(View arg0) {

            Intent i = new Intent(
                    Intent.ACTION_PICK,
                    android.provider.MediaStore.Images.Media.INTERNAL_CONTENT_URI);

            startActivityForResult(i, RESULT_LOAD_IMAGE);
        }
    });

}


/**
 * In this method, we check if the activity that was triggered was indeed Image Gallery
 * (It is common to trigger different intents from the same activity and expects result from each).
 * For this we used RESULT_LOAD_IMAGE integer that we passed previously to startActivityForResult() method
 * @param requestCode
 * @param resultCode
 * @param data
 */

@Override
public void onActivityResult(int requestCode, int resultCode, Intent data) {

    if (requestCode == RESULT_LOAD_IMAGE &amp;&amp; resultCode == RESULT_OK &amp;&amp; data != null &amp;&amp; data.getData() != null) {

        Uri uri = data.getData();

        try {
            Bitmap bitmap = MediaStore.Images.Media.getBitmap(getContentResolver(), uri);

            ImageView imageView = (ImageView) findViewById(R.id.imgView);

            imageView.setImageBitmap(bitmap);

            // imageBitmap is the Bitmap image you're trying to process for text
            if (bitmap != null) {

                TextRecognizer textRecognizer = new TextRecognizer.Builder(this).build();

                if (!textRecognizer.isOperational()) {
                    // Note: The first time that an app using a Vision API is installed on a
                    // device, GMS will download a native libraries to the device in order to do detection.
                    // Usually this completes before the app is run for the first time.  But if that
                    // download has not yet completed, then the above call will not detect any text,
                    // barcodes, or faces.
                    // isOperational() can be used to check if the required native libraries are currently
                    // available.  The detectors will automatically become operational once the library
                    // downloads complete on device.
                    Log.w(TAG, ""Detector dependencies are not yet available."");

                    // Check for low storage.  If there is low storage, the native library will not be
                    // downloaded, so detection will not become operational.
                    IntentFilter lowstorageFilter = new IntentFilter(Intent.ACTION_DEVICE_STORAGE_LOW);
                    boolean hasLowStorage = registerReceiver(null, lowstorageFilter) != null;

                    if (hasLowStorage) {
                        Toast.makeText(this, ""Low Storage"", Toast.LENGTH_LONG).show();
                        Log.w(TAG, ""Low Storage"");
                    }
                }


                Frame imageFrame = new Frame.Builder()
                        .setBitmap(bitmap)
                        .build();

                SparseArray&lt;TextBlock&gt; textBlocks = textRecognizer.detect(imageFrame);


                for (int i = 0; i &lt; textBlocks.size(); i++) {

                    TextBlock textBlock = textBlocks.get(textBlocks.keyAt(i));

                    String text = textBlock.getValue();

                    Toast.makeText(this, text , Toast.LENGTH_SHORT).show();

                }
            }
        } catch (FileNotFoundException e) {
            e.printStackTrace();
        } catch (IOException e) {
            e.printStackTrace();
        }

    }
 }
}
</code></pre>

<blockquote>
  <p>Android Manifest </p>
</blockquote>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
   &lt;manifest xmlns:android=""http://schemas.android.com/apk/res/android""
package=""com.prova.ocr""&gt;
&lt;uses-permission android:name=""android.permission.WRITE_EXTERNAL_STORAGE""/&gt;
&lt;uses-permission android:name=""android.permission.READ_EXTERNAL_STORAGE""/&gt;
&lt;application
    android:allowBackup=""true""
    android:icon=""@mipmap/ic_launcher""
    android:label=""@string/app_name""
    android:supportsRtl=""true""
    android:theme=""@style/AppTheme""&gt;
    &lt;meta-data
        android:name=""com.google.android.gms.version""
        android:value=""@integer/google_play_services_version"" /&gt;
    &lt;meta-data
        android:name=""com.google.android.gms.vision.DEPENDENCIES""
        android:value=""ocr"" /&gt;

    &lt;activity android:name="".MainActivity""&gt;
        &lt;intent-filter&gt;
            &lt;action android:name=""android.intent.action.MAIN"" /&gt;

            &lt;category android:name=""android.intent.category.LAUNCHER"" /&gt;
        &lt;/intent-filter&gt;
    &lt;/activity&gt;
&lt;/application&gt;
</code></pre>

<p></p>

<blockquote>
  <p>Gradle</p>
</blockquote>

<pre><code>apply plugin: 'com.android.application'
android {
compileSdkVersion 25
buildToolsVersion ""25.0.1""
defaultConfig {
    applicationId ""com.prova.ocr""
    minSdkVersion 19
    targetSdkVersion 25
    versionCode 1
    versionName ""1.0""
    testInstrumentationRunner ""android.support.test.runner.AndroidJUnitRunner""
}
buildTypes {
    release {
        minifyEnabled false
        proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
    }
}
}

dependencies {
compile fileTree(dir: 'libs', include: ['*.jar'])
androidTestCompile('com.android.support.test.espresso:espresso-core:2.2.2', {
    exclude group: 'com.android.support', module: 'support-annotations'
    })
compile 'com.android.support:appcompat-v7:25.0.1'
testCompile 'junit:junit:4.12'
compile 'com.android.support:support-v4:25.0.1'
compile 'com.android.support:design:25.0.1'
compile 'com.google.android.gms:play-services-vision:10.0.0+'
}
</code></pre>",,0,3,,2016-12-01 22:24:21.210 UTC,1,2016-12-01 22:24:21.210 UTC,,,,,6486954,1,3,android|ocr|google-vision|text-recognition|google-keep,2416
Bounding box in AWS Rekognition,54351470,Bounding box in AWS Rekognition,"<p>I'm trying to get bounding box from an image in Rekognition, i get the label but i get:</p>

<blockquote>
  <p>Keyerror'instances' in response['instances']</p>
</blockquote>

<pre><code>def detect_labels(bucket, key, max_labels=10, min_confidence=90, region=""eu-west-1""):
    rekognition = session.client(""rekognition"", region)
    response = rekognition.detect_labels(
        Image={
            ""S3Object"": {
                ""Bucket"": bucket,
                ""Name"": key,
            }
        }, MaxLabels=10
    )
    return response

if __name__ == ""__main__"":

response= detect_labels(BUCKET, KEY)

    print('Detected labels for ' + photo) 
    print()   
    for label in response['Labels']:

        for instance in label['Instances']:
            print (""  Bounding box"")
            print (""    Top: "" + str(instance['BoundingBox']['Top']))

        print (""----------"")
        print ()
</code></pre>",54351715,1,0,,2019-01-24 16:41:20.900 UTC,,2019-01-24 20:41:29.157 UTC,2019-01-24 18:05:12.713 UTC,,174777,,7375337,1,0,amazon-web-services|deep-learning|amazon-rekognition,107
How to get specific data from nested JSON generated by Google Cloud Vision API,48019749,How to get specific data from nested JSON generated by Google Cloud Vision API,"<p>I am developing a system and trying to get a specific data from JSON that is generated from Google Cloud Vision API and would like to show the keyword on the html. You can see the nested JSON (data) as followed. On the decription, I'd like to show ""dog"" in my html.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>{
  ""responses"": [
    {
      ""labelAnnotations"": [
        {
          ""mid"": ""/m/0kpmf"",
          ""description"": ""dog breed"",
          ""score"": 0.9408917
        },
        {
          ""mid"": ""/m/0bt9lr"",
          ""description"": ""dog"",
          ""score"": 0.91643894
        },
        {
          ""mid"": ""/m/01z5f"",
          ""description"": ""dog like mammal"",
          ""score"": 0.86234033
        },
        {
          ""mid"": ""/m/02kysw"",
          ""description"": ""spaniel"",
          ""score"": 0.84150785
        },
        {
          ""mid"": ""/m/02xl47d"",
          ""description"": ""dog breed group"",
          ""score"": 0.83750784
        },
        {
          ""mid"": ""/m/05mqq3"",
          ""description"": ""snout"",
          ""score"": 0.7590523
        },
        {
          ""mid"": ""/m/01p2lr"",
          ""description"": ""american cocker spaniel"",
          ""score"": 0.7300941
        },
        {
          ""mid"": ""/m/02wcn0z"",
          ""description"": ""russian spaniel"",
          ""score"": 0.6928253
        },
        {
          ""mid"": ""/m/03ht9m"",
          ""description"": ""field spaniel"",
          ""score"": 0.6872984
        },
        {
          ""mid"": ""/m/02wbgd"",
          ""description"": ""english cocker spaniel"",
          ""score"": 0.65664136
        }
      ]
    }
  ]
}</code></pre>
</div>
</div>
</p>

<p>I write following javascript code but still ""undefined"" answer for that</p>

<blockquote>
  <p>document.getElementById(""demo"").innerHTML = data.responses;</p>
</blockquote>

<p>fyi, </p>

<blockquote>
  <p>data</p>
</blockquote>

<p>is the overall result from JSON (Google vision API).
Really appreciate your big help!!</p>",,1,5,,2017-12-29 08:36:28.813 UTC,,2017-12-29 12:52:16.553 UTC,,,,,9152069,1,1,javascript|json,125
How can I convert an image containing tabulated data to an Excel sheet?,52857016,How can I convert an image containing tabulated data to an Excel sheet?,"<p>I have a bunch of images similar to this:</p>

<p><img src=""https://i.stack.imgur.com/6Z9pX.jpg"" alt=""Image""> </p>

<p>And I need to extract the data and store it in an Excel sheet. 
I tried using Google Vision and it is able to detect all the text, however since the image has curved horizontal lines, Google Vision gives incorrect line ordering, that is, the data of one row gets mixed up with data of other rows. How can I handle this situation and generate an excel sheet with best possible accuracy?</p>",,0,0,,2018-10-17 14:13:43.620 UTC,,2018-10-17 19:03:38.050 UTC,2018-10-17 19:03:38.050 UTC,,10223668,,10518932,1,0,computer-vision|ocr|extract|text-extraction|data-extraction,17
Google Vision OCR data form,52326489,Google Vision OCR data form,"<p>I'm exploring the Google Vision API for OCR. We have lots of forms that are computer generated and filled by users. Like the Medical Reports and Registration Forms. 
We need to process those images and get the character out of it. I've tried Google Vision API and its works great in case of computer generated form, but the ones filled by hand are creating issues. Like If fill the form with the data a little above the y axis the words is considered as previous/next line. Like below is the output</p>

<pre><code>Study Contact Name:
Test
</code></pre>

<p>expected</p>

<pre><code>Study Contact Name: Test
</code></pre>

<p><a href=""https://i.stack.imgur.com/C2vnr.jpg"" rel=""nofollow noreferrer"">The Form used</a></p>

<p>Code reference: <a href=""https://cloud.google.com/vision/docs/detecting-text#vision-text-detection-java"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/detecting-text#vision-text-detection-java</a></p>

<p>Is there a way to get this in one line, or understand if its part of that line?</p>

<p>Any other API that can help in this scenario?</p>",,2,0,,2018-09-14 06:58:38.237 UTC,,2018-09-14 08:16:35.260 UTC,,,,,10362145,1,1,ocr|google-vision,243
Python - response.read() runs smoothly in Windows but not at all in Raspbian,48672879,Python - response.read() runs smoothly in Windows but not at all in Raspbian,"<p>I'm trying to run the program which uses Microsoft Emotion API program here in my raspberry pi3. 
I tried it on Windows on version Python3.6.4 and it runs perfectly alright. 
However, when I try it on Raspberrypi3, using the default python3.5 (which does not have http.client) AND also python3.6.2 IDLE (<a href=""https://stackoverflow.com/questions/42003141/how-to-start-idle-that-comes-with-python-3-6"">How to start IDLE that comes with Python 3.6</a>), the program either (1) does NOT run at all, (2) or gives a time-out error, (3) or a </p>

<pre><code>b'{""error"":{""code"":""InvalidImageSize"",""message"":""Image size is too small or too big.""}}' error.
</code></pre>

<p>I have also tried to run it on python3.6 under env environment but to no avail. It did not even print happiness which is supposed to return a float value. Please advice why and how can I make it work on Raspberry Pi, thank you very much! Have it got to do with the python version? The API? </p>

<p>The codes are as follows:</p>

<pre><code>import http.client, urllib.request, urllib.parse, urllib.error, base64, sys
import json


headers = {
    'Content-Type': 'application/octet-stream',
    'Ocp-Apim-Subscription-Key': 'MY-KEY',
}

params = urllib.parse.urlencode({
        })

body = open('1.jpg','rb').read()

print ('begin')
conn = http.client.HTTPSConnection('westus.api.cognitive.microsoft.com')
print ('connection')
conn.request(""POST"", ""/emotion/v1.0/recognize?%s"" % params, body, headers)
print ('request')
response = conn.getresponse()
print ('donerequest')
data = response.read()
print(data)

faces = json.loads(data)
happiness = faces[0]['scores']['happiness'] if len(faces)&gt;0 else 0
print (happiness)
with open('result.txt', 'a') as out:
    out.write(""1""+ '\n')
conn.close()
</code></pre>

<p>Problem: 
stops at <code>print(data)</code>: <code>b'{""error"":{""code"": ""timeout"", ""message"": ""the operation was timeout."" }}'
Traceback (most recent call last): File.... in line 31 in &lt;module&gt; 
happiness = faces[0]['scores']['happiness'] if len(faces)&gt;0 else 0 keyerror: 0.</code> </p>

<p>It seems that the problem happens at 
data = response.read() as print(data) returns with ""operation is timeout"" error"". May I know how should I edit it in raspbian as it works well in windows for that?
I did try the following to no avail:</p>

<pre><code>data = response.read().decode(""utf8"")
</code></pre>",,0,5,,2018-02-07 20:41:13.037 UTC,2,2018-02-08 02:22:06.890 UTC,2018-02-08 02:22:06.890 UTC,,9212686,,9212686,1,0,python|windows|raspbian|microsoft-cognitive,69
How to calculate font size of text in image using Google Vision,56014699,How to calculate font size of text in image using Google Vision,"<p>I am using google vision api for detect text in image.
Now, I want to know font size of text in image. How do it calculate ?
or Do google vision api support?</p>",,0,0,,2019-05-07 02:23:34.143 UTC,,2019-05-07 02:23:34.143 UTC,,,,,11462245,1,-3,ruby-on-rails|ruby,19
Add languageHints to Google Cloud Vision,54224197,Add languageHints to Google Cloud Vision,"<p>how can I add languageHints to my google cloud vision python code.
From <a href=""https://cloud.google.com/vision/docs/languages"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/languages</a> I know that it is supported but I do not know how to implement it into the code.</p>

<pre><code>from google.cloud import vision
client = vision.ImageAnnotatorClient()

with io.open(path, 'rb') as image_file:
    content = image_file.read()

image = vision.types.Image(content=content)

response = client.text_detection(image=image)
texts = response.text_annotations
print('Texts:')

for text in texts:
    print('\n""{}""'.format(text.description))

    vertices = (['({},{})'.format(vertex.x, vertex.y)
                for vertex in text.bounding_poly.vertices])

    print('bounds: {}'.format(','.join(vertices)))
</code></pre>

<p>I think I have to do it like this:</p>

<pre><code>context = imageContext.setLanguageHints(""ko"")
response = client.text_detection(image=image, context=context)
</code></pre>",,0,0,,2019-01-16 19:41:08.303 UTC,,2019-01-16 19:41:08.303 UTC,,,,,7123714,1,0,python|google-vision,24
Google Vision API JSON Response in English only,51611001,Google Vision API JSON Response in English only,"<p>Its been so much of time exploring the Google vision API, I am trying to get the Vision API Response in English Language only , below is my request object to API which has language hints :</p>

<pre><code> {
    ""requests"": [
        {
            ""features"": [
                {
                    ""type"": ""IMAGE_PROPERTIES""
                },
                {
                    ""type"": ""LANDMARK_DETECTION""
                },
                {
                    ""type"": ""LABEL_DETECTION""
                },
                {
                    ""type"": ""WEB_DETECTION""
                },
                {
                    ""type"": ""FACE_DETECTION""
                },
                {
                    ""type"": ""SAFE_SEARCH_DETECTION""
                },
                {
                    ""type"": ""TEXT_DETECTION""
                },
                {
                    ""type"": ""LOGO_DETECTION""
                }
            ],
            ""image"": {
                ""source"": {
                    ""imageUri"": ""https://images.dreamstream.com/prodds/prddsimg/OM_pasteIt22_12_2017_2_34_7806303.jpeg""
                }
            },
      ""imageContext"": {
        ""languageHints"": [
          ""en""
        ]
      }
        }
    ]
}
</code></pre>

<p>Even this request object not getting correct response(multiple languages) from Vision API  ..</p>

<p>if there is any steps is there to get response in English only please let me know, as of now response contains multiple languages like below : </p>

<pre><code>{
            ""url"": ""https://www.tummyummi.com/food/menu-aryaas-restaurant"",
            ""pageTitle"": ""Aryaas India Restaurant - مطعم ارياس لبهند - TummYummi Restaurants"",
            ""fullMatchingImages"": [
              {
                ""url"": ""https://www.tummyummi.com/food/upload/1509868727-Curd-Vada.jpg""
              }
            ]
          },
</code></pre>",,2,0,,2018-07-31 10:16:00.830 UTC,,2018-08-24 10:32:13.947 UTC,2018-08-03 05:12:11.650 UTC,,7854597,,7854597,1,0,java|json|response|google-vision|non-english,217
How secure the Face-based user verification in AWS Rekognition?,52968434,How secure the Face-based user verification in AWS Rekognition?,"<p>I'm planning to use <code>AWS Rekognition</code> service for <strong>forgot password</strong> in my iOS and android apps. Flow will be like, whenever user initiate forgot password, I will be checking whether actual user is initiating the forgot password for particular mobile number. For this I will be asking user to take one live pic of himself/herself and check this against reference image. But I'm facing one scenario in this,</p>

<blockquote>
  <p><strong>E.g:</strong> If user A got the User B's mobile and initiate forgot password from B's device, if we have only OTP authentication, A can
  easily change B's password using forgot password, since A have access to B's messages.</p>
</blockquote>

<p>So I want to add extra layer of security before initiating forgot password flow by ensuring using live picture of person who is initiating forgot password action. </p>

<blockquote>
  <p>In this case what if User A have User B's image in A's mobile and take
  photo of it from user B's mobile and initiate forgot password?</p>
</blockquote>

<p>How to restrict this kind of scenario? and I just want to know whether this is suggested way to proceed or not. </p>

<p>Please advice.  </p>",,1,0,,2018-10-24 11:57:46.913 UTC,,2018-10-24 22:12:15.897 UTC,,,,,6355922,1,0,amazon-web-services|authentication|amazon-rekognition|facial-identification,123
How can I use Google VISION api to extract specific fields?,52323135,How can I use Google VISION api to extract specific fields?,"<p>I have a template like this</p>

<p><img src=""https://www.domesticalegal.com.br/wp-content/uploads/2016/06/7.png"" alt=""image""></p>

<p>and wanted to use google vision api to extract certain fields. Example, field CPF would be 76497127887</p>

<p>I've got a working code that looks like this</p>

<pre><code>from base64 import b64encode
import json
import requests

def request_ocr(api_key, imgname):
    #format image for google vision
    ENDPOINT_URL='https://vision.googleapis.com/v1/images:annotate'
    with open(imgname, 'rb') as f:
        ctxt = b64encode(f.read()).decode()
        img_requests={
                'image': {'content': ctxt},
                'features': [{
                    'type': 'DOCUMENT_TEXT_DETECTION',
                    'maxResults': 1
                }]
        }

    #create response
    response = requests.post(ENDPOINT_URL,
                         data=json.dumps({""requests"": img_requests }).encode(),
                         params={'key': api_key},
                         headers={'Content-Type': 'application/json'})
    return response
</code></pre>

<p>which gets the response but I need to find a way to search through it to get the value for the desired field. Can someone suggest a path?</p>

<p>Thks </p>",,0,1,,2018-09-13 23:41:30.850 UTC,,2018-09-13 23:49:10.117 UTC,2018-09-13 23:49:10.117 UTC,,9994744,,9994744,1,0,python-3.x|ocr|google-vision,112
How to identify and tightly crop faces out of images in nodejs (not a rectangle),55929639,How to identify and tightly crop faces out of images in nodejs (not a rectangle),"<p>I've written node.js application that identifies and pulls a face out of an image using AWS Rekognition which gives me a rectangular bounding box:</p>

<p><a href=""https://s3.amazonaws.com/botworx/face_identification/face.png"" rel=""nofollow noreferrer"">Example</a></p>

<p>I want to tightly crop out the face itself so that I can merge it with a snazzy background like the might do in a theme park.</p>

<p>Can anyone suggest a node package or web-service?
thanks!</p>",,0,0,,2019-04-30 22:55:38.350 UTC,1,2019-05-01 21:53:03.367 UTC,2019-05-01 21:53:03.367 UTC,,899719,,11434856,1,1,javascript|node.js|image|computer-vision|face-recognition,24
How to install google.cloud with Python pip?,41836486,How to install google.cloud with Python pip?,"<p>I am relatively new to Python and I am stuck on something which is probably relatively easy to resolve.</p>

<p>I have installed the following packages:</p>

<pre><code>pip install --upgrade google-api-python-client
pip install --upgrade google-cloud
pip install --upgrade google-cloud-vision
</code></pre>

<p>In my Python file I have:</p>

<pre><code>import cv2
import io
import os

# Imports the Google Cloud client library
from google.cloud import vision

...etc...
</code></pre>

<p>And this gives me the error:</p>

<pre><code>Traceback (most recent call last):
  File ""test.py"", line 6, in &lt;module&gt;
    from google.cloud import vision
ImportError: No module named 'google.cloud'
</code></pre>

<p>What am I missing and where should I look (logs?) to find the answer in the future.</p>

<p>PS:<br>
Pip installs of <code>google-cloud</code> and <code>google-cloud-vision</code> have the output:</p>

<pre><code>Cannot remove entries from nonexistent file /Users/foobar/anaconda/lib/python3.5/site-packages/easy-install.pth
</code></pre>

<p><strong>UPDATE</strong>:<br>
Running <code>pip freeze</code> doesn't show the packages to be installed...</p>",41926648,3,3,,2017-01-24 18:38:37.777 UTC,3,2018-08-30 12:42:27.237 UTC,2017-01-26 17:38:15.923 UTC,,5231007,,1501285,1,8,python|pip|google-cloud-vision,10998
Using Tesseract / Any OCR in PHP / JAVASCRIPT,39029331,Using Tesseract / Any OCR in PHP / JAVASCRIPT,"<p>i wanted to implement a fully working OCR into my webpage , the webpage uses several languages and two of them are js and php i was trying to find ocr which will work <strong>in atleast one</strong> of these languages. (the webpage is being hosted at google appengine) </p>

<p><strong>what i found</strong> : </p>

<p><strong>1:</strong> google cloud vision API , i also found their project on github <a href=""https://github.com/GoogleCloudPlatform/cloud-vision/tree/master/js"" rel=""nofollow"">github link for js implementation of cloud vision api</a> the code which is there doesnot work also , it doesnot output the text and it also doesnot output any errors , i have no idea why it doesnot communicate i did everything i should (there are steps that you should follow)</p>

<p><strong>2:</strong> i also found tesseract ocr which is wrapper for tesseract ocr engine i found website but the 5 line sample after copying and linkind the js source file doesnot do anything there is no documentation at all. <a href=""http://tesseract.projectnaptha.com/"" rel=""nofollow"">tesseract ocr wrapper for javascript link to webpage</a></p>

<p><strong>3:</strong> i also found tesseract ocr wrapper in php from user thiagoalessio on github (i cant post more than 2 link) but since im running google appengine hosting , i have no idea how should i implement it into the appengine project , maybe this one will work ? can someone help me ?</p>

<p><strong><em>thanks</em></strong></p>",,1,0,,2016-08-19 00:14:24.613 UTC,,2016-08-23 19:03:17.273 UTC,,,,,6706753,1,0,javascript|php|google-app-engine,400
ComputerVision API doesn't allow BlobURL with SAS Token,54769503,ComputerVision API doesn't allow BlobURL with SAS Token,"<p>I'm trying to analyse my images using <a href=""https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/"" rel=""nofollow noreferrer"">Azure Computer Vision API</a> <strong>(Azure Cognitive Service)</strong></p>

<p>But the issue is my Image is stored in Blob container with <strong>Private access</strong> which means without a SAS token it will not able to access. So when I tried to call the Computer Vision API with my <strong>image URL + SAS</strong> .It's giving <strong>bad request</strong></p>

<p>You can easily repro this issue in <a href=""https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/"" rel=""nofollow noreferrer"">this site</a> too</p>

<p><a href=""https://i.stack.imgur.com/UuPzb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UuPzb.png"" alt=""enter image description here""></a></p>",54775459,2,0,,2019-02-19 15:16:03.403 UTC,,2019-02-19 21:47:50.857 UTC,,,,,7073340,1,0,azure|computer-vision|azure-storage-blobs|azure-cognitive-services|sas-token,32
Specify a target for Google vision api to search for similar images,51095417,Specify a target for Google vision api to search for similar images,"<p>After spending some time learning the Google vision api i could successfully send requests and get results ( landmarks,labels,similar images url,etc).</p>

<p>However, now i am trying to make a local search and find similar images in my database. Some suggested that i should use SIFT algorithm but it seems complicated for a beginner like me.</p>

<p>Is it possible to set a target for Google api to search for similar images(instead of searching the whole web)?that way i can upload my database images somewhere and get query results using google api </p>

<p>Thanks for taking the time to help.</p>",,0,0,,2018-06-29 06:19:14.193 UTC,,2018-06-29 06:19:14.193 UTC,,,,,7508195,1,0,google-api|google-vision|google-image-search,56
Is there a full list of potential labels that Google's Vision API will return?,38363182,Is there a full list of potential labels that Google's Vision API will return?,"<p>I've been testing out Google's Vision API to attach labels to different images.</p>

<p>For a given image, I'll get back something like this:</p>

<pre><code>""google_labels"": {
            ""responses"": [{
                ""labelAnnotations"": [{
                    ""score"": 0.8966763,
                    ""description"": ""food"",
                    ""mid"": ""/m/02wbm""
                }, {
                    ""score"": 0.80512983,
                    ""description"": ""produce"",
                    ""mid"": ""/m/036qh8""
                }, {
                    ""score"": 0.73635191,
                    ""description"": ""juice"",
                    ""mid"": ""/m/01z1kdw""
                }, {
                    ""score"": 0.69849229,
                    ""description"": ""meal"",
                    ""mid"": ""/m/0krfg""
                }, {
                    ""score"": 0.53875387,
                    ""description"": ""fruit"",
                    ""mid"": ""/m/02xwb""
                }]
            }]
        }
</code></pre>

<p>--> My questions are:</p>

<ol>
<li>Does anybody know if Google has published their full list of labels (<code>['produce', 'meal', ...]</code>) and where I could find that?</li>
<li>Are those labels structured in any way? - e.g. is it known that 'food' is a superset of 'produce', for example.</li>
</ol>

<p>I'm guessing 'No' and 'No' as I haven't been able to find anything, but, maybe not. Thanks!</p>",,1,4,,2016-07-13 23:12:45.290 UTC,5,2017-07-22 09:31:59.937 UTC,2016-07-14 16:07:13.880 UTC,,2521469,,2521469,1,14,python|google-app-engine|google-cloud-vision,2561
Using Android-Vision to detect Hebrew,45772514,Using Android-Vision to detect Hebrew,"<p>I'm trying to create a simple application such as in the tutorial for the android vision API, only to recognize Hebrew, instead of English.</p>

<p>so far, from what i searched <a href=""https://developers.google.com/android/reference/com/google/android/gms/vision/text/package-summary"" rel=""nofollow noreferrer"" title=""vision API"">mobile vision api</a>, and from playing around with other languages, the application recognize many Latin based languages (French, Spanish, for example) but no non-Latin character languages i tried (Chinese, Hebrew, Arabic comes to mind).</p>

<p>the question is, is it possible to use Google's mobile vision to read non-Latin text? if yes, how would i change my simple app to read other type of characters? and if not, what are my alternatives? i have found tessaract and Google cloud vision, but i prefer to have as little interaction with outside sources as i can, i want the app to be installed and used freely without relaying on outside engines or servers, any help and pointing in the right direction is appreciated.</p>",,1,0,,2017-08-19 14:06:27.267 UTC,,2017-09-09 12:33:30.670 UTC,,,,,1716131,1,1,android|mobile|ocr|android-vision,482
ImportError: cannot import name _args_from_interpreter_flags,47466195,ImportError: cannot import name _args_from_interpreter_flags,"<p>For an application, I have to use Google Vision API.
I am able to use <code>from google.cloud import vision</code> and do image analysis in my computer.
But, when I deploy my app on developmental server I am getting error:</p>

<pre><code>File ""C:\MyApp\detect.py"", line 26, in &lt;module&gt;
    from google.cloud import vision
  File ""C:\Anaconda2\lib\site-packages\google\cloud\vision\__init__.py"", line 34, in &lt;module&gt;
    __version__ = get_distribution('google-cloud-vision').version
  File ""C:\Program Files (x86)\Google\Cloud SDK\google-cloud-sdk\platform\google_appengine\lib\setuptools-0.6c11\pkg_resources.py"", line 311, in get_distribution
    if isinstance(dist,Requirement): dist = get_provider(dist)
  File ""C:\Program Files (x86)\Google\Cloud SDK\google-cloud-sdk\platform\google_appengine\lib\setuptools-0.6c11\pkg_resources.py"", line 197, in get_provider
    return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]
  File ""C:\Program Files (x86)\Google\Cloud SDK\google-cloud-sdk\platform\google_appengine\lib\setuptools-0.6c11\pkg_resources.py"", line 666, in require
    needed = self.resolve(parse_requirements(requirements))
  File ""C:\Program Files (x86)\Google\Cloud SDK\google-cloud-sdk\platform\google_appengine\lib\setuptools-0.6c11\pkg_resources.py"", line 565, in resolve
    raise DistributionNotFound(req)  # XXX put more info here
DistributionNotFound: google-cloud-vision
</code></pre>

<p>When I create <code>appengine_config.py</code> file that contains:</p>

<pre><code>from google.appengine.ext import vendor
vendor.add('C:\Anaconda2\Lib\site-packages')
</code></pre>

<p>I am getting error:</p>

<pre><code>File ""C:\Users\MyApp\detect.py"", line 26, in &lt;module&gt;
    from google.cloud import vision
  File ""C:\Anaconda2\lib\site-packages\google\cloud\vision\__init__.py"", line 36, in &lt;module&gt;
    from google.cloud.vision.client import Client
  File ""C:\Anaconda2\lib\site-packages\google\cloud\vision\client.py"", line 23, in &lt;module&gt;
    from google.cloud.vision._gax import _GAPICVisionAPI
  File ""C:\Anaconda2\lib\site-packages\google\cloud\vision\_gax.py"", line 17, in &lt;module&gt;
    from google.cloud.gapic.vision.v1 import image_annotator_client
  File ""C:\Anaconda2\lib\site-packages\google\cloud\gapic\vision\v1\image_annotator_client.py"", line 31, in &lt;module&gt;
    from google.gax import api_callable
  File ""C:\Anaconda2\lib\site-packages\google\gax\__init__.py"", line 36, in &lt;module&gt;
    import multiprocessing as mp
  File ""C:\Anaconda2\lib\multiprocessing\__init__.py"", line 65, in &lt;module&gt;
    from multiprocessing.util import SUBDEBUG, SUBWARNING
  File ""C:\Anaconda2\lib\multiprocessing\util.py"", line 41, in &lt;module&gt;
    from subprocess import _args_from_interpreter_flags
ImportError: cannot import name _args_from_interpreter_flags
</code></pre>

<p>When I tried the hack mentioned in the link below:<br>
<a href=""https://github.com/GoogleCloudPlatform/google-cloud-python/issues/1893"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/google-cloud-python/issues/1893</a> <br>
<a href=""https://github.com/googleapis/gax-python/issues/149"" rel=""nofollow noreferrer"">https://github.com/googleapis/gax-python/issues/149</a> <br>
<a href=""https://gist.github.com/nilleb/419122f2f6f1228650dd2dccbd01c5af"" rel=""nofollow noreferrer"">https://gist.github.com/nilleb/419122f2f6f1228650dd2dccbd01c5af</a> <br></p>

<p>I am getting error:</p>

<pre><code>File ""C:\Users\MyApp\detect.py"", line 11, in &lt;module&gt;
    class DummyProcessing(ModuleType):
NameError: name 'ModuleType' is not defined
</code></pre>

<p>Then I followed the instruction here: <a href=""https://cloud.google.com/appengine/docs/standard/python/tools/using-libraries-python-27#copying_a_third-party_library"" rel=""nofollow noreferrer"">https://cloud.google.com/appengine/docs/standard/python/tools/using-libraries-python-27#copying_a_third-party_library</a> <br>
And used this command:</p>

<pre><code>pip install -t lib google-cloud-vision==0.28.0
</code></pre>

<p>And updated <code>appengine_config.py</code> file:</p>

<pre><code>from google.appengine.ext import vendor
vendor.add('lib')
</code></pre>

<p>I am getting error:</p>

<pre><code>File ""C:\Users\MyApp\detect.py"", line 26, in &lt;module&gt;
    from google.cloud import vision
  File ""C:\Anaconda2\lib\site-packages\google\cloud\vision\__init__.py"", line 36, in &lt;module&gt;
    from google.cloud.vision.client import Client
  File ""C:\Anaconda2\lib\site-packages\google\cloud\vision\client.py"", line 23, in &lt;module&gt;
    from google.cloud.vision._gax import _GAPICVisionAPI
  File ""C:\Anaconda2\lib\site-packages\google\cloud\vision\_gax.py"", line 17, in &lt;module&gt;
    from google.cloud.gapic.vision.v1 import image_annotator_client
  File ""C:\Anaconda2\lib\site-packages\google\cloud\gapic\vision\v1\image_annotator_client.py"", line 31, in &lt;module&gt;
    from google.gax import api_callable
  File ""C:\Anaconda2\lib\site-packages\google\gax\__init__.py"", line 36, in &lt;module&gt;
    import multiprocessing as mp
  File ""C:\Anaconda2\lib\multiprocessing\__init__.py"", line 65, in &lt;module&gt;
    from multiprocessing.util import SUBDEBUG, SUBWARNING
  File ""C:\Anaconda2\lib\multiprocessing\util.py"", line 41, in &lt;module&gt;
    from subprocess import _args_from_interpreter_flags
ImportError: cannot import name _args_from_interpreter_flags
</code></pre>

<p>Then I followed the instruction here:
<a href=""https://stackoverflow.com/questions/27057935/using-gcloud-python-in-gae/28095663#28095663"">Using gcloud-python in GAE</a></p>

<p>And used this command:</p>

<pre><code>pip install -t vendor google-cloud-vision==0.28.0
</code></pre>

<p>I copied <code>appengine_config.py</code> and <code>darth.py</code> files from here:
<a href=""https://github.com/dhermes/test-gcloud-on-gae/tree/8a850fb8b5676ca03e07c4f9dcfba5efb8c77b0a/application"" rel=""nofollow noreferrer"">https://github.com/dhermes/test-gcloud-on-gae/tree/8a850fb8b5676ca03e07c4f9dcfba5efb8c77b0a/application</a></p>

<p>I am getting error:</p>

<pre><code>File ""C:\Users\MyApp\detect.py"", line 26, in &lt;module&gt;
    from google.cloud import vision
  File ""C:\Anaconda2\lib\site-packages\google\cloud\vision\__init__.py"", line 36, in &lt;module&gt;
    from google.cloud.vision.client import Client
  File ""C:\Anaconda2\lib\site-packages\google\cloud\vision\client.py"", line 23, in &lt;module&gt;
    from google.cloud.vision._gax import _GAPICVisionAPI
  File ""C:\Anaconda2\lib\site-packages\google\cloud\vision\_gax.py"", line 17, in &lt;module&gt;
    from google.cloud.gapic.vision.v1 import image_annotator_client
  File ""C:\Anaconda2\lib\site-packages\google\cloud\gapic\vision\v1\image_annotator_client.py"", line 31, in &lt;module&gt;
    from google.gax import api_callable
  File ""C:\Anaconda2\lib\site-packages\google\gax\__init__.py"", line 36, in &lt;module&gt;
    import multiprocessing as mp
  File ""C:\Anaconda2\lib\multiprocessing\__init__.py"", line 65, in &lt;module&gt;
    from multiprocessing.util import SUBDEBUG, SUBWARNING
  File ""C:\Anaconda2\lib\multiprocessing\util.py"", line 41, in &lt;module&gt;
    from subprocess import _args_from_interpreter_flags
ImportError: cannot import name _args_from_interpreter_flags
</code></pre>

<p>I don’t know what to do next. I am completely stuck right now.</p>",,1,2,,2017-11-24 04:02:03.250 UTC,,2017-11-24 18:44:06.523 UTC,2017-11-24 18:44:06.523 UTC,,7448860,,7448860,1,2,python|google-cloud-platform|google-cloud-vision,1524
run Celery on same server as django?,44733018,run Celery on same server as django?,"<p>I am running my Django app in a load balanced Elastic Beanstalk Environment. I want to add a Celery daemon process to do the following things:</p>

<ul>
<li>Upload files to S3 in background and send a success response to my Android app</li>
<li>Send SMS to users to notify them about their upcoming EMIs (using celery beat)</li>
<li>My app uses Google Cloud vision for some features, that takes 10sec to run, so I can run that in the background</li>
</ul>

<p>Now, I want to know if it is the right way to deploy celery on the same server as Django is running using Amazon SQS? If yes, how do I set that up?</p>

<p>And if multiple servers on Elastic Beanstalk can cause duplicate tasks because of celery beat?</p>",44733227,1,0,,2017-06-24 05:15:29.493 UTC,,2017-06-24 05:46:56.607 UTC,,,,,5554104,1,0,django|django-celery|amazon-elb|celerybeat|celeryd,215
How to use Google Vision QR Code detector,55993846,How to use Google Vision QR Code detector,"<p>I am trying to scan QR Codes using the Google Vision API, but I keep getting an empty array as a result.</p>

<p>I'm running on a Xiaomi Mi A1 device with API level 27, minSdkVersion 22, targetSdkVersion 28 and Google Vision API version 17.02.</p>

<p>What I'm doing as a test is reading a PNG resource (a QR Code image) as a Bitmap object and then feeding it to the BarcodeDetector:</p>

<pre><code>val detector = BarcodeDetector.Builder(applicationContext)
                .setBarcodeFormats(Barcode.QR_CODE)
                .build()

val myBitmap : Bitmap = BitmapFactory.decodeResource(
            applicationContext.resources,
            R.drawable.qrcode)

val frame = Frame.Builder().setBitmap(myBitmap).build()

if (detector.isOperational) {
    val result : SparseArray&lt;Barcode&gt; = detector.detect(frame)
    i (""QRCodeExample"", ""QR Codes count: ${result.size()}"") // prints 0
}
</code></pre>

<p>As shown in the code above, I checked the <code>detector.isOperational</code> flag and the detector seems to be working, but the SparseArray is empty.</p>

<p>""External"" stuff I tried without success:</p>

<ul>
<li>Deleting the Google Play Services cache</li>
<li>Deleting the application data</li>
<li>Updating the Vision API version from 15.02 to 17.02</li>
</ul>",,0,0,,2019-05-05 16:00:38.740 UTC,,2019-05-05 16:16:03.477 UTC,,,,,11455616,1,0,android|image|kotlin|qr-code|google-vision,29
Text Detection problems with Google Vision,51103236,Text Detection problems with Google Vision,"<p>Not sure what the issues is as my code just stopped working overnight, but the text detection on Google Vision is either returning nil or returning words that are non-existent on the subject.</p>

<p>Here's my request function:</p>

<pre><code>func createRequest(with imageBase64: String) {

    let jsonRequest = [
        ""requests"": [
            ""image"": [
                ""content"": imageBase64 ],
            ""features"": [
                [""type"": ""TEXT_DETECTION""],
                [""type"": ""IMAGE_PROPERTIES""]
            ]
        ]
    ]

    let jsonData = try? JSONSerialization.data(withJSONObject: jsonRequest)
    var request = URLRequest(url: googleURL)
    request.httpMethod = ""POST""
    request.addValue(""application/json"", forHTTPHeaderField: ""Content-Type"")
    request.addValue(Bundle.main.bundleIdentifier ?? """", forHTTPHeaderField: ""X-Ios-Bundle-Identifier"")

    request.httpBody = jsonData

    DispatchQueue.global().async {
        let task: URLSessionDataTask = self.URLsession.dataTask(with: request) { (encodedObject, response, error) in
            guard let data = encodedObject, error == nil else {
                print(error?.localizedDescription ?? ""no data"")
                return
            }
            self.analyzeResults(data)
        }
        task.resume()
    }
}
</code></pre>

<p>Part of my analyze results function:</p>

<pre><code>func analyzeResults(_ data: Data) {
    DispatchQueue.main.async {
        guard let response = try? JSONDecoder().decode(Root.self, from: data) else { return }
        guard let responseArray = response.responses else { return }
        print(response)
</code></pre>",,0,2,,2018-06-29 13:46:24.483 UTC,,2018-06-29 13:46:24.483 UTC,,,,,5981327,1,1,ios|swift|google-vision,55
gRPC/proto Google cloud client libraries vs GAPIC Google cloud client libraries,46752575,gRPC/proto Google cloud client libraries vs GAPIC Google cloud client libraries,"<p>I've mostly found working with google's client libraries easy to work with, intuitive, and well suited to idiomatic python, with the notable exception of auth (there is a special place in hell for whoever came up with the oAuth dance)  Although in the past, most of my work was on Gsuite, I'm tinkering with the google cloud client libraries,</p>

<p>Looking for a specific library i realised they come in two flavors now: gRPC and GAPIC. although both come with a side of pickles, I could not find any reference about which flavor would be preferable to the other, (if any). </p>

<h2>Gapic Flavor</h2>

<pre><code>(ame) hector@trantor ~/a/envs ❯❯❯ pip search gapic
gapic-google-iam-admin-v1 (0.10.0)                     - GAPIC library for the Google IAM Admin API
gapic-google-cloud-spanner-admin-instance-v1 (0.15.3)  - GAPIC library for the Cloud Spanner Instance Admin API
gapic-google-cloud-spanner-admin-database-v1 (0.15.3)  - GAPIC library for the Cloud Spanner Database Admin API
gapic-google-cloud-speech-v1 (0.15.3)                  - GAPIC library for the Google Cloud Speech API
gapic-google-cloud-language-v1beta2 (0.15.3)           - GAPIC library for the Google Cloud Natural Language API
google-cloud-trace (0.15.5)                            - GAPIC library for the Stackdriver Trace API
gapic-google-cloud-functions-v1beta2 (0.15.3)          - GAPIC library for the Google Cloud Functions API
gapic-google-cloud-spanner-v1 (0.15.3)                 - GAPIC library for the Cloud Spanner API
gapic-google-cloud-language-v1 (0.15.3)                - GAPIC library for the Google Cloud Natural Language API
gapic-google-cloud-monitoring-v3 (0.15.3)              - GAPIC library for the Stackdriver Monitoring API
gapic-google-cloud-error-reporting-v1beta1 (0.15.3)    - GAPIC library for the Stackdriver Error Reporting API
gapic-google-cloud-pubsub-v1 (0.15.4)                  - GAPIC library for the Google Cloud Pub/Sub API
gapic-google-cloud-language-v1beta1 (0.11.1)           - GAPIC library for the Google Language API
gapic-google-cloud-logging-v2 (0.91.3)                 - GAPIC library for the Stackdriver Logging API
gapic-google-cloud-datastore-v1 (0.90.4)               - GAPIC library for the Google Cloud Datastore API
gapic-google-cloud-speech-v1beta1 (0.15.3)             - GAPIC library for the Google Cloud Speech API
gapic-google-cloud-vision-v1 (0.90.3)                  - GAPIC library for the Google Cloud Vision API
gapic-google-monitoring-v3 (0.11.1)                    - GAPIC library for the Stackdriver Monitoring API
gapic-google-longrunning (0.11.2)                      - GAPIC library for the Google Google API
gapic-google-maps-streetview_publish-v1 (0.1.4)        - GAPIC library for the Street View Publish API
gapic-google-pubsub-v1 (0.11.1)                        - GAPIC library for the Google Pubsub API
</code></pre>

<h3>gRPC/protocol flavor:</h3>

<pre><code>(ame) hector@trantor ~/a/envs ❯❯❯ pip search grpc | grep google                                        I
grpc-google-iam-admin-v1 (0.10.0)                      - GRPC library for the google-iam-admin-v1 service
proto-google-cloud-spanner-admin-database-v1 (0.15.3)  - GRPC library for the Cloud Spanner Database Admin API
proto-google-cloud-spanner-admin-instance-v1 (0.15.3)  - GRPC library for the Cloud Spanner Instance Admin API
google-assistant-grpc (0.0.2)                          - Google Assistant API gRPC bindings
proto-google-cloud-language-v1beta2 (0.15.3)           - GRPC library for the Google Cloud Natural Language API
grpc-google-cloud-pubsub-v1 (0.14.0)                   - GRPC library for the Google Pubsub service
grpc-google-cloud-monitoring-v3 (0.14.0)               - GRPC library for the Stackdriver Monitoring API service
proto-google-cloud-speech-v1 (0.15.3)                  - GRPC library for the Google Cloud Speech API
grpc-google-cloud-language-v1 (0.14.0)                 - GRPC library for the Google Language service
grpc-google-cloud-error-reporting-v1beta1 (0.14.0)     - GRPC library for the Stackdriver Error Reporting API
proto-google-cloud-logging-v2 (0.91.3)                 - GRPC library for the Stackdriver Logging API
grpc-google-cloud-logging-v2 (0.90.0)                  - GRPC library for the Stackdriver Logging service
proto-google-cloud-functions-v1beta2 (0.15.3)          - GRPC library for the Google Cloud Functions API
grpc-google-cloud-datastore-v1 (0.14.0)                - GRPC library for the Google Datastore service
grpc-google-cloud-speech-v1beta1 (0.14.0)              - GRPC library for the Google Speech service
proto-google-cloud-spanner-v1 (0.15.3)                 - GRPC library for the Cloud Spanner API
proto-google-cloud-speech-v1beta1 (0.15.3)             - GRPC library for the Google Cloud Speech API
proto-google-cloud-monitoring-v3 (0.15.3)              - GRPC library for the Stackdriver Monitoring API
proto-google-cloud-language-v1 (0.15.3)                - GRPC library for the Google Cloud Natural Language API
proto-google-cloud-error-reporting-v1beta1 (0.15.3)    - GRPC library for the Stackdriver Error Reporting API
proto-google-cloud-vision-v1 (0.90.3)                  - GRPC library for the Google Cloud Vision API
proto-google-cloud-datastore-v1 (0.90.4)               - GRPC library for the Google Cloud Datastore API
proto-google-cloud-pubsub-v1 (0.15.4)                  - GRPC library for the Google Cloud Pub/Sub API
grpc-google-cloud-vision-v1 (0.14.0)                   - GRPC library for the Google Cloud Vision API service
grpc-google-cloud-language-v1beta1 (0.11.1)            - GRPC library for the google-cloud-language-v1beta1 service
grpc-google-monitoring-v3 (0.11.1)                     - GRPC library for the google-monitoring-v3 service
grpc-google-longrunning-v2 (0.8.1)                     - GRPC library for the google-longrunning-v2 service
proto-google-maps-streetview_publish-v1 (0.1.4)        - GRPC library for the Street View Publish API
grpc-google-iam-v1 (0.11.4)                            - GRPC library for the google-iam-v1 service
grpc-google-pubsub-v1 (0.11.1)                         - GRPC library for the google-pubsub-v1 service`
</code></pre>

<p>To make maters more confusing , most libraries exist at the same revision number in both flavors with an older gRPC version around:</p>

<pre><code>google-cloud-datastore-v1 (0.14.0)                - GRPC library for the Google Datastore service
google-cloud-datastore-v1 (0.90.4)               - GAPIC library for the Google Cloud Datastore API
google-cloud-datastore-v1 (0.90.4)               - GRPC library for the Google Cloud Datastore API
</code></pre>

<p>and</p>

<pre><code>google-cloud-logging-v2 (0.90.0)                  - GRPC library for the Stackdriver Logging service
google-cloud-logging-v2 (0.91.3)                 - GAPIC library for the Stackdriver Logging API
google-cloud-logging-v2 (0.91.3)                 - GRPC library for the Stackdriver Logging API
</code></pre>

<p>also, the<code>assistant</code> API client comes only in gRPC <code>cloud-trace</code> is the opposite.</p>

<p>which client library should I choose to develop my app? is there any material difference either in idiomatic features or performance-wise? (i'd expect the gRPC libs to render a client more performant, but this is the internet and we're not all on reliable bandwidth) so, the trivial case of ""<em>YMMV</em>"" and ""<em>choose the tool that will do the job</em>"" are assumed.</p>

<p>documentation does not specify anything to the effect of which kind to choose, specially when both flavors are at the same version label.</p>

<p>Your insights are much appreciated.</p>",46774453,1,0,,2017-10-15 07:07:28.703 UTC,,2017-10-16 16:00:11.743 UTC,,,,,4600913,1,0,python|google-api|google-cloud-platform|grpc,225
How to force Google cloud vision to use a specific charset?,52397039,How to force Google cloud vision to use a specific charset?,"<p>I have a large set of difficult words to recognize through ᴏᴄʀ.</p>

<p>The image are all of lowercase latin alphabet without diatrics, but despite using languagehints, I end up with results like <code>学会学论發及</code> in a lot of cases.</p>

<p>So how to force Google cloud vision to use a specific set of letters and not just hint ?</p>",52410191,1,1,,2018-09-19 02:11:16.677 UTC,,2018-09-19 16:13:22.333 UTC,,,,,2284570,1,0,google-cloud-vision,25
Visual Studio Community 2017 - Google Vision API - Can't find my error,44543092,Visual Studio Community 2017 - Google Vision API - Can't find my error,"<p>I'm currently on an internship, and my project is to automate a drone, to make it recognize free parking spots or used parking spots. 
For that, I'm using the Google Vision API with Visual Studios Community 2017.</p>

<p>I already made all the "" Before beginning "" steps on Google Cloud Platform ( Create project, enable billing, get the Compute Engine Machine, and all these stuff ), I connected my Cloud Platform to Visual Studios, and Installed the packages in my project with the command line "" Install-Packages Google.Cloud.Vision.V1 -Pre "" in the Packet Manager.</p>

<p>I wrote this code : </p>

<pre><code>using System;
using System.Collections.Generic;
using System.ComponentModel;
using System.Data;
using System.Drawing;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using System.Windows.Forms;
using Google.Cloud.Vision.V1;

namespace WinImageRecognize
{
public partial class Form1 : Form
{
    public Form1()
    {
        InitializeComponent();
    }

    private void Form1_Load(object sender, EventArgs e)
    {

    }


    private void Button1_Click(object sender, EventArgs e)
    {
        var client = ImageAnnotatorClient.Create();
        // Load image file into memory
        var image = Google.Cloud.Vision.V1.Image.FromFile(""C:/Users/Stian/Pictures/Internship/car.jpg"");
        // Does label detection on the image file
        var response = client.DetectLabels(image);


        textBox1.Text = """";

        foreach ( var annotation in response)
        {
            if (annotation.Description != null)
            {
                textBox1.Text += annotation.Description + ""\r\n"";
            }
        }
    }
   }
}
</code></pre>

<p>This code opens me my WindowsForm, looking like this : <a href=""https://i.stack.imgur.com/Q7QPo.jpg"" rel=""nofollow noreferrer"">Car parked</a></p>

<p>But when I run the application ( No errors ), and clic on the button, nothing happens ! </p>

<p>And I can't fix my error ... </p>

<p>Any ideas ?</p>

<p>Thanks a lot !</p>",,0,0,,2017-06-14 11:09:41.470 UTC,,2017-06-14 15:06:03.510 UTC,2017-06-14 15:06:03.510 UTC,,4495081,,8160001,1,1,.net|visual-studio|google-compute-engine|google-vision,100
Differences between Google Cloud Vision OCR in browser demo and via python,50868017,Differences between Google Cloud Vision OCR in browser demo and via python,"<p>I am fairly new to the Google Cloud Vision API so my apologies if there is an obvious answer to this. I am noticing that for some images I am getting different OCR results between the Google Cloud Vision API Drag and Drop (<a href=""https://cloud.google.com/vision/docs/drag-and-drop"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/drag-and-drop</a>) and from local image detection in python.</p>

<p>My code is as follows</p>

<pre><code>import io
# Imports the Google Cloud client library
from google.cloud import vision
from google.cloud.vision import types

# Instantiates a client
client = vision.ImageAnnotatorClient()

# The name of the image file to annotate
file_name = ""./test0004a.jpg""

# Loads the image into memory
with io.open(file_name, 'rb') as image_file:
    content = image_file.read()

image = types.Image(content=content)

response = client.text_detection(image=image)
texts = response.text_annotations

print('Texts:')
for text in texts:
#    print('\n""{}""'.format(text.description.encode('utf-8')))
    print('\n""{}""'.format(text.description.encode('ascii','ignore')))

    vertices = (['({},{})'.format(vertex.x, vertex.y)
                for vertex in text.bounding_poly.vertices])

    print('bounds: {}'.format(','.join(vertices)))
</code></pre>

<p>A sample image that highlights this is attached <a href=""https://i.stack.imgur.com/mIrYw.jpg"" rel=""nofollow noreferrer"">Sample Image</a></p>

<p>The python code above doesn't return anything, but in the browser using drag and drop it correctly identifies ""2340"" as the text. 
Shouldn't both python and the browser return the same result?. And if not, why not?, Do I need to include additional parameters in the code?. </p>",50872211,1,0,,2018-06-15 01:26:01.447 UTC,1,2018-06-15 08:57:53.700 UTC,,,,,3513328,1,1,python|ocr|google-cloud-vision,561
Need assistance with google vision API,55994493,Need assistance with google vision API,"<p>I am creating an application using google vision api to extract handwritten texts from some images documents. I already have a google cloud ready, with the json file saved on my local folder with the code. I used google cloud storage to create a bucket, and the name of the bucket is passed to the code. I have managed to solve some errors (or maybe, did I actually destroy it?!), but I am still receiving this giant one in the command line that:</p>

<p>What does it mean? How can I solve it?</p>

<pre><code>    Input directory path is valid
    Processing File .DS_Store ...
    Traceback (most recent call last):
    File ""/Users/****j/anaconda3/lib/python3.7/site-packages/google     /api_core/grpc_helpers.py"", line 57, in error_remapped_callable
    return callable_(*args, **kwargs)
    File ""/Users/******j/anaconda3/lib/python3.7/site-packages     /grpc/_channel.py"", line 562, in __call__
    return _end_unary_response_blocking(state, call, False, None)
    File ""/Users/****j/anaconda3/lib/python3.7/site-packages                               /grpc/_channel.py"", line 466, in _end_unary_response_blocking
    raise _Rendezvous(state, None, None, deadline)
    grpc._channel._Rendezvous: &lt;_Rendezvous of RPC that terminated with:
status = StatusCode.NOT_FOUND
details = ""Error opening file: gs://ocad_images/001/.DS_Store.""
debug_error_string =   ""{""created"":""@1557074238.415495000"",""description"":""Error received from peer  ipv6:[2607:f8b0:400b:80f::200a]:443"",""file"":""src/core/lib/surface /call.cc"",""file_line"":1041,""grpc_message"":""Error opening file: gs://buck_images/001/.DS_Store."",""grpc_status"":5}""

    The above exception was the direct cause of the following exception:

    Traceback (most recent call last):
    File ""input_ml.py"", line 63, in &lt;module&gt;
    detect_hand_writtent_text(gcs_source_uri,gcs_destination_uri ,   os.path.abspath(data_dir)+""/output/""+input_file)
    File ""/Users/*****j/Environments/GoogleCloud/handwriting/doc_txt_detect.py"", line 36, in detect_hand_writtent_text
    requests=[async_request])
    File ""/Users/*****j/anaconda3/lib/python3.7/site-packages/google/cloud/vision_v1/gapic/image_annotator_client.py"", line 308, in  async_batch_annotate_files
    request, retry=retry, timeout=timeout, metadata=metadata
    File ""/Users/******j/anaconda3/lib/python3.7/site-packages/google /api_core/gapic_v1/method.py"", line 143, in __call__
    return wrapped_func(*args, **kwargs)
    File ""/Users/****j/anaconda3/lib/python3.7/site-packages/google/api_core/retry.py"", line 270, in retry_wrapped_func
    on_error=on_error,
    File ""/Users/*****j/anaconda3/lib/python3.7/site-packages/google/api_core/retry.py"", line 179, in retry_target
    return target()
    File ""/Users/*****j/anaconda3/lib/python3.7/site-packages/google/api_core/timeout.py"", line 214, in func_with_timeout
    return func(*args, **kwargs)
    File ""/Users/****j/anaconda3/lib/python3.7/site-packages/google  /api_core/grpc_helpers.py"", line 59, in error_remapped_callable
    six.raise_from(exceptions.from_grpc_error(exc), exc)
    File ""&lt;string&gt;"", line 3, in raise_from
    google.api_core.exceptions.NotFound: 404 Error opening file:  gs://ocad_images/001/.DS_Store.
</code></pre>",,0,1,,2019-05-05 17:18:58.287 UTC,,2019-05-05 17:18:58.287 UTC,,,,,8374636,1,0,exception|google-cloud-platform|ocr|vision,33
How to get the vertices of a bounding box?,55897523,How to get the vertices of a bounding box?,"<p>I am just getting my head around using Google Cloud Vision to automatically detect objects in a series of photos. These photos all have the same ""scene"" i.e. a fixed landscape, but objects in the image change-so for example birds flying in the sky. The camera taking the photos doesn't move but is taking a series of photos of the same shot over time.</p>

<p>So, I have all of my images, and I am trying to work through the steps here:
<a href=""https://cloud.google.com/vision/automl/object-detection/docs/csv-format?_ga=2.72217555.-1590639087.1553718635#csv"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/automl/object-detection/docs/csv-format?_ga=2.72217555.-1590639087.1553718635#csv</a></p>

<p>I am at the point where I have uploaded my photos. I now need to write a CSV file which leads to each image. Each row in the CSV file will point to a photo, within which I want to select objects in a bounding box and label them. The objects I select will be the objects I want to train Vision to ID.</p>

<p>This problem seems very simple but I have no idea how to select a bounding box or to get the vertices. I've tried Photoshop but can only get pixel dimensions, which isn't suitable.</p>

<p>What software should I use to get bounding box vertices, is basically my question I think??</p>

<p>Any other basic tips would be appreciated, I am a complete novice here as is probably quite obvious. </p>

<p>I've looked at existing questions on here, all of the Q&amp;A are more advanced than what I'm looking for. </p>

<p>I don't have any code as I am using a CSV file to complete this part of the task.</p>",,0,0,,2019-04-29 05:37:07.423 UTC,,2019-04-29 05:37:07.423 UTC,,,,,7969214,1,0,bounding-box|google-cloud-automl,11
Cropping webcam video frame to match window,54133439,Cropping webcam video frame to match window,"<p>So, we are using the google vision api to get facial landmarks as coordinates by grabbing a frame from the webcam and sending it to the api. The problem is that by centering the webcam video and flipping it so that it responds naturally the returned points don't map back onto the video correctly. </p>

<p>From what i can see this is because the grabbed frame is 640x480 and the window size is 1200x640. So the video is resized and centered to fit the window size using :</p>

<pre><code>const Video = styled.video`
  position: absolute;
  transform: rotateY(180deg);
  height: 100vh;
  width: 100vw;
  left: 50%;
  margin-left: -50%;
  object-fit: cover;
  object-position: center;
`;
</code></pre>

<p>Basically making the video fill the screen then centering.</p>

<p>So i need to crop the grabbed frame from so that it matches what is seen on the screen, but i have done a lot of searching but cant quite compile how to do it all at once.</p>

<p>Before the frame is uploaded i turn it into base64 using a canvas as follows:</p>

<pre><code>  imageAsBase64 = image =&gt; {
    var canvas = document.createElement(""canvas"");
    canvas.width = window.innerWidth;
    canvas.height = window.innerHeight;

    let context = canvas.getContext(""2d"");
    context.drawImage(image, 0, 0);
    context.setTransform(1,0,0,1,0,0);

    return canvas.toDataURL(""image/png"", 0.5);
  };
</code></pre>

<p>I know in here I have to crop the image to match thewebcam video in the window but dont know how. Can anyone help or point me in the right direction?</p>",,0,0,,2019-01-10 16:53:25.647 UTC,,2019-01-10 16:53:25.647 UTC,,,,,6734301,1,0,javascript|reactjs,21
Maven - Unable to resolve dependency conflict b/w google-vision beta & aws-sdk sub-components,45976281,Maven - Unable to resolve dependency conflict b/w google-vision beta & aws-sdk sub-components,"<p>I'm trying to use google-vision to fetch text from an image (uploaded to AWS S3) and store it in AWS Dynamo DB.  I'm encountering dependency conflicts on jackson-core as both google-api and aws-java-sdk are using two different versions.</p>

<hr>

<p><strong>Dependency Hierarchy</strong></p>

<blockquote>
  <p>google-api-client: 1.22.0 uses jackson-core: 2.1.3</p>
  
  <p>google-cloud-vision: 0.22.0-beta uses jackson-core: 2.1.3</p>
  
  <p>aws-java-sdk: 1.11.106 uses jackson-core: 2.6.6</p>
</blockquote>

<p>I tried ""exclusions"" and added explicit dependency in pom.xml to use jackson-core: 2.6.6.  Google-vision api works fine with that change.  However, AmazonDynamoDBClientBuilder fails with below error:</p>

<hr>

<pre><code>Exception in thread ""main"" java.lang.IllegalAccessError: tried to access method com.amazonaws.AmazonWebServiceClient.&lt;init&gt;(Lcom/amazonaws/client/AwsSyncClientParams;)V from class com.amazonaws.services.dynamodbv2.AmazonDynamoDBClientBuilder
    at com.amazonaws.services.dynamodbv2.AmazonDynamoDBClientBuilder.build(AmazonDynamoDBClientBuilder.java:60)
    at com.amazonaws.services.dynamodbv2.AmazonDynamoDBClientBuilder.build(AmazonDynamoDBClientBuilder.java:26)
    at com.amazonaws.client.builder.AwsSyncClientBuilder.build(AwsSyncClientBuilder.java:46)
    at com.oneglint.ImageProcessing.AddItem.main(AddItem.java:133)
</code></pre>

<p>Following error is displayed when there was version conflict</p>

<pre><code>Exception in thread ""main"" java.lang.NoSuchMethodError: com.fasterxml.jackson.core.JsonFactory.requiresPropertyOrdering()Z
    at com.fasterxml.jackson.databind.ObjectMapper.&lt;init&gt;(ObjectMapper.java:537)
    at com.fasterxml.jackson.databind.ObjectMapper.&lt;init&gt;(ObjectMapper.java:448)
    at com.amazonaws.partitions.PartitionsLoader.&lt;clinit&gt;(PartitionsLoader.java:51)
    at com.amazonaws.regions.RegionMetadataFactory.create(RegionMetadataFactory.java:30)
    at com.amazonaws.regions.RegionUtils.initialize(RegionUtils.java:64)
    at com.amazonaws.regions.RegionUtils.getRegionMetadata(RegionUtils.java:52)
    at com.amazonaws.regions.RegionUtils.getRegion(RegionUtils.java:105)
    at com.amazonaws.client.builder.AwsClientBuilder.withRegion(AwsClientBuilder.java:239)
    at com.oneglint.ImageProcessing.AddItem.main(AddItem.java:132)
</code></pre>

<hr>

<p>What am I missing here? Thanks for the help..</p>

<p>BTW, I'm using example code from github to achieve this.  Here are the links:</p>

<p>DynamoDB example: <a href=""https://github.com/awsdocs/aws-doc-sdk-examples/tree/master/java/example_code/dynamodb"" rel=""nofollow noreferrer"">https://github.com/awsdocs/aws-doc-sdk-examples/tree/master/java/example_code/dynamodb</a></p>

<p>Google Vision DetectText example: <a href=""https://github.com/GoogleCloudPlatform/java-docs-samples/blob/master/vision/cloud-client/src/main/java/com/example/vision/Detect.java"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/java-docs-samples/blob/master/vision/cloud-client/src/main/java/com/example/vision/Detect.java</a></p>

<hr>

<h2>Additional Details</h2>

<p>Both the examples are working fine if executed as independent projects.  The problem occurs ONLY when both <code>PutItem</code> (AWS) &amp; <code>Detect</code> (google-vision) classes are brought together in a single project, with appropriate code changes.</p>",45994115,2,0,,2017-08-31 08:02:15.830 UTC,,2017-09-01 06:02:18.463 UTC,2017-08-31 10:32:49.803 UTC,,8541830,,8541830,1,3,java|maven|amazon-web-services|amazon-dynamodb|google-vision,345
Google Vision for PDF,56176858,Google Vision for PDF,"<p>I need to send a PDF file to Google Vision to extract and return text. From documentation I understood that DPF file must be located on Google Storage, so I am putting the file to my Google Storage bucket like this:</p>

<pre><code>require '../vendor/autoload.php';

use Google\Cloud\Storage\StorageClient;

$storage = new StorageClient([
    'keyFilePath' =&gt; '/my-keyfile.json',
    'projectId' =&gt; PROJECT_ID
]);

$bucket = $storage-&gt;bucket(BUCKET_NAME);

$bucket-&gt;upload(
    fopen($_SESSION['local_pdf_url'], 'r')
);
</code></pre>

<p>It works. After I redirect to another page that is suppose to get that file to Vision, and that's where it fails. I found an <a href=""https://cloud.google.com/vision/docs/pdf#vision-pdf-detection-gcs-php"" rel=""noreferrer"">example function</a>. Here's the code:</p>

<pre><code>require '../vendor/autoload.php';

use Google\Cloud\Storage\StorageClient;
use Google\Cloud\Vision\V1\AnnotateFileResponse;
use Google\Cloud\Vision\V1\AsyncAnnotateFileRequest;
use Google\Cloud\Vision\V1\Feature;
use Google\Cloud\Vision\V1\Feature\Type;
use Google\Cloud\Vision\V1\GcsDestination;
use Google\Cloud\Vision\V1\GcsSource;
use Google\Cloud\Vision\V1\ImageAnnotatorClient;
use Google\Cloud\Vision\V1\InputConfig;
use Google\Cloud\Vision\V1\OutputConfig;

$storage = new StorageClient([
    'keyFilePath' =&gt; '/my-keyfile.json',
    'projectId' =&gt; PROJECT_ID
]);

$path = 'gs://my-bucket/'.$_SESSION['pdf_file_name'];
</code></pre>

<p>When I run the second script I get the following errors:</p>

<blockquote>
  <p>Fatal error: Uncaught DomainException: Could not load the default
  credentials. Browse to
  <a href=""https://developers.google.com/accounts/docs/application-default-credentials"" rel=""noreferrer"">https://developers.google.com/accounts/docs/application-default-credentials</a>
  for more information in
  /home/domain/vendor/google/auth/src/ApplicationDefaultCredentials.php:168
  Stack trace: #0
  /home/domain/vendor/google/gax/src/CredentialsWrapper.php(197):
  Google\Auth\ApplicationDefaultCredentials::getCredentials(Array,
  Object(Google\Auth\HttpHandler\Guzzle6HttpHandler), NULL, NULL) #1
  /home/domain/vendor/google/gax/src/CredentialsWrapper.php(114):
  Google\ApiCore\CredentialsWrapper::buildApplicationDefaultCredentials(Array,
  Object(Google\Auth\HttpHandler\Guzzle6HttpHandler)) #2
  /home/domain/vendor/google/gax/src/GapicClientTrait.php(326):
  Google\ApiCore\CredentialsWrapper::build(Array) #3
  /home/domain/vendor/google/gax/src/GapicClientTrait.php(308):
  Google\Cloud\Vision\V1\Gapic\ImageAnnotatorGapicClient->createCredentialsWrapper(NULL,
  Array) #4
  /home/domain/vendor/google/cloud/Vision/src/V1/Gapic/ImageAnnotatorGapicClient.php(216):
  Google\Clou in
  /home/domain/vendor/google/gax/src/CredentialsWrapper.php on line 200</p>
</blockquote>

<p>How do I authenticate for this service? What am I missing?</p>",56317830,2,0,,2019-05-16 21:31:23.173 UTC,,2019-05-26 22:26:59.093 UTC,2019-05-20 21:23:41.547 UTC,,434218,,434218,1,8,php|google-vision,205
Find image that fit together the best,55450247,Find image that fit together the best,"<p>Given a batch of images i have to find the images that fit together the best like in the example given below, but my solutions are not working:</p>

<h3>Left image</h3>

<p><a href=""https://i.stack.imgur.com/YrVmR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YrVmR.png"" alt=""https://i.stack.imgur.com/YrVmR.png""></a></p>

<h3>Right image</h3>

<p><a href=""https://i.stack.imgur.com/AfWwL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AfWwL.png"" alt=""https://i.stack.imgur.com/AfWwL.png""></a></p>

<p>I tried firstly with google cloud Vision API but it wasn't giving good results, then i trained a model over with ludwig but it will take forever to try all the possible combinations of images, as i have 2500 left images and 2500 right images.</p>

<p>is there a way to find this out or decrease the possible cases so that i can use it in my model.</p>",55461704,1,5,,2019-04-01 07:45:25.703 UTC,1,2019-04-01 20:05:32.057 UTC,2019-04-01 07:53:36.500 UTC,,1401510,,11289348,1,0,opencv|computer-vision,506
How to Scan only Recharge card pins using Google Vision Api while ignoring other text,53882629,How to Scan only Recharge card pins using Google Vision Api while ignoring other text,"<p>I am developing an Android App that will scan Recharge card Pins and automatically recharge.
I have tried to integrate different sdk`s of which have not reached what I want to achieve.</p>

<p>I have finally managed to find and use google vision to integrate scanning in my app, but the problem is that it scans every text.
What I want is to scan only the Recharge card pins and ignore other text.</p>

<p>Currently <a href=""https://i.stack.imgur.com/zEwYJ.png"" rel=""nofollow noreferrer"">it does this</a> </p>

<p>The app should achieve something like this <a href=""https://i.stack.imgur.com/M0VzM.png"" rel=""nofollow noreferrer"">App screen</a></p>

<p>How can i achieve this? 
Thank you.</p>",,0,1,,2018-12-21 09:57:21.540 UTC,,2018-12-21 10:05:38.417 UTC,2018-12-21 10:05:38.417 UTC,,5793873,,10803401,1,0,android-studio|google-cloud-platform|google-vision|text-recognition,33
React native face detection not working for me (iOS),53654683,React native face detection not working for me (iOS),"<p>I am trying to build a face detection module in objective-c using react native. I am using <a href=""https://github.com/react-native-community/react-native-camera/blob/master/docs/RNCamera.md#onfacesdetected"" rel=""nofollow noreferrer"">onFacesdetected</a> from react native. But it is not working for me.
Below is the code :</p>

<pre><code>&lt;RNCamera
        ref={ref =&gt; {
          this.camera = ref;
        }}
        style = {styles.preview}
        type={RNCamera.Constants.Type.front}
        flashMode={RNCamera.Constants.FlashMode.off}
        faceDetectionMode={RNCamera.Constants.FaceDetection.Mode.fast}
        onFacesDetected={(d)=&gt;{console.log('onFacesDetected',d);}}
        permissionDialogTitle={'Permission to use camera'}
        permissionDialogMessage={'We need your permission to use your camera phone'}
        onGoogleVisionBarcodesDetected={({ barcodes }) =&gt; {
          console.log(barcodes)
        }}
    /&gt;
</code></pre>

<p>When the camera opens for the first time it prints <strong>'onFacesDetected', { type: 'face', faces: [], target: 27 }</strong> after that upon bringing the face in front of camera it does nothing.</p>

<p>Any help would be appreciated.</p>",,1,0,,2018-12-06 15:30:51.717 UTC,,2018-12-08 00:52:14.450 UTC,,,,,5791180,1,0,ios|objective-c|react-native|face-detection,134
How to Authenticate to use Google Cloud Vision API,53735551,How to Authenticate to use Google Cloud Vision API,"<p>I am currently trying to use Google Cloud Vision API on C#.</p>

<p>After downloading JSON file for google cloud authentication, I have set the system environment variable as the path of the JSON file and compiled my code. It was all good.</p>

<p>However, when I created DLL with the source it seems like the DLL could not get the Google Application Credentials value from the system environment variable.</p>

<p>So that I studied some of the Google Credential Authentication documents to put a code at the very first line of C# code to deliver my JSON file path to recognize my vision api calls.</p>

<p>However, <strong>the code is not working to properly authenticating my JSON file to call Google Vision API.</strong></p>

<p>Please enlighten me with your knowledge! Thanks.</p>

<p>Here is my code.</p>

<pre><code>using System;
using System.IO;
using System.Text;
using System.Threading;
using System.Threading.Tasks;

using Google.Apis.Auth.OAuth2;
using Google.Apis.Util.Store;
using Google.Cloud.Vision.V1;
using Image = Google.Cloud.Vision.V1.Image;
using Grpc.Core;
using Grpc.Auth;


namespace dotNetFramework461_Console
{
    class Program
    {
        public string GVA_API()
        {
            string jsonPath = ""D:\\blablablah\\abcdefghijk.json"";
            string imgPath = ""C:\\Users\\blablablah\\image.jpg"";

            var stream = new FileStream(jsonPath, FileMode.Open, FileAccess.Read);
            var credential = GoogleCredential.FromStream(stream).CreateScoped(ImageAnnotatorClient.DefaultScopes);
            var channel = new Grpc.Core.Channel(ImageAnnotatorClient.DefaultEndpoint.ToString(), credential.ToChannelCredentials());
            var client = ImageAnnotatorClient.Create(channel);
            var image = Image.FromFile(imgPath);

            TextAnnotation text = client.DetectDocumentText(image);
            return text.Text;
        }
    }
}
</code></pre>",,0,2,,2018-12-12 03:17:49.020 UTC,,2018-12-12 16:58:11.650 UTC,2018-12-12 09:47:11.287 UTC,,4834002,,4834002,1,0,c#|google-cloud-platform|google-cloud-vision,185
How do I convert local .JPG file to Base64 to work with Boto3 and Detect_Text?,47542341,How do I convert local .JPG file to Base64 to work with Boto3 and Detect_Text?,"<p>Relevant Code:</p>

<pre><code>import boto3
from PIL import Image
import base64

client = boto3.client('rekognition')

filename = r'C:\Users\H-63\Pictures\scantests\Rekognition test.JPG'

with open(filename, 'rb') as image_file:
    image = image_file.read()

image = base64.b64encode(image).decode('UTF-8')

response = client.detect_text(
    Image={'Bytes': image
        })
</code></pre>

<p>However,
When I run this, I get an error:</p>

<pre><code>An error occurred (InvalidImageFormatException) when calling the DetectText operation: Request has Invalid image format
</code></pre>

<p>How do I get my image to be the right format for detect_text? The documentation says it has to be base64 encoding. </p>",,3,2,,2017-11-28 22:57:52.877 UTC,1,2019-02-03 15:19:00.050 UTC,,,,,7903466,1,2,python|amazon-web-services|boto3|amazon-rekognition,1041
PDF/TIFF Document Text Detection gcsDestinationBucketName,56257028,PDF/TIFF Document Text Detection gcsDestinationBucketName,"<p>I'm working on Pdf to text file conversion using google cloud vision api.</p>

<p>I got an initial code help through there side, image to text conversion working fine with json key which  i got through registration and activation,</p>

<p>here is a code which i got for pdf to text conversion</p>

<pre><code>private static object DetectDocument(string gcsSourceUri,
string gcsDestinationBucketName, string gcsDestinationPrefixName)
{
var client = ImageAnnotatorClient.Create();

var asyncRequest = new AsyncAnnotateFileRequest
{
    InputConfig = new InputConfig
    {
        GcsSource = new GcsSource
        {
            Uri = gcsSourceUri
        },
        // Supported mime_types are: 'application/pdf' and 'image/tiff'
        MimeType = ""application/pdf""
    },
    OutputConfig = new OutputConfig
    {
        // How many pages should be grouped into each json output file.
        BatchSize = 2,
        GcsDestination = new GcsDestination
        {
            Uri = $""gs://{gcsDestinationBucketName}/{gcsDestinationPrefixName}""
        }
    }
};

asyncRequest.Features.Add(new Feature
{
    Type = Feature.Types.Type.DocumentTextDetection
});

List&lt;AsyncAnnotateFileRequest&gt; requests =
    new List&lt;AsyncAnnotateFileRequest&gt;();
requests.Add(asyncRequest);

var operation = client.AsyncBatchAnnotateFiles(requests);

Console.WriteLine(""Waiting for the operation to finish"");

operation.PollUntilCompleted();

// Once the rquest has completed and the output has been
// written to GCS, we can list all the output files.
var storageClient = StorageClient.Create();

// List objects with the given prefix.
var blobList = storageClient.ListObjects(gcsDestinationBucketName,
    gcsDestinationPrefixName);
Console.WriteLine(""Output files:"");
foreach (var blob in blobList)
{
    Console.WriteLine(blob.Name);
}

// Process the first output file from GCS.
// Select the first JSON file from the objects in the list.
var output = blobList.Where(x =&gt; x.Name.Contains("".json"")).First();

var jsonString = """";
using (var stream = new MemoryStream())
{
    storageClient.DownloadObject(output, stream);
    jsonString = System.Text.Encoding.UTF8.GetString(stream.ToArray());
}

var response = JsonParser.Default
            .Parse&lt;AnnotateFileResponse&gt;(jsonString);

// The actual response for the first page of the input file.
var firstPageResponses = response.Responses[0];
var annotation = firstPageResponses.FullTextAnnotation;

// Here we print the full text from the first page.
// The response contains more information:
// annotation/pages/blocks/paragraphs/words/symbols
// including confidence scores and bounding boxes
Console.WriteLine($""Full text: \n {annotation.Text}"");

return 0;
}
</code></pre>

<p>this function required  3 parameters 
string gcsSourceUri,
string gcsDestinationBucketName, 
string gcsDestinationPrefixName</p>

<p>I dont understand which value should i set for those 3 params.
I never worked on third party api before so its little bit confusing for me</p>",,0,0,,2019-05-22 12:39:34.390 UTC,,2019-05-22 12:39:34.390 UTC,,,,,10132302,1,0,c#|asp.net|google-cloud-vision|google-cloud-visualstudio,13
Does Google Cloud Vision API support reading handwritten or typed text from pdf/image having comb fields,54008514,Does Google Cloud Vision API support reading handwritten or typed text from pdf/image having comb fields,"<p>I am trying to read the handwritten or typed text from a form having comb fields as shown in the following image</p>

<p><a href=""https://i.stack.imgur.com/F86an.jpg"" rel=""nofollow noreferrer"">test image with comb fields</a>.</p>

<p>I tried using Cloud Vision API to read PDF and Handwriting OCR (with DOCUMENT_TEXT_DETECTION/TEXT_DETECTION type) but it is not returning correct data. The field separator(|) is being read as I
So,
Does Google Cloud Vision API support reading handwritten or typed text from pdf/image having <strong>comb fields</strong>?
Or 
Is there an option to blur or remove the pipes in between the letters before reading the text?</p>",54040503,1,0,,2019-01-02 14:57:35.580 UTC,,2019-01-04 14:06:19.647 UTC,,,,,6459122,1,0,google-cloud-vision,106
Google Vision - How to check if the text is inside an area,52471647,Google Vision - How to check if the text is inside an area,"<p>currently my app is working well with the text detection, using google vision, but it alway display every text caught inside the camera preview, i want to limit the detect zone to a square, what is the solution for it?
or maybe just let the app detect all text, but then the user can take a picture and select which to keep, like Google image translate.
i have tried getboundingbox(), but i dont know what to do next</p>

<p>this is my code for the text detector</p>

<pre><code>textRecognizer.setProcessor(new Detector.Processor&lt;TextBlock&gt;() {
            @Override
            public void release() {

            }

            @Override
            public void receiveDetections(Detector.Detections&lt;TextBlock&gt; detections) {


                final SparseArray&lt;TextBlock&gt; items = detections.getDetectedItems();
                if(items.size()!=0){
                    textView.post(new Runnable() {
                        @Override
                        public void run() {
                            StringBuilder stringBuilder = new StringBuilder();
                            for(int i=0;i&lt;items.size();++i){
                                TextBlock item = items.valueAt(i);
                                stringBuilder.append(item.getValue());
                                stringBuilder.append(""\n"");
                            }
                            textView.setText(stringBuilder.toString());
                        }
                    });
                    textView2.post(new Runnable() {
                        @Override
                        public void run() {
                            StringBuilder stringBuilder2 = new StringBuilder();
                            for(int j=0;j&lt;items.size();j++){
                                TextBlock item2 = items.valueAt(j);
                                //stringBuilder2.append(item2.getCornerPoints());
                                stringBuilder2.append(item2.getBoundingBox());
                                stringBuilder2.append(""\n"");
                            }
                            textView2.setText(stringBuilder2.toString());
                        }
                    });
                }
            }
        });
</code></pre>",,0,0,,2018-09-24 01:16:09.720 UTC,,2018-09-24 01:16:09.720 UTC,,,,,10390096,1,0,java|android,27
Face Api VideoFrameAnalyzer Library: package restore error,51523206,Face Api VideoFrameAnalyzer Library: package restore error,"<p>I am trying to use VideoFrameAnalyzer library from Microsoft Face API product, so I cloned its Github <a href=""https://github.com/Microsoft/Cognitive-Samples-VideoFrameAnalysis"" rel=""nofollow noreferrer"">repo</a> and tried to build BasicConsoleSample as well as LiveCameraSample.</p>

<p>When I tried using Nuget to install opencvsharp, it gives me this error:
An error occurred while tring to restore packages: The specified path, file name, or both are too long. </p>

<p>For those who have got the near real time video face analysis working on Visual Studio 2017, any hint on getting it to work?</p>",,0,0,,2018-07-25 15:57:22.730 UTC,,2018-07-25 15:57:22.730 UTC,,,,,10134487,1,0,azure|face-api,29
Fetch an AWS S3 object to use in Rekognition when uploaded via Carrierwave,45518029,Fetch an AWS S3 object to use in Rekognition when uploaded via Carrierwave,"<p>I have a Gallery and Attachment models. A gallery has_many attachments and essentially all attachments are images referenced in the ':content' attribute of Attachment. </p>

<p>The images are uploaded using <a href=""https://github.com/carrierwaveuploader/carrierwave"" rel=""nofollow noreferrer"">Carrierwave gem</a> and are stored in Aws S3 via <a href=""https://github.com/fog/fog-aws"" rel=""nofollow noreferrer"">fog-aws gem</a>. This works OK. However, I'd like to conduct image recognition to the uploaded images with <a href=""https://aws.amazon.com/rekognition/"" rel=""nofollow noreferrer"">Amazon Rekognition</a>.</p>

<p>I've installed <a href=""https://github.com/aws/aws-sdk-ruby"" rel=""nofollow noreferrer"">aws-sdk gem</a> and I'm able to instantiate Rekognition without a problem until I call the <code>detect_labels</code> method at which point I have been unable to use my attached images as arguments of this method.</p>

<p>So fat I've tried:    </p>

<pre><code>@attachement = Attachment.first
client = Aws::Rekognition::Client.new
resp = client.detect_labels(
         image: @attachment
       )
# I GET expected params[:image] to be a hash... and got class 'Attachment' instead
</code></pre>

<p>I've tried using:</p>

<pre><code>client.detect_labels( image: { @attachment })
client.detect_labels( image: { @attachment.content.url })
client.detect_labels( image: { @attachment.content })
</code></pre>

<p>All with the same error. I wonder how can I fetch the s3 object form  @attachment and, even if I could do that, how could I use it as an argument in <code>detect_labels</code>.  </p>

<p>I've tried also fetching directly the s3 object to try this last bit:</p>

<pre><code>s3 = AWS:S3:Client.new
s3_object = s3.list_objects(bucket: 'my-bucket-name').contents[0]

# and then

client.detect_labels( image: { s3_object })
</code></pre>

<p>Still no success...</p>

<p>Any tips?</p>",45544674,1,0,,2017-08-05 03:58:27.960 UTC,,2017-08-07 10:23:49.303 UTC,,,,,6733104,1,1,ruby-on-rails-3|amazon-web-services|amazon-s3|amazon-rekognition|aws-sdk-ruby,209
Can google vision API detect specific faces?,35748095,Can google vision API detect specific faces?,"<p>Can I use google's vision API to not only detect faces on a specific picture but to detect which person is in the picture ?<br>
Can this be done for celebrities (or ppl which can be easily find via a google search) automatically ? For unfamiliar ppl via some learning/look-alike mechanism ?
Thanks.</p>",35748318,1,0,,2016-03-02 13:06:38.090 UTC,,2016-03-02 13:28:43.887 UTC,,,,,1083305,1,2,google-vision,1439
Access JSON kind of data from a div?,48499254,Access JSON kind of data from a div?,"<p>So I have a div which displays the results from Microsoft emotion API such as:</p>

<pre><code>anger: 0.28446418
contempt: 0.00341128884
disgust: 0.000332433876
fear: 0.009447911
happiness: 0.02609423
neutral: 0.6288482
sadness: 0.00180563633
surprise: 0.04559612
</code></pre>

<p>Here is the code for displaying the data:</p>

<pre><code> .done(function(data) {
        // Get face rectangle dimensions
        var faceRectangle = data[0].faceRectangle;
        var faceRectangleList = $('#faceRectangle');
        var data1="""";

        // Append to DOM
        for (var prop in faceRectangle) {
            data1 += ""&lt;li&gt; "" + prop + "": "" + faceRectangle[prop] + ""&lt;/li&gt;"";
        }
        faceRectangleList.html(data1);

        // Get emotion confidence scores
        var scores = data[0].scores;
        var scoresList = $('#scores');
        var data2="""";
        // Append to DOM
        for(var prop in scores) {
            data2 += ""&lt;li&gt; "" + prop + "": "" + scores[prop] + ""&lt;/li&gt;"";
        }

        scoresList.html(data2);

    }).fail(function(err) {
        alert(""Error: "" + JSON.stringify(err));
    });
});



}

function make_graph(){


}
</code></pre>

<p>Now I need to plot a line from these scores.Can you tell me how to access each separate value of these scores in another function so that I can plot them as points in my graph?</p>",48499675,4,5,,2018-01-29 10:39:20.077 UTC,,2018-01-30 10:33:14.923 UTC,2018-01-29 10:51:14.600 UTC,,9274031,,9274031,1,0,javascript,60
Post file and data to API using Python,19902877,Post file and data to API using Python,"<p>I'm experimenting a bit with an API which can detect faces from an image. I'm using Python and want to be able to upload an image that specifies an argument (in the console). For example:</p>

<pre><code>python detect.py jack.jpg
</code></pre>

<p>This is meant to send file <strong>jack.jpg</strong> up to the API. And afterwards print <strong>JSON</strong> response. Here is the documentation of the API to identify the face.</p>

<p><a href=""http://rekognition.com/developer/docs#facerekognize"" rel=""nofollow"">http://rekognition.com/developer/docs#facerekognize</a></p>

<p>Below is my code, I'm using Python 2.7.4</p>

<pre><code>#!/usr/bin/python

# Imports
import sys
import requests
import json
# Facedetection.py sends us an argument with a filename
filename = (sys.argv[1])
# API-Keys
rekognition_key = """"
rekognition_secret = """"
array = {'api_key': rekognition_key,
                'api_secret': rekognition_secret,
                'jobs': 'face_search',
                'name_space': 'multify',
                'user_id': 'demo',
                'uploaded_file': open(filename)
                }  

endpoint = 'http://rekognition.com/func/api/'
response = requests.post(endpoint, params= array)

data = json.loads(response.content)
print data
</code></pre>

<p>I can see that everything looks fine, but my console gets this output:</p>

<pre><code>Traceback (most recent call last):
  File ""upload.py"", line 23, in &lt;module&gt;
    data = json.loads(response.content)
  File ""/usr/lib/python2.7/json/__init__.py"", line 338, in loads
    return _default_decoder.decode(s)
  File ""/usr/lib/python2.7/json/decoder.py"", line 365, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/lib/python2.7/json/decoder.py"", line 383, in raw_decode
    raise ValueError(""No JSON object could be decoded"")
ValueError: No JSON object could be decoded
</code></pre>

<p>What is wrong?</p>",19903629,1,5,,2013-11-11 09:28:42.770 UTC,,2013-11-11 10:07:28.143 UTC,2013-11-11 09:54:52.033 UTC,,1308651,,1308651,1,2,python|post|python-2.7|python-requests|face-recognition,1433
"Google Cloud Vision Api only return ""name""",55022748,"Google Cloud Vision Api only return ""name""","<p>I am trying to use Google Cloud Vision API.</p>

<p>I am using the REST API in this <a href=""https://cloud.google.com/vision/docs/reference/rest/v1/files/asyncBatchAnnotate?hl=ko&amp;apix_params=%7B%22resource%22%3A%7B%22requests%22%3A%5B%7B%22inputConfig%22%3A%7B%22gcsSource%22%3A%7B%22uri%22%3A%22gs%3A%2F%2Fredaction-vision%2Fpdf_page1_employment_request.pdf%22%7D%2C%22mimeType%22%3A%22application%2Fpdf%22%7D%2C%22features%22%3A%5B%7B%22type%22%3A%22DOCUMENT_TEXT_DETECTION%22%7D%5D%2C%22outputConfig%22%3A%7B%22gcsDestination%22%3A%7B%22uri%22%3A%22gs%3A%2F%2Fredaction-vision%22%7D%7D%7D%5D%7D%7D#GcsSource"" rel=""nofollow noreferrer"">link</a>.</p>

<p><strong>POST <a href=""https://vision.googleapis.com/v1/files:asyncBatchAnnotate"" rel=""nofollow noreferrer"">https://vision.googleapis.com/v1/files:asyncBatchAnnotate</a></strong></p>

<p>My request is</p>

<pre><code>{
    ""requests"": [
        {
            ""inputConfig"": {
                ""gcsSource"": {
                    ""uri"": ""gs://redaction-vision/pdf_page1_employment_request.pdf""
                },
                ""mimeType"": ""application/pdf""
            },
            ""features"": [
                {
                    ""type"": ""DOCUMENT_TEXT_DETECTION""
                }
            ],
            ""outputConfig"": {
                ""gcsDestination"": {
                    ""uri"": ""gs://redaction-vision""
                }
            }
        }
    ]
}
</code></pre>

<p>But the response is always only ""name"" like below:</p>

<pre><code>{
    ""name"": ""operations/a7e4e40d1e1ac4c5""
}
</code></pre>

<p>My ""gs"" location is valid.
When I write the wrong path in ""gcsSource"", 404 not found error is coming.
Who knows why my response is weird?</p>",55023169,2,0,,2019-03-06 12:05:43.470 UTC,,2019-03-15 18:14:54.277 UTC,,,,,1936853,1,0,google-cloud-platform|google-cloud-vision|vision-api,81
Google Cloud Video intelligence and embedded video content,48834479,Google Cloud Video intelligence and embedded video content,"<p>Does Google Cloud Video intelligence work with embedded video content from Vimeo or YouTube. Will it be able to create tags, see faces etc... since the content is not directly uploaded?</p>",,1,0,,2018-02-16 20:34:19.283 UTC,,2018-02-22 16:20:30.720 UTC,,,,,9371444,1,0,youtube|google-cloud-platform|artificial-intelligence|vimeo|video-intelligence-api,163
Google Vision API,41923595,Google Vision API,"<p>I'm trying to post data to Google Vision API. Butt i always get 400 response code - bad request. I have no ideas already.</p>

<p>I have read and tried to use those links:</p>

<p><a href=""https://cloud.google.com/vision/docs/requests-and-responses"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/requests-and-responses</a>
<a href=""https://cloud.google.com/vision/docs/detecting-text"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/detecting-text</a>
<a href=""https://developers.google.com/apis-explorer/?hl=ru#p/vision/v1/vision.images.annotate"" rel=""nofollow noreferrer"">https://developers.google.com/apis-explorer/?hl=ru#p/vision/v1/vision.images.annotate</a></p>

<p>And i came up to this:</p>

<p>Here is my data to post:</p>

<pre><code>var dataToSend = {
        ""requests"": [
           {
               ""image"": {
                   ""content"": imageData
               },
               ""features"": [
                  {
                      ""type"": ""TYPE_UNSPECIFIED"",
                      ""maxResults"": 50
                  },
                  {
                      ""type"": ""LANDMARK_DETECTION"",
                      ""maxResults"": 50
                  },
                  {
                      ""type"": ""FACE_DETECTION"",
                      ""maxResults"": 50
                  },
                  {
                      ""type"": ""LOGO_DETECTION"",
                      ""maxResults"": 50
                  },
                  {
                      ""type"": ""LABEL_DETECTION"",
                      ""maxResults"": 50
                  },
                  {
                      ""type"": ""TEXT_DETECTION"",
                      ""maxResults"": 50
                  },
                  {
                      ""type"": ""SAFE_SEARCH_DETECTION"",
                      ""maxResults"": 50
                  },
                  {
                      ""type"": ""IMAGE_PROPERTIES"",
                      ""maxResults"": 50
                  }
               ]
           }
        ]
    };
</code></pre>

<p>And here is my post:</p>

<pre><code>$.ajax({
        url: ""https://vision.googleapis.com/v1/images:annotate?fields=responses&amp;key={MY CREATED KEY}"",
        type: ""POST"",
        data: dataToSend,
        success: function (reponse) {
            console.log(reponse);
        },
    });
</code></pre>

<p>Here is data from console (<strong>THE CONTENT IS BLANK FOR EXAMPLE</strong> ( not to post wole base64 )):</p>

<p><a href=""https://i.stack.imgur.com/GMZEy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GMZEy.png"" alt=""enter image description here""></a></p>

<p>And here is the response:</p>

<pre><code>{
  ""error"": {
    ""code"": 400,
    ""message"": ""Invalid JSON payload received. Unknown name \""requests[0][features][0][type]\"": Cannot bind query parameter. Field 'requests[0][features][0][type]' could not be found in request message.\nInvalid JSON payload received. Unknown name \""requests[0][features][0][maxResults]\"": Cannot bind query parameter. Field 'requests[0][features][0][maxResults]' could not be found in request message.\nInvalid JSON payload received. Unknown name \""requests[0][image][content]\"": Cannot bind query parameter. Field 'requests[0][image][content]' could not be found in request message."",
    ""status"": ""INVALID_ARGUMENT"",
    ""details"": [
      {
        ""@type"": ""type.googleapis.com/google.rpc.BadRequest"",
        ""fieldViolations"": [
          {
            ""description"": ""Invalid JSON payload received. Unknown name \""requests[0][features][0][type]\"": Cannot bind query parameter. Field 'requests[0][features][0][type]' could not be found in request message.""
          },
          {
            ""description"": ""Invalid JSON payload received. Unknown name \""requests[0][features][0][maxResults]\"": Cannot bind query parameter. Field 'requests[0][features][0][maxResults]' could not be found in request message.""
          },
          {
            ""description"": ""Invalid JSON payload received. Unknown name \""requests[0][image][content]\"": Cannot bind query parameter. Field 'requests[0][image][content]' could not be found in request message.""
          }
        ]
      }
    ]
  }
}
</code></pre>

<p><strong>Where is my mistake here?</strong></p>",42143495,3,1,,2017-01-29 17:07:34.437 UTC,1,2017-09-09 05:14:57.860 UTC,2017-01-30 17:37:28.113 UTC,,2235287,,2235287,1,0,javascript|json|google-api|google-cloud-platform|google-cloud-vision,1268
Extracting structured data from any retail store receipt (Azure Computer Vision API OCR),50174426,Extracting structured data from any retail store receipt (Azure Computer Vision API OCR),"<p>How can we extract structured data ( Merchant , Purchase Date , Tax , Total etc . ) from the text generated by Azure Computer Vision API OCR after scanning the  any retail store receipt . Thanks</p>",,1,0,,2018-05-04 11:59:44.287 UTC,,2018-05-04 14:50:44.983 UTC,2018-05-04 14:50:44.983 UTC,,4685471,,3116069,1,0,azure|machine-learning|computer-vision|ocr|vision-api,1256
How to set up AWS mobile SDK in iOS project in Xcode,52046473,How to set up AWS mobile SDK in iOS project in Xcode,"<p>The aws docs for this are really confusing. Following the steps from here <a href=""https://docs.aws.amazon.com/aws-mobile/latest/developerguide/getting-started.html#ios-swift"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/aws-mobile/latest/developerguide/getting-started.html#ios-swift</a> , I created the awsconfiguration.json using amplify, but it seems to be empty, it looks like this:</p>

<pre><code>   {
    ""UserAgent"": ""aws-amplify/cli"",
    ""Version"": ""0.1.0"",
    ""IdentityManager"": {
        ""Default"": {}
    }
}
</code></pre>

<p>I dragged that json into the root of my xcode project, but when I run the project trying to call an aws api (specifically rekognition), I get this error:  </p>

<blockquote>
  <p>Terminating app due to uncaught exception
  'NSInternalInconsistencyException', reason: 'The service configuration
  is <code>nil</code>. You need to configure <code>awsconfiguration.json</code>, <code>Info.plist</code>
  or set <code>defaultServiceConfiguration</code> before using this method.'</p>
</blockquote>

<p>I don't know if that's because the json isn't being read properly, or because it's empty, or what. This whole setup just seems to be a mess. </p>",,1,0,,2018-08-27 20:21:50.093 UTC,,2018-09-15 20:38:43.383 UTC,2018-08-27 20:38:34.063 UTC,,8594124,,8594124,1,0,swift|xcode|amazon-web-services|aws-mobilehub|amazon-rekognition,141
Google Vision - OCR - Request must specify image and features,46731393,Google Vision - OCR - Request must specify image and features,"<p>I am trying to Implement Google Vision OCR Request. Here is My Code,</p>

<pre><code> func performImageRecognition(image: UIImage){

    //1. Convert Image into base64 encoding
    let imageData: Data = UIImageJPEGRepresentation(image, 1.0)!
    let encodedString: String = imageData.base64EncodedString()

    //2. Request Body for Vision OCR
    let postBody: [String: Any] = getPOSTBody(base64: encodedString)

    //3. API Call
    AppDelegate.makeRequest(url: Request.url, requestBody: postBody, completionHandler: {
        data, response, error in
        print(error!)

        do{
            let dictionary = try JSONSerialization.jsonObject(with: data!, options: [])
            print(dictionary)
            self.activityindicator.stopAnimating()
        }catch{
            print(""Error Parsing Data: \(error)"" )
        }

    })

}

/*
 * Request Body
 */
func getPOSTBody(base64: String) -&gt; [String: Any]{

    let json: [String: Any] = [
        ""requests"": [[""image"": [""content"": base64]],
                     [""features"": [[""type"": ""TEXT_DETECTION""]]]
                    ]
    ]

    return json
}
</code></pre>

<p>Request Handler</p>

<pre><code>class func makeRequest(url: URL, requestBody: [String: Any],completionHandler: @escaping (Data?, Int?, String?) -&gt; Void){

    var requestData: Data!
    var urlRequest = URLRequest(url: url, cachePolicy: .reloadIgnoringLocalCacheData, timeoutInterval: 60)

    // 1. Serialize the request body to Data
    do{
        requestData = try JSONSerialization.data(withJSONObject: requestBody, options: [])
    }catch{
        print(""ERROR:: Generating data from JSON Body : \(error) "")
    }

    // 2. Setting up the required Header Fields
    urlRequest.httpBody = requestData
    urlRequest.addValue(""\(requestData.count)"", forHTTPHeaderField: ""Content-Length"")
    urlRequest.addValue(""application/json; charset=UTF-8"", forHTTPHeaderField: ""Content-Type"")
    urlRequest.httpMethod = ""POST""

    // 3. Creating the Session
    let session = URLSession(configuration: .default)
    let dataTask: URLSessionDataTask = session.dataTask(with: urlRequest, completionHandler: {
        data, response, error in

        if (error != nil){
            print(""Error is: \(error?.localizedDescription ?? ""None"")"")
            return
        }

        let resp = response as? HTTPURLResponse

        DispatchQueue.main.async {
            completionHandler(data, resp?.statusCode ?? 0, error?.localizedDescription ?? ""None"")
        }

    })

    dataTask.resume()
}
</code></pre>

<p>Problem is getting ""Bad Request, 400 Status, Request must specify image and features."". </p>

<p>I've Checked the Request body for <code>isValidJSONObject</code>, getting true. API is working fine on Postman.
Please let me know if i am missing something, Any Help will be appreciated.</p>

<p>Thank You</p>",46743159,1,0,,2017-10-13 13:43:09.990 UTC,,2017-10-14 09:48:51.560 UTC,2017-10-14 02:41:21.007 UTC,,8675237,,8675237,1,2,ios|swift4,1115
React-Native-Camera only works every now and then,53135337,React-Native-Camera only works every now and then,"<p>I'm having trouble setting up the react-native-camera, when I create a new account on the app it lets me take pictures sometimes, but it doesn't get sent to the server even though I have the route set up, I've tried logging the Data that the image generates and i get the base64 and uri as it should. 
The catch is that it only works sometimes and even then, only when i create a new account in the app, it's also not sending the image to the server. </p>

<p>Here's how the Camera.js file is written:</p>

<hr>

<pre><code>class Camera extends Component {
  constructor(props) {
    super(props);
    this.state = {
      userId: this.props.navigation.state.params
    };
  }

  render() {
    return (
      &lt;View style={styles.container}&gt;
        &lt;RNCamera
          ref={ref =&gt; {
            this.camera = ref;
          }}
          style={styles.preview}
          type={RNCamera.Constants.Type.back}
          flashMode={RNCamera.Constants.FlashMode.on}
          permissionDialogTitle={""Permissao para usar a camera""}
          permissionDialogMessage={
            ""Precisamos da sua permissao para utilizar a camera do dispositivo""
          }
          onGoogleVisionBarcodesDetected={({ barcodes }) =&gt; {
            console.log(barcodes);
          }}
        /&gt;
        &lt;View
          style={{ flex: 0, flexDirection: ""row"", justifyContent: ""center"" }}
        &gt;
          &lt;TouchableOpacity
            onPress={this.takePicture.bind(this)}
            style={styles.capture}
          &gt;
            &lt;Text style={{ fontSize: 14 }}&gt;
              &lt;Icon
                name=""camera""
                size={20}
                color=""#669999""
                paddingHorizontal={5}
              /&gt;
              FOTO
            &lt;/Text&gt;
          &lt;/TouchableOpacity&gt;
        &lt;/View&gt;
      &lt;/View&gt;
    );
  }

  takePicture = async function() {
    if (this.camera) {
      const options = { quality: 0.5, base64: true };
      const imageData = await this.camera.takePictureAsync(options);
      const turbo = Turbo({ site_id: ""5bd242567d324e00130545a1"" });
      console.log(imageData);
      const cdnResp = await turbo.uploadFile({
        uri: imageData.uri,
        name: ""camera_pic"",
        type: ""image/jpeg""
      });
      const resp = await fetch(
        config.baseUrl + ""/users/"" + this.state.userId + ""/photo"",
        {
          method: ""POST"",
          headers: {
            Accept: ""application/json"",
            ""Content-Type"": ""application/json""
          },
          body: JSON.stringify({ imageUrl: cdnResp.result.url })
        }
      );
      console.log(resp);
    }
  };
}
</code></pre>

<p>Does anyone know why it only works every now and then?
Here's the route that the camera.js uses in the api:</p>

<pre><code>router.get(""/users/:id/photo"", function(req, res) {
    const userId = req.params.id;
    console.log(req.params);
    const myPhoto = {
      url: req.body.imageUrl,
      user: req.params.id
    };
    turbo
      .create(""photo"", myPhoto)
      .then(data =&gt; {
        res.json({
          confirmation: ""success"",
          data: data
        });
        return;
      })
      .catch(err =&gt; {
        res.json({
          confirmation: ""fail"",
          message: ""Sorry, something went wrong""
        });
        return;
      });
  });
</code></pre>

<p>It was supposed to create a new resource in Turbo named photo, but the picture doesn't get there, does anyone know how to fix this or give me a direction?</p>",,0,0,,2018-11-03 20:40:50.117 UTC,,2018-11-03 20:40:50.117 UTC,,,,,9819884,1,0,javascript|android|react-native,139
TypeError: Cannot read property 'detectedLanguages' of null,54292200,TypeError: Cannot read property 'detectedLanguages' of null,"<p><strong>TLDR</strong>: How does one catch runtime errors that occur with the detectedLanguages() google vision call?</p>

<p>Certain images that we provide to the Cloud Vision API appear to crash the Vision API Client in nodejs.</p>

<p>Full details of the crash are as follows:</p>

<pre><code>/myapp/node_modules/@google-cloud/vision/src/index.js:1971
        languages: page.property.detectedLanguages.map(prop('languageCode')),
                                 ^

TypeError: Cannot read property 'detectedLanguages' of null
    at /myapp/node_modules/@google-cloud/vision/src/index.js:1971:34
    at Array.map (&lt;anonymous&gt;)
    at Vision.formatFullTextAnnotation_ (/myapp/MarlboroScanner-Node.js/node_modules/@google-cloud/vision/src/index.js:1969:6)
    at /myapp/node_modules/@google-cloud/vision/src/index.js:597:18
    at Array.map (&lt;anonymous&gt;)
    at decorateAnnotations (/myapp/node_modules/@google-cloud/vision/src/index.js:607:16)
    at Array.map (&lt;anonymous&gt;)
    at /myapp/node_modules/@google-cloud/vision/src/index.js:431:10
    at /myapp/node_modules/@google-cloud/vision/src/index.js:126:5
    at _combinedTickCallback (internal/process/next_tick.js:138:11)
</code></pre>

<p>We are using the readDocument call with a callback function.</p>

<pre><code>const vision = require('@google-cloud/vision')({
  projectId: config.PROJECT_ID,
  keyFilename: config.SERVICE_ACCT_CONFIG
});

vision.readDocument(fileData, detectOptions, function (docErr, docText, docApiResponse) { 
  print docErr;
  print docText;
  print docApiResponse;
});
</code></pre>

<p>The callback can be completely empty (so it isn't the callback that is causing the crash).</p>

<p>Since this appears to be related to the library I can't really debug this issue. How does one catch an error like this? I have tried surrounding it with try-catch, I have tried appending a .catch() call to the end of the function call. Neither work. Any advice?</p>

<p>An example image that is causing the crash is: <a href=""https://i.stack.imgur.com/l2a8C.jpg"" rel=""nofollow noreferrer"">here</a></p>",,0,0,,2019-01-21 14:36:13.273 UTC,,2019-01-21 15:11:48.177 UTC,2019-01-21 15:11:48.177 UTC,,10945259,,10945259,1,0,node.js|crash|google-vision,24
How can I get the confidence of comparing faces using amazon rekognition with php?,47779841,How can I get the confidence of comparing faces using amazon rekognition with php?,"<p>I am using Amazon rekognition to compare faces in php (AWS SDK for php
<a href=""http://docs.aws.amazon.com/aws-sdk-php/v3/api/api-rekognition-2016-06-27.html#comparefaces"" rel=""nofollow noreferrer"">Here</a> the documentation.</p>

<p>The code below</p>

<pre><code>&lt;?php
require 'aws.phar';

$s3 = new \Aws\Rekognition\RekognitionClient([
'version' =&gt; 'latest',
'region'  =&gt; 'us-east-1',
]);

$result = $s3-&gt;compareFaces([
    'SimilarityThreshold' =&gt; 90,
    'SourceImage' =&gt; [
          'Bytes' =&gt; file_get_contents(""https://images.clarin.com/2017/10/23/BkeLV_s6W_930x525.jpg"")        
    ],
    'TargetImage' =&gt; [
          'Bytes' =&gt; file_get_contents(""https://pbs.twimg.com/profile_images/653558348273569792/joxg8DZD_400x400.png"")
    ],
]);

?&gt;
</code></pre>

<p>I don't know php. 
How can I get the confidence of the picture?
I try some things but I cant get it.</p>",,1,0,,2017-12-12 19:06:06.137 UTC,,2018-01-07 09:41:14.463 UTC,,,,,7442761,1,0,php|amazon-rekognition,644
C# POST JSON Request to Google Vision,49652382,C# POST JSON Request to Google Vision,"<p>Being locked on .NET 2.0, i can't use the GOOGLE VISION C# API which is only available since.NET 4.0.</p>

<p>So I wanted to use this API with web request like this :</p>

<pre><code>string webAddr = ""https://vision.googleapis.com/v1/images:annotate?key=XXXXXXXXXXX"";

var httpWebRequest = (HttpWebRequest)WebRequest.Create(webAddr);
httpWebRequest.ContentType = ""application/json; charset=utf-8"";
httpWebRequest.Method = ""POST"";
using (var streamWriter = new StreamWriter(httpWebRequest.GetRequestStream()))
{
    string json = "" { \""requests\"": [ { \""image\"": { \""content\"": "" + this.b64 + "" }, \""features\"": [ {\""type\"": \""TEXT_DETECTION\"" }] }] }"";

    streamWriter.Write(json);
    streamWriter.Flush();
}
var httpResponse = (HttpWebResponse)httpWebRequest.GetResponse();
using (var streamReader = new StreamReader(httpResponse.GetResponseStream()))
{
    var responseText = streamReader.ReadToEnd();
    Console.WriteLine(responseText); 
}
</code></pre>

<p>The problem is that i permanently have a 400 return from google and after a lot of searching i can't find a solution.</p>

<p>Can you give me a way to proceed to solve this problem please ?
Thank you very much.</p>

<p>(Sorry for bad english...)</p>",50401541,1,2,,2018-04-04 13:32:22.497 UTC,0,2018-05-17 22:52:39.323 UTC,2018-04-04 13:52:12.710 UTC,,2541423,,8295735,1,2,c#|post,406
"Google Cloud Vision API (labels), how many possible labels are there?",49123296,"Google Cloud Vision API (labels), how many possible labels are there?","<p>I have been searching for this for hours.. but still cannot get a proper answer.....</p>

<p>Where can I get a full list of labels that Google Cloud Vision API can possibly produce ?</p>

<p>example of google api result ) 
<a href=""https://i.stack.imgur.com/Vnwut.png"" rel=""nofollow noreferrer"">img</a> </p>

<p>Thanks!</p>",49189848,1,0,,2018-03-06 04:16:18.240 UTC,,2018-03-09 08:50:58.007 UTC,2018-03-06 06:22:46.740 UTC,,9378581,,9378581,1,0,machine-learning|google-api|artificial-intelligence|google-cloud-vision|vision-api,346
Reading key/value pairs with Google Cloud Vision + Natural Language = Possible?,49527406,Reading key/value pairs with Google Cloud Vision + Natural Language = Possible?,"<p>I'm currently doing a spike for a project and was hoping the community may be able to shed some light on things.</p>

<p>I would like to use Google Cloud Vision to scan the below image and then derive the key/value pairs from it (such as Title: Ground Rod..., Last Revision: June 27, 2012). This is a basic example, it could have much more data and the layout may be different to this.</p>

<p>Since there is no easy correlation between the key/values i'm not sure if this possible? Is it possible to train the google vision with example images? Or are there any other solutions that may be able to do this?</p>

<p>Thank you!</p>

<p><a href=""https://i.stack.imgur.com/kfK8Y.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kfK8Y.jpg"" alt=""example image""></a></p>",,1,0,,2018-03-28 06:19:00.727 UTC,,2018-03-28 16:03:54.987 UTC,,,,,2885993,1,1,ocr|google-cloud-vision,439
View stays black when using multiple DecoratedBarcodeViews,48924950,View stays black when using multiple DecoratedBarcodeViews,"<p>I am trying to use a QR Code reader in multiple tabs. After having problems with Google Vision API i tried to switch to zxing. First i tried to use the library <a href=""https://github.com/journeyapps/zxing-android-embedded"" rel=""nofollow noreferrer"">zxing-android-embedded</a>.</p>

<p>I tried their tabbed sample which contains a barcodereader and a cameraview. If I replace the cameraview with an additional barcodereader the view in the first tab stays black.</p>

<p><a href=""https://i.stack.imgur.com/vdDIpm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vdDIpm.png"" alt=""black cameraview""></a></p>

<p>I used two <code>ScanFragments</code> in the <code>SectionsPagerAdapter</code> in <code>TabbedScanning.java</code>:</p>

<pre><code>@Override
public android.support.v4.app.Fragment getItem(int position) {
    if(position == 0) {
        return ScanFragment.newInstance();
    } else {
        return ScanFragment.newInstance();
    }
}
</code></pre>

<p>After switching tab or changing screen orientation everything works fine but before the first tab stays black.</p>

<p><a href=""https://i.stack.imgur.com/PeLNom.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PeLNom.jpg"" alt=""working cameraview""></a></p>

<p>I also found the following error in the logfile which i don't know how i can resolve this.</p>

<pre><code>02-22 09:37:42.164 20935-20974/example.zxing E/CameraInstance: Failed to configure camera
    java.lang.RuntimeException: getParameters failed (empty parameters)
    at android.hardware.Camera.native_getParameters(Native Method)
    at android.hardware.Camera.getParameters(Camera.java:3099)
    at com.journeyapps.barcodescanner.camera.CameraManager.setParameters(CameraManager.java:379)
    at com.journeyapps.barcodescanner.camera.CameraManager.configure(CameraManager.java:159)
    at com.journeyapps.barcodescanner.camera.CameraInstance$4.run(CameraInstance.java:203)
    at android.os.Handler.handleCallback(Handler.java:836)
    at android.os.Handler.dispatchMessage(Handler.java:103)
    at android.os.Looper.loop(Looper.java:203)
    at android.os.HandlerThread.run(HandlerThread.java:61)
</code></pre>

<p>What can i do that the view doesn't stay black and shows a working cameraview?</p>",48929569,1,0,,2018-02-22 10:22:38.993 UTC,,2018-02-22 14:13:35.133 UTC,2018-02-22 10:43:03.327 UTC,,5268730,,5268730,1,2,android|android-fragments|android-camera|qr-code|zxing,134
Getting InvalidImageFormatException as I try pass image to AWS Rekognition,49631726,Getting InvalidImageFormatException as I try pass image to AWS Rekognition,"<p>I get <code>InvalidImageFormatException</code> as I am trying to pass an image to <code>AWS Rekognition</code>. I make an HTTP request to an <a href=""http://www.xsjjys.com/data/out/43/WHDQ-511895361.jpg"" rel=""nofollow noreferrer"">Image URL</a>, download the image and pass the buffer to the aws function.</p>

<p>Here is the code for it:</p>

<pre><code>const AWS = require('aws-sdk');
const axios = require('axios');
let rekognition = new AWS.Rekognition({apiVersion: '2016-06-27', region: 'us-east-1'});

let url = 'http://www.xsjjys.com/data/out/43/WHDQ-511895361.jpg'

axios({
    method: 'GET',
    url,
    responseType: 'arraybuffer'
})
    .then(response =&gt; {
            console.log(response)
            let params = {
                    Image: {
                            Bytes: response.data
                    },
                    Attributes: ['ALL']
            };

            rekognition.detectFaces(params, (err, data) =&gt; {
                    if(err) console.log(err);
                    else console.log(JSON.stringify(data));
            });
    })
    .catch(err =&gt; console.log(err));
</code></pre>

<p>I do not understand what could be the reason for this error. It works for other images but throws an error for some of the images. What could be the reason for this?</p>

<p>Here is the trace for this:</p>

<pre><code>{ InvalidImageFormatException: Invalid image encoding
        at Request.extractError (/home/suhail/nodejs/test/node_modules/aws-sdk/lib/protocol/json.js:48:27)
        at Request.callListeners (/home/suhail/nodejs/test/node_modules/aws-sdk/lib/sequential_executor.js:105:20)
        at Request.emit (/home/suhail/nodejs/test/node_modules/aws-sdk/lib/sequential_executor.js:77:10)
        at Request.emit (/home/suhail/nodejs/test/node_modules/aws-sdk/lib/request.js:683:14)
        at Request.transition (/home/suhail/nodejs/test/node_modules/aws-sdk/lib/request.js:22:10)
        at AcceptorStateMachine.runTo (/home/suhail/nodejs/test/node_modules/aws-sdk/lib/state_machine.js:14:12)
        at /home/suhail/nodejs/test/node_modules/aws-sdk/lib/state_machine.js:26:10
        at Request.&lt;anonymous&gt; (/home/suhail/nodejs/test/node_modules/aws-sdk/lib/request.js:38:9)
        at Request.&lt;anonymous&gt; (/home/suhail/nodejs/test/node_modules/aws-sdk/lib/request.js:685:12)
        at Request.callListeners (/home/suhail/nodejs/test/node_modules/aws-sdk/lib/sequential_executor.js:115:18)
    message: 'Invalid image encoding',
    code: 'InvalidImageFormatException',
    time: 2018-04-03T13:28:43.404Z,
    requestId: 'edd8fccf-3742-11e8-95a3-c58fca411044',
    statusCode: 400,
    retryable: false,
    retryDelay: 55.96279161227613 }
</code></pre>",49632085,1,0,,2018-04-03 13:45:46.460 UTC,,2018-04-03 14:02:15.023 UTC,,,,,9515643,1,0,node.js|amazon-web-services|axios|amazon-rekognition,188
How to submit in-memory images to Visual Recognition using Python,55894824,How to submit in-memory images to Visual Recognition using Python,"<p>I'm working for the first time with IBM Watson Visual Recognition. My Python app needs to pass images that it's managing in memory to the service. However, the rather limited documentation and sample code I've been able to find from IBM shows calls to the API as referencing saved files. The file is passed to the call as an io.BufferedReader.</p>

<pre><code>with open(car_path, 'rb') as images_file:
    car_results = service.classify(
        images_file=images_file,
        threshold='0.1',
        classifier_ids=['default']
    ).get_result()
</code></pre>

<p>My application will be working with images from memory and I don't want to have to save every image to file before I can make a call. I tried replacing the BufferedReader with an io.BytesIO stream, and I got back an error saying I was missing an images_filename param. When I added a mock filename (e.g. 'xyz123.jpg') I get back the following error:</p>

<pre><code>TypeError: a bytes-like object is required, not 'float'
</code></pre>

<p>Can I make calls to the analysis API using an image from memory? If so, how?</p>

<p>EDIT:</p>

<p>This is essentially what I'm trying to do:</p>

<pre><code>def analyze_image(pillow_img: PIL.Image):
    byte_stream = io.BytesIO()
    pillow_img.save(byte_stream, format='JPEG')
    bytes_img = byte_stream.getvalue()

    watson_vr = VisualRecognitionV3(
        '2019-04-30',
        url='https://gateway.watsonplatform.net/visual-recognition/api',
        iam_apikey='&lt;API KEY&gt;'
    )

    result_json = watson_vr.classify(
        images_file=bytes_img,
        threshold=0.1,
        classifier_ids=['default']
    ).get_result()
</code></pre>

<p>Thanks</p>",,1,2,,2019-04-28 21:43:40.467 UTC,,2019-05-02 16:15:19.393 UTC,2019-04-30 16:53:20.237 UTC,,11406422,,11406422,1,0,python|ibm-watson|visual-recognition,58
How to transfer video from kinesis video stream to AWS Rekognition and perform data analytics on that video?,56256801,How to transfer video from kinesis video stream to AWS Rekognition and perform data analytics on that video?,<p>I am streaming video from raspberry pi to amazon kinesis video stream (this part is done). Now i want to send video to AWS Rekognition and perform face detection on the live video. Kindly answer in detail and with links. Thankyou!</p>,,0,0,,2019-05-22 12:26:59.190 UTC,,2019-05-22 12:26:59.190 UTC,,,,,11438404,1,0,amazon-web-services|amazon-kinesis|amazon-rekognition,13
Custom object detection,55744464,Custom object detection,<p>Does Amazon has something similar to the Azure Custom Vision service where you easily can define your own custom objects? Like Coca Cola brands or what ever you would like to detect? </p>,,0,0,,2019-04-18 10:49:35.447 UTC,,2019-04-18 10:49:35.447 UTC,,,,,1406168,1,-1,amazon-rekognition,8
IBM Watson Classifier ID and multiple classifying,36977715,IBM Watson Classifier ID and multiple classifying,"<p>With the IBM Watson Visual Recognition API, how do you know the Classifier ID when we create the train? How do you do multiple classifying?</p>",,1,3,,2016-05-02 08:00:58.720 UTC,,2016-11-07 07:08:50.280 UTC,2016-05-09 21:32:16.327 UTC,,3198917,,6238579,1,0,node.js|ibm-cloud|ibm-watson|visual-recognition,410
Uploading and saving files in server through flask gives error,49732664,Uploading and saving files in server through flask gives error,"<p>So, I'm trying to make a flask mini-app that has an upload button which will upload images and then save that image in another folder named ""upimgs"". We need to do some image processing operation on the uploaded image later using google cloud vision api. The code is :</p>

<pre><code>app = Flask(__name__)
@app.route('/')
def upload_file():
    return '''&lt;html&gt;&lt;h1&gt;Upload a file&lt;/h1&gt;
            &lt;body&gt;&lt;form action = ""http://35.231.238.112:8085/upload"" method = ""POST"" enctype = ""multipart/form-data""&gt;
            &lt;input type=""file"" name=""file""/&gt;&lt;input type=""submit""/&gt;
            &lt;/form&gt;
            &lt;/body&gt;&lt;/html&gt; '''

@app.route('/upload',methods=[""GET"",""POST""])
def upload_lnk():
    if request.method == 'POST':
            f = request.files['file']
            f.save(os.path.join(app.config['/upimgs/'], filename))
            cmd2 = ""python pj2new.py upmigs/"" + f.filename
            cmd2 = str(cmd2)
            #some more code to run that output in flask browser
</code></pre>

<p>However it shows the following error :</p>

<pre><code>KeyError: '/upimgs/'
</code></pre>

<p>Where is the probable error? I looked up to this : <a href=""https://stackoverflow.com/questions/30328586/refering-to-a-directory-in-a-flask-app-doesnt-work-unless-the-path-is-absolute"">Refering to a directory in a Flask app doesn&#39;t work unless the path is absolute</a> However the problem is not similar. I'm facing issue with uploading images in the server whether that one have file path issue.</p>",49733552,1,3,,2018-04-09 12:10:53.380 UTC,,2018-04-10 06:03:54.543 UTC,2018-04-10 06:03:54.543 UTC,,5770501,,5770501,1,-1,python|python-3.x|flask|flask-restful,1547
Face Recognition by using IBM Watson Visual Recognition,40843164,Face Recognition by using IBM Watson Visual Recognition,"<p>I am currently evaluating capabilities of IBM Watson Visual Recognition service to recognize faces. So that System should identify the each person that we have trained. Individuals may come with different clothes, and other possible variations. But system should identify each individual by looking at each face. </p>

<p>As per IBM, IBM visual recognition do not support face recognition but only face detection.</p>

<blockquote>
  <p>Face Recognition: Visual Recognition is capable of face detection
  (detecting the presence of faces) not face recognition (identifying
  individuals).</p>
</blockquote>

<p>Can we use the custom classifiers by adding different types of images for each individuals? </p>

<p>What is the significant pre/post-work from the developer to get at least 90% accuracy ? </p>",40854338,1,0,,2016-11-28 11:24:48.053 UTC,1,2016-11-28 22:03:52.553 UTC,,,,,1170843,1,1,ibm-cloud|ibm-watson|visual-recognition,938
Google Vision Color extraction,55194095,Google Vision Color extraction,"<p>I am trying to read the image properties from links stored in a csv with Google Vision and write the results back to a new column.</p>

<p>With an adapted script of a friend of mine, I managed to read the csv, to download the picture, send it to Google, to retrieve the results - but not to write them back to the csv. </p>

<p>Do you have any suggestions?</p>

<pre><code>#!/usr/bin/python
import sys
import csv
import os
import urllib
import io
from google.cloud import vision
#from google.cloud.vision import types

os.environ[""GOOGLE_APPLICATION_CREDENTIALS""] = ""./CREDENTIALS/tourism-219409-61b527f98297.json""
vision_client = vision.ImageAnnotatorClient()


# run with ""python start.py &lt;csv-file-path&gt; &lt;csv-header-row-url&gt; &lt;image path&gt;""
# example
# python start.py ./destinations2.csv fotolink ./images

# ------------------
#   FUNCTIONS
# ------------------
def printInLine(data):
    sys.stdout.write(data)


def getCsvlHeaderIndexOfUrl(csvPath, csvHeaderRowUrl, delimit):
    index = None
    with open(csvPath, 'r', encoding='UTF8') as csvfile:
        reader = csv.reader(csvfile, delimiter=delimit)
        interestingrows = [row for idx, row in enumerate(reader) if idx in (0, 0)]
        for row in interestingrows:
            for i, name in enumerate(row):
                if (name == csvHeaderRowUrl):
                    index = i
        print(interestingrows)                    
        print('found: ' + csvHeaderRowUrl + ' @ position: ' + str(index))
        return index


def getCsvlValueByHeaderindexAndLinenumber(csvPath, headerUrlIndex, lineNumber, delimit):
    print('headerUrlIndex')
    print(headerUrlIndex)
    print('---')
    value = None
    with open(csvPath, 'r', encoding='UTF8') as csvfile:
        reader = csv.reader(csvfile, delimiter=delimit)
        interestingrows = [row for idx, row in enumerate(reader) if idx in (lineNumber, lineNumber)]
        print(interestingrows)
        value = interestingrows[0][headerUrlIndex]

        print('found: ' + value + ' @ lineNumber: ' + str(lineNumber))
        return value


def saveLabelsBackToCsvByIndex(csvPath, lineNumber, labelString):
    # print('labelString: ' + labelString + str(lineNumber))
    # with is like your try .. finally block in this case
    with open(csvPath, 'r', encoding='UTF8') as file:
        # read a list of lines into data
        data = file.readlines()
        line = data[lineNumber].strip().replace('\""', '') + labelString + '\n'
        # cleanup string
        # line = line.replace(';', '')


        # print (""Result: "" + line)
        data[lineNumber] = line

        # write everything back
        with open(csvPath, 'w', encoding='UTF8') as file:
            file.writelines(data)
        print(' --&gt; ' + labelString)


def downloadImage(url, imageStorePath, index):
    cleanUrl = url.replace('\""', '')

    filename, file_extension = os.path.splitext(cleanUrl)
    image = imageStorePath + '/' + str(index) + file_extension
    print(' --&gt; ' + image)
    try:
        urllib.request.urlretrieve(cleanUrl, image)
        return image
    except Exception:
        print('ERROR: downloadImage --&gt;', url)
        pass
        # urllib.request.urlretrieve(cleanUrl, image)
        return None


def sendImageToGoogleVision(path):

    """"""Detects image properties in the file.""""""
    from google.cloud import vision
    client = vision.ImageAnnotatorClient()

    with io.open(path, 'rb') as image_file:
        content = image_file.read()

    image = vision.types.Image(content=content)

    response = client.image_properties(image=image)
    props = response.image_properties_annotation
    print('Properties:')

    for color in props.dominant_colors.colors:
        print('fraction: {}'.format(color.pixel_fraction))
        print('\tr: {}'.format(color.color.red))
        print('\tg: {}'.format(color.color.green))
        print('\tb: {}'.format(color.color.blue))
        print('\ta: {}'.format(color.color.alpha))
# ------------------
#   START
# ------------------

# checking parameters
if len(sys.argv) != 4:
    printInLine('parameters not correct (should be 5): --&gt; ')
    print(len(sys.argv))
    printInLine(' ... Quiting app')
    sys.exit()

# setting parameters if OK
csvPath = sys.argv[1]
csvHeaderRowUrl = sys.argv[2]
imageStorePath = sys.argv[3]
delimit = ';'

# parameters are OK
if (csvPath and csvHeaderRowUrl and imageStorePath):
    print(""csvPath: "" + csvPath + "" csvHeaderRowUrl: "" + csvHeaderRowUrl + "" imageStorePath: "" + imageStorePath)
    print('parameters OK ... starting...')

    # get index (position) of csv-header url
    headerUrlIndex = getCsvlHeaderIndexOfUrl(csvPath, csvHeaderRowUrl, delimit)
    # print('headerUrlIndex: ' + str(headerUrlIndex))
    # start counter with 1 (after header)
    numberOfLines = sum(1 for line in open(csvPath, 'r', encoding='UTF8'))
    # print('numberOfLines: ' + str(numberOfLines))

    # loop over each line in csv
    for i in range(1, numberOfLines):
        # row in csv
        lineNumber = i
        # get url from csv-file
        url = getCsvlValueByHeaderindexAndLinenumber(csvPath, headerUrlIndex, lineNumber, delimit)
        # download image and get path to stored file (filePath)
        filePath = downloadImage(url, imageStorePath, lineNumber)
        if filePath:
            # send image to vision and get colours
            labels = sendImageToGoogleVision(filePath)
            # build list of labels
            labelString = delimit

 #                                                      ------ This is where I need your help......


            saveLabelsBackToCsvByIndex(csvPath, lineNumber, labelString)

else:
    print('parameters missing ... Quiting app')
</code></pre>",,1,0,,2019-03-16 06:32:41.433 UTC,,2019-03-18 08:01:52.230 UTC,,,,,10829406,1,0,python|image|colors|vision,22
Response 202 from Emotion API in R but Operation Location Says Invalid Key,42044047,Response 202 from Emotion API in R but Operation Location Says Invalid Key,"<p>I am trying to reproduce in R the Microsoft Emotion API program located <a href=""https://blog.exploratory.io/analyzing-emotions-using-facial-expressions-in-video-with-microsoft-ai-and-r-8f7585dd0780#.2kdv78a61"" rel=""nofollow noreferrer"">here</a>. I obtained my API key from Microsoft's website and plugged it into the following code (also from the above linked page:</p>

<pre><code>library(httr)

apiUrl &lt;- 'https://api.projectoxford.ai/emotion/v1.0/recognizeInVideo?outputStyle=perFrame'

key &lt;- '2a0a91c98b57415a....'

urlVideo &lt;- 'https://drive.google.com/file/d/0B5EuiMJDiDhWdWZSUGxvV3lJM0U/view?usp=sharing'

mybody &lt;- list(url = urlVideo)

faceEMO &lt;- httr::POST(
    url = apiUrl,
    httr::content_type('application/json'),
    httr::add_headers(.headers = c('Ocp-Apim-Subscription-Key' = key)),
    body = mybody,
    encode = 'json'
)

operationLocation &lt;- httr::headers(faceEMO)[[""operation-location""]]
</code></pre>

<p>The request appears to go through successfully as the ""faceEMO"" object returns a Response 202 code which according to Microsoft's website means:</p>

<blockquote>
  <p>The service has accepted the request and will start the process later.
  In the response, there is a ""Operation-Location"" header. Client side
  should further query the operation status from the URL specified in
  this header.</p>
</blockquote>

<p>However, when I open the given URL from the 'operationLocation' object, it says:</p>

<blockquote>
  <p>{ ""error"": { ""code"": ""Unauthorized"", ""message"": ""Access denied due to
  invalid subscription key. Make sure you are subscribed to an API you
  are trying to call and provide the right key."" } }</p>
</blockquote>

<p>This seems to indicate that my request <em>didn't</em> go through after all. </p>

<p>Not sure why it would say my key is invalid. I tried to regenerate the key and run it again but received the same result. Maybe it has something to do with the fact that Microsoft gives me 2 keys? Do I need to use both?</p>

<p>To add additional info, I also tried running the next few lines of code given from the linked web site:</p>

<pre><code>while(TRUE){
    ret &lt;- httr::GET(operationLocation,
                     httr::add_headers(.headers = c('Ocp-Apim-Subscription-Key' = key)))
    con &lt;- httr::content(ret)
    if(is.null(con$status)){
        warning(""Connection Error, retry after 1 minute"")
        Sys.sleep(60)
    } else if (con$status == ""Running"" | con$status == ""Uploading""){
        cat(paste0(""status "", con$status, ""\n""))
        cat(paste0(""progress "", con$progress, ""\n""))
        Sys.sleep(60)
    } else {
        cat(paste0(""status "", con$status, ""\n""))
        break()
    }
}
</code></pre>

<p>When I run this I'm shown a message indicating ""status Running"" and ""progress 0"" for about 30 seconds. Then it shows ""status Failed"" and stops running without giving any indication of what caused the failure.</p>",42048523,1,0,,2017-02-04 18:34:36.367 UTC,,2017-02-05 04:23:32.823 UTC,2017-02-04 21:58:42.507 UTC,,7433679,,7433679,1,0,r|microsoft-cognitive,154
Add faces in a collection through Python on Amazon Rekognition,56388041,Add faces in a collection through Python on Amazon Rekognition,"<p>The following script, allows to add to a collection only a single image at a time.</p>

<p>How can I add the whole S3 bucket into a collection?</p>

<pre><code>if __name__ == ""__main__"":

    bucket='bucket'
    collectionId='MyCollection'
    photo='photo'

    client=boto3.client('rekognition')

    response=client.index_faces(CollectionId=collectionId,
                                Image={'S3Object':{'Bucket':bucket,'Name':photo}},
                                ExternalImageId=photo,
                                MaxFaces=1,
                                QualityFilter=""AUTO"",
                                DetectionAttributes=['ALL'])

    print ('Results for ' + photo)  
    print('Faces indexed:')                     
    for faceRecord in response['FaceRecords']:
         print('  Face ID: ' + faceRecord['Face']['FaceId'])
         print('  Location: {}'.format(faceRecord['Face']['BoundingBox']))

    print('Faces not indexed:')
    for unindexedFace in response['UnindexedFaces']:
        print(' Location: {}'.format(unindexedFace['FaceDetail']['BoundingBox']))
        print(' Reasons:')
        for reason in unindexedFace['Reasons']:
            print('   ' + reason)
</code></pre>",,0,1,,2019-05-31 02:48:18.057 UTC,,2019-05-31 02:51:08.730 UTC,2019-05-31 02:51:08.730 UTC,,11162165,,11512993,1,0,python|amazon-s3|amazon-rekognition,8
Google Cloud Vision Client Library (PHP) Image Labels - Number of results?,55276425,Google Cloud Vision Client Library (PHP) Image Labels - Number of results?,"<p>I am using the upper mentioned library (Google Cloud Vision Client Library v1) in PHP to assign labels to images... so far so good. It all works, except it returns fewer results than on the google test page... as far as I understand it has to do with a ""max_results"" parameter which defaults to 10, but I am not able to find where/how to set it manually...
There was a similar question here on Python and there it was as simple as passing it as a parameter - I have tried many options to do this in PHP, but apparently I am doing something wrong...</p>

<p>Here is a link to the documentation : <a href=""https://googleapis.github.io/google-cloud-php/#/docs/cloud-vision/v0.19.3/vision/v1/imageannotatorclient?method=labelDetection"" rel=""nofollow noreferrer"">https://googleapis.github.io/google-cloud-php/#/docs/cloud-vision/v0.19.3/vision/v1/imageannotatorclient?method=labelDetection</a>
I am guessing I have to pass it to the ""optionalArgs"" parameter... but not exactly sure how to do this... </p>

<p>Here is more or less what my code is:</p>

<pre><code>require __DIR__ . '/vendor/autoload.php';

use Google\Cloud\Vision\V1\ImageAnnotatorClient;

$this-&gt;client = new ImageAnnotatorClient();
$response = $this-&gt;client-&gt;labelDetection(...THE IMAGE...);
$labels = $response-&gt;getLabelAnnotations();

if ($labels) {
    foreach ($labels as $label) {
        // do something with $label-&gt;getDescription()
    }
}
</code></pre>

<p>Anyone got an idea how to get <strong>more results</strong> in the $labels array?</p>",55342953,2,0,,2019-03-21 08:34:41.557 UTC,1,2019-03-26 19:38:35.857 UTC,2019-03-21 10:13:29.377 UTC,,4796499,,4796499,1,0,php|google-api|google-api-php-client|google-vision,71
Reconstruct TF layer structure from .pb file,56126281,Reconstruct TF layer structure from .pb file,"<p>I have a .pb / .tflite file from which I would like to construct the original model architecture in Tensorflow code. What would be the easiest way to do it? (The .pb / .tflite file was obtained by Google Vision Auto ML, but I would like now to manually train that specific model)</p>

<p>Via <a href=""https://github.com/lutzroeder/netron"" rel=""nofollow noreferrer"">Netron</a> and Tensorboard I already looked visually at the network layers and printed the names of the network as described <a href=""https://stackoverflow.com/questions/50632258/how-to-restore-tensorflow-model-from-pb-file-in-python"">here</a>. The names were not really helpful, but Netron revealed something about the layers. I just don't see what is the easiest way to construct the TF layers in TF code. Also, I don't know which layers where frozen during the training process.</p>",,0,0,,2019-05-14 08:40:33.403 UTC,,2019-05-14 18:22:31.390 UTC,2019-05-14 18:22:31.390 UTC,,8558778,,8558778,1,0,python|tensorflow|keras|protocol-buffers|google-cloud-automl,26
How to send Base64 image to Google Cloud Vision API label detection in Ruby?,45048382,How to send Base64 image to Google Cloud Vision API label detection in Ruby?,"<p>Hi I'm building a program in Ruby to generate alt attributes for images on a webpage. I'm scraping the page for the images then sending their src, in other words a URL, to google-cloud-vision for label detection and other Cloud Vision methods. It takes about 2-6 seconds per image. I'm wondering if there's any way to reduce response time. I first used TinyPNG to compress the images. Cloud Vision was a tad faster but the time it took to compress more than outweighed the improvement. How can I improve response time? I'll list some ideas.</p>

<p>1) Since we're sending a URL to Google Cloud, it takes time for Google Cloud to receive a response, that is from the img_src, before it can even analyze the image. Is it faster to send a base64 encoded image? What's the fastest form in which to send (or really, for Google to receive) an image?</p>

<pre><code>cloud_vision = Google::Cloud::Vision.new project: PROJECT_ID
@vision = cloud_vision.image(@file_name)
@vision.labels #or @vision.web, etc.
</code></pre>

<p>2) My current code for label detection. First question: is it faster to send a JSON request rather than call Ruby (label or web) methods on a Google Cloud Project? If so, should I limit responses? Labels with less than a 0.6 confidence score don't seem of much help. Would that speed up image rec/processing time? </p>

<p>Open to any suggestions on how to speed up response time from Cloud Vision.</p>",,2,2,,2017-07-12 04:21:07.360 UTC,,2017-07-16 08:40:14.200 UTC,2017-07-14 19:50:19.013 UTC,,6727850,,6727850,1,1,ruby|google-app-engine|web-scraping|google-cloud-vision,1148
AWS Image getBytes returning null,49801592,AWS Image getBytes returning null,"<p>I am trying to convert and <code>AWS Rekognition Image</code> to java <code>BufferedImage</code>. In order to do this I need the byte array from the AWS Image. However, when I call the <code>getBytes</code> method it returns null instead of returning <code>ByteBuffer</code>. My code is as below:</p>

<pre><code>    //Load an Rekognition Image object from S3
    Image inputImage = new Image()
            .withS3Object(new com.amazonaws.services.rekognition.model.S3Object().withName(key).withBucket(bucket));

    DetectFacesRequest request = new DetectFacesRequest().withImage(inputImage).withAttributes(Attribute.ALL);

    try {
        DetectFacesResult result = amazonRekognition.detectFaces(request);
        List&lt;FaceDetail&gt; faceDetails = result.getFaceDetails();
        System.out.println(""Number of faces: "" + faceDetails.size());
        int count = 1;

        // I do get a number of FaceDetails back which proves that I am reading the image correctly from S3
        for (FaceDetail faceDetail : faceDetails) {
          BoundingBox faceBox = faceDetail.getBoundingBox();
            try {

                 // Load image
                  ByteBuffer imageBytes=inputImage.getBytes();
                  if (imageBytes == null) {
                      System.out.println(""Why is this null?"");
                      return false;
                  }
     ...
</code></pre>

<p>The input image is only 80KB in size, not sure if size matters.</p>",,0,4,,2018-04-12 16:24:46.743 UTC,,2018-04-12 16:24:46.743 UTC,,,,,1140783,1,0,java|amazon-web-services|aws-sdk|aws-java-sdk|amazon-rekognition,103
Wrong image recognition on Azure Custom Vision,56175138,Wrong image recognition on Azure Custom Vision,"<p>Wrong image recognition on Azure Custom Vision Service.</p>

<p>I have a doubt. I'm using Azure Custom Vision Service for image recognition.</p>

<p>I uploaded my photos and I put the coca cola tag,
I added 20 similar photos and to all I put their tag
<a href=""https://i.stack.imgur.com/RyJkl.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RyJkl.jpg"" alt=""enter image description here""></a></p>

<p>but at the time of doing the test, I get these results.
I'm doing a test with this image.</p>

<p><a href=""https://i.stack.imgur.com/uRaDy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uRaDy.png"" alt=""enter image description here""></a></p>

<p>Why does Custom Vision Service say that other soft drinks are Coca-Cola?</p>

<p>Do I have to do other things specifically?
Are my tags wrong?
Thanks.</p>",,2,0,,2019-05-16 19:08:29.080 UTC,,2019-05-17 08:34:40.493 UTC,2019-05-17 08:34:40.493 UTC,,1537195,,3814000,1,0,azure|microsoft-custom-vision,24
Get multiple barcode strings from Google Vision,49532350,Get multiple barcode strings from Google Vision,"<p>I'm using Google vision for a week now. but the problem is, when it sees multiple barcodes, it randomly chooses any barcodes &amp; returns only one string. I'm trying to get all the barcodes it sees.</p>

<p><a href=""https://i.stack.imgur.com/56kgo.jpg"" rel=""nofollow noreferrer"">Barcode</a></p>

<p>In the picture above, there are multiple Barcodes. Right now, for one barcode, I'm doing this in BarcodeCaptureActivity:</p>

<pre><code>@Override
    public void onBarcodeDetected(Barcode barcode) {
        //do something with barcode data returned

        Log.d(TAG, ""...::: BARCODE :::..."");
        Log.d(TAG, """" + barcode.rawValue);
        Log.d(TAG, ""...::: BARCODE :::..."");

    }
</code></pre>

<p>I'm trying to make it return an array of string with all the barcodes. But how to do it?</p>",,0,2,,2018-03-28 10:40:53.640 UTC,,2018-03-28 10:40:53.640 UTC,,,,,9563431,1,0,android|google-vision,91
AWS personalize service not available using boto3,56026129,AWS personalize service not available using boto3,"<p>I am trying to setup a boto3 client to use the AWS personalize service along the lines of what is done here:</p>

<p><a href=""https://docs.aws.amazon.com/personalize/latest/dg/data-prep-importing.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/personalize/latest/dg/data-prep-importing.html</a></p>

<p>I have faithfully followed the tutorial up to this point. I have my s3 bucket set up, and I have an appropriately formatted csv.</p>

<p>I configured my access and secret tokens and can successfully perform basic operations on my s3 bucket, so I think that part is working:</p>

<pre><code>import s3fs
fs = s3fs.S3FileSystem()
fs.ls('personalize-service-test')
'personalize-test/data'
</code></pre>

<p>When I try to create my service, things start to fail:</p>

<pre><code>import boto3
personalize = boto3.client('personalize')

---------------------------------------------------------------------------
UnknownServiceError                       Traceback (most recent call last)
&lt;ipython-input-13-c23d30ee6bd1&gt; in &lt;module&gt;
----&gt; 1 personalize = boto3.client('personalize')

/anaconda3/envs/ds_std_3.6/lib/python3.6/site-packages/boto3/__init__.py in client(*args, **kwargs)
     89     See :py:meth:`boto3.session.Session.client`.
     90     """"""
---&gt; 91     return _get_default_session().client(*args, **kwargs)
     92 
     93 

/anaconda3/envs/ds_std_3.6/lib/python3.6/site-packages/boto3/session.py in client(self, service_name, region_name, api_version, use_ssl, verify, endpoint_url, aws_access_key_id, aws_secret_access_key, aws_session_token, config)
    261             aws_access_key_id=aws_access_key_id,
    262             aws_secret_access_key=aws_secret_access_key,
--&gt; 263             aws_session_token=aws_session_token, config=config)
    264 
    265     def resource(self, service_name, region_name=None, api_version=None,

/anaconda3/envs/ds_std_3.6/lib/python3.6/site-packages/botocore/session.py in create_client(self, service_name, region_name, api_version, use_ssl, verify, endpoint_url, aws_access_key_id, aws_secret_access_key, aws_session_token, config)
    836             is_secure=use_ssl, endpoint_url=endpoint_url, verify=verify,
    837             credentials=credentials, scoped_config=self.get_scoped_config(),
--&gt; 838             client_config=config, api_version=api_version)
    839         monitor = self._get_internal_component('monitor')
    840         if monitor is not None:

/anaconda3/envs/ds_std_3.6/lib/python3.6/site-packages/botocore/client.py in create_client(self, service_name, region_name, is_secure, endpoint_url, verify, credentials, scoped_config, api_version, client_config)
     77             'choose-service-name', service_name=service_name)
     78         service_name = first_non_none_response(responses, default=service_name)
---&gt; 79         service_model = self._load_service_model(service_name, api_version)
     80         cls = self._create_client_class(service_name, service_model)
     81         endpoint_bridge = ClientEndpointBridge(

/anaconda3/envs/ds_std_3.6/lib/python3.6/site-packages/botocore/client.py in _load_service_model(self, service_name, api_version)
    115     def _load_service_model(self, service_name, api_version=None):
    116         json_model = self._loader.load_service_model(service_name, 'service-2',
--&gt; 117                                                      api_version=api_version)
    118         service_model = ServiceModel(json_model, service_name=service_name)
    119         return service_model

/anaconda3/envs/ds_std_3.6/lib/python3.6/site-packages/botocore/loaders.py in _wrapper(self, *args, **kwargs)
    130         if key in self._cache:
    131             return self._cache[key]
--&gt; 132         data = func(self, *args, **kwargs)
    133         self._cache[key] = data
    134         return data

/anaconda3/envs/ds_std_3.6/lib/python3.6/site-packages/botocore/loaders.py in load_service_model(self, service_name, type_name, api_version)
    376             raise UnknownServiceError(
    377                 service_name=service_name,
--&gt; 378                 known_service_names=', '.join(sorted(known_services)))
    379         if api_version is None:
    380             api_version = self.determine_latest_version(

UnknownServiceError: Unknown service: 'personalize'. Valid service names are: acm, acm-pca, alexaforbusiness, amplify, apigateway, apigatewaymanagementapi, apigatewayv2, application-autoscaling, appmesh, appstream, appsync, athena, autoscaling, autoscaling-plans, backup, batch, budgets, ce, chime, cloud9, clouddirectory, cloudformation, cloudfront, cloudhsm, cloudhsmv2, cloudsearch, cloudsearchdomain, cloudtrail, cloudwatch, codebuild, codecommit, codedeploy, codepipeline, codestar, cognito-identity, cognito-idp, cognito-sync, comprehend, comprehendmedical, config, connect, cur, datapipeline, datasync, dax, devicefarm, directconnect, discovery, dlm, dms, docdb, ds, dynamodb, dynamodbstreams, ec2, ecr, ecs, efs, eks, elasticache, elasticbeanstalk, elastictranscoder, elb, elbv2, emr, es, events, firehose, fms, fsx, gamelift, glacier, globalaccelerator, glue, greengrass, guardduty, health, iam, importexport, inspector, iot, iot-data, iot-jobs-data, iot1click-devices, iot1click-projects, iotanalytics, kafka, kinesis, kinesis-video-archived-media, kinesis-video-media, kinesisanalytics, kinesisanalyticsv2, kinesisvideo, kms, lambda, lex-models, lex-runtime, license-manager, lightsail, logs, machinelearning, macie, marketplace-entitlement, marketplacecommerceanalytics, mediaconnect, mediaconvert, medialive, mediapackage, mediastore, mediastore-data, mediatailor, meteringmarketplace, mgh, mobile, mq, mturk, neptune, opsworks, opsworkscm, organizations, pi, pinpoint, pinpoint-email, pinpoint-sms-voice, polly, pricing, quicksight, ram, rds, rds-data, redshift, rekognition, resource-groups, resourcegroupstaggingapi, robomaker, route53, route53domains, route53resolver, s3, s3control, sagemaker, sagemaker-runtime, sdb, secretsmanager, securityhub, serverlessrepo, servicecatalog, servicediscovery, ses, shield, signer, sms, sms-voice, snowball, sns, sqs, ssm, stepfunctions, storagegateway, sts, support, swf, textract, transcribe, transfer, translate, waf, waf-regional, workdocs, worklink, workmail, workspaces, xray
</code></pre>

<p>Indeed the service name 'personalize' is missing from the list.  </p>

<p>I already tried upgrading <code>boto3</code> and <code>botocore</code> to their latest version and restarting my kernel.</p>

<pre><code>boto3           1.9.143 
botocore        1.12.143
</code></pre>

<p>Any idea as to what to try next would be great.</p>",,3,0,,2019-05-07 15:38:50.990 UTC,,2019-05-08 13:08:55.683 UTC,2019-05-08 00:52:18.010 UTC,,174777,,2690677,1,0,amazon-web-services|boto3|amazon-personalize,51
Google Cloud Vision API - Partial matching image,52446033,Google Cloud Vision API - Partial matching image,"<p>I am using the Google Cloud Vision API to search similar images (web detection) and it works pretty well. Google detects full matching images and partial matching images (cropped versions).</p>

<p>I am looking for a way to detect more different versions. For example, when I look for a logo, I would like to detect large, small, square, rectangular ... versions of this logo. For now, I detect images that match exactly the one I upload and cropped versions.</p>

<p>Do you know if this is possible and how can I do that?</p>",,1,1,,2018-09-21 14:37:28.003 UTC,,2018-09-21 15:56:06.327 UTC,,,,,10396962,1,0,search-engine|google-cloud-vision,253
Google Cloud Vision IMAGE_PROPERTIES returns 10 colors/results no matter what image and what value for maxResults parameters,40079213,Google Cloud Vision IMAGE_PROPERTIES returns 10 colors/results no matter what image and what value for maxResults parameters,"<p>I am new to Google Cloud Vision API and I wanted to extract the colors from an image using their dominant color functionality. below is my code which is base on <a href=""http://terrenceryan.com/blog/index.php/working-with-cloud-vision-api-from-php/"" rel=""nofollow"">Terrence Ryan's Blog</a></p>

<pre><code>    $cvurl = ""https://vision.googleapis.com/v1/images:annotate?key=API_KEY"";

    $data = file_get_contents($cache_job);
    $base64 = base64_encode($data);
    //Create this JSON
    $r_json ='{
        ""requests"": [
            {
              ""image"": {
                ""content"":""' . $base64. '""
              },
              ""features"": [
                  {
                    ""type"": ""IMAGE_PROPERTIES"",
                    ""maxResults"": 200
                  }
              ]
            }
        ]
    }';

    $curl = curl_init();
    curl_setopt($curl, CURLOPT_URL, $cvurl);
    curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);
    curl_setopt($curl, CURLOPT_HTTPHEADER, array(""Content-type: application/json""));
    curl_setopt($curl, CURLOPT_POST, true);
    curl_setopt($curl, CURLOPT_POSTFIELDS, $r_json);
    $json_response = curl_exec($curl);
    $status = curl_getinfo($curl, CURLINFO_HTTP_CODE);
    curl_close($curl);

    if ( $status != 200 ) {
        die(""Error: $cvurl failed status $status"" );
    }
</code></pre>

<p>The code works but i got some issues with it. There are some colors that are obviously on the image but weren't included on the API response. And so I thought increasing the number of result will solve it but then i discovered that changing the ""maxResults"" (Google API docs: the number of results to be returned) parameter to any value does not affect anything on the response. The number of result are fixed to 10 colors even if I set the parameter to less than 10 and even if I change the image. Google's API documentation doesn't say anything about it so i was wondering if any of you guys here have experienced it.</p>",,1,0,,2016-10-17 05:42:36.690 UTC,,2016-10-18 15:05:52.180 UTC,2016-10-18 15:05:52.180 UTC,,322020,,1886062,1,0,php|google-cloud-vision,239
android studio Multiple dex files define Lcom/google/common/reflect/Types$WildcardTypeImpl;,53331004,android studio Multiple dex files define Lcom/google/common/reflect/Types$WildcardTypeImpl;,"<p>I use firebase database, storage, auth and google cloud vision.</p>

<p>I got this error</p>

<pre><code>Multiple dex files define Lcom/google/common/reflect/Types$WildcardTypeImpl;
</code></pre>

<p>I think that this project gradle conflicts</p>

<p>this is my gradle(Module:app)</p>

<pre><code>apply plugin: 'com.android.application'

android {
    compileSdkVersion 27
    buildToolsVersion ""27.0.3""

    defaultConfig {
        applicationId ""*************************""
        minSdkVersion 19
        targetSdkVersion 27
        versionCode 1
        versionName ""1.0""
    }
    buildTypes {
        release {
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
        }
        buildTypes.each {
            it.buildConfigField 'String', 'API_KEY', '""**********************""'
        }
    }
    compileOptions {
        sourceCompatibility JavaVersion.VERSION_1_8
        targetCompatibility JavaVersion.VERSION_1_8
    }
}

dependencies {
    implementation fileTree(include: ['*.jar'], dir: 'libs')
    testCompile 'junit:junit:4.12'
    implementation 'com.google.api-client:google-api-client-android:1.23.0' exclude module: 'httpclient'
    implementation 'com.google.http-client:google-http-client-gson:1.23.0' exclude module: 'httpclient'
    implementation 'com.google.apis:google-api-services-vision:v1-rev369-1.23.0'

    implementation 'com.android.support.constraint:constraint-layout:1.1.3'

    implementation 'com.android.support:appcompat-v7:27.1.1'
    androidTestImplementation 'com.android.support.test:runner:1.0.2'
    androidTestImplementation 'com.android.support.test.espresso:espresso-core:3.0.2'
    implementation 'com.android.support:animated-vector-drawable:27.1.1'
    implementation 'com.android.support:support-media-compat:27.1.0'

    implementation 'com.github.bumptech.glide:glide:4.8.0'
    annotationProcessor 'com.github.bumptech.glide:compiler:4.8.0'
    implementation 'com.firebaseui:firebase-ui-storage:3.2.2'
    implementation 'com.jaredrummler:colorpicker:1.0.1'
    implementation 'com.android.support:multidex:1.0.3'

    //firebase 구글 로그인 원래 16.0.2
    implementation 'com.google.android.gms:play-services-auth:16.0.0'

    //    ++++++++ 2018.11.08 ++++++++
    implementation 'com.android.support:design:27.1.1'

    implementation 'com.firebaseui:firebase-ui-database:0.6.2'
    implementation 'com.android.support:recyclerview-v7:27.1.1'
    implementation 'com.android.support:cardview-v7:27.1.1'
    implementation 'com.squareup.picasso:picasso:2.5.2'

    implementation 'org.parceler:parceler-api:1.1.11'
    annotationProcessor 'org.parceler:parceler:1.1.11'

    //////////
    implementation 'com.google.firebase:firebase-core:16.0.4'
    implementation 'com.google.firebase:firebase-database:16.0.3'
    implementation 'com.google.firebase:firebase-firestore:17.1.1'
    implementation 'com.google.firebase:firebase-storage:16.0.3'
    implementation 'com.google.firebase:firebase-auth:16.0.4'
    implementation 'com.google.firebase:firebase-messaging:17.3.3'
    implementation 'com.google.firebase:firebase-config:16.0.1'
    implementation 'com.google.firebase:firebase-invites:16.0.4'
    implementation 'com.google.firebase:firebase-ads:16.0.1'
    implementation 'com.google.firebase:firebase-appindexing:16.0.2'
    implementation 'com.google.firebase:firebase-perf:16.1.2'
    implementation 'com.google.firebase:firebase-functions:16.1.1'
    implementation 'com.google.firebase:firebase-ml-vision:17.0.1'
    implementation 'com.google.firebase:firebase-ml-model-interpreter:16.2.2'
}

apply plugin: 'com.google.gms.google-services'
</code></pre>

<p>this is my gradle(Project)</p>

<pre><code>// Top-level build file where you can add configuration options common to all sub-projects/modules.

buildscript {
    repositories {
        google()
        jcenter()
        maven {
            url 'https://maven.google.com/'
            name 'Google'
        }
    }
    dependencies {
//        classpath 'com.android.tools.build:gradle:2.2.3'
//        classpath 'com.google.gms:google-services:3.1.0'
        classpath 'com.android.tools.build:gradle:3.0.1'
        classpath 'com.google.gms:google-services:4.0.1'
        // NOTE: Do not place your application dependencies here; they belong
        // in the individual module build.gradle files
    }
}

allprojects {
    repositories {
        google()
        jcenter()
        mavenCentral()
        maven {
            url 'https://maven.google.com/'
            name 'Google'
        }
    }
}

task clean(type: Delete) {
    delete rootProject.buildDir
}
</code></pre>

<p>What can I do? ;(
I think gradle has some problems, but I dont' know what to do. 
I put almost all codes found from googling
Help me :(</p>",,1,0,,2018-11-16 03:30:08.707 UTC,0,2018-11-16 15:25:40.660 UTC,,,,,10619721,1,0,android|firebase,31
Microsoft cognitive services face API expired and free version,47138550,Microsoft cognitive services face API expired and free version,"<p>I have a simple question.
Microsoft cognitive services face API has a free option. I am currently on my 30 days trial for it.
When the face API will expire (in 12 days), will it still be available to use using the free version ? Thanks</p>",47148368,1,0,,2017-11-06 13:54:05.933 UTC,,2017-11-07 01:07:32.387 UTC,,,,,4751420,1,0,microsoft-cognitive|azure-cognitive-services,338
similar face recognition using google cloud vision API in android studio,47563346,similar face recognition using google cloud vision API in android studio,"<p>i am trying to build an app in android which takes a an image (original image) and identify the face/faces in the image using google cloud vision API  and this is easy to do, then i want to insert a new image which also contain face/faces and compare between the original image and the new image, so i want to know if the new image has a face similar to the original image.
so can google cloud API  do this? it can identify faces, but can i use it to get similar faces to the identified face in other images? 
here is a simple code of how to detect faces</p>

<pre><code>List&lt;FaceAnnotation&gt; faces = batchResponse.getResponses()
                                .get(0).getFaceAnnotations();

// Count faces
int numberOfFaces = faces.size();



// Display toast on UI thread
runOnUiThread(new Runnable() {
    @Override
    public void run() {
        Toast.makeText(getApplicationContext(),
                message, Toast.LENGTH_LONG).show();
    }
});
</code></pre>",,0,0,,2017-11-29 23:06:16.400 UTC,1,2017-11-29 23:06:16.400 UTC,,,,,4748115,1,1,android|face-recognition|google-vision,352
Google API Vision - Could not load the default credentials,43759806,Google API Vision - Could not load the default credentials,"<p>I am trying to use the google Vision API.
I did the following steps:</p>

<ol>
<li>Enabled the Google Vision API</li>
<li>Created the service account id</li>
<li>Setted the enviorement varialbe 'GOOGLE_APPLICATION_CREDENTIALS' with the json path</li>
<li>downloaded the API with composer</li>
<li><p>wrote the following code:</p>

<pre><code>$vision = new VisionClient([
  'projectId' =&gt; $googleApiKey,
]);
</code></pre>

<p>$image = $vision->image(file_get_contents($path), ['FACE_DETECTION']);
$result = $vision->annotate($image);</p></li>
</ol>

<p>When I lunch this code I obtain the following error:</p>

<pre><code> Fatal error: Uncaught exception 'Google\Cloud\Core\Exception\ServiceException' with message 'Could not load the default credentials. 
</code></pre>

<p>Somebody can help me to resolve this problem?</p>

<p>Thanks a lot!!!</p>",,1,0,,2017-05-03 12:16:49.693 UTC,,2017-05-04 19:55:09.923 UTC,,,,,880386,1,0,php|google-app-engine|google-vision,708
get java.net.SocketTimeoutException: connect timed out when running vision sample code,41560252,get java.net.SocketTimeoutException: connect timed out when running vision sample code,"<p>My project has a OCR requirement and I want to use the google cloud Vision API. I download the sample code via GIT, but it report follow errors:</p>

<blockquote>
  <p>Exception in thread ""main"" java.net.SocketTimeoutException: connect
  timed out     at java.net.DualStackPlainSocketImpl.waitForConnect(Native
  Method)   at
  java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:85)
    at
  java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
    at
  java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
    at
  java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
    at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)   at
  java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)    at
  java.net.Socket.connect(Socket.java:589)  at
  sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:668)    at
  sun.net.NetworkClient.doConnect(NetworkClient.java:175)   at
  sun.net.www.http.HttpClient.openServer(HttpClient.java:432)   at
  sun.net.www.http.HttpClient.openServer(HttpClient.java:527)   at
  sun.net.www.protocol.https.HttpsClient.(HttpsClient.java:264)
    at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367)
    at
  sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191)
    at
  sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138)
    at
  sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032)
    at
  sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:177)
    at
  sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1316)
    at
  sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1291)
    at
  sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250)
    at
  com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:77)
    at
  com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981)
    at
  com.google.api.client.auth.oauth2.TokenRequest.executeUnparsed(TokenRequest.java:283)
    at
  com.google.api.client.auth.oauth2.TokenRequest.execute(TokenRequest.java:307)
    at
  com.google.api.client.googleapis.auth.oauth2.GoogleCredential.executeRefreshToken(GoogleCredential.java:384)
    at
  com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:489)
    at
  com.google.api.client.auth.oauth2.Credential.intercept(Credential.java:217)
    at
  com.google.api.client.http.HttpRequest.execute(HttpRequest.java:868)
    at
  com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419)
    at
  com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352)
    at
  com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469)
    at
  com.google.cloud.vision.samples.label.LabelApp.labelImage(LabelApp.java:136)
    at
  com.google.cloud.vision.samples.label.LabelApp.main(LabelApp.java:71)</p>
</blockquote>

<p>I don`t modify any code and I could get the successfully test results on the API browser explorer. Has anyone met this kind of issue before?Could you please give me any suggestion?</p>",,2,1,,2017-01-10 02:38:57.527 UTC,,2017-03-18 03:41:27.800 UTC,2017-01-10 04:05:13.630 UTC,,4505446,,7397211,1,0,google-cloud-platform|google-cloud-vision,1865
Creating back-end database on cloud,44125520,Creating back-end database on cloud,"<p>I have created an Xamarin Android apps which uses Google Vision API. Now, I want to create a cloud base database to retrieve some information. For example, I will get some 'brand name' from Google Vision API and on the base of that result i want to display some description about that particular brand and those description will be stored on back-end database. </p>

<p>I have created mobile service account on Azure Portal, tried few tutorials but could n't get any idea how to connect app with back end data.
Any links of suggestion will be great help to move forward.</p>",44131360,3,0,,2017-05-23 03:53:50.427 UTC,,2017-05-23 21:35:39.727 UTC,,,,,7120682,1,0,sql|asp.net-mvc|web-services|rest|azure,52
How create custom clasifier with hierarchy - watson visual recognition,42135072,How create custom clasifier with hierarchy - watson visual recognition,"<p>I am working with Watson Visual Recognition and have successfully created a custom classifier. </p>

<p>I notice the build in default classifier can return a hierarchy eg: Animals/Dog, But how to create a custom classifier return response contains ""type_hierarchy"" such as default classifier ?</p>

<p>It may be necessary to train a custom classifier with more positive class and negative class or it is possibly due to me being on the trial version  ?!!</p>",42151788,1,0,,2017-02-09 11:08:39.313 UTC,,2017-02-10 04:55:13.657 UTC,2017-02-09 11:32:15.423 UTC,,7398565,,7398565,1,0,watson|visual-recognition,179
Google Vision: How do I choose all types of detection,51102444,Google Vision: How do I choose all types of detection,"<p>i'm playing around with <strong>Google Cloud Vision API</strong> and have implemented it on my own application. For now I can only implement one ""type"" in the POST, but I want to have more than one. In the Vision API - Drag and Drop Demo (<a href=""https://cloud.google.com/vision/docs/drag-and-drop"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/drag-and-drop</a>), you can output more than one type and I want to do the same. </p>

<p>After reading the documentation for the API I thought the solution was to set the ""type"" to ""TYPE_UNSPECIFIED"", but after trying that I couldn't get any response.</p>

<p>""type"" is a ENUM and I listed the documentation under:</p>

<blockquote>
<pre><code>*// Type: The feature type.
    //
    // Possible values:
    //   ""TYPE_UNSPECIFIED"" - Unspecified feature type.
    //   ""FACE_DETECTION"" - Run face detection.
    //   ""LANDMARK_DETECTION"" - Run landmark detection.
    //   ""LOGO_DETECTION"" - Run logo detection.
    //   ""LABEL_DETECTION"" - Run label detection.
    //   ""TEXT_DETECTION"" - Run text detection / optical character*
</code></pre>
</blockquote>

<p>I need help to implement more than one ""type""..
Any ideas?</p>",51153803,1,0,,2018-06-29 13:03:55.617 UTC,,2018-07-03 11:44:20.413 UTC,2018-07-02 09:15:02.370 UTC,,10011018,,10011018,1,0,api|cloud|google-cloud-vision|vision,123
Is there any way to stop AWS Rekognition operation?,54122545,Is there any way to stop AWS Rekognition operation?,"<p>Is there any way to stop/cancel any Rekognition operation which was started earlier through its jobId or similar thing?</p>

<p>To elaborate it, lets assume that I have started a label detection operation using startLabelDetection method through which I get a jobId. I want to have an option to cancel/stop it ( also it would be great to have pause option ;) while the process is in progress.</p>

<p>I went through the documentation but did't find any clue.</p>",54122634,1,0,,2019-01-10 05:43:44.240 UTC,,2019-01-10 05:52:33.327 UTC,,,,,651262,1,0,amazon-web-services|amazon-rekognition,30
How to save/load Google-Cloud-Vision OCR protobuf response to disk?,55617828,How to save/load Google-Cloud-Vision OCR protobuf response to disk?,"<p>I'm trying to save responses from Google-Cloud-Vision OCR to disk and found gzipping and storing the actual protobuf is the most space efficient option for later processing. That part was easy! Now how do I retrieve and parse that back from disk into its original format?</p>

<p>My question is: Where/how do I rebuild the message_pb2 file to parse the file back into protobuf</p>

<p>Following <a href=""https://developers.google.com/protocol-buffers/docs/pythontutorial"" rel=""nofollow noreferrer"">documentation</a> Here's my code so far:</p>

<pre><code>#!/usr/bin/python3
# coding: utf-8

from google.cloud import vision
import gzip, os, io


def ocr_document(path):
    """"""
    Detects document features in an image.
    Returns response protobuf from API.
    """"""
    client = vision.ImageAnnotatorClient()

    with io.open(path, 'rb') as image_file:
        content = image_file.read()

    image = vision.types.Image(content=content)

    response = client.document_text_detection(image=image)

    return(response)

response = ocr_document('handwritten-scan.jpg')
serialized = response.SerializeToString()


with gzip.open('response.pb.gz', 'wb') as f:
    f.write(serialized)
print(os.path.getsize('response.pb.gz'), 'bytes') # Output: 11032 bytes

# Figure this part out!

with gzip.open('response.pb.gz', 'rb') as f:
    serialized=f.read()
    ### parsed = message_pb2.Message()  # &lt; - Protobuf message I'm missing
    parsed.ParseFromString(serialized)
    print(parsed)
</code></pre>",,1,0,,2019-04-10 17:00:05.867 UTC,,2019-04-12 06:14:28.603 UTC,2019-04-12 06:07:53.303 UTC,,1632150,,1632150,1,0,python|google-cloud-platform|protocol-buffers|google-cloud-vision,44
Best practice for `pip install` red warnings?,50870964,Best practice for `pip install` red warnings?,"<p>I have it many times that when installing with pip I get success messages followed by <em>red</em> lines with warnings. An example:</p>

<blockquote>
  <p>Successfully built qgrid</p>
  
  <p>spacy 2.0.11 has requirement regex==2017.4.5, but you'll have regex 2017.11.9 which is incompatible.</p>
  
  <p>proto-google-cloud-vision-v1 0.90.3 has requirement oauth2client&lt;4.0dev,>=2.0.0, but you'll have oauth2client 4.1.2 which is incompatible.</p>
  
  <p>proto-google-cloud-speech-v1beta1 0.15.3 has requirement oauth2client&lt;4.0dev,>=2.0.0, but you'll have oauth2client 4.1.2 which is incompatible.</p>
  
  <p>proto-google-cloud-spanner-v1 0.15.3 has requirement oauth2client&lt;4.0dev,>=2.0.0, but you'll have oauth2client 4.1.2 which is incompatible.</p>
</blockquote>

<p>How do I reconcile the state of the install in such cases? do these messages simply imply that older versions will be used so that the compatibility of previously installed packages will preserve? does it also imply that the newly installed package has requested newer versions and may or may not work well, with the old ones present?</p>

<p>How do you usually proceed in these cases?</p>

<p>Obviously working with virtual environments may remove the clashes, but I can see this happening also in virtual environments.</p>

<p>Thanks!</p>",,1,0,,2018-06-15 07:29:43.140 UTC,,2018-06-15 09:29:34.887 UTC,2018-06-15 07:36:49.757 UTC,,1509695,,1509695,1,5,python|pip,259
"OCR using google-cloud-vision - Result does not contain uni characters for Polish, German, etc",40695516,"OCR using google-cloud-vision - Result does not contain uni characters for Polish, German, etc","<p>I am trying to use OCR feature in Google Vision API but not able to receive expected result. I expect to see ü for German and ć, ń, ó, ś, ź, ł, ę, ą for Polish in the results. Is there a way I can do it? </p>

<p>Obtained text does not contain uni characters for many languages: Polish, German. But this languages in the list of supported languages and language was detected correctly. </p>

<p><a href=""https://i.stack.imgur.com/Op9vb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Op9vb.png"" alt=""enter image description here""></a></p>

<p>I use drag&amp;drop option here <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/</a> and CloudVision Android Sample. Thank you for any advices. </p>",40760531,1,1,,2016-11-19 16:54:24.663 UTC,,2016-11-23 09:30:29.387 UTC,2016-11-19 17:01:17.170 UTC,,3627736,,3627736,1,2,google-cloud-vision,254
Microsoft Azure Custom Vision Python SDK - use image file from computer for prediction,54839576,Microsoft Azure Custom Vision Python SDK - use image file from computer for prediction,"<p><a href=""https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/python-tutorial"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/python-tutorial</a></p>

<p>I followed the above tutorial for using the Azure Custom Vision Python SDK. Instead of using an image on the internet for prediction (as shown in the tutorial), I would like to use an image file from my computer. How can I do that? Thanks!</p>",54867703,1,0,,2019-02-23 08:15:02.270 UTC,,2019-02-25 13:49:17.040 UTC,,,,,11105090,1,0,azure|sdk|microsoft-custom-vision,116
API to compare similarity between two images,39768788,API to compare similarity between two images,"<p>Is there any API to compare two images and gives me similarity score?</p>

<p>I have tried using IBM watson and Google vision services.But they are doing sophisticated stuffs of matching similar images across net. Is there any straight forward API where I can mention URL of two images and it gives me similarity score between them</p>",,3,1,,2016-09-29 11:17:15.733 UTC,,2017-11-30 21:29:07.267 UTC,,,,,2625282,1,2,api|image-processing,9167
Specify output filename of Cloud Vision request,54790692,Specify output filename of Cloud Vision request,"<p>So I'm sitting with Google Cloud Vision (for Node.js) and I'm trying to dynamically upload a document to a Google Cloud Bucket, process it using Google Cloud Vision API, and then downloading the .json afterwards. However, when Cloud Vision processes my request and places it in my bucket for saved text extractions, it appends <code>output-1-to-n.json</code> at the end of the filename. So let's say I'm processing a file called <code>foo.pdf</code> that's 8 pages long, the output will not be <code>foo.json</code> (even though I specified that), but rather be <code>foooutput1-to-8.json</code>. </p>

<p>Of course, this could be remedied by checking the page count of the PDF before uploading it and appending that to the path I search for when downloading, but that seems like such an unneccesary hacky solution. I can't seem to find anything in the documentation about not appending <code>output-1-to-n</code> to outputs. Extremely happy for any pointers!</p>",55168370,1,0,,2019-02-20 16:09:24.663 UTC,1,2019-03-15 18:20:31.253 UTC,,,,,6502832,1,0,google-cloud-platform|google-cloud-storage|google-cloud-vision,50
How to detect only one value from Google Vision QR Scanner?,54677464,How to detect only one value from Google Vision QR Scanner?,"<p>I have implemented a QR scanner(QRScanner class) using Google Vision API. Once a value is detected it is passed to another activity(Info class) using Intents. The problem is that once a QR code is scanned the Info class gets opened several times.I want to limit the QRScanner class to get only one QR value and Info classed to be opened only once.</p>

<p><strong>QRScanner Class</strong></p>

<pre><code>@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.activity_qr_scanner);

    cameraPreview = (SurfaceView) findViewById(R.id.camera_surface);
    qrResult = (TextView) findViewById(R.id.scannerResult);
    setupCamera();
}

private void setupCamera() {

    BarcodeDetector barcodeDetector = new BarcodeDetector.Builder(this).setBarcodeFormats(Barcode.QR_CODE).build();
    final CameraSource cameraSource = new CameraSource.Builder(this, barcodeDetector)
            .setAutoFocusEnabled(true)
            .setRequestedPreviewSize(1600, 1024)
            .build();

    cameraPreview.getHolder().addCallback(new SurfaceHolder.Callback() {
        @Override
        public void surfaceCreated(SurfaceHolder holder) {

            if (ActivityCompat.checkSelfPermission(QrScanner.this, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {
                // TODO: Consider calling
                //    ActivityCompat#requestPermissions
                // here to request the missing permissions, and then overriding
                //   public void onRequestPermissionsResult(int requestCode, String[] permissions,
                //                                          int[] grantResults)
                // to handle the case where the user grants the permission. See the documentation
                // for ActivityCompat#requestPermissions for more details.
                return;
            }
            try {
                cameraSource.start(cameraPreview.getHolder());
            } catch (IOException e) {
                e.printStackTrace();
            }
        }

        @Override
        public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {

        }

        @Override
        public void surfaceDestroyed(SurfaceHolder holder) {
            cameraSource.stop();
        }
    });


    barcodeDetector.setProcessor(new Detector.Processor&lt;Barcode&gt;() {
        @Override
        public void release() {

        }

        @Override
        public void receiveDetections(Detector.Detections&lt;Barcode&gt; detections) {

            final SparseArray&lt;Barcode&gt; qrCodes = detections.getDetectedItems();

            if(qrCodes.size()&gt;0)
            {
                qrResult.post(new Runnable() {
                    @Override
                    public void run() {

                        qrResult.setText(qrCodes.valueAt(0).displayValue);
                        Intent intent = new Intent(QrScanner.this,Info.class);
                        intent.putExtra(QR_CODE,qrCodes.valueAt(0).displayValue);
                        startActivity(intent);
                    }
                });
            }
        }
    });
}
</code></pre>

<p><strong>Info Class</strong></p>

<pre><code>Intent intent = getIntent();
    QRCODE = (String) intent.getStringExtra(QrScanner.QR_CODE);

    DB = FirebaseDatabase.getInstance();
    ref = DB.getReference().child(""Animals"").child(QRCODE);
    ref.addValueEventListener(new ValueEventListener() {
        @Override
        public void onDataChange(@NonNull DataSnapshot dataSnapshot) {

            String DBAnimalClass = dataSnapshot.child(""class"").getValue().toString();
            String DBAnimalFamily = dataSnapshot.child(""family"").getValue().toString();
            String DBAnimalOrder = dataSnapshot.child(""order"").getValue().toString();

        }

        @Override
        public void onCancelled(@NonNull DatabaseError databaseError) {

        }
    });
</code></pre>

<p><strong>Currently once a QR is detected the Info class gets called several times. I want the QRScanner to get only one value and Info class to get called only once</strong></p>",,0,1,,2019-02-13 18:55:24.837 UTC,,2019-02-13 18:55:24.837 UTC,,,,,6737536,1,0,android|firebase|qr-code|barcode-scanner|google-vision,50
how can i take a specific element from this list in python?,44826567,how can i take a specific element from this list in python?,"<p>I'm working with the Microsoft Azure face API and I want to get only the glasses response.
heres my code: </p>

<pre><code>########### Python 3.6 #############
import http.client, urllib.request, urllib.parse, urllib.error, base64, requests, json

###############################################
#### Update or verify the following values. ###
###############################################

# Replace the subscription_key string value with your valid subscription key.
subscription_key = '(MY SUBSCRIPTION KEY)'

# Replace or verify the region.
#
# You must use the same region in your REST API call as you used to obtain your subscription keys.
# For example, if you obtained your subscription keys from the westus region, replace 
# ""westcentralus"" in the URI below with ""westus"".
#
# NOTE: Free trial subscription keys are generated in the westcentralus region, so if you are using
# a free trial subscription key, you should not need to change this region.
uri_base = 'https://westcentralus.api.cognitive.microsoft.com'

# Request headers.
headers = {
    'Content-Type': 'application/json',
    'Ocp-Apim-Subscription-Key': subscription_key,
}

# Request parameters.
params = {
    'returnFaceAttributes': 'glasses',
}

# Body. The URL of a JPEG image to analyze.
body = {'url': 'https://upload.wikimedia.org/wikipedia/commons/c/c3/RH_Louise_Lillian_Gish.jpg'}

try:
    # Execute the REST API call and get the response.
    response = requests.request('POST', uri_base + '/face/v1.0/detect', json=body, data=None, headers= headers, params=params)

    print ('Response:')
    parsed = json.loads(response.text)
    info = (json.dumps(parsed, sort_keys=True, indent=2))
    print(info)

except Exception as e:
    print('Error:')
    print(e)
</code></pre>

<p>and it returns a list like this: </p>

<pre><code>[
  {
    ""faceAttributes"": {
      ""glasses"": ""NoGlasses""
    },
    ""faceId"": ""0f0a985e-8998-4c01-93b6-8ef4bb565cf6"",
    ""faceRectangle"": {
      ""height"": 162,
      ""left"": 177,
      ""top"": 131,
      ""width"": 162
    }
  }
]
</code></pre>

<p>I want just the glasses attribute so it would just return either ""Glasses"" or ""NoGlasses""
Thanks for any help in advance!</p>",,4,0,,2017-06-29 13:30:25.810 UTC,,2017-06-29 14:00:21.530 UTC,,,,,8168776,1,0,python|api|azure,56
Rails Active Storage + AWS Rekognition detect_labels: filtered labels,55158595,Rails Active Storage + AWS Rekognition detect_labels: filtered labels,"<p>I'm trying to get AWS Rekognition to work with Rails 6 rc3 with photos stored in S3 via Active Storage. </p>

<pre><code>Aws.config.update({
      region: 'us-west-2',
      credentials: Aws::Credentials.new(Rails.application.credentials.aws[:access_key], Rails.application.credentials.aws[:secret_access_key])
    })

    rekognition = Aws::Rekognition::Client.new(region: Aws.config[:region], credentials: Aws.config[:credentials])
    @uri = @user.avatar.service_url
    @dir = @uri.split(""/"").fourth
    @key = @dir.split(""?"").first

    response = rekognition.detect_labels(
      {image:
        {s3_object:
          {bucket: 'bucket',
            name: @key,
          },
        },
        max_labels: 5,
        min_confidence: 70
      }
    )
    puts response
    @user.update(notes: response)
</code></pre>

<p>However the labels in the response shows 'FILTERED' </p>

<pre><code>{:labels=&gt;[{:name=&gt;""[FILTERED]"", :confidence=&gt;99.28252410888672, :instances=&gt;[], :parents=&gt;[{:name=&gt;""[FILTERED]""}
</code></pre>

<p>Doing the same thing over aws-cli shows the labels. Why does it show 'filtered' and how can I show the labels?</p>",,0,0,,2019-03-14 09:08:10.307 UTC,,2019-03-14 09:08:10.307 UTC,,,,,1835311,1,0,ruby-on-rails|amazon-rekognition,24
Face recognition with Azure Face Api using Angular 7,56193287,Face recognition with Azure Face Api using Angular 7,"<p>I need to develop a face recognition system in using Angular with Azure Face API. However, the documentation for Azure Face API is in C#. Could anyone help me rewrite it to typescript?</p>

<p><a href=""https://docs.microsoft.com/en-us/azure/cognitive-services/face/face-api-how-to-topics/howtoidentifyfacesinimage"" rel=""nofollow noreferrer"">This</a> is the guildline for face recognition in Azure Face API</p>

<pre><code>https://westus.api.cognitive.microsoft.com/face/v1.0/detect[?returnFaceId][&amp;returnFaceLandmarks][&amp;returnFaceAttributes]
&amp;subscription-key=&lt;Subscription key&gt;

faceServiceClient = new FaceServiceClient(""&lt;Subscription Key&gt;"");

// Create an empty PersonGroup
string personGroupId = ""myfriends"";
await faceServiceClient.CreatePersonGroupAsync(personGroupId, ""My Friends"");

// Define Anna
CreatePersonResult friend1 = await faceServiceClient.CreatePersonAsync(
    // Id of the PersonGroup that the person belonged to
    personGroupId,    
    // Name of the person
    ""Anna""            
);

// Directory contains image files of Anna
const string friend1ImageDir = @""D:\Pictures\MyFriends\Anna\"";

foreach (string imagePath in Directory.GetFiles(friend1ImageDir, ""*.jpg""))
{
    using (Stream s = File.OpenRead(imagePath))
    {
        // Detect faces in the image and add to Anna
        await faceServiceClient.AddPersonFaceAsync(
            personGroupId, friend1.PersonId, s);
    }
}
</code></pre>",,1,1,,2019-05-17 20:40:31.183 UTC,,2019-05-20 09:23:40.613 UTC,2019-05-17 21:02:28.047 UTC,,1474204,,11518040,1,-2,c#|angular|typescript|azure|face-recognition,37
Google Vision API java client: how to set API credentials explicitly in code(without using environment variable),53249139,Google Vision API java client: how to set API credentials explicitly in code(without using environment variable),"<p>I'm trying to integrate my project with <a href=""https://cloud.google.com/vision/docs/quickstart-client-libraries"" rel=""nofollow noreferrer"">Google Vision API</a></p>

<p>Here is the maven dependency for you to check the client version I'm trying to integrate with:</p>

<pre><code>&lt;dependency&gt;
   &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;
   &lt;artifactId&gt;google-cloud-vision&lt;/artifactId&gt;
   &lt;version&gt;1.51.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>The way the documentation offers to set the API authentication credentials is the following:</p>

<pre><code>Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the 
file path of the JSON file that contains your service account key
</code></pre>

<p>I'm wondering if there is a way to set the credentials explicitly in code as that is more convenient than setting environment variables in each and every environment we are running our project on.</p>

<p>As I know for a former client version <code>1.22</code> that was possible doing the following:</p>

<pre><code>def credentialFile = this.class.classLoader
.getResource(grailsApplication.config.credentialsFile)
GoogleCredential credential = 
GoogleCredential.fromStream(credentialFile.openStream())
        .createScoped(grailsApplication.config.googleScope)

Vision vision = new 
Vision.Builder(GoogleNetHttpTransport.newTrustedTransport(), 
jsonFactory, credential).build()
</code></pre>

<p>But for the new client API I was not able to find the way and documentation doesn't say anything in that regards.</p>",,1,1,,2018-11-11 13:19:41.647 UTC,,2019-02-25 01:05:24.150 UTC,,,,,3115229,1,0,java|credentials|google-vision|explicit|client-library,87
"Google Api Vision, ""before_request"" error",48846121,"Google Api Vision, ""before_request"" error","<p>So I'm trying to use Google's vision api, where it takes recognizes the labeles,facial, and text detections....etc</p>

<p>But it unfortunately, I can't fix an error that's causing us to fall behind . </p>

<p><strong>SOURCE CODE</strong></p>

<pre><code>import io
import os

# Imports the Google Cloud client library
from google.cloud import vision
from google.cloud.vision import types
from google.cloud import storage

def getCredentials():
    storage_client = storage.Client.from_service_account_json(
            'eyeHear-cad56858f9be.json')
    return storage_client
    # buckets = list(storage_client.list_buckets())
    # print(buckets)

def instantiateClient():
    # Instantiates a client
    client = vision.ImageAnnotatorClient(
        credentials=getCredentials())

    # The name of the image file to annotate
    file_name = os.path.join(
        os.path.dirname(__file__),
        'images/demo-image.jpg')

    # Loads the image into memory
    with io.open(file_name, 'rb') as image_file:
        content = image_file.read()

    image = types.Image(content=content)

    # Performs label detection on the image file
    response = client.label_detection(image=image)
    # labels = response.label_annotations

    # print('Labels:')
    # for label in labels:
    #     print(label.description)
</code></pre>

<p>It says ""Client"" object has no attribute, ""before_request""</p>

<p>That's where I'm stuck, and I'm not sure what to do from there.</p>

<p><strong>ERROR</strong></p>

<pre><code>ERROR:root:AuthMetadataPluginCallback ""&lt;google.auth.transport.grpc.AuthMetadataPlugin object at 0x104746ef0&gt;"" raised exception!
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/grpc/_plugin_wrapping.py"", line 77, in _call_
    callback_state, callback))
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/auth/transport/grpc.py"", line 77, in _call_
    callback(self._get_authorization_headers(context), None)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/auth/transport/grpc.py"", line 61, in _get_authorization_headers
    self._credentials.before_request(
AttributeError: 'Client' object has no attribute 'before_request'
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/api_core/grpc_helpers.py"", line 54, in error_remapped_callable
    return callable_(*args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/grpc/_channel.py"", line 487, in _call_
    return _end_unary_response_blocking(state, call, False, deadline)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/grpc/_channel.py"", line 437, in _end_unary_response_blocking
    raise _Rendezvous(state, None, None, deadline)
grpc._channel._Rendezvous: &lt;_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Getting metadata from plugin failed with error: 'Client' object has no attribute 'before_request')&gt;

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""testAPI.py"", line 43, in &lt;module&gt;
    run_quickstart()
  File ""testAPI.py"", line 33, in run_quickstart
    response = client.text_detection(image=image)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/cloud/vision_helpers/decorators.py"", line 110, in inner
    return self.annotate_image(request, retry=retry, timeout=timeout)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/cloud/vision_helpers/__init__.py"", line 67, in annotate_image
    r = self.batch_annotate_images([request], retry=retry, timeout=timeout)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/cloud/vision_v1/gapic/image_annotator_client.py"", line 158, in batch_annotate_images
    request, retry=retry, timeout=timeout)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/api_core/gapic_v1/method.py"", line 139, in _call_
    return wrapped_func(*args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/api_core/grpc_helpers.py"", line 56, in error_remapped_callable
    six.raise_from(exceptions.from_grpc_error(exc), exc)
  File ""&lt;string&gt;"", line 3, in raise_from
google.api_core.exceptions.ServiceUnavailable: 503 Getting metadata from plugin failed with error: 'Client' object has no attribute 'before_request'
</code></pre>",,1,3,,2018-02-17 21:30:57.537 UTC,,2018-02-17 22:15:35.113 UTC,,,,,7896427,1,1,python|api,893
aws Rekognition not initializing on iOS,51561234,aws Rekognition not initializing on iOS,"<p>there seems to be very little to no documentation for AWS iOS text recognition inside an image.  I have gone through the process of AWS create IAM with permissions to do Rekognition etc, I created my ""mobile app"" on AWS from that profile, and I got a json file which is included in my project.</p>

<p><a href=""https://i.stack.imgur.com/pFsn7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pFsn7.png"" alt=""enter image description here""></a></p>

<p>I am initializing the AWS ""stack"" with no problems also in App Delegate</p>

<pre><code>    func application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplicationLaunchOptionsKey: Any]?) -&gt; Bool {
    // Override point for customization after application launch.

    AWSDDLog.add(AWSDDTTYLogger.sharedInstance)
    AWSDDLog.sharedInstance.logLevel = .info

    return AWSMobileClient.sharedInstance().interceptApplication(
        application,
        didFinishLaunchingWithOptions: launchOptions)
}
</code></pre>

<p>I get a crash in my ViewController :</p>

<pre><code>    override func viewDidLoad() {
    super.viewDidLoad()

    let rekognitionClient = AWSRekognition.default() // CRASH HERE BOOM

    let sourceImage = UIImage(named: ""corolla"")

    let image = AWSRekognitionImage()
    image!.bytes = UIImageJPEGRepresentation(sourceImage!, 0.7)

    guard let request = AWSRekognitionDetectLabelsRequest() else {
        puts(""Unable to initialize AWSRekognitionDetectLabelsRequest."")
        return
    }

    request.image = image
    request.maxLabels = 3
    request.minConfidence = 90

    rekognitionClient.detectLabels(request) { (response:AWSRekognitionDetectLabelsResponse?, error:Error?) in
        if error == nil {
            print(response!)
        }
    }

}
</code></pre>

<p>The crash shows this:</p>

<pre><code>2018-07-27 11:22:10.064126-0400 Plater[13491:5229604] *** Terminating app due    to uncaught exception 'NSInternalInconsistencyException', reason: 'The service configuration is `nil`. You need to configure `awsconfiguration.json`, `Info.plist` or set `defaultServiceConfiguration` before using this method.'
*** First throw call stack:
(
0   CoreFoundation                      0x0000000110d3e1e6 __exceptionPreprocess + 294
1   libobjc.A.dylib                     0x000000010d7d4031 objc_exception_throw + 48
2   AWSRekognition                      0x000000010caf19ac __36+[AWSRekognition defaultRekognition]_block_invoke + 492
3   libdispatch.dylib                   0x0000000111dc97ec _dispatch_client_callout + 8
4   libdispatch.dylib                   0x0000000111dcad64 dispatch_once_f + 285
5   AWSRekognition                      0x000000010caf1794 +[AWSRekognition defaultRekognition] + 84
6   Plater                              0x000000010bd6c5ec _T06Plater14ViewControllerC11viewDidLoadyyF + 124
7   Plater                              0x000000010bd6d364 _T06Plater14ViewControllerC11viewDidLoadyyFTo + 36
8   UIKit                               0x000000010e214131 -
</code></pre>

<p>From what I can gather, it seems that I am somehow supposed to configure Rekognition inside of my json file?  I did not see that option when the json file was being created on the AWS web site...</p>

<p>Any ideas?</p>",,1,1,,2018-07-27 15:35:02.827 UTC,1,2018-07-28 18:48:34.063 UTC,,,,,174602,1,2,ios|swift|aws-sdk|amazon-rekognition,120
Amazon Rekognition for text detection,42842441,Amazon Rekognition for text detection,<p>I have images of receipts and I want to store the text in the images separately. Is it possible to detect text from images using Amazon Rekognition?</p>,,6,0,,2017-03-16 18:45:33.817 UTC,2,2019-01-06 19:24:31.933 UTC,2017-03-20 01:21:36.690 UTC,,1667977,,2219441,1,12,amazon-web-services|amazon-rekognition,15387
Using Multilayer Perceptron (MLP) to categorise images and its performance,49120897,Using Multilayer Perceptron (MLP) to categorise images and its performance,"<p>I am new to Machine/Deep learning area!</p>

<p>If I understood correctly, when I am using images as an input,</p>

<p>the number of neurons at input layer = the number of pixels (i.e resolution)</p>

<p>The weights and biases are updated through back-propagation to achieive low as possible error-rate.</p>

<p><strong>Question 1.</strong> </p>

<p><strong>So, even one single image data will adjust the values of weights &amp; biases (through back-propagation algorithm), then how does adding more similar images into this MLP improve the performance?</strong>
(I must be missing something big.. however to me, it seems like it will only be optimised for the given single image and if i input the next one (of similar img), it will only be optimised for the next one )</p>

<p><strong>Question 2.</strong></p>

<p>If I want to train my MLP to recognise certain types of images ( Let's say clothes / animals ) , what is a good number of training set for each label(i.e clothes,animals)? I know more training set will produce better result, however how much number would be ideal for good enough performance?</p>

<p><strong>Question 3. (continue)</strong></p>

<p>A bit different angle question,
There is a google cloud vision API , which will take images as an input, and produce label/probability as an output. So this API will give me an output of 100 (lets say) labels and the probabilities of each label.</p>

<p>(e.g, when i put an online game screenshot, it will produce as below,)
<a href=""https://i.stack.imgur.com/uWwbX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uWwbX.png"" alt=""example""></a></p>

<p>Can this type of data be used as an input to MLP to categorise certain type of images? 
( Assuming I know all possible types of labels that Google API produces and using all of them as input neurons )</p>

<p>Pixel values represent an image. But also, I think this type of API output results can represent an image in different angle.</p>

<p>If so, what would be the performance difference ?
e.g) when classifying 10 different types of images, 
<strong>(pixels trained model) vs (output labels trained model)</strong></p>",49121310,1,0,,2018-03-05 23:00:48.383 UTC,,2018-03-06 00:29:34.690 UTC,2018-03-06 00:29:34.690 UTC,,9378581,,9378581,1,-1,machine-learning|neural-network|deep-learning|artificial-intelligence,111
Request module crashes in Cloud Functions,47634218,Request module crashes in Cloud Functions,"<p>I try text_detection to photo. However, strange problem happens now.
I post very simply debug program.</p>

<p>the flow is:</p>

<p>1:defining variable(photo, google storage's path, vision's url)</p>

<p>2:generating the bearer token. Add it to ServiceToken.
(ref:<a href=""https://cloud.google.com/vision/docs/auth"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/auth</a>)</p>

<p>3: rbody is Google Cloud Vision's request.</p>

<p>4: Finally, using Request module. If Vision detects text, outputting json of the detected result(body:~~). </p>

<pre><code>const request = require('request');

var filePath = ""Photo01.jpg"";
const imagePath = 'gs://***'+filePath;
const urlvision = 'https://vision.googleapis.com/v1/images:annotate';
var ServiceToken;
ServiceToken = """";

//OCR
let rbody = {
    requests: [{
        image: {
            source: {
                imageUri:imagePath
            }
        }, 
        features: [{type: 'TEXT_DETECTION'}]
    }]
};
let headers = {'Content-Type':'application/json'};

console.log(JSON.stringify(rbody));

request.post({
    url: urlvision,
    headers:headers,
    body:JSON.stringify(rbody)
},function(error,response,body){
    console.log(""body:""+body);

    console.log(""error:""+error);
}).auth(null,null,true,ServiceToken);
</code></pre>

<p>In my local pc, I confirmed that program works normally.Thus, I combined between firebase storage handler in cloud functions and this program. However, in Cloud functions, request module outputted this error.</p>

<pre><code>error:TypeError: The header content contains invalid characters
</code></pre>

<p>I can not understand this error. I think that this error indicates a header. But headers's variable is not modified. Why behavior of request module changes in cloud functions? Does anyone know the details?</p>",,0,0,,2017-12-04 13:12:40.913 UTC,,2017-12-04 13:12:40.913 UTC,,,,,8887246,1,1,firebase|request|google-cloud-functions|google-cloud-vision,98
mobile google vision faster drawing,51778072,mobile google vision faster drawing,"<p>I am using the OCR functionality of mobile google vision. I used the sample example where there is a box surrounding the text being detected. </p>

<p>I noticed as  I move my camera, the box is lagging behind and moving in a jerky way trying to follow the text. How can  I improve the performance so that the box is more realtime when the camera is moving ?
Thank you</p>",,0,0,,2018-08-10 02:26:36.143 UTC,,2018-08-10 02:26:36.143 UTC,,,,,1217820,1,0,android|google-vision,12
"""No module named google.cloud"" error when Python script is imported into a test, but not when run directly",56122848,"""No module named google.cloud"" error when Python script is imported into a test, but not when run directly","<p>Not sure if this is more google-cloud-related or pytest-related. See files below.</p>

<p>When I run either <code>python app/my_script.py</code> or <code>python -m app.my_script</code>, the script runs fine.</p>

<p>But when I run <code>pytest</code>, the line in the script <code>from google.cloud import vision</code> throws ""ModuleNotFoundError: No module named 'google.cloud'"". </p>

<p>I have tried unsuccessfully to add various package names into the requirements.txt file and/or run <code>pip install google-cloud</code> and <code>pip install google-cloud-language</code> with and without <code>--upgrade</code> flags. What steps can I take to overcome this error?</p>

<hr>

<p>conftest.py: (empty)</p>

<p>requirements.txt:</p>

<pre><code>google-cloud-vision
</code></pre>

<p>app/my_script.py:</p>

<pre><code>from google.cloud import vision
from google.cloud.vision import types

def new_client():
    client = vision.ImageAnnotatorClient()
    return client

if __name__ == ""__main__"":
    client = new_client()
    # etc...
</code></pre>

<p>test/test_my_script.py:</p>

<pre><code>from app.my_script import new_client

# tests here... 
</code></pre>",,0,3,,2019-05-14 04:04:53.527 UTC,,2019-05-14 04:04:53.527 UTC,,,,,670433,1,0,python|python-3.x|pytest|google-cloud-vision|google-cloud-python,24
Posting An Image from Webcam to Azure Face Api,46718939,Posting An Image from Webcam to Azure Face Api,"<p>I am trying to upload an image that I get from my webcam to the Microsoft Azure Face Api. I get the image from canvas.toDataUrl(‘image/png’) which contains the Data Uri. I change the Content Type to application/octet-stream and when I attach the Data Uri to the post request, I get a Bad Request (400) Invalid Face Image. If I change the attached data to a Blob, I stop receiving errors however I only get back an empty array instead of a JSON object. I would really appreciate any help for pointing me in the right direction.</p>

<p>Thanks!</p>",46940611,3,9,,2017-10-12 21:09:22.647 UTC,1,2017-10-29 19:45:49.220 UTC,,,,,4962592,1,1,azure|microsoft-cognitive,1651
Getting incorrect vertices from google vision,51815006,Getting incorrect vertices from google vision,"<p>I'm creating an app that uses the phone's frame to find a logo (I'm not taking a picture, I'm just grabbing the frame every few seconds).</p>

<p>The logo detection is working perfectly, but now I need the location of the logo on the screen (from the frame's image). The vertices array that gets returned is completely off.</p>

<p>Here is an example <a href=""https://i.stack.imgur.com/exH41.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/exH41.jpg"" alt=""enter image description here""></a></p>

<p>After capturing the frame I am currently displaying it on my screen for test purposes. This is the image taken from the frame and sent to google vision logo detection. It detects Walmart perfectly. The red numbers are the vertices I get returned. Obviously, they are completely wrong.</p>

<p>The only guess I have is that either when I send the image through the base64 encoded Image class it gets shrunk, thus returning the shrunk version of the vertices, or for some reason, it shrinks the results.</p>",,0,1,,2018-08-13 03:56:04.617 UTC,,2018-08-13 03:56:04.617 UTC,,,,,4092656,1,0,android|google-vision,26
How to extract micr code from bank cheques accurately using google vision api?,55568129,How to extract micr code from bank cheques accurately using google vision api?,"<p><a href=""https://i.stack.imgur.com/yRj9k.png"" rel=""nofollow noreferrer"">micr code image</a> I want to extract MICR codes from bank cheques using google vision api ,currently vision API is not giving adequate results specially it is not reading the fonts of MICR correctly. How to use this API more appropriately so that I can extract MICR accurately.</p>",,0,2,,2019-04-08 07:20:49.220 UTC,,2019-04-08 07:20:49.220 UTC,,,,,11327506,1,0,express|google-cloud-vision,25
OCR on binary image,55283215,OCR on binary image,"<p>I have a binary text image like this one <a href=""https://i.stack.imgur.com/ORW90.png"" rel=""nofollow noreferrer"">black on white text - cat</a> </p>

<p>I want to perform OCR on images like these. They contain no more than one word.
I have tried tesseract and Google cloud vision but both of them return no results.
I'm using python 3.6 and Windows 10.</p>

<pre><code># export GOOGLE_APPLICATION_CREDENTIALS=kyourcredentials.json
import io
import cv2
from PIL import Image

# Imports the Google Cloud client library
from google.cloud import vision
from google.cloud.vision import types

# Instantiates a client
client = vision.ImageAnnotatorClient()

with io.open(""test.png"", 'rb') as image_file:
    content = image_file.read()

image = types.Image(content=content)
response = client.text_detection(image=image)
texts = response.text_annotations
resp = ''

for text in texts:
    resp+=' ' + text.description

print(resp)

from PIL import Image as im
import pytesseract as ts
print(ts.image_to_string(im.fromarray(canvas.reshape((480,640)),'L'))) # canvas contains the Mat object from which the image is saved to png
</code></pre>

<p>This image should be a simple task for either of the two and I feel I'm missing something in my code. Please help me out!</p>

<p>EDIT:</p>

<p>Thanks to <strong>F10</strong> for pointing me in the right direction. This is how I got it to work with a local image.</p>

<pre><code># export GOOGLE_APPLICATION_CREDENTIALS=kyourcredentials.json
import io
import cv2
from PIL import Image

# Imports the Google Cloud client library
from google.cloud import vision
from google.cloud.vision import types
from google.cloud.vision import enums

# Instantiates a client
client = vision.ImageAnnotatorClient()

with io.open(""test.png"", 'rb') as image_file:
    content = image_file.read()

features = [
    types.Feature(type=enums.Feature.Type.DOCUMENT_TEXT_DETECTION)
]


image = types.Image(content=content)

request = types.image_annotator_pb2.AnnotateImageRequest(image=image, features=features)
response = client.annotate_image(request)

print(response)
</code></pre>",55284207,1,0,,2019-03-21 14:50:55.113 UTC,,2019-03-21 19:21:14.440 UTC,2019-03-21 19:21:14.440 UTC,,5672476,,5672476,1,0,ocr|google-cloud-vision|python-tesseract,61
Google Vision API CameraSourcePreview FPS,48608334,Google Vision API CameraSourcePreview FPS,"<p>I am trying to develop android face recognition app. Starting from <a href=""https://github.com/googlesamples/android-vision/tree/master/visionSamples/FaceTracker"" rel=""nofollow noreferrer"">Google Vision API FaceTracker example</a> - <strong>how can I get camera preview FPS value</strong>? </p>

<p>I understand that I can set requested FPS on <code>CameraSource.Builder()</code> with <code>setRequestedFps()</code>, but I want <strong>actual</strong> FPS value updated every second or every new frame. Is it possible to get or calculate it?</p>",,1,0,,2018-02-04 13:03:17.687 UTC,,2018-02-04 16:02:44.787 UTC,2018-02-04 15:17:24.490 UTC,,2649012,,2568647,1,1,android|android-camera|frame-rate|face-recognition|google-vision,277
Google Vision API with Selenium,54996104,Google Vision API with Selenium,"<p>We are doing <code>Selenium Automation</code> and we do have a requirement to validate text and graph in images.
I tried <code>tesseract</code> but it is not giving a consistent result.
I came to know about <code>Google Vision API</code>, but don't know how to set it up with Eclipse Java.</p>

<p>Can someone tell me,</p>

<ol>
<li>Is it paid?</li>
<li>How to set it up with the existing selenium maven project.</li>
</ol>

<p>Any other accurate alternative for validating images?</p>

<p>Thanks,
Nilesh</p>",,0,0,,2019-03-05 05:40:39.580 UTC,,2019-03-05 10:42:48.073 UTC,2019-03-05 10:42:48.073 UTC,,10814567,,10762815,1,1,java|maven|google-app-engine|selenium-webdriver,31
Google Vision Api: Differentiate Real Face and A Photo,45563012,Google Vision Api: Differentiate Real Face and A Photo,<p>I'm trying develop a facial recognition system. But the issue is it can be by-passed by a photo. I'm using google vision api to detect faces. Is there a way to avoid detecting faces in a photo? Just want to know if there is a real person standing in front of the camera.</p>,,1,0,,2017-08-08 08:21:24.697 UTC,,2017-08-08 21:05:25.260 UTC,,,,,2214782,1,0,android|face-recognition|google-vision,485
Need answer about some Machine Learning related questions?,44149272,Need answer about some Machine Learning related questions?,"<p>Recently, we planned to build a system for image processing to extract info from images. At present we are using <code>AWS</code> Rekognition to do that. But, in some cases, we are not getting accurate information from AWS. So, we've planned to build our own custom one.</p>

<p>We've 4/5 months to do that. At least a <code>POC</code> version. Also, we've planned to use <code>Tensorflow</code> for that. We all have no prior experience about Machine Learning &amp; Deep Learning but already have 5/6yrs of experience on Computer Programming by using different languages. </p>

<p>Currently, I'm studying ML from a course of <code>Udemy</code> &amp; my approach to solve this problem is...</p>

<ul>
<li>Learn Machine Learning(ML)</li>
<li>Learn Deep Learning(DL)</li>
<li>Above ML &amp; DL maybe I'll be ready to understand the whole thing &amp; can able to build a system for Image Processing.</li>
</ul>

<p>In abstract what I've understood is, I've to write one Deep Learning program in <code>Python</code> by using <code>Tensorflow</code>. By using that Program I've to build a Model. Then I've to train that Model by using some training data. Then, when my Model achieves a certain level of accuracy I'll use some test data. </p>

<p>Now, there some places at where I've bit confused &amp; here are my questions regarding that confusion...</p>

<ol>
<li><p>I know <code>tensorflow</code> is a library but at some places, it's also mentioned as a system. So, is it really a library(piece of code) only &amp; something more than that? </p></li>
<li><p>I got some Image Processing Python code in <code>Tensorflow</code> tutorial section (<a href=""https://www.tensorflow.org/tutorials/image_recognition"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/image_recognition</a>). We've tested that code &amp; it's working exactly the way <code>AWS</code> Recognition service work. So, here my doubt is... can I use this Python code as it is in our production work?</p></li>
<li><p>After train a model with some training data does those training data get part of the whole system or Machine Learning Model extract some META info from those training data &amp; keep with itself rather whole raw training data(in my case it'll be raw images).</p></li>
<li><p>Can I do all these ML+DL programmings over my Linux System? It has Pentium 4 with 8GB RAM.</p></li>
<li><p>Also, want to know... the approach which I've mentioned to build a solution for my problem is sufficient or I need to do something else also.</p></li>
</ol>

<p>Need some guidance to clear out all these confusion.</p>

<p>Thanks</p>",44150906,1,4,,2017-05-24 04:55:57.067 UTC,,2017-05-24 09:16:08.143 UTC,2017-05-24 09:16:08.143 UTC,,304215,,304215,1,1,machine-learning|tensorflow|deep-learning,98
"AWS lambda, Unknown service: 'quicksight'",53893575,"AWS lambda, Unknown service: 'quicksight'","<p>I am using boto3 API to create groups in quicksight from lambda, having role full access to quicksight</p>

<pre><code>import boto3

quicksight_client = boto3.client('quicksight', region_name='us-east-1', 
                                 aws_access_key_id=ACCESS_ID, 
                                 aws_secret_access_key=ACCESS_KEY)

def lambda_handler(event, context):
    # list groups present in quicksight
    response = quicksight_client.list_groups(
        AwsAccountId=event['AWS Account ID'],
        Namespace='default'
    )
</code></pre>

<p>facing following error</p>

<p><code>Unknown service: 'quicksight'. Valid service names are: acm, acm-pca, alexaforbusiness, apigateway, application-autoscaling, appstream, appsync, athena, autoscaling, autoscaling-plans, batch, budgets, ce, cloud9, clouddirectory, cloudformation, cloudfront, cloudhsm, cloudhsmv2, cloudsearch, cloudsearchdomain, cloudtrail, cloudwatch, codebuild, codecommit, codedeploy, codepipeline, codestar, cognito-identity, cognito-idp, cognito-sync, comprehend, config, connect, cur, datapipeline, dax, devicefarm, directconnect, discovery, dlm, dms, ds, dynamodb, dynamodbstreams, ec2, ecr, ecs, efs, eks, elasticache, elasticbeanstalk, elastictranscoder, elb, elbv2, emr, es, events, firehose, fms, gamelift, glacier, glue, greengrass, guardduty, health, iam, importexport, inspector, iot, iot-data, iot-jobs-data, iot1click-devices, iot1click-projects, iotanalytics, kinesis, kinesis-video-archived-media, kinesis-video-media, kinesisanalytics, kinesisvideo, kms, lambda, lex-models, lex-runtime, lightsail, logs, machinelearning, macie, marketplace-entitlement, marketplacecommerceanalytics, mediaconvert, medialive, mediapackage, mediastore, mediastore-data, mediatailor, meteringmarketplace, mgh, mobile, mq, mturk, neptune, opsworks, opsworkscm, organizations, pi, pinpoint, polly, pricing, rds, redshift, rekognition, resource-groups, resourcegroupstaggingapi, route53, route53domains, s3, sagemaker, sagemaker-runtime, sdb, secretsmanager, serverlessrepo, servicecatalog, servicediscovery, ses, shield, sms, snowball, sns, sqs, ssm, stepfunctions, storagegateway, sts, support, swf, transcribe, translate, waf, waf-regional, workdocs, workmail, workspaces, xray</code></p>

<p>when same code is executed from my computer its working, but not from lambda</p>

<p>can anyone help me with this?</p>",53897197,2,4,,2018-12-22 06:33:49.553 UTC,,2018-12-22 16:02:07.643 UTC,2018-12-22 06:42:27.733 UTC,,119861,,4277485,1,1,python|amazon-web-services|lambda|boto3,302
Is the Open Sourced Google Vision API CameraSource class Out-Dated?,39797164,Is the Open Sourced Google Vision API CameraSource class Out-Dated?,"<p>In this <a href=""https://stackoverflow.com/questions/32051973/google-vision-api-samples-get-the-camerasource-to-focus?rq=1"">SO Thread</a>, <a href=""https://stackoverflow.com/users/5231007/pm0733464"">pm0733464</a>, says this: </p>

<blockquote>
  <p>we open sourced the CameraSource class, which has an auto focus method as well. This one allows you to set a specific focus mode as opposed to the ""continuous video"" mode that the official API defaults to:</p>
</blockquote>

<p>Which was great. But it seems that the Google Vision API has moved on and the open Sourced version has not. The official API now has a new type of processor called: FocusingProcessor -- which allows the detector to only respond on the OnFocus event. </p>

<pre><code> barcodeDetector = new BarcodeDetector.Builder(this)
            .setBarcodeFormats(Barcode.QR_CODE | Barcode.PDF417)
            .build();
 barcodeDetector.setProcessor(new BarcodeFocusingProcessor(
            barcodeDetector,
            new NullTracker()));
 CameraSource.Builder builder = new CameraSource.Builder(getApplicationContext(), barcodeDetector)
            .setFacing(CameraSource.CAMERA_FACING_BACK)
            .setRequestedPreviewSize(1600, 1024)
            .setAutoFocusEnabled(true)
            .setRequestedFps(24.0f);
    cameraSource = builder.build();
</code></pre>

<p>In my experiments this ""finds"" barcodes much faster than using the processor that the examples show in the <a href=""https://github.com/googlesamples/android-vision/blob/master/visionSamples/barcode-reader/app/src/main/java/com/google/android/gms/samples/vision/barcodereader/ui/camera/CameraSource.java"" rel=""nofollow noreferrer"">Official Google Vision API Samples</a></p>

<p>Am I missing something somewhere? Or is the CameraSource in the Google.Vision libraries not the same one they are showing in the open source?</p>

<p>[EDIT] 
Sharing code by request of pm0733464:</p>

<p>For the record, I began with the fork of the vision api Demo which allows for <a href=""https://github.com/wax911/Vision-Barcode-Scanner"" rel=""nofollow noreferrer"">automatically detecting barcode</a></p>

<p>My code makes some simple changes. First off, I add PDF417 to the scanable barcodes. Then I set the processor to an autofocus-er. I turn the tracker into a nullTracker because I don't need the graphic displaying, and I hoped that would speed some things up</p>

<p>in  <strong>BarcodeCaptureActivity</strong> I change <strong>createCameraSource</strong> where it defined the barcode detector like to this:</p>

<pre><code>  BarcodeDetector barcodeDetector =
                new BarcodeDetector.Builder(context)
                        .setBarcodeFormats(Barcode.PDF417)
                        .build();


        barcodeDetector.setProcessor(new MyCameraFocusingProcessor(
                barcodeDetector,
                new NullTracker()));


// Creates and starts the camera.  Note that this uses a higher resolution in comparison
        // to other detection examples to enable the barcode detector to detect small barcodes
        // at long distances.

        CameraSource.Builder builder = new CameraSource.Builder(getApplicationContext(), barcodeDetector)
                .setFacing(CameraSource.CAMERA_FACING_BACK)
                .setRequestedPreviewSize(1600, 1024)
                .setRequestedFps(24.0f);

        // make sure that auto focus is an available option
        if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.ICE_CREAM_SANDWICH)
        {
            builder = builder.setFocusMode(
                    autoFocus ? Camera.Parameters.FOCUS_MODE_CONTINUOUS_PICTURE : null);
        }

        mCameraSource = builder
                .setFlashMode(useFlash ? Camera.Parameters.FLASH_MODE_TORCH : null)
                .build();
    }
</code></pre>

<p>My FocusProcessor (in the same class) looks like this:</p>

<pre><code>private class MyCameraFocusingProcessor implements Detector.Processor&lt;Barcode&gt;
    {
        public MyCameraFocusingProcessor(BarcodeDetector barcodeDetector, NullTracker emptyTracker)
        {

        }

        @Override
        public void release()
        {

        }

        @Override
        public void receiveDetections(Detector.Detections&lt;Barcode&gt; detections)
        {
            //  boolean chk = detections.detectorIsOperational();
            int sizeCheck = detections.getDetectedItems().size();
            if (sizeCheck &gt; 0)
            {
                SparseArray&lt;Barcode&gt; codes = detections.getDetectedItems();
                for (int i = 0; i &lt; sizeCheck; i++)
                {
                    Barcode barcode = codes.valueAt(i);
                    try
                    {
                        if (barcode.format == Barcode.PDF417)
                        {
                            Intent data = new Intent();
                            data.putExtra(BarcodeObject, barcode);
                            setResult(CommonStatusCodes.SUCCESS, data);
                            finish();
                            break;
                        }
                    }
                    catch (Exception ex)
                    {
                        Log.d(""Detect"", ""Error: "" + ex.getMessage());
                    }
                }
            }
            return;
        }
    }

    private class NullTracker
    {

    }
</code></pre>",,1,0,,2016-09-30 17:42:43.950 UTC,1,2016-10-10 16:06:43.417 UTC,2017-05-23 12:00:40.917 UTC,,-1,,628887,1,1,android|android-camera|google-play-services|android-vision|google-vision,1384
Appsync Image Process then Upload using Lambda as Data Source,53794706,Appsync Image Process then Upload using Lambda as Data Source,"<p>I would like to know best practice or example of how to set this up.</p>

<p>My use case is for a React.js client app to upload a user avatar photo to AWS via AppSync API endpoint for processing, replication, and storage.  A lambda function on the backend will process the image for cropping and resizing to multiple sizes for storage to S3 (e.g. sm, md, lg, xl). I also want to apply Image Rekognition to automate flagging of inappropriate content as part of the image processing pipeline.</p>

<p>I understand that AppSync / Amplify can be used to upload complex objects by defining an S3Object type in the graphl schema.  But this approach seems to limiting for what I would like to do.</p>",,0,0,,2018-12-15 15:42:38.847 UTC,,2018-12-16 01:56:27.783 UTC,2018-12-16 01:56:27.783 UTC,,174777,,10666046,1,0,amazon-web-services|amazon-s3|aws-lambda,36
How can I fix response text?,40349933,How can I fix response text?,"<p>I use Google Cloud Vision Api (Text_Detection) it is working normal but when I return answer from the Google, message style like image </p>

<p>I want just one text e.g ""ACADEMIC PLANNER"" so how can I remove front of Academic ""null:"" and other words?</p>

<p>Image e.g </p>

<p><a href=""https://i.stack.imgur.com/OE5U4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OE5U4.png"" alt=""enter image description here""></a></p>

<p>And here is my code;</p>

<pre><code>private String convertResponseToString(BatchAnnotateImagesResponse response) {
    String message = ""I found these things:\n\n"";

    List&lt;EntityAnnotation&gt; labels = response.getResponses().get(0).getTextAnnotations();
    if (labels != null) {
        for (EntityAnnotation label : labels) {
            message += String.format(""%.3f: %s"", label.getScore(), label.getDescription());
            message += ""\n"";
        }
    } else {
        message += ""nothing"";
    }

    return message;
}
</code></pre>",40487021,3,0,,2016-10-31 20:17:06.903 UTC,,2018-10-28 01:36:27.577 UTC,2018-10-28 01:36:27.577 UTC,,1033581,,5770137,1,-2,android|response,183
Extract text using Google Vision Api combined with Credit Card Scanner?,52448751,Extract text using Google Vision Api combined with Credit Card Scanner?,"<p>I am trying to understand how text recognition works in Android, so I decided to create an app that can scan credit card and extract info (card number and expiry date).</p>

<p>I found this open source: <a href=""https://github.com/faceterteam/PayCards_Android"" rel=""nofollow noreferrer"">https://github.com/faceterteam/PayCards_Android</a> and I hoped that it would work properly. </p>

<p>It turns out that this can capture and extract numbers well IF the numbers aren't printed flat on the card.</p>

<p>Now, I know that the Google Vision Api makes it possible for me to make my phone recognize printed numbers on cards, but not embossed numbers.</p>

<p>So I would love to combine these two. Unfortunately, I don't know how to, yet.</p>

<p>I found out that the Google Vision Api can recognize numbers from bitmap. But the point is, I am not familiar to how cameras work in Android.</p>

<p>My plan is to use the PayCards for Android, and while it continuously tries to detect embossed numbers, frame by frame, use Google Vision on these frames to check if there are printed numbers instead of embossed numbers.</p>

<p>Is there a way to get a bitmap image out of a camera preview for me to use Google Vision on? I just don't know where to put my Google Vision codes.</p>

<p>Help me, please.</p>",,1,0,,2018-09-21 17:46:14.500 UTC,0,2018-09-22 13:51:24.967 UTC,,,,,3630606,1,1,android|opencv|ocr|text-recognition,300
Image size is too small Azure Face API Android,49388341,Image size is too small Azure Face API Android,"<p>I am trying to use the Azure Face API on android. I am capturing an image from the device camera and then converting it to an InputStream to be sent to the detect method. I keep getting the error ""com.microsoft.projectoxford.face.rest.ClientException: Image size is too small""</p>

<p>I checked the documentation and the image size is 1.4Mb which is within the 1Kb-4Mb range. I don't understand why it isn't working.</p>

<pre><code>Bitmap bitmap = cameraKitImage.getBitmap();
ByteArrayOutputStream bos = new ByteArrayOutputStream();
bitmap.compress(Bitmap.CompressFormat.PNG, 100, bos);
bitmapdata = bos.toByteArray();

new FaceTask().execute(new ByteArrayInputStream(bitmapdata));

Face[] faces = faceServiceClient.detect(inputStreams[0], true, false, null);
</code></pre>",,0,4,,2018-03-20 15:34:27.023 UTC,1,2018-12-21 01:33:33.110 UTC,2018-03-22 02:53:04.560 UTC,,9523712,,9523712,1,4,android|azure|microsoft-cognitive|face-api,563
Can I use Amazon Rekognition video api without storing videos in S3 buckets?,54897189,Can I use Amazon Rekognition video api without storing videos in S3 buckets?,<p>All of Amazon documentation on their Video Rekognition API are examples of videos that are stored in S3 bucket. Is there anyone out there who have tried using the API without storing the videos in S3 i.e. on local machine or GCS?</p>,,1,0,,2019-02-27 02:36:57.307 UTC,,2019-02-27 03:44:26.023 UTC,2019-02-27 03:41:50.037 UTC,,174777,,7692719,1,0,amazon-web-services|amazon-s3|amazon-rekognition,27
Can i give aspect ratio in Google Vision api?,44304400,Can i give aspect ratio in Google Vision api?,"<blockquote>
  <p>I was exploring google vision and in the specific function
  'detectCrops', gives me the crop hints. what does this means exactly?</p>
</blockquote>

<p>I tried hitting the api with a sample image and got a response of an array with four coordinates. what does this coordinates signify? is the aspect ratio fixed? can i specify a specific aspect ratio? . The documentation is not clear or i am not able to understand.</p>

<p><strong>My code</strong></p>

<pre><code>var vision = require('@google-cloud/vision')({
  projectId: Credentials.PROJECT_ID,
  credentials: Credentials.AUTH_KEY
})

vision.detectCrops('img.jpg', function(err, crops, apiResponse) {
  console.log('err', err)
  console.log('crops', crops)
  console.log('apiResponse', apiResponse)
})
</code></pre>

<p><strong>The response</strong></p>

<pre><code>err null

crops [ [ { x: 0, y: 0 },
  { x: 649, y: 0 },
  { x: 649, y: 399 },
  { x: 0, y: 399 } ] ]

apiResponse { responses:
 [ { faceAnnotations: [],
   landmarkAnnotations: [],
   logoAnnotations: [],
   labelAnnotations: [],
   textAnnotations: [],
   fullTextAnnotation: null,
   safeSearchAnnotation: null,
   imagePropertiesAnnotation: null,
   cropHintsAnnotation: [Object],
   webDetection: null,
   error: null } ] }
</code></pre>",44321450,1,0,,2017-06-01 09:58:23.883 UTC,1,2017-06-02 05:31:00.150 UTC,,,,,5924523,1,0,node.js|google-vision,157
Accessing the Camera from com.google.android.gms.vision.CameraSource and Increasing/Decreasing the Preview Brightness,49752955,Accessing the Camera from com.google.android.gms.vision.CameraSource and Increasing/Decreasing the Preview Brightness,"<p>I Have to implement in Google Vision API's CameraSource for build the camera and do the face detection on before capturing the image. Now I have faced few issues, So I need to access the camera object from CameraSource.</p>

<ol>
<li>How could I achieve the increase or Decrease the Camera Preview Brightness using CameraSource?</li>
</ol>

<p>This is my CameraSource Builder</p>

<pre><code> mCameraSource = new CameraSource.Builder(context, detector)
                .setRequestedPreviewSize(640, 480)
                .setFacing(CameraSource.CAMERA_FACING_FRONT)
                .setRequestedFps(30.0f)
                .build();
</code></pre>

<p>Here I have to try to access/get the camera from mCameraSource object.</p>

<pre><code>Field[] declaredFields = CameraSource.class.getDeclaredFields();

        for (Field field : declaredFields) {
            if (field.getType() == Camera.class) {
                field.setAccessible(true);
                try {
                    Camera camera = (Camera) field.get(mCameraSource);
                    if (camera != null) {
                        Camera.Parameters params = camera.getParameters();                        
                        params.setExposureCompensation(1500);
                        camera.setParameters(params);
                    }
                } catch (IllegalAccessException e) {
                    e.printStackTrace();
                }

            }
        }
</code></pre>

<p>but the <code>camera</code> returns null only, And My 2nd Question is how to do brightness option...</p>

<pre><code>if (camera != null) {
     Camera.Parameters params = camera.getParameters();                        
     params.setExposureCompensation(1500);
     camera.setParameters(params);
 }
</code></pre>",49767713,1,3,,2018-04-10 11:52:54.003 UTC,,2018-04-11 06:11:43.553 UTC,2018-04-10 12:41:56.357 UTC,,5230881,,5230881,1,2,java|android|camera|surfaceview|vision-api,782
How to Analyze video using google cloud vision and nodejs,55766327,How to Analyze video using google cloud vision and nodejs,<p>I have videos stored at amazon s3 cloud and i need to analyze it using google vision cloud and nodejs. Please help me.</p>,,0,0,,2019-04-19 18:39:59.840 UTC,,2019-04-19 18:39:59.840 UTC,,,,,11385412,1,0,node.js|cloud|vision,11
What are the rules for class names in IBM Watson Visual Recognition service?,38679651,What are the rules for class names in IBM Watson Visual Recognition service?,"<p>I'm exploring IBM Watson Visual Recognition service and when I create a classifier using classnames like '<strong>black-dog</strong>' (i.e. black-dog_positive_example),  this classname is later returned as '<strong>black_dog</strong>' (with <strong>underscore</strong> replacing <strong>dash</strong>) when I classify an image using the <code>/v3/classify</code> endpoint. </p>

<p>But when I retrieve the classifier details with the <code>/v3/classifiers/{classifier_id}</code> the class is correctly listed as 'black-dog'.</p>

<p>So, my result for <code>GET /v3/classifiers/{classifier_id}</code> is like:</p>

<pre><code>{
    ""classifier_id"": ""dog_561932172"",
    ""name"": ""dog"",
    ""owner"": ""xxxxxxxx-xxx-xxx-xxx-xxxxxxxxxxxx"",
    ""status"": ""ready"",
    ""created"": ""2016-07-30T22:06:39.327Z"",
    ""classes"": [
        {""class"": ""black-dog""}
    ]
}
</code></pre>

<p>While my result for <code>GET /v3/classify</code> is</p>

<pre><code>{
  ""custom_classes"": 1,
  ""images"": [
    {
      ""classifiers"": [
        {
          ""classes"": [
            {
              ""class"": ""black_dog"",
              ""score"": 0.546941
            }
          ],
          ""classifier_id"": ""dog_561932172"",
          ""name"": ""dog""
        }
      ],
      ""image"": ""20160620_142113.jpg""
    }
  ],
  ""images_processed"": 1
}
</code></pre>

<p>So is this expected or a defect? Should I avoid using ""-"" in class names? Are there any other rules for the value of a classname?</p>",39359093,2,1,,2016-07-31 00:12:59.513 UTC,1,2016-09-10 20:25:08.797 UTC,2016-07-31 01:14:30.717 UTC,,6145796,,1956187,1,1,ibm-cloud|ibm-watson|visual-recognition,107
Error parsing 'parameters' JSON Watson Visual Recognition,49126860,Error parsing 'parameters' JSON Watson Visual Recognition,"<p>I am getting the below error while calling Watson Visual Recognition API through Java. Any help will be highly appreciated.</p>

<pre><code>       VisualRecognition service = new VisualRecognition(VisualRecognition.VERSION_DATE_2016_05_20);
       service.setApiKey(""api_key"");

       InputStream imagesStream = new FileInputStream(""C:\\fruitbowl.jpg"");
       ClassifyOptions classifyOptions =
               new ClassifyOptions.Builder().imagesFile(imagesStream).imagesFilename(""fruitbowl.jpg"")
                       .parameters(""{\""classifier_ids\"": [\""fruits_1462128776\"", + \""SatelliteModel_6242312846\""],\""threshold\"": 0.6}"")
                       .build();
       ClassifiedImages result = service.classify(classifyOptions).execute();
       System.out.println(result);
</code></pre>

<p>Stacktrace:</p>

<pre><code>  SEVERE: POST https://gateway-a.watsonplatform.net/visual-recognition  /api/v3/classify?version=2016-05-20&amp;api_key=0b5b96d2428f020c207a9388f2bb1ee840e57c9c, status: 400, error: {
""error"": {
    ""code"": 400,
    ""error_id"": ""input_error"",
    ""description"": ""Error parsing 'parameters' JSON. Ensure threshold is a float; owner and classifier-ids are string arrays; url is a string.""
}}
</code></pre>",,2,0,,2018-03-06 08:58:03.387 UTC,,2018-03-08 19:08:46.997 UTC,2018-03-08 01:40:51.633 UTC,,3198917,,1305448,1,0,java|ibm-cloud|ibm-watson|visual-recognition,132
Amazon aws-cpp-sdk Index Faces giving segfault,49444278,Amazon aws-cpp-sdk Index Faces giving segfault,"<p>I'm having a problem with <a href=""https://github.com/aws/aws-sdk-cpp/blob/master/aws-cpp-sdk-rekognition/include/aws/rekognition/model/IndexFacesRequest.h"" rel=""nofollow noreferrer"">Index_Faces</a> using Amazon <strong>aws-cpp-sdk</strong>. I'm getting <strong>segmentation</strong> fault in following program.</p>

<pre><code>Image *fileData;

Image&amp; imageToBytes(const char* str)
{
    FILE *fp;
    size_t length = 0;

    fp= fopen(str, ""rb"");
    if(fp == NULL)
    {
        exit(0);
    }
    fseek(fp, 0, SEEK_END);
    length= ftell(fp);
    rewind(fp);
    fileData= (Image*)malloc((length+1)*sizeof(char));
    fread(fileData, length, 1, fp);
    return *fileData;
}

int main()
{
   Aws::SDKOptions options;

   Aws::InitAPI(options);
   {
       RekognitionClient *rekClient = new RekognitionClient();
       CreateCollectionRequest *clRequest = new CreateCollectionRequest();

       CreateCollectionRequest str = clRequest-&gt;WithCollectionId(""collection7981"");

       CreateCollectionOutcome respose = rekClient-&gt;CreateCollection(str);
       std::cout&lt;&lt;""Collection Successfully Created...""&lt;&lt;std::endl;

       IndexFacesRequest iFaceRequest;

       iFaceRequest.WithImage(imageToBytes(""/home/msc/Profile_Pics/ms.JPG""));

   }
   Aws::ShutdownAPI(options);
   return 0;
}
</code></pre>

<p>So, how to provide an image file from my local system to amazon aws-cpp-sdk?</p>",,1,3,,2018-03-23 07:13:50.410 UTC,1,2018-03-23 07:47:06.340 UTC,2018-03-23 07:37:39.123 UTC,,5612562,,5612562,1,-1,c++|c++11|segmentation-fault|aws-sdk-cpp,57
cx_Freeze module with version number,54418688,cx_Freeze module with version number,"<p>I would like to specify multiple modules to install by version number.</p>

<p>If this is my <code>setup.py</code>:</p>

<p><strong>How can I adjust the contents of the mods list to specify the version number?</strong></p>

<pre class=""lang-python prettyprint-override""><code>    from cx_Freeze import setup, Executable
    import sys

    base = None

    if sys.platform == 'win32':
        base = None


    executables = [Executable(""main.py"", base=base)]

    mods = [""cachetools==3.0.0"",
    ""pyexcel-xlsx==0.5.6"",
    ""idna""]

    packages = mods
    options = {
        'build_exe': {
            'packages':packages,
        },

    }

    setup(
        name = ""file_rename"",
        options = options,
        version = ""1"",
        description = 'renames pdf consent forms',
        executables = executables
    )
</code></pre>

<p><strong>Edit:</strong>
The utilities.py file begins:</p>

<pre><code>import os
from os.path import dirname
import json
import io
from google.cloud import vision
</code></pre>

<p>The error I get after applying the advice from @jpeg, is as follows:
<a href=""https://i.stack.imgur.com/hdueD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hdueD.png"" alt=""error""></a></p>

<p>My pip freeze is:</p>

<pre><code>appdirs==1.4.3
beautifulsoup4==4.7.1
cachetools==3.0.0
certifi==2018.11.29
chardet==3.0.4
cx-Freeze==5.1.1
dialogflow==0.5.1
dj-database-url==0.5.0
Django==2.1.4
django-heroku==0.3.1
et-xmlfile==1.0.1
google==2.0.1
google-api-core==1.6.0
google-auth==1.6.1
google-cloud==0.34.0
google-cloud-core==0.28.1
google-cloud-storage==1.13.1
google-cloud-vision==0.35.1
google-resumable-media==0.3.1
googleapis-common-protos==1.5.5
grpcio==1.16.1
gunicorn==19.6.0
idna==2.7
image==1.5.27
jdcal==1.4
lml==0.0.7
numpy==1.15.4
openpyxl==2.5.12
packaging==19.0
pandas==0.23.4
pdf2image==1.1.0
Pillow==5.3.0
protobuf==3.6.1
psycopg2==2.7.6.1
pyasn1==0.4.4
pyasn1-modules==0.2.2
pyexcel==0.5.10
pyexcel-io==0.5.11
pyexcel-xlsx==0.5.6
pyparsing==2.3.1
pypng==0.0.19
python-dateutil==2.7.5
pytz==2018.7
requests==2.21.0
rsa==4.0
six==1.11.0
soupsieve==1.7.3
texttable==1.5.0
urllib3==1.24.1
whitenoise==3.3.1
wincertstore==0.2
XlsxWriter==1.1.2
</code></pre>

<p>The issue doesn't show itself when running the script normally, only after my exe is created and run, does an error appear.</p>",,1,0,,2019-01-29 10:17:03.570 UTC,,2019-02-05 12:18:59.440 UTC,2019-02-05 10:09:24.163 UTC,,8297514,,8297514,1,0,python|cx-freeze,110
Used Google-Cloud-Vision does not return the Bangla Texts,52475518,Used Google-Cloud-Vision does not return the Bangla Texts,"<p>I have connected my python program with Google-cloud-vision through API. I am getting the label_detection, Text_Detections both work and it returns only English text detections and ignore the Bangla strings/char part from the Image. In both Python and JSON output I am successfully getting English Text, but No Bangla text.  Could you please help how to solve Bangla detection part.  So that I can get both (English and Bangla Text) from the Image, for hint, same Image (Bangla+English mixed) give proper output in Google-Cloud-Vision <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/</a> page, where it says TYR THIS API. </p>",,2,0,,2018-09-24 08:41:43.610 UTC,,2018-11-08 14:21:35.293 UTC,,,,,10406791,1,0,google-cloud-vision,73
How to set a rectangular focus area on Google Vision Camera Source?,49217996,How to set a rectangular focus area on Google Vision Camera Source?,"<p>I'm currently having problems with Google vision. There is nothing wrong with the library actually, its works great. All what I'm trying to accomplish now is set a rectangular area where the CameraSource will focus only on. The aim is try to capture text within that particular rectangular box only. I've tried many examples on StackOverflow but they all didn't seem to work. My current working code with the Google Vision ( 11.8.0 ) is </p>

<pre><code>cameraView = findViewById(R.id.surfaceview);
        output = findViewById(R.id.output);

        TextRecognizer textRecognizer = new TextRecognizer.Builder(ScanVoucher.this).build();

        if(!textRecognizer.isOperational()) {
            show_alert(""Text Recognition not supported on this device"");
        } else {

            cameraSource = new CameraSource.Builder(ScanVoucher.this, textRecognizer)
                    .setAutoFocusEnabled(true)
                    .setFacing(CameraSource.CAMERA_FACING_BACK)
                    .setRequestedFps(2.0f)
                    .setRequestedPreviewSize(300, 300)
                    .build();

            cameraView.getHolder().addCallback(new SurfaceHolder.Callback() {
                @Override
                public void surfaceCreated(SurfaceHolder surfaceHolder) {
                    try {
                        cameraSource.start(cameraView.getHolder());
                    } catch (IOException e) {
                        show_alert(""Unable to access camera"");
                    } catch (SecurityException e) {
                        finish();
                    } catch(Exception e) {
                        //Kill
                        finish();
                    }
                }

                @Override
                public void surfaceChanged(SurfaceHolder surfaceHolder, int i, int i1, int i2) {

                }

                @Override
                public void surfaceDestroyed(SurfaceHolder surfaceHolder) {
                    cameraSource.stop();
                }
            });

            textRecognizer.setProcessor(new Detector.Processor&lt;TextBlock&gt;() {
                @Override
                public void release() {

                }

                @Override
                public void receiveDetections(Detector.Detections&lt;TextBlock&gt; detections) {
                    final SparseArray&lt;TextBlock&gt; items = detections.getDetectedItems();

                    if(items.size() != 0) {
                        output.post(new Runnable() {
                            @Override
                            public void run() {
                                StringBuilder builder = new StringBuilder();

                                for(int i = 0; i &lt; items.size(); i++) {
                                    TextBlock item = items.valueAt(i);
                                    builder.append(item.getValue());
                                }

                                try {
                                    output.setText(builder.toString());
                                } catch (Exception e) {
                                    output.setText(e.getMessage());
                                }

                            }
                        });
                    }
                }
            });
        }
</code></pre>

<p>As I said, everything works great. Wondering how I could set the rectangular box like preview area so that the camera only captures text with the box just like the QR or bar code scanner apps. Thanks in advance.</p>",,1,2,,2018-03-11 08:10:49.417 UTC,,2019-03-17 18:14:37.240 UTC,2018-03-11 10:55:18.820 UTC,,2649012,,9474482,1,0,java|android,140
Google vision API giving sporadic 403 errors,40450515,Google vision API giving sporadic 403 errors,"<p>I have a very basic python app that calls the google vision API and asks for OCR on an image.</p>

<p>It was working fine a few days ago using a basic API key. I have since created a modified version that uses a service account as well, which also worked. </p>

<p>All my images are ~500kB</p>

<p>However, today about 80% of all calls return ""403 reauthorized"" when I try to run OCR on the image. The remainder run as they always have done...</p>

<p>The google quotas limit page lists:</p>

<pre>
MB per image    4 MB
MB per request  8 MB
Requests per second 10
Requests per feature per day    700,000
Requests per feature per month  20,000,000
Images per second   8
Images per request  16
</pre>

<p>And I am way below any of these limits (by orders of magnitude) - any idea what might be going on? </p>

<p>It seems strange that simply running the same code, with the same input images, will sometimes give a 403 and sometimes not....perhaps the error is indicative of the API struggling with demand? </p>",,1,2,,2016-11-06 14:36:08.617 UTC,,2016-11-09 01:04:58.317 UTC,2016-11-06 19:34:41.320 UTC,,1447903,,1447903,1,1,api|google-cloud-vision,631
Grpc.Core.RpcException StatusCode Unavailable Channel is in state TRANSIENT_FAILURE,54581027,Grpc.Core.RpcException StatusCode Unavailable Channel is in state TRANSIENT_FAILURE,"<p>I am using Google Vision API to get associated labels for an image.</p>

<pre><code> var client = ImageAnnotatorClient.Create();
 var image = Image.FromFile(@""C:\Users\Scorpio\Desktop\th.jpg"");
 var response = client.DetectLabels(image); // error
 foreach (var annotation in response)
 {
     if (annotation.Description != null)
           Console.WriteLine(annotation.Description);
 }
</code></pre>

<p><a href=""https://i.stack.imgur.com/RKIlr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RKIlr.png"" alt=""enter image description here""></a></p>

<p>Any idea how can we resolve this issue? I tried using very common images like country flags but still it gives error.</p>",,2,1,,2019-02-07 19:38:02.703 UTC,,2019-03-18 15:59:09.003 UTC,,,,,1700747,1,1,c#|google-cloud-platform|google-cloud-vision,620
Using a variable in the later part of the program (python),53460840,Using a variable in the later part of the program (python),"<p>I'm trying to connect two separate codes into one program. I need to put one string from first to second part.</p>

<p>First:</p>

<pre><code>import boto3

if __name__ == ""__main__"":

    bucket='BUCKET-NAME'
    collectionId='COLLECTION-ID'
    fileName='input.jpg'
    threshold = 70
    maxFaces=1

    client=boto3.client('rekognition')


    response=client.search_faces_by_image(CollectionId=collectionId,
                                Image={'S3Object':{'Bucket':bucket,'Name':fileName}},
                                FaceMatchThreshold=threshold,
                                MaxFaces=maxFaces)


    faceMatches=response['FaceMatches']
    for match in faceMatches:
            print (match['Face']['FaceId'])
</code></pre>

<p>Second:</p>

<pre><code>import boto3
from boto3.dynamodb.conditions import Key, Attr

dynamodb = boto3.resource('dynamodb')

table = dynamodb.Table('faces')

response = table.scan(
    FilterExpression=Attr('faceid').eq('FaceId')
)
items = response['Items']
print(items)
</code></pre>

<p>I need to put ID shown by <code>print (match['Face']['FaceId'])</code> from first code to <code>FaceId</code> in second code. </p>

<p>I tried to define a variable and put a value into it and then get it later but I could not do it correctly</p>",,1,4,,2018-11-24 17:45:16.770 UTC,,2018-11-24 18:13:03.903 UTC,,,,,9590393,1,1,python|amazon-web-services,31
Access Microsoft Face API IOS with Swift,47439799,Access Microsoft Face API IOS with Swift,"<p>I am trying to enter to the API from Microsoft FACE API. I created an account on Azure and created the service that they provide me the keys.
The point is that I am trying to get access  to the API and it's all time a 401 error</p>

<pre><code>let client = MPOFaceServiceClient(subscriptionKey: ""MY_API_KEY"")
</code></pre>

<p>All time returns the same error:</p>

<pre><code>Error Domain=POFaceServiceClient error - http response is not success : 
                    {""error"":{""code"":""Unspecified"",""message"":""Access denied due to invalid subscription key. Make sure you are subscribed to an API you are trying to call and provide the right key.""}}
                         Code=401 ""(null)""
</code></pre>

<p>There is no almost documentation for the API in IOS and then it's just objective-c, no swift. Can someone figure out why is this returning all time the 401 error???</p>

<p>Edit:
As well I tried  let client = <code>MPOFaceServiceClient(endPointAndSubscriptionKey: ""MY_SERVER_AREA"", key: ""MY_API_KEY"")</code>
but this one returns all time 404 error, resource was not found.</p>",47506062,1,0,,2017-11-22 16:38:45.180 UTC,,2018-01-06 11:37:35.017 UTC,,,,,6362007,1,0,ios|swift|azure|microsoft-cognitive|face,300
How to convert java.nio.DirectByteBuffer to java.nio.HeapByteBuffer,45170228,How to convert java.nio.DirectByteBuffer to java.nio.HeapByteBuffer,"<p>I want compare image between source image file </p>

<pre><code>InputStream  inputStream;
</code></pre>

<p>and set to </p>

<pre><code>ByteBuffer sourceImageBytes = ByteBuffer.wrap(IOUtils.toByteArray(inputStream))
</code></pre>

<p>and source Webcam  </p>

<pre><code>webcam.getImageBytes()
</code></pre>

<p>but have problem compare(use AmazonRekognition) between two source</p>

<p>print content</p>

<ul>
<li>source InputStream : <code>java.nio.HeapByteBuffer</code></li>
<li>source webcam : <code>java.nio.HeapByteBuffer</code></li>
</ul>

<p>How to convert <code>java.nio.DirectByteBuffer</code> to <code>java.nio.HeapByteBuffer</code> ?</p>",,0,2,,2017-07-18 14:51:29.990 UTC,,2017-07-18 18:23:13.253 UTC,2017-07-18 18:23:13.253 UTC,,862594,,5146789,1,0,java|amazon-web-services|javafx|amazon-rekognition,97
An example of calling AWS Rekognition HTTP API from php,48527517,An example of calling AWS Rekognition HTTP API from php,<p>I am studying Amazon Rekognition API. I would to like to know if it is possible to call Amazon Rekognition API via curl?</p>,,1,2,,2018-01-30 17:47:09.660 UTC,1,2018-09-30 15:07:24.627 UTC,2018-02-02 11:07:20.480 UTC,,880386,,880386,1,1,curl|amazon|amazon-rekognition,457
how to use old google play service maps library beside newer google play service APIs,52877398,how to use old google play service maps library beside newer google play service APIs,"<p>The problem I am facing is my application needs to support older devices ( at least API 14 ) so I am using google play service maps 7.3.0.</p>

<p>However, I am unable to use newer play service APIs such as Google Vision because my google play service version is low.</p>

<p>The problem is my users are unable to update their google play services app so I cannot simply upgrade my library.</p>",,0,0,,2018-10-18 15:22:09.557 UTC,,2018-10-18 18:41:46.757 UTC,2018-10-18 18:41:46.757 UTC,,4842949,,6877472,1,0,java|android,16
Google play services and relation with android version,40671175,Google play services and relation with android version,"<p>I will be developing an app that uses Google Vision API in order to scan barcode. I am successfully able to write and test the app. However, I found out that the API has to be supported for Android's ICS i.e. version 4 and above. I am using Google Play Services 8.4 version. Will I be able to use this app? I have just created a prototype of app only.</p>

<p>In short is there any relationship between google play services and android version? If yes where can I find it. Thanks.</p>",40740035,2,0,,2016-11-18 07:02:42.410 UTC,0,2016-11-22 10:56:56.983 UTC,,,,,3773776,1,-2,android|google-play-services,64
Amazon Rekognition detect_labels does not return Instances or Parents,54916175,Amazon Rekognition detect_labels does not return Instances or Parents,"<p>Per <a href=""https://docs.aws.amazon.com/rekognition/latest/dg/labels-detect-labels-image.html#detectlabels-response"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/rekognition/latest/dg/labels-detect-labels-image.html#detectlabels-response</a> and <a href=""https://docs.aws.amazon.com/rekognition/latest/dg/API_DetectLabels.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/rekognition/latest/dg/API_DetectLabels.html</a> , Amazon Rekognition should return Instances (bounding box details) and Parents with each label. However, upon successfully running detect_labels with an implementation similar to that of the above links, the only keys in my response are 'Name' and 'Confidence'; 'Instances' and 'Parents' are not even keys, let alone keys with empty values. </p>

<p>Does anyone have any thoughts? </p>

<p>My code is below: </p>

<pre><code>def _bounding_box(imageFile):

    client = boto3.client('rekognition')

    with open(imageFile, 'rb') as image:
        response = client.detect_labels(Image={'Bytes': image.read()})

    print('Detected labels in ' + imageFile)
    for label in response['Labels']:

        print(label)
        print(""Label: "" + label['Name'])
        print(""Confidence: "" + str(label['Confidence']))
        print(""Instances:"")
        for instance in label['Instances']:
            print(""  Bounding box"")
            print(""    Top: "" + str(instance['BoundingBox']['Top']))
            print(""    Left: "" + str(instance['BoundingBox']['Left']))
            print(""    Width: "" + str(instance['BoundingBox']['Width']))
            print(""    Height: "" + str(instance['BoundingBox']['Height']))
            print(""  Confidence: "" + str(instance['Confidence']))
            print()
        print('Parents: ')
        for parent in label['Parents']:
            print(""   "" + parent['Name'])
        print(""----------"")
        print()
</code></pre>",54917865,1,0,,2019-02-27 23:37:51.927 UTC,,2019-02-28 03:15:43.547 UTC,,,,,7519956,1,0,amazon-web-services|amazon-rekognition,62
Google Vision API - label detection polygon is null,50907959,Google Vision API - label detection polygon is null,"<p>I'm using <a href=""https://cloud.google.com/vision/docs/detecting-labels#vision-label-detection-nodejs"" rel=""nofollow noreferrer"">Image labels detection</a> of Google vision API in my node application.
I'm trying to get label polygon but it returns null.</p>

<pre><code>const client = new visionClient.ImageAnnotatorClient();
client
    .labelDetection('./test.jpg')
    .then(results =&gt; {
        console.log(results[0]);
        const labels = results[0].labelAnnotations;

        console.log('Labels:');
        labels.forEach(label =&gt; console.log(label.description));
    })
    .catch(err =&gt; {
        console.error('ERROR:', err);
    });
</code></pre>

<p>results:</p>

<pre><code>[ { locations: [],
       properties: [],
       mid: '/m/0j272k5',
       locale: '',
       description: 'eyewear',
       score: 0.9805809855461121,
       confidence: 0,
       topicality: 0.9805809855461121,
       boundingPoly: null }]
</code></pre>

<p><strong>boundingPoly: null</strong></p>",,0,0,,2018-06-18 10:59:23.903 UTC,,2018-06-18 10:59:23.903 UTC,,,,,3166417,1,0,node.js|vision-api,40
Google Vision API - Project not passed,43628002,Google Vision API - Project not passed,"<p>I am trying to use the Google VISION API and I want to use the programm ""quickstart.py"" at <a href=""https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/vision/cloud-client/quickstart/quickstart.py"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/vision/cloud-client/quickstart/quickstart.py</a>.
I have created an account at Google itself and set the variable ""GOOGLE_APPLICATION_CREDENTIALS"". I created a test project then stored my credentials locally.</p>

<p>However, when running the application I first authenticated via ""gcloud auth application-default login"" and run the code of the application. But unfortunately I received the message
""OSError: Project was not passed and could not be determined from the environment"".</p>

<p>What change do I need to make in order to run this example?</p>

<p>Thanks,
Andi</p>",,1,0,,2017-04-26 07:46:47.600 UTC,,2017-05-11 02:18:42.513 UTC,,,,,1684315,1,0,gcloud,1029
Rajawali transparent background,33515465,Rajawali transparent background,"<p>I am trying to overlay a <code>RajawaliSurfaceView</code> with the Earth Demo-Renderer over the <a href=""https://github.com/googlesamples/android-vision/tree/master/visionSamples/FaceTracker"" rel=""nofollow noreferrer"">Google Vision Face Tracker sample</a>.</p>

<p>I use this to make the background transparent:</p>

<pre><code>rajawaliSurfaceView.getHolder().setFormat(PixelFormat.TRANSPARENT);
</code></pre>

<p>This is the layout file of my activity:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;FrameLayout xmlns:android=""http://schemas.android.com/apk/res/android""
    xmlns:surfaceview=""http://schemas.android.com/apk/res-auto""
    android:id=""@+id/topLayout""
    android:orientation=""vertical""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""
    android:keepScreenOn=""true""&gt;
    &lt;com.google.android.gms.samples.vision.face.facetracker.ui.camera.CameraSourcePreview
      android:id=""@+id/preview""
      android:layout_width=""match_parent""
      android:layout_height=""match_parent""&gt;
    &lt;com.google.android.gms.samples.vision.face.facetracker.ui.camera.GraphicOverlay
        android:id=""@+id/faceOverlay""
        android:layout_width=""match_parent""
        android:layout_height=""match_parent"" /&gt;
  &lt;/com.google.android.gms.samples.vision.face.facetracker.ui.camera.CameraSourcePreview&gt;
    &lt;org.rajawali3d.surface.RajawaliSurfaceView
        android:layout_width=""match_parent""
        android:layout_height=""match_parent""
        android:id=""@+id/rajawali_surface""
        android:layout_gravity=""left|top"" /&gt;
&lt;/FrameLayout&gt;
</code></pre>

<p>It compiles without a problem, but I do not see the 3d model or the FaceGraphic from the Google Demo project.
I also get this error when trying to preview the layout xml in Android Studio:</p>

<pre><code>java.lang.NullPointerException
at android.opengl.GLSurfaceView.onResume(GLSurfaceView.java:562)
at org.rajawali3d.surface.RajawaliSurfaceView.onResume(RajawaliSurfaceView.java:106)
at org.rajawali3d.surface.RajawaliSurfaceView.onVisibilityChanged(RajawaliSurfaceView.java:116)
at android.view.View.dispatchAttachedToWindow(View.java:14537)
at android.view.ViewGroup.dispatchAttachedToWindow(ViewGroup.java:2843)
at android.view.ViewGroup.dispatchAttachedToWindow(ViewGroup.java:2843)
at android.view.ViewGroup.dispatchAttachedToWindow(ViewGroup.java:2843)
at android.view.ViewGroup.dispatchAttachedToWindow(ViewGroup.java:2843)
at android.view.AttachInfo_Accessor.setAttachInfo(AttachInfo_Accessor.java:42)
at com.android.layoutlib.bridge.impl.RenderSessionImpl.inflate(RenderSessionImpl.java:232)
at com.android.layoutlib.bridge.Bridge.createSession(Bridge.java:426)
at com.android.ide.common.rendering.LayoutLibrary.createSession(LayoutLibrary.java:350)
at com.android.tools.idea.rendering.RenderTask$2.compute(RenderTask.java:510)
at com.android.tools.idea.rendering.RenderTask$2.compute(RenderTask.java:498)
at com.intellij.openapi.application.impl.ApplicationImpl.runReadAction(ApplicationImpl.java:888)
at com.android.tools.idea.rendering.RenderTask.createRenderSession(RenderTask.java:498)
at com.android.tools.idea.rendering.RenderTask.access$600(RenderTask.java:72)
at com.android.tools.idea.rendering.RenderTask$3.call(RenderTask.java:610)
at com.android.tools.idea.rendering.RenderTask$3.call(RenderTask.java:607)
at com.android.tools.idea.rendering.RenderService.runRenderAction(RenderService.java:366)
at com.android.tools.idea.rendering.RenderTask.render(RenderTask.java:607)
at com.android.tools.idea.rendering.RenderTask.render(RenderTask.java:629)
at org.jetbrains.android.uipreview.AndroidLayoutPreviewToolWindowManager.doRender(AndroidLayoutPreviewToolWindowManager.java:652)
at org.jetbrains.android.uipreview.AndroidLayoutPreviewToolWindowManager.access$1700(AndroidLayoutPreviewToolWindowManager.java:80)
at org.jetbrains.android.uipreview.AndroidLayoutPreviewToolWindowManager$7$1.run(AndroidLayoutPreviewToolWindowManager.java:594)
at com.intellij.openapi.progress.impl.CoreProgressManager$2.run(CoreProgressManager.java:152)
at com.intellij.openapi.progress.impl.CoreProgressManager.registerIndicatorAndRun(CoreProgressManager.java:452)
at com.intellij.openapi.progress.impl.CoreProgressManager.executeProcessUnderProgress(CoreProgressManager.java:402)
at com.intellij.openapi.progress.impl.ProgressManagerImpl.executeProcessUnderProgress(ProgressManagerImpl.java:54)
at com.intellij.openapi.progress.impl.CoreProgressManager.runProcess(CoreProgressManager.java:137)
at org.jetbrains.android.uipreview.AndroidLayoutPreviewToolWindowManager$7.run(AndroidLayoutPreviewToolWindowManager.java:589)
at com.intellij.util.ui.update.MergingUpdateQueue.execute(MergingUpdateQueue.java:320)
at com.intellij.util.ui.update.MergingUpdateQueue.execute(MergingUpdateQueue.java:310)
at com.intellij.util.ui.update.MergingUpdateQueue$2.run(MergingUpdateQueue.java:254)
at com.intellij.util.ui.update.MergingUpdateQueue.flush(MergingUpdateQueue.java:269)
at com.intellij.util.ui.update.MergingUpdateQueue.flush(MergingUpdateQueue.java:227)
at com.intellij.util.ui.update.MergingUpdateQueue.run(MergingUpdateQueue.java:217)
at com.intellij.util.concurrency.QueueProcessor.runSafely(QueueProcessor.java:238)
at com.intellij.util.Alarm$Request$1.run(Alarm.java:351)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
</code></pre>

<p><br><strong>UPDATE 1:</strong></p>

<p>I removed this line (<code>rajawaliSurfaceView.getHolder().setFormat(PixelFormat.TRANSPARENT);</code>), hoping to see the 3d scene at least, but nothing changed, I still only see the camera preview.</p>

<p>However, when resuming the app from anywhere I see the rotating earth for the fraction of a second, before the camera preview is started.</p>

<p>According to <a href=""https://stackoverflow.com/questions/4182486/placing-overlappingz-index-a-view-above-another-view-in-android"">this answer</a>, it should work as I expected it to. What do I have to do?</p>

<p><br><strong>UPDATE 2:</strong><br>(using <a href=""https://andrewhague.wordpress.com/2011/05/24/how-to-set-a-transparent-glsurfaceview/"" rel=""nofollow noreferrer"">this</a> tutorial)</p>

<p>OK - Adding the line <code>rajawaliSurfaceView.setZOrderOnTop(true);</code> renders the 3d scene on top of my camera preview, but I still have a problem.</p>

<p>Neither</p>

<pre><code>rajawaliSurfaceView.setEGLConfigChooser(8, 8, 8, 8, 16, 0);
rajawaliSurfaceView.getHolder().setFormat(PixelFormat.TRANSLUCENT);
</code></pre>

<p>nor</p>

<pre><code>rajawaliSurfaceView.setEGLConfigChooser(8, 8, 8, 8, 16, 0);
rajawaliSurfaceView.getHolder().setFormat(PixelFormat.RGBA_8888);
</code></pre>

<p>only clear the background of the scene.
I allways get a fully transparent SurfaceView. Any ideas how I can resolve this?</p>

<p>(Apparently the xml error only seems to be a mild anoyance I'll have to ignore.)</p>",,0,0,,2015-11-04 06:53:53.983 UTC,,2017-11-26 07:48:42.283 UTC,2017-05-23 12:14:34.787 UTC,,-1,,4427841,1,3,android|xml|surfaceview|glsurfaceview|rajawali,728
How to setLanguageHint in Google OCR SDK?,49732951,How to setLanguageHint in Google OCR SDK?,"<p>I am testing few sample code for OCR using Google Cloud Vision API. I observed the APIs can able to detect <strong>English</strong> language very easily from an Image but  in other language like <strong>Hindi</strong>, the APIs are not able to detect. </p>

<p><strong>MyCode :</strong> </p>

<pre><code>public static void detectText(String filePath) throws Exception, IOException {
    System.out.println(""Detect Text\n"");

    List&lt;AnnotateImageRequest&gt; requests = new ArrayList&lt;&gt;();

    ByteString imgBytes = ByteString.readFrom(new FileInputStream(filePath));

    Image img = Image.newBuilder().setContent(imgBytes).build();
    Feature feat = Feature.newBuilder().setType(Feature.Type.TEXT_DETECTION).build();

    AnnotateImageRequest request = AnnotateImageRequest.newBuilder().addFeatures(feat).setImage(img).build();
    requests.add(request);


    Credentials myCredentials = ServiceAccountCredentials.fromStream(
            new FileInputStream(jsonPath));
    ImageAnnotatorSettings imageAnnotatorSettings =
            ImageAnnotatorSettings.newBuilder()
                    .setCredentialsProvider(FixedCredentialsProvider.create(myCredentials))
                    .build();
    try (ImageAnnotatorClient client = ImageAnnotatorClient.create(imageAnnotatorSettings)) {
        BatchAnnotateImagesResponse response = client.batchAnnotateImages(requests);
        List&lt;AnnotateImageResponse&gt; responses = response.getResponsesList();

        for (AnnotateImageResponse res : responses) {
            if (res.hasError()) {
                System.out.printf(""Error: %s\n"", res.getError().getMessage());
                return;
            }
            // For full list of available annotations, see http://g.co/cloud/vision/docs
            for (EntityAnnotation annotation : res.getTextAnnotationsList()) {
                System.out.printf(""Text: %s\n"", annotation.getDescription());
            }
        }
    }
}
</code></pre>

<p><strong>Image :</strong> </p>

<p><a href=""https://i.stack.imgur.com/u9Rfe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u9Rfe.png"" alt=""enter image description here""></a></p>

<p>But the same image, i have tried in Google Drive, all the text from the image is easily detected.</p>

<p>Please let me know, same image how can i use in the code to detect the text?</p>",,2,0,,2018-04-09 12:25:45.917 UTC,,2018-04-20 04:33:33.567 UTC,,,,,1453704,1,0,ocr|google-vision|hindi,261
How to convert IBM Watson cURL commands to PHP,49589780,How to convert IBM Watson cURL commands to PHP,"<p>I need to convert this cURL command in PHP to use it on my site in WordPress.</p>

<pre><code>curl -X POST -F ""images_file=@fruitbowl.jpg"" -F ""parameters=@fruit.json"" ""https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classify?api_key={api-key}&amp;version=2016-05-20""
</code></pre>

<p>Parameters object that I'm using:</p>

<pre><code>{
    ""classifier_ids"": [
        ""My_Model_ID"",
        ""default""
    ],
    ""owners"": [""me""],
    ""threshold"": 0.6
}
</code></pre>

<p><strong>This is my attempt:</strong></p>

<pre><code>&lt;?php
//Here is the JSON Parameters Object
$arr = array('classifier_ids' =&gt; array('My_Model_ID', 'default'), 'owners' =&gt; array('me'), 'threshold' =&gt; 0.6);

//Here is the endpoint URL
$url = 'https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classify?api_key=MY_KEY&amp;version=2016-05-20';

// IMPORTANT - Image that is uploaded on my site 
$filename = file_get_contents('@/wp-content/uploads/2018/03/raiox_img02.jpg');
$cfile = curl_file_create($filename,'image/jpeg');

//cURL
$ch = curl_init();
    curl_setopt($ch, CURLOPT_URL, $url); //URL
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);
    //curl_setopt($ch, CURLOPT_BINARYTRANSFER,1);
    curl_setopt($ch, CURLOPT_POST, 1); //POST
    curl_setopt($ch, CURLOPT_FILE, $cFile); //Try pass image
    curl_setopt($ch, CURLOPT_POSTFIELDS,  json_encode($arr)); //Try pass JSON

//Result
$result = curl_exec($ch);
if (curl_errno($ch)) {
    echo 'Erro: ' . curl_error($ch); //Error
}
curl_close ($ch); //Finish

echo $result;
</code></pre>

<p>This is the error:</p>

<pre><code>Warning: file_get_contents(@/wp-content/uploads/2018/03/raiox_img02.jpg): failed to open stream: No such file or directory in /srv/bindings/.. ../code/wp-content/plugins/insert-php-code-snippet/shortcode-handler.php(65) : eval()'d code on line 9 Warning: curl_setopt(): supplied argument is not a valid File-Handle resource in /srv/bindings/.. ../code/wp-content/plugins/insert-php-code-snippet/shortcode-handler.php(65) : eval()'d code on line 18
</code></pre>

<p>This is the JSON I get as a return:</p>

<pre><code>{
 ""error"":
     {
       ""code"": 400,
       ""error_id"": ""input_error"",
       ""description"": ""No images were specified.""
     },
 ""images_processed"": 0
}
</code></pre>

<p>I want use my custom model of IBM Watson Visual Recognition.
I left commenting exactly how I use it, because with the syntax I'm using I can not use the image I need.</p>

<p><strong>Using WordPress</strong></p>

<p>Version: 9.4.4</p>

<p>Plugin: <a href=""https://wordpress.org/plugins/insert-php-code-snippet/"" rel=""nofollow noreferrer"">XYZ PHP Code</a></p>

<p>I am using the following links to guide me:</p>

<p><a href=""https://stackoverflow.com/questions/1939609/convert-command-line-curl-to-php-curl"">Convert command line cURL to PHP cURL</a></p>

<p><a href=""https://incarnate.github.io/curl-to-php/"" rel=""nofollow noreferrer"">https://incarnate.github.io/curl-to-php/</a></p>

<p><a href=""https://gist.github.com/germanattanasio/ca22c0d47755d6f023f1"" rel=""nofollow noreferrer"">https://gist.github.com/germanattanasio/ca22c0d47755d6f023f1</a></p>

<p><a href=""https://stackoverflow.com/questions/42227768/ibm-watson-api-of-visual-recognition-add-image-issue-in-collectio-using-curl"">IBM watson api of visual recognition add image issue in collectio using curl</a></p>

<p><a href=""https://console.bluemix.net/docs/services/visual-recognition/tutorial-custom-classifier.html#classify"" rel=""nofollow noreferrer"">https://console.bluemix.net/docs/services/visual-recognition/tutorial-custom-classifier.html#classify</a></p>

<p>Remember that I am not installing any library or using Composer.</p>",49592000,1,8,,2018-03-31 16:07:31.767 UTC,1,2018-03-31 20:16:42.833 UTC,,,,,8793012,1,3,php|wordpress|curl|ibm-watson|visual-recognition,329
google vision OCR text detection,44806711,google vision OCR text detection,"<p>I am using google vision API for text detection and I get the expected result for English but when test on Arabic I get no result, I found in google decumentation that it support many languages includes Arabic by using parameter languagehint ar 
but i don't know where to use this parameter.
I use this <a href=""http://www.truiton.com/2016/11/optical-character-recognition-android-ocr/"" rel=""nofollow noreferrer"">tutorial</a>.</p>",,1,0,,2017-06-28 15:23:19.627 UTC,,2017-09-05 04:49:38.860 UTC,2017-06-29 22:28:53.057 UTC,,2612002,,3605952,1,0,android|ocr|google-vision,434
"inner json array cannot be accessed in javascript, but can console log the complete json",53718088,"inner json array cannot be accessed in javascript, but can console log the complete json","<p><code>console.log(e.responseText);
  testNature(e.responseText.responses[0]);</code></p>

<p>I cannot use the inner array of a JSON response it says:-</p>

<p>'Uncaught TypeError: Cannot read property '0' of undefined'</p>

<p>when I console log the e.responseText I get:-</p>

<pre><code> {
  ""responses"": [
    {
      ""labelAnnotations"": [
        {
          ""mid"": ""/m/06mb1"",
          ""description"": ""rain"",
          ""score"": 0.930509,
          ""topicality"": 0.930509
        },
        {
          ""mid"": ""/m/0838f"",
          ""description"": ""water"",
          ""score"": 0.91255623,
          ""topicality"": 0.91255623
        },
        {
          ""mid"": ""/m/01ctsf"",
          ""description"": ""atmosphere"",
          ""score"": 0.86684966,
          ""topicality"": 0.86684966
        },
        {
          ""mid"": ""/m/04k84"",
          ""description"": ""light"",
          ""score"": 0.8194458,
          ""topicality"": 0.8194458
        },
        {
          ""mid"": ""/m/01bqvp"",
          ""description"": ""sky"",
          ""score"": 0.7569251,
          ""topicality"": 0.7569251
        }
      ]
    }
  ]
}
</code></pre>

<p>but cannot use the inner array <code>e.responseText.responses[0]</code> to call a function ie testNature(e.responseText.responses[0]) . I got the JSON from google cloud vision API</p>",53718847,2,3,,2018-12-11 05:46:01.923 UTC,,2018-12-11 06:51:38.153 UTC,,,,,10185513,1,0,javascript|arrays|json|google-cloud-vision,33
"How to remove the ""jsonoutput-1-to-1.json"" from Google API Vision?",55369637,"How to remove the ""jsonoutput-1-to-1.json"" from Google API Vision?","<p>I am developing this system using Google Vision API and Google Cloud storage.
When I upload a PDF file to Google Cloud Storage it will then translate it to .json file.
It works, but the problem is, I cant seem to find where to remove the jsonoutput-1-to-1. </p>

<p>example : </p>

<p>filename.pdf** is translated to <strong>filename.pdf.jsonoutput-1-to-1.json</strong></p>

<p>I want to remove the <strong>jsonoutput-1-to-1</strong> and make the file become </p>

<p><strong>filename.pdf.json</strong></p>

<p>Here is my code. </p>

<pre><code>async function detectPdfText(fileNameExtract)
{      
 const bucketName = 'testbucket';     

 const fileName = fileNameExtract;     

 const outputPrefix = 'results'



 const gcsSourceUri = `gs://${bucketName}/${fileName}`;

 const gcsDestinationUri = `gs://${bucketName}/${fileName}${outputPrefix}`;         



 const inputConfig = {
Supported mime_types are: 'application/pdf' and 'image/tiff'

   mimeType: 'application/pdf',

   gcsSource: {

     uri: gcsSourceUri,

   },

 };



 const outputConfig = {

   gcsDestination: {

     uri: gcsDestinationUri,

   },

 };



 const features = [{type: 'DOCUMENT_TEXT_DETECTION'}];

 const request = {

   requests: [

     {

       inputConfig: inputConfig,

       features: features,

       outputConfig: outputConfig,

     },

   ],

 };     
 const [operation] = await client.asyncBatchAnnotateFiles(request);

 const [filesResponse] = await operation.promise();

 const destinationUri = filesResponse.responses[0].outputConfig.gcsDestination.uri;    
}
async function listFiles() 
{     

const {Storage} = require('@google-cloud/storage');                        

const storage = new Storage();                                        

const bucketName = 'testbucket';                 

const [files] = await storage.bucket(bucketName).getFiles();                         

  files.forEach(file =&gt;

    {

      console.log(`The List Of File In GBucket :` + file.name);

    });}  
</code></pre>

<p>I manage to list down the file but Would like to have without jsonoutput-1-to-1</p>",,0,0,,2019-03-27 04:05:15.883 UTC,,2019-03-27 10:28:33.620 UTC,2019-03-27 10:28:33.620 UTC,,5468463,,11158273,1,0,node.js|reactjs,25
how to get the prediction or results from custom vision api?,49231034,how to get the prediction or results from custom vision api?,"<p>I try using custom vision service that could read bank notes. I came up with this code shown below(through the code samples here...
<a href=""https://southcentralus.dev.cognitive.microsoft.com/docs/services/57982f59b5964e36841e22dfbfe78fc1/operations/5a3044f608fa5e06b890f164"" rel=""nofollow noreferrer"">Microsoft custom vision sample codes</a></p>

<pre><code> ByteArrayOutputStream output = new ByteArrayOutputStream();
    bitmapPicture.compress(Bitmap.CompressFormat.JPEG, 100, output);

    OkHttpClient client = new OkHttpClient();

    Request.Builder requestBuilder = new Request.Builder();
    RequestBody requestBody = null;

    requestBuilder.addHeader(""Prediction-Key"",""___________________________"");
    requestBuilder.addHeader(""Content-Type"",""application/octet-stream"");
    requestBody = RequestBody.create(MediaType.parse(""application/octet-stream; charset=utf-8""), output.toByteArray());

    requestBuilder.method(""POST"", requestBody);

    requestBuilder.url(""https://southcentralus.api.cognitive.microsoft.com/customvision/v1.1/Prediction/__________/image"");

    Request request = requestBuilder.build();
    Response response = client.newCall(request).execute();

    if (response.isSuccessful()) {
        return response.body().string();
    } else {
        throw new IOException(""Exception: response code "" + response.code());
    }
</code></pre>

<p>And based from this <a href=""https://www.henkboelman.com/recognizing-things-with-the-custom-vision-service/"" rel=""nofollow noreferrer"">link</a> it uses an sdk from microsoft to get results. However, I want to build an android app.
How am I be able to get the tag result and its prediction? Thank you in advance.</p>",,0,7,,2018-03-12 08:38:07.823 UTC,1,2018-03-30 18:35:54.157 UTC,2018-03-12 08:58:22.477 UTC,,9477194,,9477194,1,0,java|android|machine-learning|computer-vision,224
"Code 403 : ""The request is missing a valid API key."" PERMISSION_DENIED Service Account Key Google Cloud Vision PHP",55465835,"Code 403 : ""The request is missing a valid API key."" PERMISSION_DENIED Service Account Key Google Cloud Vision PHP","<p>I want to implement the Google Cloud Vision with ImageAnnotator using a service key. What i have try is like below :</p>

<p><strong>Error :</strong> </p>

<blockquote>
  <p><p>Message: {
        ""error"": {
          ""code"": 403,
          ""message"": ""The request is missing a valid API key."",
          ""status"": ""PERMISSION_DENIED""
        }
      }</p>
</blockquote>

<p><strong>When try this code :</strong></p>

<pre><code>defined('BASEPATH') OR exit('No direct script access allowed');

use Google\Cloud\Vision\VisionClient;

class Admin_center extends CI_Controller {
    function __construct() {
        parent::__construct();

        include APPPATH . 'third_party/vendor/autoload.php';
    }

    public function index() {
        $this-&gt;load-&gt;view('index');
    }

    function upload_ocr_image() {               
        $img_data = $this-&gt;upload-&gt;data();                                          

        $vision = new VisionClient(['keyfile' =&gt; json_decode(file_get_contents(base_url().'assets/google_cloud_vision/credentials.json'), true)]);

        $imageRes = fopen($img_data['full_path'], 'r');
        $image = $vision-&gt;image($imageRes,['Text_Detection']);
        $result = $vision-&gt;annotate($image);

        print_r($result);
    }
}
</code></pre>

<p>I used a <strong>service account</strong> key.</p>

<p>Why i got error : 403 Permissin Denied and Missing a valid API Key ?</p>

<p><strong>Edited :</strong></p>

<p>I have follow this youtube tutorial :</p>

<p><a href=""https://www.youtube.com/watch?v=K-tpjOT7k-o"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=K-tpjOT7k-o</a></p>

<p><a href=""https://www.youtube.com/watch?v=PqAXE67fwu8&amp;t=2s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=PqAXE67fwu8&amp;t=2s</a></p>

<p>Thank You</p>",55467167,2,0,,2019-04-02 01:49:41.913 UTC,,2019-04-02 06:48:53.633 UTC,2019-04-02 06:48:53.633 UTC,,1841839,,6786634,1,0,php|codeigniter|google-api|google-cloud-platform|google-cloud-vision,145
I want to get the insights of the image stored on the Softlayer cloud WITHOUT DOWNLOADING IT LOCALLY using ibm watson Visual Recognition,35602395,I want to get the insights of the image stored on the Softlayer cloud WITHOUT DOWNLOADING IT LOCALLY using ibm watson Visual Recognition,<p>I want to do some Analytics on the image hosted on the cloud using IBM Watson Visual recognition. Currently I am downloading the image and storing it locally and then give it to the Watson visual Recognition service. <strong>I dont want to download the image locally</strong>.I am using JAVA</p>,,2,1,,2016-02-24 12:36:40.367 UTC,,2016-03-12 13:32:11.087 UTC,,,,,5816183,1,1,java|image|ibm-watson|ibm-cloud-infrastructure|visual-recognition,71
How can I DetectFaces in Amazon Rekognition AWS with Android Studio?,42041693,How can I DetectFaces in Amazon Rekognition AWS with Android Studio?,"<p>I have tried so many way but i can't succeed. I haven't found any source code examples for Android(about rekognition)</p>

<p>there's a source code in JAVA in the Developer Guide but i cannot implement that even though I tried TT</p>

<p>I try to detect faces by sending an image file from an external storage(from the emulator)
I don't know what i did wrong(I'm not good at coding)
Here is my code</p>

<pre><code>AmazonRekognitionClient amazonRekognitionClient;
Image getAmazonRekognitionImage;
DetectFacesRequest detectFaceRequest;
DetectFacesResult detectFaceResult;
File file = new File(Environment.getExternalStorageDirectory(),""sungyeol.jpg.jpg"");

public void test_00(View view) {
 ByteBuffer imageBytes;
 try{
        InputStream inputStream = new FileInputStream(file.getAbsolutePath().toString());
        imageBytes = ByteBuffer.wrap(IOUtils.toByteArray(inputStream));
        Log.e(""InputStream: "",""""+inputStream);
        Log.e(""imageBytes: "","""");
        getAmazonRekognitionImage.withBytes(imageBytes);

        // Initialize the Amazon Cognito credentials provider
        CognitoCachingCredentialsProvider credentialsProvider = new CognitoCachingCredentialsProvider(
                getApplicationContext(),
                ""us-east-2:......."", // Identity Pool ID
                Regions.US_EAST_2 // Region
        );

        //I want ""ALL"" attributes
        amazonRekognitionClient = new AmazonRekognitionClient(credentialsProvider);

        detectFaceRequest = new DetectFacesRequest()
                .withAttributes(Attribute.ALL.toString())
                .withImage(getAmazonRekognitionImage);

        detectFaceResult = amazonRekognitionClient.detectFaces(detectFaceRequest);
        detectFaceResult.getFaceDetails();

    }
 catch(Exception ex){
   Log.e(""Error on something:"",""Message:""+ex.getMessage());
 }
</code></pre>

<p>and here is my errors</p>

<pre><code>02-04 09:30:07.268 29405-29405/? E/InputStream:: java.io.FileInputStream@a9b23e7
02-04 09:30:07.271 29405-29405/? E/Error on something:: Message:Attempt to invoke virtual method 'com.amazonaws.services.rekognition.model.Image com.amazonaws.services.rekognition.model.Image.withBytes(java.nio.ByteBuffer)' on a null object reference
</code></pre>

<p>what is a null object reference?
i try to change the file path but he said no such file ... and when I change to this path, there's errors above.
by the way I've already asked a user for a permission to access a folder from Emulator in Android</p>

<p>please help me
PS. sorry for my bad English</p>

<p>Thank you in advance.</p>",42064256,1,0,,2017-02-04 14:48:37.370 UTC,4,2019-01-09 19:11:18.830 UTC,,,,,7450998,1,5,java|amazon-web-services|android-studio|amazon-rekognition,2679
Where do I run this AWS rekognition code and do I need to install anything else? What result will I get?,53464278,Where do I run this AWS rekognition code and do I need to install anything else? What result will I get?,"<p>`<br>
     import boto3</p>

<pre><code> if __name__ == ""__main__"":

    bucket='random_name'
    photo='b4.png'

    client=boto3.client('rekognition')


    response=client.detect_text(Image={'S3Object': 
    {'random_name':bucket,'b4.png':photo}})


    textDetections=response['TextDetections']
    print(response)
    print('Matching faces')
    for text in textDetections:
         print('Detected text:' + text['DetectedText'])
         print('Confidence: ' + ""{:.2f}"".format(text['Confidence']) + ""%"")
         print('Id: {}'.format(text['Id']))
         if 'ParentId' in text:
               print('Parent Id: {}'.format(text['ParentId']))
               print('Type:' + text['Type'])
               print()`
</code></pre>

<p>This is the code recognizing images(OCR) yet I do not know where I should paste this code to run. Do I run this in Jupyter notebooks and would I need to install extra things? Do I run it in the Anaconda Prompt? I've tried both. In Jupyter, I get an error: |ParamValidationError: Parameter validation failed: Unknown parameter in Image.S3Object: ""random_name"", must be one of: Bucket, Name, Version Unknown parameter in Image.S3Object: ""b4.png"", must be one of: Bucket, Name, Version| and Anaconda prompt has much more errors. I've installed AWS already and curious whether there is more to install. It would be greatly appreciated if anyone helped me.</p>",,1,0,,2018-11-25 02:57:13.390 UTC,,2018-11-25 05:10:29.937 UTC,,,,,10700690,1,-1,python|amazon-web-services|machine-learning|ocr,29
Why is Google PDF DOCUMENT_TEXT_DETECTION API much slower than Google JPG DOCUMENT_TEXT_DETECTION API,56213065,Why is Google PDF DOCUMENT_TEXT_DETECTION API much slower than Google JPG DOCUMENT_TEXT_DETECTION API,"<p>I noticed the Google Vision PDF OCR DOCUMENT_TEXT_DETECTION takes about 15 seconds to detect a single PDF page <a href=""https://cloud.google.com/vision/docs/pdf"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/pdf</a>. <br/>
But if I submit the same PDF page as JPG it takes less than 3seconds to detect texts <a href=""https://cloud.google.com/vision/docs/detecting-fulltext"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/detecting-fulltext</a> </p>

<p>I used the code provided here (C#)<a href=""https://cloud.google.com/vision/docs/pdf#vision-pdf-detection-gcs-csharp"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/pdf#vision-pdf-detection-gcs-csharp</a></p>

<p>I noticed it takes about 15 seconds for the following line of code to say all text in PDF is detected and saved to gsBucket
<code>operation.PollUntilCompleted();</code></p>

<ul>
<li>My GsBucket is ""Multi-Regional Storage"" US</li>
<li>I'm also uploading from a US location</li>
</ul>

<p>I was wondering what else I can do to speed up the process or this is expected?</p>",,0,5,,2019-05-20 00:24:12.510 UTC,,2019-05-20 00:44:57.730 UTC,2019-05-20 00:44:57.730 UTC,,11525519,,11525519,1,1,c#|asp.net|google-cloud-platform|google-api|google-vision,68
Multiprocessing causes Python to crash and gives an error may have been in progress in another thread when fork() was called,50168647,Multiprocessing causes Python to crash and gives an error may have been in progress in another thread when fork() was called,"<p>I am relatively new to Python and trying to implement a Multiprocessing module for my for loop.</p>

<p>I have an array of Image url's stored in img_urls which I need to download and apply some Google vision.</p>

<pre><code>if __name__ == '__main__':

    img_urls = [ALL_MY_Image_URLS]
    runAll(img_urls)
    print(""--- %s seconds ---"" % (time.time() - start_time)) 
</code></pre>

<p>This is my runAll() method</p>

<pre><code>def runAll(img_urls):
    num_cores = multiprocessing.cpu_count()

    print(""Image URLS  {}"",len(img_urls))
    if len(img_urls) &gt; 2:
        numberOfImages = 0
    else:
        numberOfImages = 1

    start_timeProcess = time.time()

    pool = multiprocessing.Pool()
    pool.map(annotate,img_urls)
    end_timeProcess = time.time()
    print('\n Time to complete ', end_timeProcess-start_timeProcess)

    print(full_matching_pages)


def annotate(img_path):
    file =  requests.get(img_path).content
    print(""file is"",file)
    """"""Returns web annotations given the path to an image.""""""
    print('Process Working under ',os.getpid())
    image = types.Image(content=file)
    web_detection = vision_client.web_detection(image=image).web_detection
    report(web_detection)
</code></pre>

<p>I am getting this as the warning when I run it and python crashes</p>

<pre><code>objc[67570]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67570]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67567]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67567]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67568]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67568]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67569]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67569]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67571]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67571]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67572]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67572]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
</code></pre>",,1,4,,2018-05-04 06:36:05.487 UTC,5,2019-01-30 04:13:13.183 UTC,,,,,5608734,1,22,python|multithreading|python-3.x,5342
i am getting com.android.volley.AuthFailureError while using google vision api to recognise text,52906415,i am getting com.android.volley.AuthFailureError while using google vision api to recognise text,"<p>Here is my vollyNetworking code</p>

<pre><code>public void callGoogleVisionAPI(JSONObject object) {
        progressBar.setVisibility(View.VISIBLE);
        RequestQueue requestQueue = Volley.newRequestQueue(context);
        String url = ""https://vision.googleapis.com/v1/images:annotate?key="" + CLOUD_VISION_API_KEY;
        JsonObjectRequest postRequest = new JsonObjectRequest(url, object,
                new Response.Listener&lt;JSONObject&gt;()
                {
                    @Override
                    public void onResponse(JSONObject response) {
                        // response
                    Log.e(""in"",""res"");
                        progressBar.setVisibility(View.GONE);
                        if(response!= null) {
                            String bCardText = getRelevantString(response);
                            try {
                                bCardText = bCardText.replace(""\n"", "" "");
                                obtainedText.setText(bCardText);
                            }catch (NullPointerException e){
                                e.printStackTrace();
                            }
                        } else{
                            Log.w(TAG, ""response is null"");
                        }
                    }
                },
                new Response.ErrorListener()
                {
                    @Override
                    public void onErrorResponse(VolleyError error) {
                        // error
                        progressBar.setVisibility(View.GONE);
                        Toast.makeText(context, ""Network error. Processing failed"", Toast.LENGTH_LONG).show();
                        Log.i(""error"", error.toString());
                    }
                }
        );
        postRequest.setRetryPolicy(new DefaultRetryPolicy(
                5000,
                DefaultRetryPolicy.DEFAULT_MAX_RETRIES,
                DefaultRetryPolicy.DEFAULT_BACKOFF_MULT));
        requestQueue.add(postRequest);
    }
</code></pre>

<p>here is my error</p>

<pre><code>Unexpected response code 403 for https://vision.googleapis.com/v1/images:annotate?key=MY_KEY

com.android.volley.AuthFailureError
</code></pre>

<p>Here my cloud console shows the exact amount of request so it means that my request is going to my console so what could be the problem?</p>",,0,0,,2018-10-20 13:53:39.687 UTC,,2018-10-20 13:53:39.687 UTC,,,,,9633929,1,0,java|android|android-studio|android-volley|google-vision,48
Microsoft Face API - different cameras problem,54212819,Microsoft Face API - different cameras problem,"<p>On my website I'm making login system by user's face. And I'm trying to verify person, who is member in <strong>largePersonGroup</strong> , has <strong>personId</strong> and five <strong>persistedFaceIds</strong> (i.e. he has shot five photos on his Samsung Galaxy s8 and for every photo he get persistedFaceId). Then he is trying to login to my website, by his Lenovo tablet, as he is making photo of his face by the camera of the tablet (Lenovo tab4 10 plus) and verifying that just shot photo with the photos he created earlier on his Samsung and here comes my problem. He can't login. Microsoft Face API doesn't recognize him.</p>

<p>My question is:</p>

<ul>
<li><strong>Is this a problem, because of photos. Where I'm using different
devices with different cameras. One device (Samsung) for shooting the person's photos and different device (Lenovo) for taking photo for verifying.</strong></li>
</ul>

<p>When I'm trying all this steps, with creating person with photos and verifying him, from one device my Lenovo tablet all is good, it recognizes him. But from different devices - can't.</p>",,0,1,,2019-01-16 08:18:48.817 UTC,,2019-01-16 08:18:48.817 UTC,,,,,7429425,1,0,microsoft-cognitive,37
Error when trying to reproduce Postman request (to send an image) on angular 7,54113520,Error when trying to reproduce Postman request (to send an image) on angular 7,"<p>I have built an application with the following architecture : </p>

<ul>
<li>Client : Angular 7</li>
<li>Back-end: Spring boot</li>
</ul>

<p>The aim is to take a screenshot of the user and compare it with a picture in a database, using AWS Rekognition API.
But I keep getting a CORS error</p>

<hr>

<ol>
<li>The front make a POST calls to the back (a REST api built with Spring) and send an image</li>
<li>The Back get the image and send it to AWSRekognition API (using AWS SDK) with the target image stored in a database</li>
<li>The AWS api send back the result</li>
<li>The Back Spring api send back to the front-end the result</li>
</ol>

<hr>

<ul>
<li><p>When I use Postman and keep the code which calls the AWS API: OK it works fine</p></li>
<li><p>When I try on my web browser and KEEP the AWS API call code : I get a CORS error
Here is the Postman screenshot : </p></li>
</ul>

<p>Here is the Angular code : 
picture.components.ts</p>

<pre><code>  /**
   * take picture from camera
   */
  takeSnap(){
this.canvas.getContext(""2d"").drawImage(this.video, 0, 0, 300, 300, 0, 0, 300, 300);

this.canvas.toBlob((res)=&gt;{
  console.log(res);
  let img: File ;
  img = res;

  if (this.selectedUserId!=null) {

    let authInfo = new FormData();

    authInfo.append(""image"" , new File([img], 'img')); 
    authInfo.append(""id"" , this.selectedUserId.toString()); 

    this.requestSer.identify(authInfo).subscribe((x)=&gt;{
      this.readVerifyResponse(x);
    });
  }
});


  }
</code></pre>

<p>Here is the request.service.ts</p>

<pre><code>export class RequestService {

 constructor(
    private http  : HttpClient,
    private globals: Globals
    ) { }

/**
 * Send image to veify identity
 * @param img File
 */
  identify(authInfo) : Observable&lt;File&gt;{
    return this.http.post&lt;File&gt;(this.globals.server_full+'rekognition/compareFaces' , authInfo);
  }
</code></pre>

<p>If someone can help me, that's would be super great,
Cheers
Alexis</p>",,0,6,,2019-01-09 15:35:36.933 UTC,0,2019-01-09 17:02:10.440 UTC,2019-01-09 17:02:10.440 UTC,,9014245,,9014245,1,0,angular|postman,36
Unity WWWForm unauthorized for azure,54110288,Unity WWWForm unauthorized for azure,"<p>I have to send an request with an image to Microsoft Azure Custom Vision with the header ""Prediciton-Key"" -> myKey  and the body should be an image (binary data).</p>

<p>First of all i tryed with postman and it works fine, but when i put it on Unity ,i get an error :401 Unauthorized.</p>

<p>Does anyone know why? (i put the api key as string)</p>

<pre><code>    WWWForm form = new WWWForm();
    form.headers.Add(""Prediction-Key"", ""xxxxxxxxxxxxxxxxxxxxxxxx"");
    form.AddBinaryData(""fileUpload"", texture.EncodeToPNG());

   UnityWebRequest req = UnityWebRequest.Post(link,form);

    yield return req.SendWebRequest();
</code></pre>",54117253,1,3,,2019-01-09 12:33:41.523 UTC,,2019-01-14 15:07:35.937 UTC,,,,,10849251,1,0,c#|forms|unity3d|networking|request,34
Microsoft Computer Vision OCR : Disable grouping text by regions,41562347,Microsoft Computer Vision OCR : Disable grouping text by regions,"<p>I've been using Microsoft Computer Vision to read receipts, trying to find an alternative to Abby's OCR as there is a substantial price difference.</p>

<p>The results I get are always grouped by regions. This obviously makes it much harder to identify the corresponding fields with their amounts. </p>

<p>Is there a way through Microsoft Vision or anyway at all that I can achieve the same aligned output as Abby's? </p>

<p>Here's an image with both results and the receipt </p>

<p>Ocr Results</p>

<p><a href=""https://i.stack.imgur.com/wWdBE.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/wWdBE.png"" alt=""enter image description here""></a></p>",,1,0,,2017-01-10 06:21:48.733 UTC,1,2017-05-03 03:44:32.763 UTC,2017-01-10 07:26:58.307 UTC,,6293599,,5771629,1,6,c#|service|ocr|microsoft-cognitive,698
Invalid api key when requesting for Google cloud Vision api in android app,44166139,Invalid api key when requesting for Google cloud Vision api in android app,"<p>Hello I'm trying to test out the google cloud vision api for my android app.
I enabled the api and created an OAuth 2.0 client ID and I'm using the sample code 
from google: <a href=""https://github.com/GoogleCloudPlatform/cloud-vision/blob/master/android/CloudVision/app/src/main/java/com/google/sample/cloudvision/MainActivity.java"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/cloud-vision/blob/master/android/CloudVision/app/src/main/java/com/google/sample/cloudvision/MainActivity.java</a></p>

<p>and here is the json resonse:</p>

<pre><code>{""code"": 400,

""errors"": [

{

""domain"": ""global"",

""message"": ""API key not valid. Please pass a valid API key."",

""reason"": ""badRequest""

}

],

""message"": ""API key not valid. Please pass a valid API key."",

""status"": ""INVALID_ARGUMENT""  }
</code></pre>

<p>I am pretty sure the api key provided is the one I am using.
What could I be doing wrong?</p>",44207864,1,0,,2017-05-24 18:33:33.877 UTC,,2017-05-26 18:15:32.967 UTC,,,,,7613290,1,0,android|api|key|google-cloud-platform,641
SSL Handshake Error while calling Visual Recognition in Watson,49067894,SSL Handshake Error while calling Visual Recognition in Watson,"<p>I am getting SSL Handshake error while trying to call Watson Visual Recognition Service through java. Any help will be highly appreciated.</p>

<pre><code>   public static void main(String[] args) throws FileNotFoundException    {
   VisualRecognition service = new VisualRecognition(VisualRecognition.VERSION_DATE_2016_05_20);
   service.setApiKey(""api_key"");

   InputStream imagesStream = new FileInputStream(""C:\\fruitbowl.jpg"");
   ClassifyOptions classifyOptions =
           new ClassifyOptions.Builder().imagesFile(imagesStream).imagesFilename(""fruitbowl.jpg"")
                   .parameters(
                           ""{\""classifier_ids\"": [\""fruits_1462128776\"", + \""SatelliteModel_6242312846\""],\""threshold\"": 0.6}"")
                   .build();
   ClassifiedImages result = service.classify(classifyOptions).execute();
   System.out.println(result);
    }}
</code></pre>

<blockquote>
  <p>Exception in thread ""main"" java.lang.RuntimeException: javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure<br>
     at com.ibm.watson.developer_cloud.service.WatsonService$1.execute(WatsonService.java:176)<br>
     at editcsvfile.csv.MainTest.main(MainTest.java:22)<br>
  Caused by: javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure</p>
</blockquote>",,0,3,,2018-03-02 11:07:51.393 UTC,2,2018-03-02 14:56:27.027 UTC,2018-03-02 14:56:27.027 UTC,,1813616,,1305448,1,0,java|ssl|ibm-watson|jdk1.6|watson,80
program type already present com.amazonaws.services.cognitoidentity.model.GetCredentialsForIdentityResult,51229325,program type already present com.amazonaws.services.cognitoidentity.model.GetCredentialsForIdentityResult,"<p>I got this error from logcat.</p>

<pre>
Program type already present: com.amazonaws.services.cognitoidentity.model.GetCredentialsForIdentityResult
Message{kind=ERROR, text=Program type already present: com.amazonaws.services.cognitoidentity.model.GetCredentialsForIdentityResult, sources=[Unknown source file], tool name=Optional.of(D8)}
</pre>

<p>and this is my build.gradle.</p>

<pre><code>apply plugin: 'com.android.application'

android {
    compileSdkVersion 28
    defaultConfig {
        applicationId ""com.example.yunjung.yunjung""
        minSdkVersion 23
        targetSdkVersion 28
        multiDexEnabled true
        versionCode 1
        versionName ""1.0""
        testInstrumentationRunner ""android.support.test.runner.AndroidJUnitRunner""
    }
    buildTypes {
        release {
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
        }
    }
}

dependencies {
    implementation fileTree(include: ['*.jar'], dir: 'libs')

    implementation 'com.android.support:appcompat-v7:28.0.0-alpha3'
    implementation 'com.android.support.constraint:constraint-layout:1.1.2'

    testImplementation 'junit:junit:4.12'

    androidTestImplementation 'com.android.support.test:runner:1.0.2'
    androidTestImplementation 'com.android.support.test.espresso:espresso-core:3.0.2'

    implementation 'com.amazonaws:aws-java-sdk:1.11.361'
    implementation('com.amazonaws:aws-android-sdk-mobile-client:2.6.24@aar') { transitive = true }

    implementation 'com.amazonaws:aws-android-sdk-core:2.6.24'
    implementation 'com.amazonaws:aws-android-sdk-s3:2.6.24'
    implementation 'com.amazonaws:aws-android-sdk-ddb:2.6.24'
    implementation 'com.amazonaws:aws-android-sdk-rekognition:2.6.24'
}
</code></pre>",,0,1,,2018-07-08 06:08:16.760 UTC,,2018-07-08 06:20:51.440 UTC,2018-07-08 06:20:51.440 UTC,,2058133,,10044814,1,0,android|gradle|runtime-error,268
Faces indexed by iOS/Android app are not detected by Android/iOS App - AWS Rekognition,53003814,Faces indexed by iOS/Android app are not detected by Android/iOS App - AWS Rekognition,"<p>So I have been working on a product (Android First and then iOS) for a long time that index faces of people using AWS Rekognition and when they are again scanned later, it identifies them. 
It's working great when I index a face from an Android device and then try to search it with an Android device. But if I try to search it later on iOS app, it doesn't find it. Same is the result if I go other way round. Index with iOS, search with Android, not found.
The collection ID is same while indexing and searching on both devices. I couldn't figure out how is it possible that a face indexed by one OS type, same region, same collection, couldn't be found while on other device.</p>

<p>If anyone here could try and help me with the issue, please do. I'll be really thankful.</p>

<p>Update 1: I have called ""listCollections"" function on both iOS and android apps. Both of them are showing different list of collections. This is the issue. But I can't figure our why it is happening. The identity pool and region is same on both of them.</p>

<p>Here is my Android Code to access Rekognition:</p>

<pre><code>mCredentialsProvider = new CognitoCachingCredentialsProvider(
        mContext,
        ""us-east-2:xbxfxexf-x5x5-xax7-x9xf-x5x0xexfx1xb"", // Identity pool ID
        Regions.US_EAST_2 // Region
);

mUUID = UUID.randomUUID().toString().replace(""-"", """");

mAmazonS3Client = new AmazonS3Client(mCredentialsProvider);
mAmazonS3Client.setRegion(Region.getRegion(Regions.US_EAST_2));
mAmazonRekognitionClient = new AmazonRekognitionClient(mCredentialsProvider);

if(!mAmazonS3Client.doesBucketExist(mFacesBucket)) {
    mAmazonS3Client.createBucket(mFacesBucket);
}

Log.i(TAG, ""Uploading image to S3 Bucket"");
mAmazonS3Client.putObject(mFacesBucket, getS3ObjectName(), new File(data[0].toString()));
Log.i(TAG, ""Image Uploaded"");

Image image = new Image();
try {
    image.setBytes(ByteBuffer.wrap(Files.toByteArray(new File(data[0].toString()))));
} catch (IOException e) {
    e.printStackTrace();
}

Log.i(TAG, ""Indexing image"");
IndexFacesRequest indexFacesRequest =new IndexFacesRequest()
        .withCollectionId(mFacesCollection)
        .withImage(image)
        .withExternalImageId(mUUID)
        .withDetectionAttributes(""ALL"");

mAmazonRekognitionClient.indexFaces(indexFacesRequest);
</code></pre>

<p>Here is my iOS code to access Rekognition:</p>

<pre><code>func uploadToCollection(img: UIImage)
    {
        let myIdentityPoolId=""us-east-2:xbxfxexf-x5x5-xax7-x9xf-x5x0xexfx1xb""

        let credentialsProvider = AWSCognitoCredentialsProvider(regionType: .USEast2, identityPoolId: myIdentityPoolId)
        //store photo in s3()
        let configuration = AWSServiceConfiguration(region: .USEast2, credentialsProvider: credentialsProvider)

        AWSServiceManager.default().defaultServiceConfiguration = configuration
        rekognitionClient = AWSRekognition.default()

        guard let request = AWSRekognitionIndexFacesRequest() else
        {
            puts(""Unable to initialize AWSRekognitionindexFaceRequest."")
            return
        }
        var go=false
        request.collectionId = ""i_faces"" + self.firebaseID.lowercased() //here iosCollection will be replaced by firebase Current UserID
        request.detectionAttributes = [""ALL"", ""DEFAULT""]
        request.externalImageId = self.UUID //this should be mUUID, passed as parameter to this function
        let sourceImage = img
        let image = AWSRekognitionImage()
        image!.bytes = sourceImage.jpegData(compressionQuality: 0.7)
        request.image = image
        self.rekognitionClient.indexFaces(request) { (response:AWSRekognitionIndexFacesResponse?, error:Error?) in
            if error == nil
            {
                print(""Upload to Collection Complete"")
            }
            go=true
            return
        }
        while(go==false){}
    }
</code></pre>",53128782,2,0,,2018-10-26 07:42:52.830 UTC,,2018-11-03 05:51:14.677 UTC,2018-10-30 16:13:21.407 UTC,,4395264,,4395264,1,2,android|ios|amazon-web-services|aws-sdk|amazon-rekognition,149
Extracting badly formatted text from a PDF,55997760,Extracting badly formatted text from a PDF,"<p>I'm trying to extract some entries from a PDF, but the bad formatting is making it inconvenient to simply parse through like a normal document. There isn't any consistent positioning for the text, so each entry is a unique scramble with no consistent pattern I can find. I only want the entry name and the info on the right, not the field name or description.</p>

<p><a href=""https://i.stack.imgur.com/tuTCS.png"" rel=""nofollow noreferrer"" title=""An image of the PDF I&#39;m trying to extract data from in the original format""><img src=""https://i.stack.imgur.com/tuTCS.png"" alt=""An image of the PDF I&#39;m trying to extract data from in the original format"" title=""An image of the PDF I&#39;m trying to extract data from in the original format""></a></p>

<p>I've tried experimenting with headers and layout info using the PyPDF2 Module but there doesn't seem to be any metadata for the PDF besides basic author info.</p>

<p>My idea was using the Google Cloud Vision API to transcribe the text, but that brings up issues of auto-positioning.</p>

<p>Does anyone know of a better methodology for this, or if not, simply how to execute the positioning for the Cloud Vision API?</p>",,0,0,,2019-05-06 01:16:54.547 UTC,,2019-05-06 04:49:00.953 UTC,2019-05-06 04:49:00.953 UTC,,1729265,,11296201,1,0,python-3.x|pdf,22
Blur people from 1000+ images,50250154,Blur people from 1000+ images,"<p>I need to anonymize people (and maybe later license plates) from thousand images automatic.</p>

<p>I search through the internet to make a solution on my own with openCV/emguCV, but so far the detection rate is rather bad.</p>

<p>Then I came across Amazon Rekognition, which also looks good but has a steep learning curve for me.</p>

<p>I am somewhat confused that there is no software out there to anonymize pictures without userinput, I though in the age of StreetView this would be easier.</p>

<p>Am I missing something out here?</p>",,1,0,,2018-05-09 09:39:53.650 UTC,1,2018-05-09 10:01:27.120 UTC,,,,,5148451,1,0,opencv|image-processing|amazon-rekognition,58
How to capture a photo and using Google Vision API translate it to text?,52081400,How to capture a photo and using Google Vision API translate it to text?,"<p>The application only able to translate the image to text with using res/drawable image but i need to create an application that able to capture a photo and display it on ImageView and let the google vision api to translate the image to text. Can anyone tell me how to do it or other way to do it.</p>

<pre><code>ImageView imgView;
Button btnCamera, btnTranslate;
TextView txtResult;

@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.activity_main);

    imgView = findViewById(R.id.imgView);
    btnCamera = findViewById(R.id.btnCamera);
    btnTranslate = findViewById(R.id.btnTranslate);
    txtResult = findViewById(R.id.txtResult);

    btnCamera.setOnClickListener(new View.OnClickListener() {
        @Override
        public void onClick(View v) {
            Intent cameraMode = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);
            startActivityForResult(cameraMode,0);
        }
    });

    btnTranslate.setOnClickListener(new View.OnClickListener() {
        @Override
        public void onClick(View v) {
            Bitmap bitmap = BitmapFactory.decodeResource(getApplicationContext().getResources(),R.id.imgView);

            TextRecognizer textRecognizer = new TextRecognizer.Builder(getApplicationContext()).build();

            if(textRecognizer.isOperational())
            {
                Frame frame = new Frame.Builder().setBitmap(bitmap).build();
                SparseArray&lt;TextBlock&gt; items = textRecognizer.detect(frame);
                StringBuilder stringBuilder = new StringBuilder();

                for(int i = 0; i &lt; items.size(); i++)
                {
                    TextBlock myItem = items.valueAt(i);
                    stringBuilder.append(myItem.getValue());
                    stringBuilder.append(""\n"");
                }

                txtResult.setText(stringBuilder.toString());
            }
            else
            {
                Toast.makeText(MainActivity.this,""Failed to get the text from image!"",Toast.LENGTH_SHORT).show();
            }
        }
    });


}



@Override
protected void onActivityResult(int requestCode, int resultCode, @Nullable Intent data) {
    super.onActivityResult(requestCode, resultCode, data);

    Bitmap bitmap = (Bitmap) data.getExtras().get(""data"");
    imgView.setImageBitmap(bitmap);
}
</code></pre>

<p>}</p>",,0,0,,2018-08-29 15:41:59.583 UTC,0,2018-08-29 15:41:59.583 UTC,,,,,7859973,1,0,java|android|api|ocr,32
"Undefined is not a constructor: AWS.Comprehend, aws JavaScript SDK",52163842,"Undefined is not a constructor: AWS.Comprehend, aws JavaScript SDK","<p>I´m trying to use the Amazon Comprehend API via aws JavaScript SDK. But I always get</p>

<blockquote>
  <p>Uncaught (in promise): TypeError: undefined is not a constructor
  (evaluating 'new AWS.Comprehend...</p>
</blockquote>

<p>' What I´m doing wrong? Thank you so much.</p>

<p>All other services e.g. Polly and Rekognition are working well.</p>

<pre><code> import * as AWS from 'aws-sdk';

 ....

 getTextAnalysis(textToAnalyze) {

   let awsCredentials = new AWS.Credentials(""XXXXXXXXXXX"", ""XXXXXXXXX"");
   let settings = {
       awsCredentials: awsCredentials,
       awsRegion: ""us-west-2""
   }

   AWS.config.credentials = settings.awsCredentials;
   AWS.config.region = settings.awsRegion;

   let sentimentAnalysis = new Promise(function (successCallback, errorCallback) {
     var comprehend = new AWS.Comprehend({apiVersion: '2017-11-27'});
     var params = {
          LanguageCode: 'en',
          Text: textToAnalyze
        }

     comprehend.detectSentiment(params, function (error, data) {
         if (error) {
             errorCallback(error)
         } else {
             console.log('comprehend: ' + JSON.stringify(data))
             successCallback(data)
         }
     });

 });

 return sentimentAnalysis;

 }
</code></pre>",53734675,1,2,,2018-09-04 10:07:58.443 UTC,0,2018-12-12 01:10:32.293 UTC,,,,,3542250,1,2,amazon-web-services|typescript|aws-sdk,185
Do you need internet connection to use Amazon rekognition or can it be used in offline app?,50417917,Do you need internet connection to use Amazon rekognition or can it be used in offline app?,<p>I would like to make an app that can utilize facial recognition from Amazon rekognition (AWS). Is internet connection required to use Amazon rekognition? </p>,50420766,2,1,,2018-05-18 19:16:19.520 UTC,,2018-05-24 13:50:47.870 UTC,2018-05-19 00:42:28.637 UTC,,174777,,9802437,1,0,amazon-web-services|amazon-rekognition,331
IBM Cloud Watson Visual Recognition image classification.. Error in uploading file,52419957,IBM Cloud Watson Visual Recognition image classification.. Error in uploading file,"<p>Through my IBM Cloud account, I have registered a Watson Visual Recognition service.
Then I tried using their API to upload an image '<code>fruitbowl.jpg</code>' to my bucket associated with this service and then get some analysis on that image.</p>

<p>I have tried the code available at <a href=""https://dataplatform.cloud.ibm.com/analytics/notebooks/v2/ee1d0b44-0fce-4cf6-8545-e1dc961d0668/view?access_token=c0489b861ab65f63be7e3c5ce962003a2a0197660e67ecb140c477c2e11b5fe3"" rel=""nofollow noreferrer"">link</a></p>

<p>Till the API and bucket validation, code works fine but throws error </p>

<pre><code>""`FileNotFoundError: [Errno 2] No such file or directory: 'fruitbowl.jpg'"" at: 

cos.upload_file(Filename='fruitbowl.jpg',Bucket=credentials['BUCKET'],Key='fruitbowl.jpg')
</code></pre>

<p>I can see that this file is available in my bucket.</p>

<p>Code:</p>

<pre><code>from ibm_botocore.client import Config
    import ibm_boto3
    def download_file_cos( credentials, local_file_name, key ):
        cos = ibm_boto3.client( service_name            = 's3',
                                ibm_api_key_id          = credentials['IBM_API_KEY_ID'],
                                ibm_service_instance_id = credentials['IAM_SERVICE_ID'],
                                ibm_auth_endpoint       = credentials['IBM_AUTH_ENDPOINT'],
                                config=Config(signature_version = 'oauth'),
                                endpoint_url            = credentials['ENDPOINT'] )
        try:
            cos.upload_file( Filename='fruitbowl.jpg',Bucket=credentials['BUCKET'],Key='fruitbowl.jpg')
        except Exception as e:
            print( Exception, e )
        else:
            print( 'File Downloaded' ) 
</code></pre>

<p>My current directory for IBM cloud notebook is </p>",,0,4,,2018-09-20 07:37:16.783 UTC,,2018-09-20 08:07:40.180 UTC,2018-09-20 08:07:40.180 UTC,,4923755,,7265885,1,0,ibm-cloud|visual-recognition,64
Different results using google-cloud-vision online and api,48075707,Different results using google-cloud-vision online and api,"<p>I used google vision online in <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/</a> to get the detected text for an image file. I received 5 results.</p>

<p>Then I used the api to get the detected text for the same image file. But I only received 3 results and only 1 is exactly the same as one of the online-result.</p>

<p>Can anyone tell how to get the same results?</p>",48120468,1,1,,2018-01-03 10:38:38.330 UTC,,2018-01-05 19:58:56.233 UTC,,,,,9087866,1,0,google-vision,103
Google vision API label detection,43383886,Google vision API label detection,"<p>in google vision api label detection, can't know where object located ? any options or idea ??
I have tried in sample, and then response json is does not include object position!</p>",,1,1,,2017-04-13 05:04:33.300 UTC,,2017-07-01 20:40:20.953 UTC,2017-04-13 18:30:03.733 UTC,,5231007,,7603159,1,0,vision|google-cloud-vision|vision-api,579
java.lang.NoClassDefFoundError: Failed resolution of: Lcom/google/android/gms/vision/face/FaceDetector;,48085989,java.lang.NoClassDefFoundError: Failed resolution of: Lcom/google/android/gms/vision/face/FaceDetector;,"<p>I'm writing an android plugin for Unity that uses android face detection. I have the plugin working with the old </p>

<pre><code>android.media.FaceDetector;
</code></pre>

<p>face detector and I want to update it to the new Google vision APIs</p>

<p>For some reason, by simply adding in a declaration of a face detector variable cause the plugin to crash as soon as it starts.</p>

<p>Here is the relevant code.</p>

<pre><code>package com.test.camerapreview;

import android.app.Activity;
//import android.content.Context;
//import android.graphics.Bitmap;
//import android.graphics.BitmapFactory;
import android.graphics.PointF;
import android.graphics.Rect;
import android.graphics.SurfaceTexture;
import android.hardware.Camera;
//import android.media.FaceDetector;
import android.opengl.GLES11Ext;
import android.opengl.GLES20;
import android.util.Log;
//import android.util.SparseArray;
import android.view.Display;
import android.view.Surface;
//import android.view.SurfaceView;

//import com.google.android.gms.vision.Frame;
import com.google.gson.Gson;
import com.unity3d.player.UnityPlayer;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

//import com.google.android.gms.common.ConnectionResult;
//import com.google.android.gms.common.GoogleApiAvailability;
//import com.google.android.gms.vision.CameraSource;
//import com.google.android.gms.vision.MultiProcessor;
//import com.google.android.gms.vision.Tracker;
//import com.google.android.gms.vision.face.Face;
import com.google.android.gms.vision.face.FaceDetector;

    public class NativeCamera implements Camera.PreviewCallback, Camera.FaceDetectionListener {

        public static NativeCamera instance;
        public static String gameObjectTargetName;
        private static Activity myActivity;

        Camera mCamera;
        int camId;
        SurfaceTexture texture;
        int nativeTexturePointer = -1;

        int unrotatedWidth;
        int unrotatedHeight;
        int rotatedHeight;
        int rotatedWidth;

        com.google.android.gms.vision.face.FaceDetector faceDetector; // Putting this here makes my app crash.

        //
        // Call this function first.
        //
        public static void Setup(String gameObjectName, Activity theActivity){
            gameObjectTargetName = gameObjectName;
            myActivity = theActivity;
            instance = new NativeCamera();
        }

    // Rest of code snipped out
}
</code></pre>

<p>The only difference to this source is the addition of the faceDetector variable. It is not used anywhere.</p>

<p>This is the error that I see in adb logcat</p>

<pre><code>01-03 14:00:45.798 19649-19787/? W/MDM: InstalledPackageMonitor - &lt;--&gt; getAppVersionForUid(-1) failed to find matching entity
01-03 14:00:46.123 19649-19787/? I/chatty: uid=10117(com.metago.astro) bmv identical 1 line
01-03 14:00:46.141 19649-19787/? W/MDM: InstalledPackageMonitor - &lt;--&gt; getAppVersionForUid(-1) failed to find matching entity
01-03 14:00:46.346 723-830/? I/ThermalEngine: vs_get_temperature: read[0] tsens_tz_sensor15 49000 mC, weight[0] 1
01-03 14:00:46.346 723-830/? I/ThermalEngine: vs_get_temperature: read[1] tsens_tz_sensor0 53000 mC, weight[1] -1
01-03 14:00:46.710 19649-19787/? W/MDM: InstalledPackageMonitor - &lt;--&gt; getAppVersionForUid(-1) failed to find matching entity
01-03 14:00:46.770 14771-14784/? I/zygote64: Waiting for a blocking GC ProfileSaver
01-03 14:00:46.792 14771-14784/? I/zygote64: WaitForGcToComplete blocked ProfileSaver on AddRemoveAppImageSpace for 21.469ms
01-03 14:00:47.348 723-830/? I/ThermalEngine: vs_get_temperature: read[0] tsens_tz_sensor15 49000 mC, weight[0] 1
01-03 14:00:47.348 723-830/? I/ThermalEngine: vs_get_temperature: read[1] tsens_tz_sensor0 52000 mC, weight[1] -1
01-03 14:00:47.492 2286-2286/? I/WearableService: Wearable Services stopping
01-03 14:00:48.350 723-830/? I/ThermalEngine: vs_get_temperature: read[0] tsens_tz_sensor15 48000 mC, weight[0] 1
01-03 14:00:48.350 723-830/? I/ThermalEngine: vs_get_temperature: read[1] tsens_tz_sensor0 52000 mC, weight[1] -1
01-03 14:00:49.352 723-830/? I/ThermalEngine: vs_get_temperature: read[0] tsens_tz_sensor15 48000 mC, weight[0] 1
01-03 14:00:49.352 723-830/? I/ThermalEngine: vs_get_temperature: read[1] tsens_tz_sensor0 52000 mC, weight[1] -1
01-03 14:00:49.904 868-2419/? I/ActivityManager: START u0 {act=android.intent.action.MAIN cat=[android.intent.category.LAUNCHER] flg=0x10200000 cmp=com.tvgla.campreview/com.unity3d.player.UnityPlayerActivity bnds=[586,1508][855,1857]} from uid 10032
01-03 14:00:49.908 671-671/? D/QCOM PowerHAL: LAUNCH HINT: ON
01-03 14:00:49.909 1950-1952/? E/ANDR-PERF-OPTSHANDLER: Warning: Resource [2, 0] not supported for core 1. Instead use resource for core 0
01-03 14:00:49.909 658-2267/? I/ACDB-LOADER: ACDB AFE returned = -19
01-03 14:00:49.909 1950-1952/? E/ANDR-PERF-RESOURCEQS: Failed to apply optimization [2, 2, 0]
01-03 14:00:49.909 658-2267/? D/hardware_info: hw_info_append_hw_type : device_name = speaker
01-03 14:00:49.909 658-2267/? D/audio_hw_primary: enable_snd_device: snd_device(2: speaker)
01-03 14:00:49.909 658-2267/? D/audio_route: Apply path: speaker
01-03 14:00:49.909 658-2267/? D/audio_hw_primary: enable_audio_route: usecase(1) apply and update mixer path: low-latency-playback speaker
01-03 14:00:49.909 658-2267/? D/audio_route: Apply path: low-latency-playback speaker
01-03 14:00:49.923 671-671/? D/QCOM PowerHAL: Activity launch hint handled
01-03 14:00:49.948 868-894/? I/ActivityManager: Start proc 14820:com.tvgla.campreview/u0a324 for activity com.tvgla.campreview/com.unity3d.player.UnityPlayerActivity
01-03 14:00:49.948 658-2267/? D/audio_hw_primary: out_write: retry previous failed cal level set
01-03 14:00:49.958 14820-14820/? I/zygote: Late-enabling -Xcheck:jni
01-03 14:00:50.090 14820-14837/? D/OpenGLRenderer: HWUI GL Pipeline
01-03 14:00:50.121 14820-14837/? I/Adreno: QUALCOMM build                   : 2941438, I916dfac403
                                           Build Date                       : 10/03/17
                                           OpenGL ES Shader Compiler Version: EV031.21.02.00
                                           Local Branch                     : O18A
                                           Remote Branch                    : 
                                           Remote Branch                    : 
                                           Reconstruct Branch               : 
01-03 14:00:50.126 14820-14837/? I/Adreno: PFP: 0x005ff087, ME: 0x005ff063
01-03 14:00:50.132 14820-14837/? I/zygote: android::hardware::configstore::V1_0::ISurfaceFlingerConfigs::hasWideColorDisplay retrieved: 0
01-03 14:00:50.133 14820-14837/? I/OpenGLRenderer: Initialized EGL, version 1.4
01-03 14:00:50.133 14820-14837/? D/OpenGLRenderer: Swap behavior 2
01-03 14:00:50.165 868-918/? I/ActivityManager: Displayed com.tvgla.campreview/com.unity3d.player.UnityPlayerActivity: +225ms
01-03 14:00:50.167 1377-1377/? I/GoogleInputMethod: onFinishInput() : Dummy InputConnection bound
01-03 14:00:50.167 1377-1377/? I/GoogleInputMethod: onStartInput() : Dummy InputConnection bound
01-03 14:00:50.173 671-671/? D/QCOM PowerHAL: LAUNCH HINT: OFF
01-03 14:00:50.192 14820-14836/? I/Unity: SystemInfo CPU = ARMv7 VFPv3 NEON, Cores = 4, Memory = 3753mb
01-03 14:00:50.192 14820-14836/? I/Unity: SystemInfo ARM big.LITTLE configuration: 2 big (mask: 12), 2 little (mask: 3)
01-03 14:00:50.192 14820-14836/? I/Unity: ApplicationInfo com.tvgla.campreview version 1.0 build 44c1fe70-80a4-4a60-8a18-207b4d37a86f
01-03 14:00:50.195 14836-14836/? W/UnityMain: type=1400 audit(0.0:91697): avc: denied { read } for name=""stat"" dev=""proc"" ino=4026532357 scontext=u:r:untrusted_app:s0:c512,c768 tcontext=u:object_r:proc_stat:s0 tclass=file permissive=0
01-03 14:00:50.262 14820-14836/? E/Unity: Unable to find AudioPluginMsHRTF
01-03 14:00:50.263 14820-14836/? E/Unity: Unable to find AudioPluginOculusSpatializer
01-03 14:00:50.264 14820-14836/? E/Unity: Unable to find unitygar
01-03 14:00:50.264 14820-14836/? E/Unity: Unable to find libAudioPluginOculusSpatializer
01-03 14:00:50.266 14820-14836/? E/Unity: Unable to find libtango_3d_reconstruction_api
01-03 14:00:50.280 14820-14836/? D/Unity:  GL_OES_EGL_image GL_OES_EGL_image_external GL_OES_EGL_sync GL_OES_vertex_half_float GL_OES_framebuffer_object GL_OES_rgb8_rgba8 GL_OES_compressed_ETC1_RGB8_texture GL_AMD_compressed_ATC_texture GL_KHR_texture_compression_astc_ldr GL_KHR_texture_compression_astc_hdr GL_OES_texture_compression_astc GL_OES_texture_npot GL_EXT_texture_filter_anisotropic GL_EXT_texture_format_BGRA8888 GL_OES_texture_3D GL_EXT_color_buffer_float GL_EXT_color_buffer_half_float GL_QCOM_alpha_test GL_OES_depth24 GL_OES_packed_depth_stencil GL_OES_depth_texture GL_OES_depth_texture_cube_map GL_EXT_sRGB GL_OES_texture_float GL_OES_texture_float_linear GL_OES_texture_half_float GL_OES_texture_half_float_linear GL_EXT_texture_type_2_10_10_10_REV GL_EXT_texture_sRGB_decode GL_OES_element_index_uint GL_EXT_copy_image GL_EXT_geometry_shader GL_EXT_tessellation_shader GL_OES_texture_stencil8 GL_EXT_shader_io_blocks GL_OES_shader_image_atomic GL_OES_sample_variables GL_EXT_texture_border_clamp GL_EXT_multisampled_render_to_texture GL_EXT_mul
01-03 14:00:50.280 14820-14836/? D/Unity: tisampled_render_to_texture2 GL_OES_shader_multisample_interpolation GL_EXT_texture_cube_map_array GL_EXT_draw_buffers_indexed GL_EXT_gpu_shader5 GL_EXT_robustness GL_EXT_texture_buffer GL_EXT_shader_framebuffer_fetch GL_ARM_shader_framebuffer_fetch_depth_stencil GL_OES_texture_storage_multisample_2d_array GL_OES_sample_shading GL_OES_get_program_binary GL_EXT_debug_label GL_KHR_blend_equation_advanced GL_KHR_blend_equation_advanced_coherent GL_QCOM_tiled_rendering GL_ANDROID_extension_pack_es31a GL_EXT_primitive_bounding_box GL_OES_standard_derivatives GL_OES_vertex_array_object GL_EXT_disjoint_timer_query GL_KHR_debug GL_EXT_YUV_target GL_EXT_sRGB_write_control GL_EXT_texture_norm16 GL_EXT_discard_framebuffer GL_OES_surfaceless_context GL_OVR_multiview GL_OVR_multiview2 GL_EXT_texture_sRGB_R8 GL_KHR_no_error GL_EXT_debug_marker GL_OES_EGL_image_external_essl3 GL_OVR_multiview_multisampled_render_to_texture GL_EXT_buffer_storage GL_EXT_external_buffer GL_EXT_blit_framebuffer_params GL_EXT_clip_cull_distance
01-03 14:00:50.280 14820-14836/? D/Unity:  GL_EXT_protected_textures GL_EXT_shader_non_constant_global_initializers GL_QCOM_framebuffer_foveated GL_QCOM_shader_framebuffer_fetch_noncoherent GL_EXT_EGL_image_array GL_NV_shader_noperspective_interpolation

                                          [ 01-03 14:00:50.316 14820:14836 D/         ]
                                          PlayerBase::PlayerBase()

                                          [ 01-03 14:00:50.317 14820:14836 D/         ]
                                          TrackPlayerBase::TrackPlayerBase()
01-03 14:00:50.317 14820-14836/? I/libOpenSLES: Emulating old channel mask behavior (ignoring positional mask 0x3, using default mask 0x3 based on channel count of 2)
01-03 14:00:50.317 14820-14836/? W/AudioTrack: notificationFrames=-10 clamped to the range -1 to -8
01-03 14:00:50.319 14820-14836/? I/AudioTrack: AUDIO_OUTPUT_FLAG_FAST successful; frameCount 1536 -&gt; 1536
01-03 14:00:50.319 14820-14836/? D/AudioTrack: Client defaulted notificationFrames to 192 for frameCount 1536
01-03 14:00:50.321 1342-1881/? D/Avrcp: AudioManager Player: ID:23 -- type:android.media.SoundPool -- u/pid:10043/1393 -- state:idle -- attr:AudioAttributes: usage=USAGE_ASSISTANCE_SONIFICATION content=CONTENT_TYPE_SONIFICATION flags=0x0 tags= bundle=null
01-03 14:00:50.321 1342-1881/? D/Avrcp: AudioManager Player: ID:15 -- type:android.media.SoundPool -- u/pid:1000/868 -- state:idle -- attr:AudioAttributes: usage=USAGE_ASSISTANCE_SONIFICATION content=CONTENT_TYPE_SONIFICATION flags=0x0 tags= bundle=null
01-03 14:00:50.321 1342-1881/? D/Avrcp: AudioManager Player: ID:31 -- type:android.media.SoundPool -- u/pid:1027/2374 -- state:idle -- attr:AudioAttributes: usage=USAGE_NOTIFICATION content=CONTENT_TYPE_SONIFICATION flags=0x0 tags= bundle=null
01-03 14:00:50.321 1342-1881/? D/Avrcp: AudioManager Player: ID:2431 -- type:android.media.MediaPlayer -- u/pid:10035/8516 -- state:idle -- attr:AudioAttributes: usage=USAGE_UNKNOWN content=CONTENT_TYPE_UNKNOWN flags=0x0 tags= bundle=null
01-03 14:00:50.321 1342-1881/? D/Avrcp: AudioManager Player: ID:2447 -- type:android.media.MediaPlayer -- u/pid:10043/1393 -- state:stopped -- attr:AudioAttributes: usage=USAGE_NOTIFICATION content=CONTENT_TYPE_SONIFICATION flags=0x0 tags= bundle=null
01-03 14:00:50.321 1342-1881/? D/Avrcp: AudioManager Player: ID:2527 -- type:OpenSL ES AudioPlayer (Buffer Queue) -- u/pid:10324/14820 -- state:started -- attr:AudioAttributes: usage=USAGE_MEDIA content=CONTENT_TYPE_UNKNOWN flags=0x0 tags= bundle=null
01-03 14:00:50.321 1342-1881/? D/Avrcp: AudioManager isPlaying: true
01-03 14:00:50.322 656-656/? W//system/bin/hw/android.hidl.allocator@1.0-service: ashmem_create_region(3840) returning hidl_memory(0x7d5f82b330, 3840)
01-03 14:00:50.323 656-656/? W//system/bin/hw/android.hidl.allocator@1.0-service: ashmem_create_region(3840) returning hidl_memory(0x7d5f82b330, 3840)
01-03 14:00:50.332 658-22045/? E/volume_listener: check_and_set_gain_dep_cal: Failed to set gain dep cal level
01-03 14:00:50.333 1342-1881/? E/Avrcp: play status change 2➡1 mPlayStatusChangedNT: 1
01-03 14:00:50.334 1342-1881/? E/bt_btif: register_notification_rsp: Avrcp device is not connected, handle: 0x0
01-03 14:00:50.334 1342-1881/? D/Avrcp: sendPlayPosNotificationRsp: Not registered or requesting.
01-03 14:00:50.334 1950-1952/? E/ANDR-PERF-OPTSHANDLER: Warning: Resource [2, 0] not supported for core 1. Instead use resource for core 0
01-03 14:00:50.334 658-9407/? I/ACDB-LOADER: ACDB AFE returned = -19
01-03 14:00:50.334 1950-1952/? E/ANDR-PERF-RESOURCEQS: Failed to apply optimization [2, 2, 0]
01-03 14:00:50.334 658-9407/? D/audio_hw_primary: enable_audio_route: usecase(5) apply and update mixer path: audio-ull-playback speaker
01-03 14:00:50.334 658-9407/? D/audio_route: Apply path: audio-ull-playback speaker
01-03 14:00:50.342 658-9407/? D/audio_hw_primary: out_write: retry previous failed cal level set
01-03 14:00:50.354 723-830/? I/ThermalEngine: vs_get_temperature: read[0] tsens_tz_sensor15 49000 mC, weight[0] 1
01-03 14:00:50.354 723-830/? I/ThermalEngine: vs_get_temperature: read[1] tsens_tz_sensor0 54000 mC, weight[1] -1
01-03 14:00:50.383 1543-1543/? D/zz: UtWallpaperService$UtEngine.onVisibilityChanged() false
01-03 14:00:50.386 1543-1543/? D/zz: UtRenderer.onNotVisible() 
01-03 14:00:50.504 2331-2331/? I/PBSessionCacheImpl: Deleted sessionId[8605084760413] from persistence.
01-03 14:00:50.533 868-1542/? I/WifiService: getWifiEnabledState uid=10048
01-03 14:00:50.550 2331-2331/? W/SearchService: Abort, client detached.
01-03 14:00:51.356 723-830/? I/ThermalEngine: vs_get_temperature: read[0] tsens_tz_sensor15 50000 mC, weight[0] 1
01-03 14:00:51.356 723-830/? I/ThermalEngine: vs_get_temperature: read[1] tsens_tz_sensor0 54000 mC, weight[1] -1
01-03 14:00:52.357 723-830/? I/ThermalEngine: vs_get_temperature: read[0] tsens_tz_sensor15 49000 mC, weight[0] 1
01-03 14:00:52.357 723-830/? I/ThermalEngine: vs_get_temperature: read[1] tsens_tz_sensor0 53000 mC, weight[1] -1
01-03 14:00:52.979 868-894/? W/SensorService: sensor 00000001 already enabled in connection 0x782da33f00 (ignoring)
01-03 14:00:53.116 658-22045/? D/audio_hw_primary: disable_audio_route: usecase(1) reset and update mixer path: low-latency-playback speaker
01-03 14:00:53.154 14820-14836/? E/Unity: AndroidJavaException: java.lang.NoSuchFieldError: no ""Ljava/lang/Object;"" field ""instance"" in class ""Lcom/test/camerapreview/NativeCamera;"" or its superclasses
                                          java.lang.NoSuchFieldError: no ""Ljava/lang/Object;"" field ""instance"" in class ""Lcom/test/camerapreview/NativeCamera;"" or its superclasses
                                              at com.unity3d.player.UnityPlayer.nativeRender(Native Method)
                                              at com.unity3d.player.UnityPlayer.c(Unknown Source:0)
                                              at com.unity3d.player.UnityPlayer$c$1.handleMessage(Unknown Source:151)
                                              at android.os.Handler.dispatchMessage(Handler.java:102)
                                              at android.os.Looper.loop(Looper.java:164)
                                              at com.unity3d.player.UnityPlayer$c.run(Unknown Source:20)
                                            at UnityEngine.AndroidJNISafe.CheckException () [0x00000] in &lt;filename unknown&gt;:0 
                                            at UnityEngine.AndroidJNISafe.GetStaticFieldID (IntPtr clazz, System.String name, System.String sig) [0x00000] in &lt;filename unknown&gt;:0 
                                            at UnityEngine._AndroidJNIHelper.GetFieldID (IntPtr jclass, System.String fieldName, System.String signature, Boolean isStatic) [0x00000] in &lt;filena
</code></pre>

<p>It is complaining about the variable called instance which is set in the Setup function that is called from Unity. Without this variable, my plugin works. With it, it crashes.</p>

<p>I did also make a change to my build.gradle file. Here it is.</p>

<pre><code>apply plugin: 'com.android.library'

android {
    compileSdkVersion 25
    buildToolsVersion ""25.0.3""

    defaultConfig {
        minSdkVersion 18
        targetSdkVersion 25
        versionCode 1
        versionName ""1.0""

    }
    buildTypes {
        release {
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
        }
    }
}

dependencies {
    compile 'com.android.support:appcompat-v7:25.3.1'
    compile 'com.google.android.gms:play-services-vision:9.4.0+' // I added this line.
    provided files('./libs/UnityPlayer.jar')
    compile files('libs/gson-2.8.2.jar')
}

android.libraryVariants.all { variant -&gt;
    // Task names.
    String variantName = ""${variant.name.capitalize()}""; // Like 'Debug'
    String deployTaskGroup = ""plugin"";
    String deployTaskName = ""deploy${variantName}PluginArchive""; // Like 'deployDebugPluginArchive'
    String dependencyTaskName = ""assemble${variantName}""; // Like 'assembleDebug'
    // Source.
    String sourceAARFolder = ""${buildDir.getPath()}/outputs/aar/"";
    String sourceAARName = ""${project.name}-${variant.name}.aar"";
    // Target.
    String targetAARFolder = ""../../../Assets/Plugins/Android""; // Navigate into 'Assets'
    String targetAARName = ""Native Camera.aar""; // The form you ship your plugin
    // Create task.
    task(deployTaskName, dependsOn: dependencyTaskName, type: Copy) {
        from(sourceAARFolder)
        into(targetAARFolder)
        include(sourceAARName)
        rename(sourceAARName, targetAARName)
    }.group = deployTaskGroup;
}
</code></pre>

<p>I added the line</p>

<pre><code>compile 'com.google.android.gms:play-services-vision:9.4.0+'
</code></pre>

<p><strong>Update</strong></p>

<p>I have tried renaing the variable from instance to mInstance and I had the same issuue.  I then tried creating the variable inside a function and returning it. Now I have a more intuitive error log, which I am not sure how to fix.</p>

<pre><code>01-03 14:38:32.505 19629-19646/? E/Unity: AndroidJavaException: java.lang.NoClassDefFoundError: Failed resolution of: Lcom/google/android/gms/vision/face/FaceDetector;
                                          java.lang.NoClassDefFoundError: Failed resolution of: Lcom/google/android/gms/vision/face/FaceDetector;
                                              at java.lang.reflect.Executable.getMethodReturnTypeInternal(Native Method)
                                              at java.lang.reflect.Method.getReturnType(Method.java:141)
                                              at java.lang.Class.getDeclaredMethods(Class.java:1880)
                                              at com.unity3d.player.ReflectionHelper.getMethodID(Unknown Source:63)
                                              at com.unity3d.player.UnityPlayer.nativeRender(Native Method)
                                              at com.unity3d.player.UnityPlayer.c(Unknown Source:0)
                                              at com.unity3d.player.UnityPlayer$c$1.handleMessage(Unknown Source:151)
                                              at android.os.Handler.dispatchMessage(Handler.java:102)
                                              at android.os.Looper.loop(Looper.java:164)
                                              at com.unity3d.player.UnityPlayer$c.run(Unknown Source:20)
                                           Caused by: java.lang.ClassNotFoundException: Didn't find class ""com.google.android.gms.vision.face.FaceDetector"" on path: DexPathList[[zip file ""/data/app/com.tvgla.campreview-C0
</code></pre>

<p>Thanks
John Lawrie</p>",,1,1,,2018-01-03 22:18:10.040 UTC,,2019-01-03 09:14:00.243 UTC,2018-01-03 22:57:25.337 UTC,,2957503,,2957503,1,1,android|crash|face-detection|vision|google-vision,700
Google cloud Vision and Clarifai doesn't Support tagging for 360 degree images and videos,47671289,Google cloud Vision and Clarifai doesn't Support tagging for 360 degree images and videos,"<p>I'm working on image processing. So far Google Cloud Vision and Clarifai are the best API's to detect objects from images and videos, but both API's doesn't support object detection from 360 degree images and videos. Is there any solution for this problem ?</p>",,0,3,,2017-12-06 09:50:45.100 UTC,,2017-12-06 09:50:45.100 UTC,,,,,5844927,1,0,image|image-processing|google-cloud-vision|object-detection-api|clarifai,39
How to handle API key regeneration on service bind in Bluemix Watson Visual Recognition?,44166894,How to handle API key regeneration on service bind in Bluemix Watson Visual Recognition?,"<p>I've got a Watson Visual Recognition service bound to a Bluemix Application. I'm managing the application deploy using <a href=""https://github.com/bluemixgaragelondon/cf-blue-green-deploy"" rel=""nofollow noreferrer"">blue-green deploy</a> with a smoke test.</p>

<p>One of the checks I'm including in the smoke test is function which depends on the Visual Recognition service. Because the smoke test runs immediately after deployment, and because it looks like the Visual Recognition service API key is regenerated on rebind - and then takes a little while to become valid - the smoke test fails. The wait to become valid is documented, but it's causing a headache. I've tried two workarounds:</p>

<ul>
<li>Add a retry-loop in my code to wait until the Visual Recognition service key is valid. My smoke test can then call this, ensuring that nothing gets pushed to live until it's got a valid key. I can see that the key becomes valid for free calls relatively quickly (within about 30s), but then it takes a couple of minutes to attach to entitlement for paid calls. Waiting for the key to be valid for paid calls adds about five minutes to my deployments, which isn't ideal, since our team push many times a day and the deployments can start to back up. </li>
<li>Create permanent credentials and use a user-provided service to bind them to my application. This means deployments can start using that key immediately, which is good, but I've bypassed the normal service binding mechanism, which seems wrong. </li>
</ul>

<p>Is there an option I've missed? </p>",,1,0,,2017-05-24 19:22:14.413 UTC,,2017-05-25 00:29:45.677 UTC,2017-05-24 19:28:29.247 UTC,,1104727,,1104727,1,2,ibm-cloud|ibm-watson|watson|visual-recognition,90
Version conflict between gms:play-services-vision and firebase,53955753,Version conflict between gms:play-services-vision and firebase,"<p>I've installed both react-native-firebase and react-native-camera. The camera was fine when play-services -vision was stuck at <code>12.0.1</code>, but I just ran into this error (<strong>Error updating property googleVisionBarcodeDetectorEnable</strong>)  <a href=""https://github.com/react-native-community/react-native-camera/issues/1844"" rel=""noreferrer"">https://github.com/react-native-community/react-native-camera/issues/1844</a> that requires an upgrade to <code>15.0.2</code>.</p>

<p>It looks like there are Google Play Services and Firebase conflicts when <code>play-services-vision</code> is bumped up to <code>15.0.2</code> from <code>12.0.1</code>:</p>

<pre><code>Dependency failing: com.google.android.gms:play-services-flags:15.0.1 -&gt; com.google.android.gms:play-services-basement@[
  15.0.1], but play-services-basement version was 16.0.1.

  The following dependencies are project dependencies that are direct or have transitive dependencies that lead to the art
  ifact with the issue.
  -- Project 'app' depends onto com.google.firebase:firebase-messaging@17.3.4
  -- Project 'app' depends onto com.google.android.gms:play-services-base@16.0.1
  -- Project 'app' depends onto com.google.firebase:firebase-core@16.0.6
  -- Project 'app' depends onto com.google.android.gms:play-services-vision@15.0.2
</code></pre>

<p>I've tried <code>com.google.android.gms:play-services-vision@16.2.0</code> but it gave me <code>exceed 64k methods</code> error. 
Bumping up to 17.0.2 would cause a version conflict from <code>com.google.android.gms:play-services-basement</code>. </p>

<p>Anyone using both react-native-firebase and react-native camera? Can you tell me how to solve this version conflict problem?</p>

<p>Here is the dependencies in android/app/build.gradle</p>

<pre><code>dependencies {

    implementation (project(':react-native-camera')) {
      exclude group: ""com.google.android.gms""
      implementation ""com.android.support:exifinterface:${rootProject.ext.supportLibVersion}""
      implementation ('com.google.android.gms:play-services-vision:12.0.1') {
        force = true
      }
    }
    implementation project(':react-native-gesture-handler')
    implementation project(':react-native-webview')
    implementation project(':react-native-fast-image')
    implementation project(':react-native-google-signin')
    implementation project(':react-native-firebase')
    implementation 'com.google.firebase:firebase-core:16.0.6'
    implementation ('com.google.android.gms:play-services-base:16.0.1')
    implementation 'com.google.firebase:firebase-messaging:17.3.4'
    implementation('com.crashlytics.sdk.android:crashlytics:2.9.5@aar') {
        transitive = true
    }
    implementation(project(':react-native-google-signin')) {
        exclude group: ""com.google.android.gms"" // very important
    }
    implementation fileTree(include: ['*.jar'], dir: 'libs')
    implementation ""com.android.support:appcompat-v7:${rootProject.ext.supportLibVersion}""
    implementation 'com.facebook.react:react-native:+'
    implementation project(':react-native-sqlite-storage')
    implementation 'me.leolin:ShortcutBadger:1.1.21@aar'
    implementation 'com.facebook.fresco:animated-gif:1.10.0'
}
</code></pre>

<p>Ext in android/build.gradle</p>

<pre><code>ext {
    buildToolsVersion = ""28.0.3""
    minSdkVersion = 19
    compileSdkVersion = 28
    targetSdkVersion = 28
    supportLibVersion = ""27.1.1""
}
</code></pre>

<p>Package:</p>

<pre><code>""react-native-camera"": ""^1.6.4"",
""react-native-firebase"": ""^5.1.1"",
</code></pre>",,3,2,,2018-12-28 08:42:55.827 UTC,,2019-01-16 20:44:02.567 UTC,2018-12-28 08:57:43.063 UTC,,2598292,,2598292,1,10,android|react-native|gradle|react-native-firebase,907
Aws Rekognition takes too long to compare face between 2 pictures,54015677,Aws Rekognition takes too long to compare face between 2 pictures,"<p>I am using the following code for Rekognition.</p>

<pre><code>AWSCredentials credentials = new BasicAWSCredentials(xx, yy);
            AmazonRekognition rekognitionClient = new AmazonRekognitionClient(credentials);
            rekognitionClient.setRegion(Region.getRegion(Regions.US_EAST_1));
            CompareFacesRequest request = new CompareFacesRequest()
                    .withSourceImage(new Image().withBytes(byteBufferSrc))
                    .withTargetImage(new Image().withBytes(byteBufferDest)).withSimilarityThreshold(90f);
            CompareFacesResult response = rekognitionClient.compareFaces(request);
            boolean matched = false;
            for (CompareFacesMatch singleMatch : response.getFaceMatches()) {
                if (singleMatch.getSimilarity() &gt;= 90f) {
                    return true;
                }
            }
</code></pre>

<p>It is taking almost 1 min to finish the face detection between the 2 images.
Is this normal? I find it excessive so I am wondering if there is a way to speed it up or if I am doing anything wrong
Thank you</p>",,0,2,,2019-01-03 02:44:52.010 UTC,,2019-01-03 02:44:52.010 UTC,,,,,1217820,1,0,android|amazon-rekognition,37
Google vision API food label detection in android,42702426,Google vision API food label detection in android,"<p>I am making an in-depth food logging application for android mobile and I would like to add some basic image recognition using the google vision API.</p>

<p>I've been experimenting with the API and using PHP with no success.
I've been looking through all the tutorials and always get stuck on some point.</p>

<p>This is the closest I've came so far in php </p>

<pre><code>&lt;?php
# Includes the autoloader for libraries installed with composer
require __DIR__ . '/vendor/autoload.php';

# Imports the Google Cloud client library
use Google\Cloud\Vision\VisionClient;

# Your Google Cloud Platform project ID
$projectId = 'foodlogging-160914';

putenv('GOOGLE_APPLICATION_CREDENTIALS=./FoodLogging-ae7e284eb66e.json');

# Instantiates a client
$vision = new VisionClient([
    'projectId' =&gt; $projectId
]);

# The name of the image file to annotate
$fileName = __DIR__ . '/hamburger.jpg';

# Prepare the image to be annotated
$image = $vision-&gt;image(fopen($fileName, 'r'), [
    'LABEL_DETECTION'
]);

# Performs label detection on the image file
$labels = $vision-&gt;annotate($image)-&gt;labels();

echo ""Labels:\n"";
foreach ($labels as $label) {
    echo $label-&gt;description() . ""\n"";
}
?&gt;
</code></pre>

<p>But then I get this error.</p>

<pre><code>Fatal error: Uncaught exception 'Google\Cloud\Exception\ServiceException' with message 'DateTime::__construct(): It is not safe to rely on the system's timezone settings. You are *required* to use the date.timezone setting or the date_default_timezone_set() function. In case you used any of those methods and you are still getting this warning, you most likely misspelled the timezone identifier. We selected the timezone 'UTC' for now, but please set date.timezone to select your timezone.' in /Library/WebServer/Documents/foodLogging/vendor/google/cloud/src/RequestWrapper.php:223 Stack trace: #0 /Library/WebServer/Documents/foodLogging/vendor/google/cloud/src/RequestWrapper.php(136): Google\Cloud\RequestWrapper-&gt;convertToGoogleException(Object(Google\Cloud\Exception\ServiceException)) #1 /Library/WebServer/Documents/foodLogging/vendor/google/cloud/src/RestTrait.php(83): Google\Cloud\RequestWrapper-&gt;send(Object(GuzzleHttp\Psr7\Request), Array) #2 /Library/WebServer/Documents/foodLogging/vendor/google/cloud/src/Vision/Connecti in /Library/WebServer/Documents/foodLogging/vendor/google/cloud/src/RequestWrapper.php on line 223
</code></pre>

<p>I've followed the entire documentation and I have no clue why it has trouble about the datetime because I never even use it.</p>

<p>Does anyone have any experience with the google vision API that can help me out? Preferably with the android part, help me get on my way or help me get started?</p>

<p>Thanks ahead.</p>",42702570,1,0,,2017-03-09 18:05:12.150 UTC,,2017-03-10 17:14:51.730 UTC,2017-03-10 17:14:51.730 UTC,,5231007,,3801533,1,1,php|android|google-play-services|google-cloud-vision,450
Swedish letters missing in google-vision Api response,38288634,Swedish letters missing in google-vision Api response,"<p>I am trying to use Google Cloud Vision with TEXT_DETECTION to perform OCR. It works perfect and i get the response in a Json object. The problem is that when I try to detect swedish text it does not return swedish letters. Api correctly detects the local. it returns the response but does not include swedish letters like (ä,ö) etc. </p>

<p>I have tried to set local in the request but it does not work.</p>

<p>I just want to get swedish letters in response. I have no idea what should i do?</p>

<p>if someone give some link to google-vision discussion thread what will be helpful.</p>",38467417,2,0,,2016-07-10 04:39:20.973 UTC,,2016-12-03 19:21:27.637 UTC,2016-07-11 16:05:33.900 UTC,,5231007,,1073229,1,2,android|ocr|google-cloud-vision,186
How to add Media Recorder to Android Google Play Service Vision Barcode scanner,35785224,How to add Media Recorder to Android Google Play Service Vision Barcode scanner,"<p>This question was asked but never answered <a href=""https://stackoverflow.com/questions/32715573/media-recorder-with-google-vision-api"">here</a> -- but it is somewhat different than my need, anyway.</p>

<p>I want to record video, while running the Google Vision library in the background, so whenever my user holds up a barcode (or approaches one closely enough) the camera will automatically detect and scan the barcode  -- and all the while it is recording the video. I know the Google Vision demo is pretty CPU intensive, but when I try a simpler version of it (i.e. without grabbing every frame all the time and handing it to the detector) I'm not getting reliable barcode reads.</p>

<p>(I am running a Samsung Galaxy S4 Mini on KitKat 4.4.3 Unfortunately, for reasons known only to Samsung, they no longer report the OnCameraFocused event, so it is impossible to know if the camera grabbed the focus and call the barcode read then. That makes grabbing and checking every frame seem like the only viable solution.)</p>

<p>So to at least prove the concept, I wanted to simply modify the Google Vision Demo. (Found <a href=""https://github.com/googlesamples/android-vision"" rel=""nofollow noreferrer"">Here</a>)</p>

<p>It seems the easiest thing to do is simply jump in the code and add a media recorder. I did this in the CameraSourcePreview method during surface create.</p>

<p>Like this:</p>

<pre><code>private class SurfaceCallback implements SurfaceHolder.Callback
{
    @Override
    public void surfaceCreated(SurfaceHolder surface)
    {
        mSurfaceAvailable = true;
        try
        {
            startIfReady();
            if (mSurfaceAvailable)
            {
                Camera camera = mCameraSource.getCameraSourceCamera();
                /** ADD MediaRecorder to Google Example  **/
                if (camera != null &amp;&amp; recordThis)
                {
                    if (mMediaRecorder == null)
                    {
                        mMediaRecorder = new MediaRecorder();
                        camera.unlock();
                        SurfaceHolder sh = mSurfaceView.getHolder();
                        mMediaRecorder.setPreviewDisplay(sh.getSurface());
                        mMediaRecorder.setCamera(camera);
                        mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER);
                        mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.CAMERA);
                        mMediaRecorder.setProfile(CamcorderProfile.get(CamcorderProfile.QUALITY_HIGH));
                        String OutputFile = Environment.getExternalStorageDirectory() + ""/"" +
                                DateFormat.format(""yyyy-MM-dd_kk-mm-ss"", new Date().getTime()) + "".mp4"";
                        File newOutPut = getVideoFile();
                        String newOutPutFileName = newOutPut.getPath();
                        mMediaRecorder.setOutputFile(newOutPutFileName);
                        Log.d(""START MR"", OutputFile);
                        try { mMediaRecorder.prepare(); } catch (Exception e) {}
                        mCameraSource.mediaRecorder = mMediaRecorder;
                        mMediaRecorder.start();
                    }
                }
            }
        }
        catch (SecurityException se)
        {
            Log.e(TAG, ""Do not have permission to start the camera"", se);
        }
        catch (IOException e)
        {
            Log.e(TAG, ""Could not start camera source."", e);
        }
    }
</code></pre>

<p>That DOES record things, while still handing each frame off to the Vision code. But, strangely, when I do that, the camera does not seem to call autofocus correctly, and the barcodes are not scanned -- since they are never really in focus, and therefore not recognized.</p>

<p>My next thought was to simply capture the frames as the barcode detector is handling the frames, and save them to the disk one by one (I can mux them together later.)</p>

<p>I did this in CameraSource.java.</p>

<p>This does not seem to be capturing all of the frames, even though I am writing them out in a separate AsyncTask running in the background, which I thought would get them eventually -- even if it took awhile to catch up. The saving was not optimized, but it looks as though it is dropping frames throughout, not just at the end. </p>

<p>To add this code, I tried putting it in the 
    private class FrameProcessingRunnable 
in the run() method. </p>

<p>Right after the FrameBuilder Code, I added this:</p>

<pre><code>            if (saveImagesIsEnabled)
            {
                if (data == null)
                {
                    Log.d(TAG, ""data == NULL"");
                }
                else
                {
                    SaveImageAsync saveImage = new SaveImageAsync(mCamera.getParameters().getPreviewSize() );
                    saveImage.execute(data.array());
                }
            }
</code></pre>

<p>Which calls this class:</p>

<pre><code>Camera.Size lastKnownPreviewSize = null;
public class SaveImageAsync extends AsyncTask&lt;byte[], Void, Void&gt;
{
    Camera.Size previewSize;

    public SaveImageAsync(Camera.Size _previewSize)
    {
        previewSize = _previewSize;
        lastKnownPreviewSize = _previewSize;
    }
    @Override
    protected Void doInBackground(byte[]... dataArray)
    {
        try
        {
            if (previewSize == null)
            {
                if (lastKnownPreviewSize != null)
                    previewSize = lastKnownPreviewSize;
                else
                    return null;
            }
            byte[] bitmapData = dataArray[0];
            if (bitmapData == null)
            {
                Log.d(""doInBackground"",""NULL: "");
                return null;
            }
                // where to put the output file (note: /sdcard requires WRITE_EXTERNAL_STORAGE permission)
                File storageDir = Environment.getExternalStorageDirectory();
                String imageFileName = baseFileName + ""_"" +  Long.toString(sequentialCount++) + "".jpg"";
                String filePath = storageDir + ""/"" + ""tmp"" + ""/"" + imageFileName;
                FileOutputStream out = null;
                YuvImage yuvimage = new YuvImage(bitmapData, ImageFormat.NV21, previewSize.width,
                        previewSize.height, null);
            try
            {
                out = new FileOutputStream(filePath);
                yuvimage.compressToJpeg(new Rect(0, 0, previewSize.width,
                        previewSize.height), 100, out);
            }
            catch (Exception e)
            {
                e.printStackTrace();
            }
            finally
            {
                try
                {
                    if (out != null)
                    {
                        out.close();
                    }
                }
                catch (IOException e)
                {
                    e.printStackTrace();
                }
            }
        }
        catch (Exception ex)
        {
            ex.printStackTrace();
            Log.d(""doInBackground"", ex.getMessage());
        }
        return null;
    }
}
</code></pre>

<p>I'm OK with the mediarecorder idea, or the brute force frame capture idea, but neither seem to be working correctly.</p>",,0,1,,2016-03-03 23:33:54.027 UTC,1,2016-03-03 23:33:54.027 UTC,2017-05-23 10:33:47.770 UTC,,-1,,628887,1,1,android|android-camera|mediarecorder|android-vision,470
How to trigger the same lambda function with multiple triggers?,53130628,How to trigger the same lambda function with multiple triggers?,"<p>The steps my app needs to take are:</p>

<ol>
<li>In the frontend, the user triggers a lambda function with API gateway which 
sends a file to s3. </li>
<li>When the file arrives in s3, trigger the same lambda function to apply video 
recognition and send jobId to SNS. </li>
<li>When SNS receives a message, trigger the same lambda function to get the 
label data and return the data back to the user with the API gateway</li>
</ol>

<p>All the steps work when I test them individually but, but I don't know how to make the code work together as described above. Should I create multiple lambda functions, use one lambda or try another option.</p>

<p>Note: the label data needs to return back to the user via API</p>

<p>Should be something like this:</p>

<pre><code>rekognition = boto3.client(""rekognition"")
sns = boto3.client(""sns"")

def lambda_handler(event, context):
    # should be triggered when s3 recives file after API call
    response = rekognition.start_label_detection(
        Video = {
            ""S3Object"": {
                ""Bucket"": BUCKET,
                ""Name"": KEY
            }
        },
        NotificationChannel = {
            ""SNSTopicArn"": SNS_TOPIC_ARN,
            ""RoleArn"": ROLE_ARN
        }
    )

    # should be triggerd when sns message has arrived
    if ""Records"" in event:
        message = event[""Records""][0][""Sns""][""Message""]
        #perform get lables here from jobId...



    # should return labels back to the user
    return {
        ""statusCode"": 200,
        ""body"": json.dumps(lables),
        ""headers"": {
        ""Access-Control-Allow-Origin"": ""*"",
        ""Content-Type"": ""application/json""

        }
    }
</code></pre>",53136325,2,1,,2018-11-03 10:55:38.640 UTC,,2018-11-03 23:13:32.757 UTC,,,,,5008598,1,0,python|amazon-web-services|amazon-s3|aws-lambda,948
How to extract the string that matches pattern/s,54615536,How to extract the string that matches pattern/s,"<p>I am performing OCR on license plates with the help of Google Cloud Vision.
The problem is it also recognizes unnecessary words in the license plate. </p>

<p>I tried to compile a pattern for the license plate and used search() on the string. However it returned None. Here's what I tried so far.</p>

<pre><code>pnum = 'REGISTERED AE 12435' #example of OCR result
plate1 = compile('^[A-Z]{2}[ ][0-9]{5}$')
result = plate1.search(pnum)
print(result)
</code></pre>",,0,6,,2019-02-10 10:37:50.960 UTC,,2019-03-03 19:42:48.063 UTC,,,,,10194940,1,0,python|regex|google-cloud-platform,25
new TextRecognizer.Builder(context).build() generates error,49982048,new TextRecognizer.Builder(context).build() generates error,"<p>I'm using <a href=""https://github.com/googlesamples/android-vision/tree/master/visionSamples/ocr-reader"" rel=""nofollow noreferrer"">google vision sample</a> to create an OCR app, but statement </p>

<pre><code>TextRecognizer textRecognizer = new TextRecognizer.Builder(context).build();
</code></pre>

<p>generates errors</p>

<blockquote>
<pre><code>&gt; E/native: jni_helper.cc:170 GetContents failed:
&gt; /data/user/0/com.google.android.gms/app_vision/ocr/data/models/rpn_lstm_engine_tfmini.bincfg
&gt; E/native: jni_helper.cc:170 GetContents failed:
&gt; /data/user/0/com.google.android.gms/app_vision/ocr/data/models/semanticlift_engine_0.2.bincfg
</code></pre>
</blockquote>

<p>Can't find any help on it anywhere.</p>",,0,0,,2018-04-23 13:13:25.143 UTC,,2018-04-23 13:13:25.143 UTC,,,,,1474829,1,2,google-vision,263
How to connect amazon video streaming kinesis with amazon rekognition?,56334124,How to connect amazon video streaming kinesis with amazon rekognition?,<p>I am streaming video the amazon kinesis from raspberry pi (This is done). Now i want to perform face detection/recognition on that video using amazon Rekognition. For that i need to connect aws kinesis with rekognition how to connect them? Thanks</p>,,0,0,,2019-05-28 01:52:54.360 UTC,,2019-05-28 01:52:54.360 UTC,,,,,11438404,1,0,amazon-kinesis|amazon-rekognition,10
How to run older version of Google Cloud Vision,55528723,How to run older version of Google Cloud Vision,<p>I am trying to run a code that uses previous version of the google cloud vision api. How do I install the earlier version? Working on Ubuntu and using Ruby</p>,,0,0,,2019-04-05 05:25:54.557 UTC,,2019-04-05 10:21:37.697 UTC,2019-04-05 10:21:37.697 UTC,,11030003,,11030003,1,0,ruby-on-rails|google-app-engine|machine-learning|nlp|google-vision,15
400 Bad Request C# Microsoft Face Api,42102280,400 Bad Request C# Microsoft Face Api,"<p>I am trying to get simple functionality from the Microsoft Face API, using this example provided (<a href=""https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236"" rel=""nofollow noreferrer"">link</a>):</p>

<pre><code>// Request headers
client.DefaultRequestHeaders.Add(""Ocp-Apim-Subscription-Key"", ""{subscription key}"");

// Request parameters
queryString[""returnFaceId""] = ""true"";
queryString[""returnFaceLandmarks""] = ""false"";
queryString[""returnFaceAttributes""] = ""{string}"";
var uri = ""https://westus.api.cognitive.microsoft.com/face/v1.0/detect?"" + queryString;

HttpResponseMessage response;

// Request body
byte[] byteData = Encoding.UTF8.GetBytes(""{body}"");

using (var content = new ByteArrayContent(byteData))
{
   content.Headers.ContentType = new MediaTypeHeaderValue(""&lt; your content type, i.e. application/json &gt;"");
   response = await client.PostAsync(uri, content);
}
</code></pre>

<p>Whenever I execute the code, I get a 400 bad request, of which I cannot how to view the specific cause. This is how mine looks:</p>

<pre><code>// Request headers
client.DefaultRequestHeaders.Add(""Ocp-Apim-Subscription-Key"", ""xxxxxxxxxxxxxxxxxxxxxxx"");

// Request parameters
queryString[""returnFaceId""] = ""true"";
queryString[""returnFaceLandmarks""] = ""false"";
queryString[""returnFaceAttributes""] = ""Age"";
var uri = ""https://westus.api.cognitive.microsoft.com/face/v1.0/detect?"" + queryString;

HttpResponseMessage response;

// Request body
byte[] byteData = Encoding.UTF8.GetBytes(""{ \""url\"":\""http://i0.kym-cdn.com/photos/images/newsfeed/000/272/907/dc1.jpg/ \""}"");

using (var content = new ByteArrayContent(byteData))
{
    content.Headers.ContentType = new MediaTypeHeaderValue(""application/json"");
    response = await client.PostAsync(uri, content);
    Console.Write(response.StatusCode);
}
</code></pre>",,1,2,,2017-02-07 23:59:40.040 UTC,,2017-02-08 01:32:34.277 UTC,2017-02-08 00:11:57.090 UTC,,1663001,,7531695,1,0,c#|httprequest,370
in Android google vision OCR how I can determine the accuracy of recognition,41104752,in Android google vision OCR how I can determine the accuracy of recognition,"<p>In google vision OCR, under Android, how I find and know what was the accuracy of a recognized text?</p>",,4,1,,2016-12-12 16:02:34.427 UTC,2,2018-03-23 05:29:51.567 UTC,2016-12-12 19:03:00.473 UTC,,1952295,,254195,1,4,android|android-vision|google-vision,1129
Xamarin google play service vision How do I access the underlying camera in a cameraSource so I can set the focus mode to macro?,50726224,Xamarin google play service vision How do I access the underlying camera in a cameraSource so I can set the focus mode to macro?,"<p>I'm using google vision to read QR tags.
everything has be going fine on the phone that I was developing for.
recently I was given an Galaxy Tab A as the target device.
I cannot get the Tab A to auto focus close enough to read the qr tags.</p>

<p>I noticed in the camera app that it has a Macro setting.  when I turn it on, it focuses up close and reads the tag just fine.</p>

<p>So...
in Xamarin how do I access the camera object's parameters when I am using the google vision cameraSource?</p>

<p>I've tried the examples I've found here, and I guess I'm missing something, cause I can't make them work.</p>

<p>Thanks for any help.</p>

<h1>update</h1>

<p>Here's the only way I have been able to get this java code to convert, and it doesn't work.  Obviously I'm doing something wrong...</p>

<pre><code>  private static bool cameraFocus(CameraSource cameraSource, String focusMode)
        {
            Java.Lang.Reflect.Field[] declaredFields =        cameraSource.Class.GetDeclaredFields();

            foreach (Java.Lang.Reflect.Field field in declaredFields) { 
                if (field.GetType() == typeof(Android.Hardware.Camera)) { 
                    field.Accessible = true; 
                    try {
                        Android.Hardware.Camera camera = (Android.Hardware.Camera)field.Get(cameraSource);
                        if (camera != null) {
                            Android.Hardware.Camera.Parameters parameters = camera.GetParameters();
                                    parameters.FocusMode = Android.Hardware.Camera.Parameters.FocusModeMacro;
                            camera.SetParameters(parameters); 
                            return true; 
                        } 

                        return false; 
                    } catch  {

                    } 

                    break; 
                } 
            } 

            return false; 
        }
</code></pre>",,1,1,,2018-06-06 17:27:46.640 UTC,,2018-06-08 01:29:40.640 UTC,2018-06-08 01:29:40.640 UTC,,8632294,,7436442,1,1,android|xamarin.android,204
Parse response from Google Cloud Vision API Python Client,45427109,Parse response from Google Cloud Vision API Python Client,"<p>I am using Python Client for Google Cloud Vision API, basically same code as in documentation <a href=""http://google-cloud-python.readthedocs.io/en/latest/vision/"" rel=""nofollow noreferrer"">http://google-cloud-python.readthedocs.io/en/latest/vision/</a></p>

<pre><code>&gt;&gt;&gt; from google.cloud import vision
&gt;&gt;&gt; client = vision.ImageAnnotatorClient()
&gt;&gt;&gt; response = client.annotate_image({
...   'image': {'source': {'image_uri': 'gs://my-test-bucket/image.jpg'}},
...   'features': [{'type': vision.enums.Feature.Type.FACE_DETECTOIN}],
... })
</code></pre>

<p>problem is that response doesn't have field ""annotations"" (as it is documentation) but based on documentation has field for each ""type"". so when I try to get response.face_annotations I get 
and basically I don't know how to extract result from Vision API from response (AnnotateImageResponse) to get something like json/dictionary like data.
version of google-cloud-vision is 0.25.1 and it was installed as full google-cloud library (pip install google-cloud).
I think today is not my day
I appreciate any clarification / help</p>",,2,0,,2017-08-01 00:14:47.020 UTC,1,2018-10-16 21:31:22.073 UTC,,,,,2521843,1,2,google-cloud-vision|google-cloud-python,851
can i access python script from ionic application,51187280,can i access python script from ionic application,"<p>I am building an <strong>offline</strong> ionic application and need to be integrate it with my python script contains a standalone ML model trained and exported from the microsoft custom vision which classifies the trees.it takes <strong>picture as an input</strong> and <strong>returns a string</strong>.</p>

<blockquote>
  <p>python server.py image.png</p>
</blockquote>

<p>this returns an output as</p>

<blockquote>
  <p>Mango tree </p>
</blockquote>

<p>How can we access the python script or how to make it as a service ?</p>",,1,0,,2018-07-05 09:02:40.827 UTC,,2018-07-05 09:14:13.610 UTC,2018-07-05 09:11:48.783 UTC,,9823478,,9823478,1,0,python|ionic-framework|machine-learning,609
i cannot find my project build.gradle file to add dependency,49296976,i cannot find my project build.gradle file to add dependency,"<p>I cannot find my build.gradle project file to add dependency only my android studio contains 1 module build.gradle but cannot find the other one to add dependency.</p>

<p><a href=""https://i.stack.imgur.com/Vhkcu.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Vhkcu.jpg"" alt=""enter image description here""></a>
i need to add the google vision dependency in my project but i am not getting where to write the dependency code
like</p>

<p><code>compile 'com.google.android.gms:play-services-vision:11.0.4'</code></p>

<p>can please anyone help me out regarding this problem</p>",,1,6,,2018-03-15 10:24:43.447 UTC,,2018-03-15 10:31:37.227 UTC,2018-03-15 10:25:51.903 UTC,,4168607,,8178577,1,-2,android,59
Match a given set of keywords from mysql database,46267416,Match a given set of keywords from mysql database,"<p>I have a column in my mysql database with set of keywords. (Specifically the lable data i'm getting from google vision api). Is there a easy way to match and return similar records when another set of lables given to the database.</p>

<p>In database:  ""Bike vehicle transport light floor""
What i'm giving as search parameters : ""light bike car green""</p>

<p>Approach i've taken currently: use the ""LIKE"" keyword with wildcard. Is there a better way to do this? 
Thanks</p>",46268774,2,3,,2017-09-17 18:20:37.013 UTC,,2017-09-17 21:02:57.140 UTC,,,,,6120928,1,1,mysql|search,42
(Duplicate zip entry [98.jar:com/google/api/client/json/JsonPolymorphicTypeMap$TypeDef.class])),51721510,(Duplicate zip entry [98.jar:com/google/api/client/json/JsonPolymorphicTypeMap$TypeDef.class])),"<p>I have added following dependency for google vision api.</p>

<p><strong>com.google.apis:google-api-services-vision:v1-rev357-1.22.0</strong></p>

<p>Now when I build the project, I am getting following error:</p>

<p><strong>(Duplicate zip entry [98.jar:com/google/api/client/json/JsonPolymorphicTypeMap$TypeDef.class]))</strong></p>

<p>How to resolve this error.</p>",,0,1,,2018-08-07 07:46:56.600 UTC,,2018-08-07 07:46:56.600 UTC,,,,,1550003,1,0,android|android-gradle|dependencies,16
How to Disable multiple barcode detection android Vision API,42112519,How to Disable multiple barcode detection android Vision API,"<p>I'm trying to disable multiple <code>Barcode</code> detection at time.
How to disable <code>MultiProcessor</code> using <code>Google Vision API</code>, I couldn't find any solution from official site <a href=""https://developers.google.com/vision/android/multi-tracker-tutorial"" rel=""noreferrer"">here</a></p>

<p>I have downloaded sample from <a href=""https://github.com/googlesamples/android-vision"" rel=""noreferrer"">here</a></p>

<p><strong>Code</strong>    </p>

<pre><code>BarcodeDetector barcodeDetector = new BarcodeDetector.Builder(context).build();
    BarcodeTrackerFactory barcodeFactory = new BarcodeTrackerFactory(mGraphicOverlay);
    barcodeDetector.setProcessor(
            new MultiProcessor.Builder&lt;&gt;(barcodeFactory).build());
</code></pre>

<p>Even if remove below line, I cannot detect at all.</p>

<pre><code> barcodeDetector.setProcessor(
        new MultiProcessor.Builder&lt;&gt;(barcodeFactory).build());
</code></pre>",42176567,2,2,,2017-02-08 11:58:57.853 UTC,2,2017-02-11 13:45:16.480 UTC,2017-02-08 12:33:16.730 UTC,,3251783,,3251783,1,9,android|barcode-scanner|android-vision,3415
Streaming webcam video to AWS Kinesis?,49575082,Streaming webcam video to AWS Kinesis?,"<p>I know this might be a relatively generic question, but I'm trying to see how I can get pointed in the right direction...</p>

<p>I'm trying to build a live face recognition app, using AWS Rekognition. I'm pretty comfortable with the API, and using static images uploaded to S3 to perform facial recognition. However, I'm trying to find out a way to stream live data into Rekognition. After reading the various articles and documentation that Amazon makes available, I found the process but can't seem to get over one hurdle.</p>

<p>According to the docs, I can use Kinesis to accomplish this. Seems pretty simple: create a Kinesis video stream, and process the stream through Rekognition. The producer produces the stream data into the Kinesis stream and I'm golden.</p>

<p>The problem I have is the producer. I found that AWS has a Java Producer library available (<a href=""https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/producer-sdk-javaapi.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/producer-sdk-javaapi.html</a>). Great... Seems simple enough, but now how do I use that producer to capture the stream from my webcam, and send off the bytes to Kinesis? The sample code that AWS provided actually uses static images from a directory, no code to get it integrated with an actual live source like a webcam.</p>

<p>Ideally, I can load my camera as an input source amd start streaming. But I can't seem to find any documentation on how to do this.</p>

<p>Any help, or direction would be greatly appreciated.</p>",50441929,2,0,,2018-03-30 13:08:52.213 UTC,,2018-06-04 02:17:08.007 UTC,,,,,6632117,1,2,java|aws-sdk|amazon-rekognition|facial-identification,2852
Programming Microsoft Emotion Api,54218010,Programming Microsoft Emotion Api,<p>I  want to use the Microsoft Emotion API but program it to detect an emotion it currently does not. Is it possible to do this? </p>,54226011,1,2,,2019-01-16 13:22:22.490 UTC,,2019-01-16 21:57:13.700 UTC,2019-01-16 13:28:02.543 UTC,,7123166,,10922603,1,-1,microsoft-cognitive|azure-cognitive-services,42
Install Google Cloud Vision on Ubuntu with Google Cloud Python SDK,51216224,Install Google Cloud Vision on Ubuntu with Google Cloud Python SDK,"<p>On Ubuntu 14.04, I've successfully installed the Python SDK following this: <a href=""https://cloud.google.com/sdk/docs/downloads-apt-get"" rel=""nofollow noreferrer"">https://cloud.google.com/sdk/docs/downloads-apt-get</a></p>

<p>Also, I am able to deploy the app to Google App Engine, using the <code>appcfg.py</code> script.</p>

<p>I do have a configuration file where I declare the <code>vendors</code> directory and all third party libraries are installed there.</p>

<p>Everything works, except when I try to import Google Cloud Vision:</p>

<blockquote>
  <p>File ""/home/vagrant/source/web/handlers/posts.py"", line 8, in
  
      from google.cloud import vision   File ""/usr/lib/google-cloud-sdk/platform/google_appengine/google/appengine/tools/devappserver2/python/runtime/sandbox.py"",
  line 1149, in load_module
      raise ImportError('No module named %s' % fullname) ImportError: No module named google.cloud.vision</p>
</blockquote>

<p>I have installed Google Cloud Vision both with:</p>

<pre><code>sudo pip install --upgrade google-cloud-vision
</code></pre>

<p>and </p>

<pre><code>pip install --upgrade -t lib google-cloud-vision 
</code></pre>

<p>None of them work. </p>

<p>How do I install locally? As a third party or globally? How will it work on Google when I deploy?</p>",,1,0,,2018-07-06 18:54:09.690 UTC,,2018-12-09 00:32:14.917 UTC,2018-12-09 00:32:14.917 UTC,,9146820,,908773,1,2,python|google-app-engine|google-cloud-platform,211
Google Cloud Vision Raw JSON Response,49615202,Google Cloud Vision Raw JSON Response,"<p>When trying out google cloud vision with the drag and drop <a href=""https://cloud.google.com/vision/docs/drag-and-drop"" rel=""nofollow noreferrer"">Try Drag and Drop</a>, the last tab has raw JSON.  What parameter do we need to pass to get that data?  </p>

<p>I'm currently doing DOCUMENT_TEXT_DETECTION but it only gives data at the level of words and not of individual characters.</p>

<p>Edit: I modified this code <a href=""https://gist.github.com/atotto/4fe3ec32d30b82d46206c459c1fd0aa8"" rel=""nofollow noreferrer"">vision test</a> and changed the feature ... </p>

<pre><code>feature := &amp;vision.Feature{
    Type: ""DOCUMENT_TEXT_DETECTION"",
}
</code></pre>

<p>and the printing to ... </p>

<pre><code>body, err := json.Marshal(res)
fmt.Println(string(body))
</code></pre>

<p>I'm only seeing textAnnotations in the output.</p>",,3,1,,2018-04-02 16:45:04.540 UTC,,2018-04-06 07:07:48.873 UTC,2018-04-03 07:13:13.943 UTC,,791842,,791842,1,0,google-cloud-vision,1366
Docker - Problem with java netty_tcnative,55708104,Docker - Problem with java netty_tcnative,"<p>I am trying to dockerize 4 services and I have a problem with one of the services. Particularly, this service is implemented is spring boot service and uses google vision API. When building the images and starting the containers everything works fine, until it gets to the part where the google vision API code is used. I then have the following runtime errors when running the containers:</p>

<pre><code>netty-tcnative unavailable (this may be normal)
java.lang.IllegalArgumentException: Failed to load any of the given libraries: [netty_tcnative_linux_x86_64, netty_tcnative_linux_x86_64_fedora, netty_tcnative_x86_64, netty_tcnative]
at io.grpc.netty.shaded.io.netty.util.internal.NativeLibraryLoader.loadFirstAvailable(NativeLibraryLoader.java:104) ~[grpc-netty-shaded-1.18.0.jar!/:1.18.0]
at io.grpc.netty.shaded.io.netty.handler.ssl.OpenSsl.loadTcNative(OpenSsl.java:526) ~[grpc-netty-shaded-1.18.0.jar!/:1.18.0]
at io.grpc.netty.shaded.io.netty.handler.ssl.OpenSsl.&lt;clinit&gt;(OpenSsl.java:93) ~[grpc-netty-shaded-1.18.0.jar!/:1.18.0]
at io.grpc.netty.shaded.io.grpc.netty.GrpcSslContexts.defaultSslProvider(GrpcSslContexts.java:244) [grpc-netty-shaded-1.18.0.jar!/:1.18.0]
at io.grpc.netty.shaded.io.grpc.netty.GrpcSslContexts.configure(GrpcSslContexts.java:171) [grpc-netty-shaded-1.18.0.jar!/:1.18.0]
at io.grpc.netty.shaded.io.grpc.netty.GrpcSslContexts.forClient(GrpcSslContexts.java:120) [grpc-netty-shaded-1.18.0.jar!/:1.18.0]
at io.grpc.netty.shaded.io.grpc.netty.NettyChannelBuilder.buildTransportFactory(NettyChannelBuilder.java:385) [grpc-netty-shaded-1.18.0.jar!/:1.18.0]
at io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:435) [grpc-core-1.18.0.jar!/:1.18.0]
at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:223) [gax-grpc-1.42.0.jar!/:1.42.0]
at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:164) [gax-grpc-1.42.0.jar!/:1.42.0]
at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:156) [gax-grpc-1.42.0.jar!/:1.42.0]
at com.google.api.gax.rpc.ClientContext.create(ClientContext.java:157) [gax-1.42.0.jar!/:1.42.0]
at com.google.cloud.vision.v1.stub.GrpcImageAnnotatorStub.create(GrpcImageAnnotatorStub.java:84) [google-cloud-vision-1.66.0.jar!/:1.66.0]
at com.google.cloud.vision.v1.stub.ImageAnnotatorStubSettings.createStub(ImageAnnotatorStubSettings.java:120) [google-cloud-vision-1.66.0.jar!/:1.66.0]
at com.google.cloud.vision.v1.ImageAnnotatorClient.&lt;init&gt;(ImageAnnotatorClient.java:136) [google-cloud-vision-1.66.0.jar!/:na]
at com.google.cloud.vision.v1.ImageAnnotatorClient.create(ImageAnnotatorClient.java:117) [google-cloud-vision-1.66.0.jar!/:na]
at com.google.cloud.vision.v1.ImageAnnotatorClient.create(ImageAnnotatorClient.java:108) [google-cloud-vision-1.66.0.jar!/:na]
</code></pre>

<p>Complete log file of the error can be found in this link:
<a href=""https://drive.google.com/file/d/1K2FYUGvZ2II4jbjUzcg3uhL6TRD626Qw/view?usp=sharing"" rel=""nofollow noreferrer"">Complete Log File</a>.</p>

<p>Here are my <strong>docker-compose.yml</strong> file and the <strong>Dockerfile</strong> of the service causing problem:</p>

<p><strong>DockerFile</strong></p>

<pre><code>FROM maven:3.6.0-jdk-8-alpine
WORKDIR /app/back
COPY src src
COPY pom.xml .
RUN mvn clean package

FROM openjdk:8-jdk-alpine
RUN apk add --no-cache curl
WORKDIR /app/back

COPY --from=0 /app/back/target/imagescanner*.jar ./imagescanner.jar
COPY --from=0 /app/back/target/classes/API-Key.json .
ENV GOOGLE_APPLICATION_CREDENTIALS ./API-Key.json
EXPOSE 8088

ENTRYPOINT [""java"", ""-jar"", ""./imagescanner.jar""]
</code></pre>

<p><strong>docker-compose.yml</strong></p>

<pre><code>version: '3'

services:
   front:
     container_name: demoLab_front
     build: ./front
     image: demolab/front:latest
     expose:
       - ""3000""
     ports:
       - ""8087:3000""
     restart: always
   back:
     container_name: demoLab_backGCV
     build: ./backGCV
     image: demolab/backgcv:latest
     depends_on:
       - lab
     ports:
       - ""8088:8088""
     restart: always
   lab:
     container_name: demoLab_labGCV
     build: ./lab
     image: demolab/labgcv:latest
     expose:
       - ""8089""
     ports:
       - ""8089:8089""
     restart: always
   sift:
     container_name: demoLab_labSIFT
     build: ./detect-label-service
     image: demolab/labsift:latest
     expose:
       - ""5000""
     ports:
       - ""5000:5000""
     restart: always
</code></pre>

<p><strong>EDIT</strong></p>

<p>After some googling I found out that: GRPC Java examples are not working on Alpine Linux since required libnetty-tcnative-boringssl-static depends on glibc. Alpine is using musl libc and application startup will fail with message similar to mine.
I found <a href=""https://github.com/netty/netty-tcnative/issues/111"" rel=""nofollow noreferrer"">this project</a> that try to build the right images but it seems broken for a lot of pepole (the build didn't work for my case)</p>",,1,2,,2019-04-16 12:24:34.797 UTC,,2019-04-16 13:12:24.680 UTC,2019-04-16 13:02:08.567 UTC,,3081406,,3081406,1,2,spring-boot|docker|docker-compose|netty|grpc-java,89
Can not import google.cloud module in python script,55268232,Can not import google.cloud module in python script,"<p>I am following <a href=""https://www.smashingmagazine.com/2019/01/powerful-image-analysis-google-cloud-vision-python/"" rel=""nofollow noreferrer"">this</a> article trying to make my Python script read labels related to an image using <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">Google Cloud Vision API</a>. The problem is that I am getting this error when trying to include a reference to <strong>vision</strong> from google.cloud module.</p>

<pre><code>import io
from google.cloud import vision
from google.cloud.vision import types
</code></pre>

<p>The error that I am getting says:</p>

<pre><code>ImportError: No module named 'google.cloud'
</code></pre>

<p>This is weird because when I do:</p>

<pre><code>$ pip show google-cloud
</code></pre>

<p>I can see it is there and its files are located at:</p>

<pre><code>/usr/local/lib/python2.7/dist-packages/google_cloud-0.34.0.dist-info/*
</code></pre>

<p>Except for that, when I do <strong>pip freeze</strong> in my working folder I can see them both that I need available:</p>

<pre><code>google-cloud==0.34.0
google-cloud-vision==0.33.0
</code></pre>

<p>I am now wondering what could be the reason for not being able to see include this module in my Python script.</p>",,2,0,,2019-03-20 18:55:09.003 UTC,,2019-03-20 19:55:04.400 UTC,2019-03-20 19:14:01.650 UTC,,2128702,,2128702,1,0,python|google-cloud-platform|pip,65
Google Cloud vision couldn't detect one character,51605428,Google Cloud vision couldn't detect one character,"<p>I have used Google Cloud Vision API for my small project to detect text from an image. The API works very well almost text in the image be detected by the API but I found when the image has only one character in a line, the API will skip it.</p>

<p>Do you have any solution for this problem? I try to change color and resize the image but it still not work.</p>

<p>for example please look : [The API can detect only 'AMATA' but not 'S']</p>

<p><a href=""https://i.stack.imgur.com/2KjE5.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2KjE5.jpg"" alt=""enter image description here""></a></p>",,1,2,,2018-07-31 04:23:43.850 UTC,,2018-08-07 15:52:32.183 UTC,2018-07-31 07:01:54.557 UTC,,305043,,10158420,1,0,google-api|google-cloud-platform|google-cloud-vision,190
How to turn numpy array image to bytes?,50630045,How to turn numpy array image to bytes?,"<p>I need to recognize image with Google Vision API. Among the examples, they use following construction:</p>

<pre><code>with io.open('test.png', 'rb') as image_file:
    content = image_file.read()
image = vision.types.Image(content=content)
</code></pre>

<p>I need to do similar, but my image comes from:</p>

<pre><code>content = cv2.imread()
</code></pre>

<p>Which returns numpy array, not bytes. I tried:</p>

<pre><code>content = content.tobytes()
</code></pre>

<p>Which converts array to bytes, but returns different bytes apparently, since it gives different result.<br>
So how to make my image array similar to one which I get by <code>open()</code> function</p>",50630390,1,16,,2018-05-31 17:49:11.927 UTC,2,2018-05-31 18:31:11.030 UTC,2018-05-31 17:53:53.643 UTC,,3204551,,4363045,1,7,python|python-3.x|numpy|opencv|google-cloud-platform,4070
How to Implement Amazon Kinesis PutMedia Method using PYTHON,51991401,How to Implement Amazon Kinesis PutMedia Method using PYTHON,"<p>I'm trying to recognize faces in a video stream using these <a href=""https://docs.aws.amazon.com/rekognition/latest/dg/recognize-faces-in-a-video-stream.html"" rel=""nofollow noreferrer"">instructions</a> but I couldn't find any help to implement  PutMedia operation using Python. I'm  using Ubuntu 16.04 and Python 3.6. Any hint please so I can implement it using Python.</p>",,1,0,,2018-08-23 17:36:41.620 UTC,,2018-12-25 09:33:46.383 UTC,,,,,10264905,1,0,python|amazon-web-services,34
InvalidSignatureException - Amazon Rekognition with PHP Client,55920132,InvalidSignatureException - Amazon Rekognition with PHP Client,"<p>I am getting the following error whenever I connect to Amazon Rekognition Service through a PHP script deployed on a hosted server (hostgator)</p>

<p>AWS HTTP error: Client error: POST <a href=""https://rekognition.us-east-2.amazonaws.com"" rel=""nofollow noreferrer"">https://rekognition.us-east-2.amazonaws.com</a> resulted in a 400 Bad Request response:
{""__type"":""InvalidSignatureException"",""message"":""Signature expired: 20190430T075732Z is now earlier than 20190430T105409 (truncated...)
InvalidSignatureException (client): Signature expired: 20190430T075732Z is now earlier than 20190430T105409Z (20190430T105909Z - 5 min.) - {""__type"":""InvalidSignatureException"",""message"":""Signature expired: 20190430T075732Z is now earlier than 20190430T105409Z (20190430T105909Z - 5 min.)""}'</p>

<p>I am using the following PHP script.</p>

<pre><code>require 'aws/aws-autoloader.php';
use Aws\Rekognition\RekognitionClient;
//Credentials for access AWS Service code parameter
$credentials = new Aws\Credentials\Credentials('xxxx', 'yyyyy');
//Get Rekognition Access
$rekognitionClient = RekognitionClient::factory(array(
'region' =&gt; ""us-east-2"",
'version' =&gt; 'latest',
'correctClockSkew' =&gt; true,
'credentials' =&gt; $credentials
));
</code></pre>

<p>I tried adding ""'correctClockSkew' => true,"" as suggested by other solutions but doesnt seem to work for me.</p>

<p>I am not sure whether it is the right way of solving this problem.</p>

<p>Please Help</p>",,0,0,,2019-04-30 11:39:51.213 UTC,,2019-04-30 11:39:51.213 UTC,,,,,6091747,1,0,php|amazon|amazon-rekognition,16
Authentication with service account. PDF/TIFF Document Text Detection service from Google Vision,54315250,Authentication with service account. PDF/TIFF Document Text Detection service from Google Vision,"<p>I'm trying to use Google Vision OCR capabilities in order to extract text from some scanned old relatories in PDF. The API is added to my project, the account service is created and has granted access to the relatories storage interval. When I try run the command to execute the operation, I receive this error message:</p>

<pre><code>curl: (3) [globbing] bad range specification in column 11
curl: (3) [globbing] unmatched brace in column 1
curl: (3) [globbing] unmatched brace in column 1
curl: (6) Could not resolve host: gcsSource
curl: (3) [globbing] unmatched brace in column 1
curl: (3) [globbing] unmatched close brace/bracket in column 47
curl: (3) [globbing] bad range specification in column 2
curl: (3) [globbing] nested brace in column 17
curl: (3) [globbing] unmatched close brace/bracket in column 28
curl: (3) [globbing] unmatched close brace/bracket in column 2

{
  ""error"": {
    ""code"": 401,
    ""message"": ""Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential. See https://developers.google.com/identity/sign-in/web/devconsole-project."",
    ""status"": ""UNAUTHENTICATED""
  }
}
</code></pre>

<p>What's the problem with the authentication? I've already tried use the default-application print-access-token instead of the service account generated print-access-token</p>

<p>I already read this:
- <a href=""https://cloud.google.com/vision/docs/pdf#vision-web-detection-gcs-protocol"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/pdf#vision-web-detection-gcs-protocol</a>
- <a href=""https://stackoverflow.com/questions/52614091/i-want-to-use-pdf-tiff-document-text-detection-service-from-google-cloud"">I want to use PDF/TIFF Document Text Detection service from google cloud</a></p>

<pre><code>curl -H ""Authorization: Bearer ""$(MY-PRINT-ACCESS-TOKEN) --header ""Content-Type: application/json""   --request POST   --data '{ ""requests"":[  {""inputConfig"": { ""gcsSource"": {  ""uri"": ""gs://sentinelasdafronteira/relatorios/1830.pdf""},""mimeType"":""application/pdf""},""features"": [{""type"":""DOCUMENT_TEXT_DETECTION""}],""outputConfig"": {""gcsDestination"":{""uri"": ""gs://sentinelasdafronteira/""},""batchSize"": 2}}]}' https://vision.googleapis.com/v1/files:asyncBatchAnnotate
</code></pre>",,0,0,,2019-01-22 19:34:30.033 UTC,,2019-01-22 19:34:30.033 UTC,,,,,10952005,1,0,google-cloud-platform|google-vision,29
How can I send a file to Google Vision that I just saved in a folder?,54640539,How can I send a file to Google Vision that I just saved in a folder?,"<p>I am trying to send an image file to Google Vision just after I saved the file using a promisified <code>fs.writeFile</code> but I am getting an error saying that the file or directory is not found.</p>

<p>I have added a function to get the file size to confim that the file exists before I send the request to Vision and the file is already created. </p>

<p>I've also called a pathname to an existing file and Google Vision just worked fine. </p>

<p>So I guess that it's just a matter of setting a time between saving the file and sending request to Google Vision.</p>

<p>Here's my code:</p>

<pre><code>let generateFileName = (suffix, ext) =&gt; {
let filename;
let date = new Date();
return filename = `${suffix}${date.getDate()}${date.getMonth()}${date.getFullYear()}${date.getTime()}${ext}`;
 }

let saveToFile = (base64Image, pathName) =&gt; new Promise((resolve, reject) =&gt; {
const writeFile = util.promisify(fs.writeFile);
writeFile(pathName, base64Image, { encoding: 'base64' })
    .then(() =&gt; getSize(pathName))
    .then((size) =&gt; size &gt; 0 ? console.log('size is good') : console.log('size is bad'))
resolve(pathName)
});

let getSize = (filename) =&gt; {
    const stats = fs.statSync(filename);
    const fileSizeInBytes = stats.size;
    console.log('Image size in bytes:' + fileSizeInBytes);
    return fileSizeInBytes;
}
app.post('/', (req, res) =&gt; {

if (req.body.image !== null) {
    let pathname = `./images/${generateFileName(""photo"", "".jpg"")}`;
    saveToFile(req.body.image, pathname)
        .catch(err =&gt; console.log(err))
        .then(pathname =&gt; visionClient.webDetection(pathname)) //
        .then(labels =&gt; sendLabel(labels))
        .then(res.send(""File saved""))
        .catch(err =&gt; {
            console.error('ERROR:', err);
        })

}
//...
}
</code></pre>

<p>Here's what I get in console:</p>

<pre><code>ERROR: { Error: ENOENT: no such file or directory, open 
'./images/photo11120191549923812361.jpg'
errno: -2,
code: 'ENOENT',
syscall: 'open',
path: './images/photo11120191549923812361.jpg' }
Image size in bytes:61282
size is good
</code></pre>

<p>How to fix this? Should I just add a setTimeout? or is there any other better solution to overcome this error?</p>",,0,1,,2019-02-11 23:13:51.770 UTC,,2019-02-11 23:13:51.770 UTC,,,,,5768431,1,0,node.js|promise|google-vision|writefile,10
Google Cloud Vision API on PHP AppEngine,36944481,Google Cloud Vision API on PHP AppEngine,"<p>I have been using the Google Cloud Vision api with a php app hosted on a private VPS for a while without issue.  I'm migrating the app to Google AppEngine and am now running into issues.</p>

<p>I'm using a CURL post to the API, but it's failing on AppEngine.  I have billing enabled and other curl requests work without issue.  Someone mentioned that calls to googleapis.com won't work on AppEngine, that I need to access the API differently.  I'm not able to find any resources online to confirm that.</p>

<p>Below is my code, CURL error #7 is returned, failed to connect to host.</p>

<pre><code>$request_json = '{
            ""requests"": [
                {
                  ""image"": {
                    ""source"": {
                        ""gcsImageUri"":""gs://bucketname/image.jpg""
                    }
                  },
                  ""features"": [
                      {
                        ""type"": ""LABEL_DETECTION"",
                        ""maxResults"": 200
                      }
                  ]
                }
            ]
        }';
$curl = curl_init();
curl_setopt($curl, CURLOPT_URL, 'https://vision.googleapis.com/v1/images:annotate?key='.GOOGLE_CLOUD_VISION_KEY);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);
curl_setopt($curl, CURLOPT_HTTPHEADER, array('Content-type: application/json'));
curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, $request_json);
$json_response = curl_exec($curl);
$status = curl_getinfo($curl, CURLINFO_HTTP_CODE);
if ($status != 200) {
    die(""Error: $status, response $json_response, curl_error "" . curl_error($curl) . ', curl_errno ' . curl_errno($curl));
}
curl_close($curl);
echo '&lt;pre&gt;';
echo $json_response;
echo '&lt;/pre&gt;';
</code></pre>",36944708,2,0,,2016-04-29 17:42:47.730 UTC,,2017-02-16 21:05:49.050 UTC,,,,,1099019,1,2,php|google-app-engine|google-cloud-vision,1060
Gradle sync fails: com.google.android.gms:play-services-basement and com.google.firebase:firebase-common,53244160,Gradle sync fails: com.google.android.gms:play-services-basement and com.google.firebase:firebase-common,"<p>I am trying to include firebase to my android project as described in the official firebase documenation. The app is very basic and uses Google's vision APIs.</p>

<p>Error that I'm getting:</p>

<blockquote>
  <p>Gradle sync failed: Failed to notify dependency resolution listener.
                      The library com.google.android.gms:play-services-basement is being requested by various other libraries at [[11.0.4,11.0.4], [15.0.1,15.0.1]], but resolves to 15.0.1. Disable the plugin and check your dependencies tree using ./gradlew :app:dependencies.</p>
</blockquote>

<p>I've already tried a few solutions from similar questions posted on the site which didn't work for me. More importantly, I'd like to understand the root cause, rather than blindly tinkering with versions.</p>

<p>Below are my gradle files:</p>

<p><strong>Module level</strong></p>

<pre><code>apply plugin: 'com.android.application'
/* ... */    
dependencies {
    implementation fileTree(include: ['*.jar'], dir: 'libs')
    testImplementation 'junit:junit:4.12'
    androidTestImplementation 'com.android.support.test:runner:2.0.2'
    androidTestImplementation 'com.android.support.test.espresso:espresso-core:3.0.2'
    implementation 'com.google.android.gms:play-services:11.0.4'
    implementation 'com.google.firebase:firebase-core:16.0.1'
}

apply plugin: 'com.google.gms.google-services'
</code></pre>

<p><strong>Project level</strong></p>

<pre><code>// Top-level build file where you can add configuration options common to all sub-projects/modules.

buildscript {

    repositories {
        google()
        jcenter()
    }
    dependencies {
        classpath 'com.android.tools.build:gradle:3.2.1'
        classpath 'com.google.gms:google-services:4.0.1'

        // NOTE: Do not place your application dependencies here; they belong
        // in the individual module build.gradle files
    }
}

allprojects {
    repositories {
        google()
        jcenter()
    }
}

task clean(type: Delete) {
    delete rootProject.buildDir
}
</code></pre>

<p>Running <code>./gradlew app:dependencies</code> gives:</p>

<blockquote>
  <p>Failed to notify dependency resolution listener.</p>
  
  <blockquote>
    <p>The library com.google.android.gms:play-services-basement is being requested by various other libraries at [[11.0.4,11.0.4], [15.0.1,15.0.1]], but resolves to 15.0.1. Disable the plugin and check your dependencies tree using ./gradlew :app:dependencies.</p>
    
    <p>The library com.google.firebase:firebase-common is being requested by various other libraries at [[11.0.4,11.0.4]], but resolves to 16.0.0. Disable the plugin and check your dependencies tree using ./gradlew :app:dependencies.</p>
  </blockquote>
</blockquote>",53244221,1,0,,2018-11-10 22:44:55.403 UTC,,2018-11-11 11:37:38.297 UTC,,,,,1112010,1,2,android|firebase|firebase-realtime-database|android-gradle,2015
"Facing Issue with google vision, Java 8 on linux",52928909,"Facing Issue with google vision, Java 8 on linux","<p>I am using google vision api for ocr with Java 8. It works well on mac os however it does not work on Linux os.</p>

<p>dependency used-</p>

<pre><code>&lt;dependency&gt;
&lt;groupId&gt;com.google.cloud&lt;/groupId&gt;
   &lt;artifactId&gt;google-cloud-vision&lt;/artifactId&gt;
   &lt;version&gt;1.32.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>Exception i am getting -</p>

<pre><code>java.lang.IllegalArgumentException: ALPN is not configured properly. See https://github.com/grpc/grpc-java/blob/master/SECURITY.md#troubleshooting for more information.
    at io.grpc.netty.shaded.io.grpc.netty.GrpcSslContexts.selectApplicationProtocolConfig(GrpcSslContexts.java:166)
    at io.grpc.netty.shaded.io.grpc.netty.GrpcSslContexts.configure(GrpcSslContexts.java:136)
    at io.grpc.netty.shaded.io.grpc.netty.GrpcSslContexts.configure(GrpcSslContexts.java:124)
    at io.grpc.netty.shaded.io.grpc.netty.GrpcSslContexts.forClient(GrpcSslContexts.java:94)
    at io.grpc.netty.shaded.io.grpc.netty.NettyChannelBuilder$NettyTransportFactory$DefaultNettyTransportCreationParamsFilterFactory.&lt;init&gt;(NettyChannelBuilder.java:546)
    at io.grpc.netty.shaded.io.grpc.netty.NettyChannelBuilder$NettyTransportFactory$DefaultNettyTransportCreationParamsFilterFactory.&lt;init&gt;(NettyChannelBuilder.java:539)
    at io.grpc.netty.shaded.io.grpc.netty.NettyChannelBuilder$NettyTransportFactory.&lt;init&gt;(NettyChannelBuilder.java:477)
    at io.grpc.netty.shaded.io.grpc.netty.NettyChannelBuilder.buildTransportFactory(NettyChannelBuilder.java:325)
    at io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:362)
    at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:206)
    at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:157)
    at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:149)
    at com.google.api.gax.rpc.ClientContext.create(ClientContext.java:151)
    at com.google.cloud.vision.v1.stub.GrpcImageAnnotatorStub.create(GrpcImageAnnotatorStub.java:84)
    at com.google.cloud.vision.v1.stub.ImageAnnotatorStubSettings.createStub(ImageAnnotatorStubSettings.java:120)
    at com.google.cloud.vision.v1.ImageAnnotatorClient.&lt;init&gt;(ImageAnnotatorClient.java:136)
    at com.google.cloud.vision.v1.ImageAnnotatorClient.create(ImageAnnotatorClient.java:117)
    at com.hp.wex.utilities.FileDirStringOp.getOcrText(FileDirStringOp.java:250)
    at com.hp.wex.MainApp.main(MainApp.java:203)
Caused by: java.lang.ClassNotFoundException: org/eclipse/jetty/alpn/ALPN
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:348)
    at io.grpc.netty.shaded.io.grpc.netty.JettyTlsUtil.isJettyAlpnConfigured(JettyTlsUtil.java:64)
    at io.grpc.netty.shaded.io.grpc.netty.GrpcSslContexts.selectApplicationProtocolConfig(GrpcSslContexts.java:153)
    ... 18 more
</code></pre>

<p>Can anyone help me with this??
Thanks in advance</p>",,0,0,,2018-10-22 12:04:24.110 UTC,,2018-10-23 10:09:08.373 UTC,2018-10-23 10:06:05.207 UTC,,8521696,,8521696,1,0,java|linux|grpc|google-vision|grpc-java,57
Watson Visual Recognition - Unauthorized,51746269,Watson Visual Recognition - Unauthorized,"<p>I am trying to use the Watson Visual Recognition service with the watson-developer-cloud NPM module. But I always get the following error. What am I doing wrong?</p>

<pre><code> Unauthorized: Access denied due to invalid credentials.
</code></pre>

<p>I already searched for hours and found many people with the same problem, but none of the answers resolved the issue.</p>

<p>My service authentication informations (just test data):</p>

<pre><code> {
   ""apikey"": ""API_KEY"",
   ""iam_apikey_description"": ""..."",
   ""iam_apikey_name"": ""..."",
   ""iam_role_crn"": ""..."",
   ""iam_serviceid_crn"": ""..."",
   ""url"": ""https://gateway.watsonplatform.net/visual-recognition/api""
 }
</code></pre>

<p>My Node.js code to create the VisualRecognizionV3 object:</p>

<pre><code> let visualRecognition = new VisualRecognizionV3({
     api_key: ""API_KEY"",
     version: ""2018-03-19""
 });
</code></pre>

<p>I will appreciate your help!</p>",51758093,1,0,,2018-08-08 12:03:58.040 UTC,,2018-08-09 02:44:08.307 UTC,2018-08-08 13:14:41.810 UTC,,1167890,,10197495,1,0,node.js|ibm-watson|visual-recognition,143
Cloud API Vision Results not appearing,54118524,Cloud API Vision Results not appearing,"<p>I'm making a request with the google vision api that appears to have worked, I get an operation number back. The problem I am having is the I am not sure how to interpret the results and nothing appeared in the output folder after running the script.</p>

<p>This is the script I ran</p>

<p><a href=""https://vision.googleapis.com/v1/files:asyncBatchAnnotate"" rel=""nofollow noreferrer"">https://vision.googleapis.com/v1/files:asyncBatchAnnotate</a></p>

<pre><code>{
      ""requests"":[
        {
          ""inputConfig"": {
            ""gcsSource"": {
              ""uri"": ""gs://somebucket/1.pdf""
            },
            ""mimeType"": ""application/pdf""
          },
          ""features"": [
            {
              ""type"": ""DOCUMENT_TEXT_DETECTION""
            }
          ],
          ""outputConfig"": {
            ""gcsDestination"": {
              ""uri"": ""gs://somebucket/output/""
            },
            ""batchSize"": 1
          }
        }
      ]
    }
</code></pre>

<p>This returns back</p>

<pre><code>{
    ""name"": ""operations/8b7534d4b21b825e""
}
</code></pre>

<p>and when I do a lookup on the operation I get this</p>

<p><a href=""https://vision.googleapis.com/v1/operations/8b7534d4b21b825e"" rel=""nofollow noreferrer"">https://vision.googleapis.com/v1/operations/8b7534d4b21b825e</a></p>

<pre><code>{
    ""name"": ""operations/8b7534d4b21b825e"",
    ""metadata"": {
        ""@type"": ""type.googleapis.com/google.cloud.vision.v1.OperationMetadata"",
        ""state"": ""CREATED"",
        ""createTime"": ""2019-01-09T21:08:57.339363096Z"",
        ""updateTime"": ""2019-01-09T21:08:57.339363096Z""
    }
}
</code></pre>

<p>However the output folder is completely empty and I am not sure what to make of the state created. </p>",54119911,1,0,,2019-01-09 21:19:17.130 UTC,,2019-01-09 23:24:50.463 UTC,,,,,61962,1,0,google-cloud-platform|google-vision,48
Android - google cloud vision with OCR: Display of detected text,49207943,Android - google cloud vision with OCR: Display of detected text,"<p>I am using Google cloud vision OCR to detect text. The displayed text is always 1. detected text, 2. each of the detected words. I only want to display the detected text.</p>

<p>I am using the code from <a href=""https://github.com/GoogleCloudPlatform/cloud-vision/tree/master/android/CloudVision"" rel=""nofollow noreferrer"">Google Cloud Platform Github</a> where I set the type to Text Detection <code>labelDetection.setType(""TEXT_DETECTION"");</code> in <code>callCloudVision</code> method.</p>

<p>I also modified the <code>convertResponseToString</code> method to:</p>

<pre><code>private String convertResponseToString(BatchAnnotateImagesResponse response) {
        String message = """";

        List&lt;EntityAnnotation&gt; labels = response.getResponses().get(0).getTextAnnotations();
        for (EntityAnnotation label : labels) {
            if (labels != null) {
                System.out.println(label.getDescription());
                message += String.format(Locale.US, ""%s"", label.getDescription()) + ""\n"";
            }
            else
            {
                message += ""nothing"";
            }
        }
        return message;
    }
</code></pre>

<p>This is my gradle:</p>

<pre><code>apply plugin: 'com.android.application'

android {
    compileSdkVersion 25
    defaultConfig {
        applicationId ""com.example.mhci""
        minSdkVersion 24
        targetSdkVersion 25
        versionCode 1
        versionName ""1.0""
        testInstrumentationRunner ""android.support.test.runner.AndroidJUnitRunner""
        multiDexEnabled true
        javaCompileOptions {
            annotationProcessorOptions {
                includeCompileClasspath false
            }
        }
    }
    buildTypes {
        release {
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
        }
    }
    packagingOptions {
        exclude 'META-INF/LICENSE'
        exclude 'META-INF/io.netty.versions.properties'
        exclude 'META-INF/INDEX.LIST'
        exclude 'META-INF/DEPENDENCIES'
    }
}

dependencies {
    compile fileTree(dir: 'libs', include: ['*.jar'])
    implementation 'com.android.support.constraint:constraint-layout:1.0.2'
    testCompile 'junit:junit:4.12'

    androidTestCompile('com.android.support.test.espresso:espresso-core:3.0.1', {
        exclude group: 'com.android.support', module: 'support-annotations'
    })

    compile 'com.android.support:appcompat-v7:25.1.1'
    compile 'com.android.support:design:25.4.0'

    compile 'com.google.api-client:google-api-client-android:1.20.0' exclude module: 'httpclient'
    compile 'com.google.http-client:google-http-client-gson:1.20.0' exclude module: 'httpclient'

    compile 'com.google.apis:google-api-services-vision:v1-rev2-1.21.0'

    compile 'com.android.support:design:25.4.0'

    compile ('com.google.apis:google-api-services-translate:v2-rev47-1.22.0') {
        exclude group: 'com.google.guava'
    }

    compile ('com.google.cloud:google-cloud-translate:0.5.0') {
        exclude group: 'io.grpc', module: 'grpc-all'
        exclude group: 'com.google.protobuf', module: 'protobuf-java'
        exclude group: 'com.google.api-client', module: 'google-api-client-appengine'
    }
}
</code></pre>

<p>The detected text of <a href=""https://i.stack.imgur.com/EWvpL.png"" rel=""nofollow noreferrer"">this image</a> that was displayed is:</p>

<pre><code>Hello world

Hello
world
</code></pre>

<p>But I want it to only display <code>Hello world</code></p>

<p>How can I do it?</p>",,1,10,,2018-03-10 10:23:13.180 UTC,,2018-03-22 18:20:46.093 UTC,2018-03-10 12:50:23.333 UTC,,8771588,,8771588,1,0,java|android|ocr|google-cloud-vision,325
Can you upload an image from a mobile phone html page on Node-red?,46152610,Can you upload an image from a mobile phone html page on Node-red?,"<p>I'm trying to take an image from a phone and then put it through Watson Visual Recognition on Node-Red.</p>

<p>I've been loading my URL on my phone, and it's able to take an image, but then instantly crashes.</p>

<p>Does anyone have any experience in this? Thanks </p>

<p>My node-red flow is here </p>

<pre><code>[{""id"":""d2139149.736cb"",""type"":""visual-recognition-v3"",""z"":""12c7584.82dbca8"",""name"":""Classify Meter"",""apikey"":"""",""image-feature"":""classifyImage"",""lang"":""en"",""x"":652.107177734375,""y"":757.9999923706055,""wires"":[[""603c48c7.3144f8"",""8f4f26d2.549338"",""a862f46a.96305""]]},{""id"":""4a935442.0b7efc"",""type"":""http in"",""z"":""12c7584.82dbca8"",""name"":"""",""url"":""/recophone"",""method"":""get"",""upload"":false,""swaggerDoc"":"""",""x"":115,""y"":733.4404983520508,""wires"":[[""f235ab46.9552a8""]]},{""id"":""1c640d2d.20b8b3"",""type"":""http response"",""z"":""12c7584.82dbca8"",""name"":""HTTP Response"",""statusCode"":"""",""headers"":{},""x"":1097.33349609375,""y"":719.3928298950195,""wires"":[]},{""id"":""f235ab46.9552a8"",""type"":""switch"",""z"":""12c7584.82dbca8"",""name"":""Check Url"",""property"":""payload.imageurl"",""propertyType"":""msg"",""rules"":[{""t"":""null""},{""t"":""else""}],""checkall"":""false"",""outputs"":2,""x"":311.5714111328125,""y"":727.4404907226562,""wires"":[[""7de01baf.d96a2c""],[""9b629cc.7379fe""]]},{""id"":""603c48c7.3144f8"",""type"":""template"",""z"":""12c7584.82dbca8"",""name"":""Report"",""field"":""payload"",""fieldType"":""msg"",""format"":""html"",""syntax"":""mustache"",""template"":""&lt;html&gt;\n&lt;head&gt;&lt;title&gt;Watson Visual Recognition on Node-RED&lt;/title&gt;&lt;/head&gt;\n&lt;style&gt; \nbody {background-color: white;}\nh1   {text-align: center;}\nh2   {color: black; text-align: center; font-family: verdana; font-size: 30px;}\np    {color:black; text-align: center; font-family: verdana; font-size: 15px;}\ntable {color:black; text-align: center; font-family: verdana; font-size: 15px;}\nform {color:black; text-align: center; font-family: verdana; font-size: 15px;}\n&lt;/style&gt;\n&lt;body&gt;\n&lt;h1&gt;&lt;img src=\""http://www.rousseau.com.pt/imagens/estudos/IBM_logo_blue_thumb1.png\""&gt;\n&lt;img src=\""https://images.reevoo.com/retailers/Center/NPOWR/brand/260x200.png\""&gt; &lt;/h1&gt;\n&lt;h2&gt;Node-RED Watson Visual Recognition output&lt;/h2&gt;\n&lt;p&gt;Analyzed image: {{payload}}&lt;br/&gt;&lt;img src=\""{{payload}}\"" height='100'/&gt;&lt;/p&gt;\n&lt;table border='1' align=\""center\""&gt; \n   &lt;thead&gt;&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Score&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;\n{{#result.images.0.classifiers.0.classes}}\n  &lt;tr&gt;&lt;td&gt;&lt;b&gt;{{class}}&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;i&gt;{{score}}&lt;/i&gt;&lt;/td&gt;&lt;/tr&gt;\n{{/result.images.0.classifiers.0.classes}}\n&lt;/table&gt;\n&lt;form  action=\""{{req._parsedUrl.pathname}}\""&gt;\n    &lt;input type=\""submit\"" value=\""Try again\""/&gt;\n&lt;/form&gt;\n&lt;/body&gt;\n&lt;/html&gt;"",""x"":903.7618408203125,""y"":756.726203918457,""wires"":[[""1c640d2d.20b8b3""]]},{""id"":""9b629cc.7379fe"",""type"":""change"",""z"":""12c7584.82dbca8"",""name"":""Convert Url"",""rules"":[{""t"":""set"",""p"":""payload"",""pt"":""msg"",""to"":""payload.imageurl"",""tot"":""msg""}],""action"":"""",""property"":"""",""from"":"""",""to"":"""",""reg"":false,""x"":459.33343505859375,""y"":757.2500228881836,""wires"":[[""d2139149.736cb""]]},{""id"":""8f4f26d2.549338"",""type"":""function"",""z"":""12c7584.82dbca8"",""name"":""get labels"",""func"":""//var labels = msg.result.images[0].classifiers[0].classes;\nvar confidence = msg.result.images[0].classifiers[0].classes;\n//msg.payload = labels.map(function(i){\n //  return i.class;\nmsg.payload = confidence.map(function(i){\nreturn i.class + \"" \"" + i.score;\n//msg.payload = confidence.map(function(i){\n  //return i.score;\n  \n});\nreturn msg;"",""outputs"":1,""noerr"":0,""x"":897.66650390625,""y"":795.1071243286133,""wires"":[[""e10c122c.626fd""]]},{""id"":""e10c122c.626fd"",""type"":""debug"",""z"":""12c7584.82dbca8"",""name"":"""",""active"":true,""console"":""false"",""complete"":""payload"",""x"":1082.428466796875,""y"":791.726203918457,""wires"":[]},{""id"":""a862f46a.96305"",""type"":""debug"",""z"":""12c7584.82dbca8"",""name"":"""",""active"":false,""console"":""false"",""complete"":""result"",""x"":901.3629150390625,""y"":832.3452758789062,""wires"":[]},{""id"":""df542a47.1a7088"",""type"":""inject"",""z"":""12c7584.82dbca8"",""name"":""URL"",""topic"":"""",""payload"":""http://visual-recognition-demo.mybluemix.net/images/samples/2.jpg"",""payloadType"":""str"",""repeat"":"""",""crontab"":"""",""once"":false,""x"":474.73809814453125,""y"":795.75,""wires"":[[""d2139149.736cb""]]},{""id"":""7de01baf.d96a2c"",""type"":""template"",""z"":""12c7584.82dbca8"",""name"":""File UploadGet Image URL"",""field"":""payload"",""fieldType"":""msg"",""format"":""html"",""syntax"":""mustache"",""template"":""&lt;html&gt;\n&lt;head&gt;\n    &lt;style&gt;\nbody {background-color: white;}\nh1   {text-align: center;}\nh2   {color: black; text-align: center; font-family: verdana; font-size: 30px;}\nh3   {color:black; text-align: center; font-family: verdana; font-size: 15px;}\nform {color:black; text-align: center; font-family: verdana; font-size: 15px;}\n&lt;/style&gt;\n&lt;title&gt;Npower Meter Watson Visual Recognition font-family: \""Playfair Display\"", serif;\nfont-size: 32px;text-align: left; &lt;/title&gt; &lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt; &lt;img src=\""http://www.rousseau.com.pt/imagens/estudos/IBM_logo_blue_thumb1.png\""&gt;\n&lt;img src=\""https://images.reevoo.com/retailers/Center/NPOWR/brand/260x200.png\""&gt; &lt;/h1&gt; \n \n &lt;h2&gt;  Welcome to this demo on identifying Meter Issues via images &lt;/h2&gt;\n &lt;h3&gt; Select an image URL&lt;/h3&gt;\n&lt;form  action=\""{{req._parsedUrl.pathname}}\""&gt;\n    //Code for taking picture from phone\n&lt;form action=\""/action_page.php\""&gt;\n  &lt;input type=\""file\"" name=\""pic\"" accept=\""image/*\""&gt;\n  &lt;input type=\""submit\""&gt;\n   &lt;img src=\""http://static.flickr.com/3087/2648843542_3101f132ec.jpg\"" height='200'/&gt;\n    &lt;img src=\""http://static.flickr.com/3281/2719178040_e80447902d.jpg\"" height='200'/&gt;\n    &lt;img src=\""http://static.flickr.com/2308/2218597585_410d3c9148.jpg\"" height='200'/&gt;\n    &lt;img src=\""http://static.flickr.com/3072/2439552803_ed8b7d6ab1.jpg\"" height='200'/&gt;\n&lt;br/&gt;&lt;b&gt;Copy above image location URL or enter any image URL:&lt;/b&gt;&lt;br/&gt;\n    &lt;input type=\""text\"" name=\""imageurl\""/&gt;\n    &lt;input type=\""submit\"" value=\""Analyze\""/&gt;\n&lt;/form&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n"",""x"":843.5714111328125,""y"":716.5833435058594,""wires"":[[""1c640d2d.20b8b3""]]},{""id"":""7bd12438.024544"",""type"":""comment"",""z"":""12c7584.82dbca8"",""name"":""Phone Upload"",""info"":"""",""x"":110,""y"":682,""wires"":[]}]
</code></pre>",,1,5,,2017-09-11 09:43:56.423 UTC,,2017-10-09 15:40:42.877 UTC,2017-09-11 09:48:17.140 UTC,,504554,,8493905,1,-2,ibm-watson|node-red|visual-recognition,299
Google Vision API Base64 decoding failed,44373059,Google Vision API Base64 decoding failed,"<p>I have sent Base64 data to Google Vision API and it works on one of my web servers, but does not work on another web server.</p>

<p>I get the error:
Invalid value at 'requests[0].image.content' (TYPE_BYTES), Base64 decoding failed for ""... base64 data here ...""</p>

<p>I try a different image on both servers and it works on both web servers and Google Vision API returns good results.</p>

<p>The base64 data that i am sending from both webservers is identical.  The Programming i am using to send (ColdFusion) is identical.</p>

<p>I would paste the Base64 data here, but it is a lot of text...</p>

<p>Is there anything on the Google Vision API console that will give me information on my failures so i can compare them to the successes?</p>",,1,0,,2017-06-05 16:06:58.053 UTC,,2018-09-23 23:04:12.300 UTC,2017-06-06 15:11:35.677 UTC,,5231007,,8115220,1,2,google-cloud-vision,318
How do I increase timeout to stop google-cloud-vision DeadlineExceededError?,44783626,How do I increase timeout to stop google-cloud-vision DeadlineExceededError?,"<p>I am using 'google-cloud-vision' gem (v0.23.0) to do some image OCR and my requests randomly fail with: DeadlineExceededError.  The error rate ranges from 1% to 99% failure, on a day-to-day basis, so it is very unpredictable.</p>

<p>When bypassing the gem and using the Google REST API, and passing in my image that is Base64Encoded, things seem flawless.  </p>

<p>I'm guessing that the DeadlineExceededError is utilizing some timeout variable, whereas the REST api is not. So, I was wondering how to increase the Timeout as I don't feel right by using raw ruby code VS a library created by the company.</p>",44784841,1,0,,2017-06-27 14:59:12.353 UTC,,2017-06-27 15:56:12.837 UTC,,,,,688266,1,0,ruby-on-rails|ruby|google-cloud-platform|google-cloud-vision,202
Create variable and dataset in a loop? (R),55155238,Create variable and dataset in a loop? (R),"<p>this is the first time I'm attempting to build a function using R. Basically my intended goal are as follows.</p>

<ul>
<li>Communicate with Google Cloud Vision API using RoogleVision package</li>
<li>The function goes through the images in the directory</li>
<li>Retrieve wanted information from Google Vision features for each picture</li>
<li>Save them in a single aggregated dataset</li>
</ul>

<p>Below is the sample code I'm using. The only part I think I'm struggling is properly ""iterating"" through the pictures and continuously creating a dataset.</p>

<p>Any helps and advice are appreciated!</p>

<p>Thanks in advance!</p>

<pre><code>googlevision &lt;- function(path) {
    path &lt;- dirname(file.choose())  # Get directory
    setwd(path)
    pic_list &lt;- list.files(path = path, pattern = ""*.png"")  # Get filename lists
    vision_data &lt;- NULL
    for (i in pic_list) {
            text &lt;- getGoogleVisionResponse(i, feature = ""TEXT_DETECTION"")
            text_lang &lt;- text[[1]][1]
            ad_text &lt;- paste(text[[2]][2:as.numeric(length(text[[2]])-20)], sep = "" "", collapse = "" "")
            vision_data &lt;- bind_rows(c(""text_lang"" = text[[1]][1], 
                                       ""ad_text"" = paste(text[[2]][2:as.numeric(length(text[[2]])-20)], sep = "" "", collapse = "" "")))
            if(colnames(getGoogleVisionResponse(i, feature = ""FACE_DETECTION""))[1] != ""error""){
                    face &lt;- getGoogleVisionResponse(i, feature = ""FACE_DETECTION"")
                    face_conf &lt;- face$detectionConfidence
                    joy &lt;- face$joyLikelihood
                    sorrow &lt;- face$sorrowLikelihood
                    anger &lt;- face$angerLikelihood
                    surprise &lt;- face$surpriseLikelihood
                    underExposed &lt;- face$underExposedLikelihood
                    blur &lt;- face$blurredLikelihood
                    headwear &lt;- face$headwearLikelihood
            } 
            if(colnames(getGoogleVisionResponse(i, feature = ""LABEL_DETECTION""))[1] != ""error""){
                    label &lt;- getGoogleVisionResponse(i, feature = ""LABEL_DETECTION"")
                    label_desc &lt;- label$description
                    label_score &lt;- label$score
            }
            visual_data &lt;- bind_rows(c(""face_conf"" = face_conf,
                               ""joy"" = joy,
                               ""sorrow"" = sorrow,
                               ""anger"" = anger, ""surprise"" = surprise, ""underExposed"" = underExposed,
                               ""blur"" = blur, ""headwear"" = headwear, ""text_lang"" = text_lang, ""ad_text"" = ad_text))
    }
</code></pre>",55157364,1,0,,2019-03-14 04:59:31.740 UTC,1,2019-03-14 07:50:43.867 UTC,,,,,11199930,1,3,r|macos|function|for-loop|if-statement,94
Why getlocationonscreen returining -180 for x?,49842534,Why getlocationonscreen returining -180 for x?,"<p>I am using the google vision library (although this might not matter for the purpose of this question). Here is the xml file</p>

<pre><code>&lt;LinearLayout
    xmlns:android=""http://schemas.android.com/apk/res/android""
    android:id=""@+id/topLayout""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""
    android:keepScreenOn=""true""&gt;

    &lt;com.google.android.gms.samples.vision.ocrreader.ui.camera.CameraSourcePreview
        android:id=""@+id/preview""
        android:layout_width=""match_parent""
        android:layout_height=""match_parent""&gt;

        &lt;com.google.android.gms.samples.vision.ocrreader.ui.camera.GraphicOverlay
            android:id=""@+id/graphicOverlay""
            android:layout_width=""match_parent""
            android:layout_height=""match_parent"" /&gt;

    &lt;/com.google.android.gms.samples.vision.ocrreader.ui.camera.CameraSourcePreview&gt;

&lt;/LinearLayout&gt;
</code></pre>

<p>Now if do </p>

<pre><code>@Override
public void onWindowFocusChanged(boolean hasFocus) {
    super.onWindowFocusChanged(hasFocus);
    int  [] loc = new int[2];
    mPreview.getLocationOnScreen(loc);
    int  [] loc2 = new int[2];
    mGraphicOverlay.getLocationOnScreen(loc2);
}
</code></pre>

<p>Then loc1 is {0,0} which is expected but loc2 is{-180,0}.</p>

<p>Why is that? Shouldn't it be also 0,0?
Thank you</p>

<p>Here is the UI part from dympsys</p>

<pre><code>  View Hierarchy:
          DecorView@55587bd[OcrLiveActivity]
            android.widget.LinearLayout{d743bae V.E...... ........ 0,0-1080,1920}
              android.view.ViewStub{b409d4f G.E...... ......I. 0,0-0,0 #1020471 android:id/action_mode_bar_stub}
              android.widget.FrameLayout{48b3cdc V.E...... ........ 0,0-1080,1920}
                android.support.v7.widget.FitWindowsLinearLayout{9fc21e5 V.E...... ........ 0,0-1080,1920 #7f08000b app:id/action_bar_root}
                  android.support.v7.widget.ViewStubCompat{ecc64ba G.E...... ......I. 0,0-0,0 #7f080017 app:id/action_mode_bar_stub}
                  android.support.v7.widget.ContentFrameLayout{593bf6b V.E...... ........ 0,0-1080,1920 #1020002 android:id/content}
                    android.support.constraint.ConstraintLayout{bfd4ac8 V.E...... ........ 0,0-1080,1920 #7f0800d6 app:id/topLayout} 

com.google.android.gms.samples.vision.ocrreader.ui.camera.CameraSourcePreview{651f761 V.E...... ........ 0,0-1080,1920 #7f080091 app:id/preview}
                        android.view.SurfaceView{4c1286 V.E...... ........ -180,0-1260,1920}               

com.google.android.gms.samples.vision.ocrreader.ui.camera.GraphicOverlay{7ace747 V.ED..... ........ -180,0-1260,1920 #7f080058 app:id/graphicOverlay}
</code></pre>",,0,6,,2018-04-15 13:35:26.403 UTC,,2018-04-17 12:06:43.580 UTC,2018-04-17 12:06:43.580 UTC,,1217820,,1217820,1,0,android|android-layout,38
Google Vision API : java.lang.NoClassDefFoundError: com/google/cloud/vision/v1/ImageAnnotatorClient ERROR,56341410,Google Vision API : java.lang.NoClassDefFoundError: com/google/cloud/vision/v1/ImageAnnotatorClient ERROR,"<p>I'm trying to run the Google Api Vision sample code but I'm getting this error:</p>

<blockquote>
  <p>java.lang.NoClassDefFoundError:com/google/cloud/vision/v1/ImageAnnotatorClient</p>
</blockquote>

<p>These are the dependencies that imported into my project.</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;dependency&gt;
  &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;
  &lt;artifactId&gt;google-cloud-vision&lt;/artifactId&gt;
  &lt;version&gt;1.74.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
  &lt;groupId&gt;com.google.api.grpc&lt;/groupId&gt;
  &lt;artifactId&gt;proto-google-common-protos&lt;/artifactId&gt;
  &lt;version&gt;1.7.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
  &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt;
  &lt;artifactId&gt;jsr305&lt;/artifactId&gt;
  &lt;version&gt;3.0.2&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
  &lt;groupId&gt;javax.annotation&lt;/groupId&gt;
  &lt;artifactId&gt;javax.annotation-api&lt;/artifactId&gt;
  &lt;version&gt;1.3.2&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>Code that I'm using. Which is provided google Vision API from: <a href=""https://cloud.google.com/vision/docs/libraries"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/libraries</a></p>

<pre class=""lang-java prettyprint-override""><code>package com.google.cloud.vision.api.utils;

//Imports the Google Cloud client library

import com.google.cloud.vision.v1.AnnotateImageRequest;
import com.google.cloud.vision.v1.AnnotateImageResponse;
import com.google.cloud.vision.v1.BatchAnnotateImagesResponse;
import com.google.cloud.vision.v1.EntityAnnotation;
import com.google.cloud.vision.v1.Feature;
import com.google.cloud.vision.v1.Feature.Type;
import com.google.cloud.vision.v1.Image;
import com.google.cloud.vision.v1.ImageAnnotatorClient;
import com.google.protobuf.ByteString;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.List;

public class QuickstartSample {

    public static void main(String... args) throws Exception {
      // Instantiates a client
      try (ImageAnnotatorClient vision = ImageAnnotatorClient.create()) {

        // The path to the image file to annotate
        String fileName = ""/content/dam/USGBoral/Australia/Website/Images/products/steel_framing/SteelFraming-335x135_en.jpg"";

        // Reads the image file into memory
        Path path = Paths.get(fileName);
        byte[] data = Files.readAllBytes(path);
        ByteString imgBytes = ByteString.copyFrom(data);

        // Builds the image annotation request
        List&lt;AnnotateImageRequest&gt; requests = new ArrayList&lt;&gt;();
        Image img = Image.newBuilder().setContent(imgBytes).build();
        Feature feat = Feature.newBuilder().setType(Type.LABEL_DETECTION).build();
        AnnotateImageRequest request = AnnotateImageRequest.newBuilder()
                                                         .addFeatures(feat)
                                                         .setImage(img)
                                                         .build();
        requests.add(request);

        // Performs label detection on the image file
        BatchAnnotateImagesResponse response = vision.batchAnnotateImages(requests);
        List&lt;AnnotateImageResponse&gt; responses = response.getResponsesList();

        for (AnnotateImageResponse res : responses) {
          if (res.hasError()) {
            System.out.printf(""Error: %s\n"", res.getError().getMessage());
            return;
          }

          for (EntityAnnotation annotation :res.getLabelAnnotationsList()) {
            annotation.getAllFields()
                      .forEach((k, v) -&gt; System.out.printf(""%s : %s\n"", k, v.toString()));
          }
        }
      }
   }
}
</code></pre>",,1,3,,2019-05-28 11:38:48.043 UTC,,2019-05-29 08:00:54 UTC,2019-05-28 13:45:45.657 UTC,,7404943,,5610650,1,0,google-cloud-vision|google-vision,15
Can AWS rekognition detect a table within an image and allow me to ingest into a dynamoDB table,54065638,Can AWS rekognition detect a table within an image and allow me to ingest into a dynamoDB table,<p>I have a lot of documents in offline books. It's in table format and I don't want to manually input these tables into a dynamoDB table. Can I use AWS rekognition to help me here OR I should look at some other service ?</p>,,1,0,,2019-01-06 20:28:27.717 UTC,,2019-01-19 14:45:56.633 UTC,2019-01-08 08:35:14.933 UTC,,4370109,,3320613,1,0,amazon-dynamodb|amazon-rekognition,20
how can I make a s3 bucket sub images iteration generic,50733603,how can I make a s3 bucket sub images iteration generic,"<p>my bucket on s3 is named as 'python' and its subfolder is  'boss' . So I want to get all images of folder boss in lambda function. currently I am hard-coding values but putting image in root not in subfolder.  </p>

<pre><code>bucket=""python""
key=""20180530105812.jpeg""
</code></pre>

<p>then I want to call this function one by one for all images</p>

<pre><code>def lambda_handler(event, context):
    # Get the object from the event
    bucket=""ais-django""
    key=""20180530105812.jpeg""

    try:

        # Calls Amazon Rekognition IndexFaces API to detect faces in S3 object 
        # to index faces into specified collection

        response = index_faces(bucket, key)

        # Commit faceId and full name object metadata to DynamoDB
</code></pre>",50736724,3,0,,2018-06-07 05:34:02.970 UTC,,2018-06-07 08:42:16.863 UTC,2018-06-07 05:41:49.103 UTC,,7749560,,7749560,1,0,python|amazon-web-services|amazon-s3|aws-lambda|amazon-rekognition,218
Google Vision API Document Text multiple images in base64 String,51008892,Google Vision API Document Text multiple images in base64 String,"<p>I use the Google Vision API OCR (Document Text Detection) to get the text from a scanned document (base64 String). It works perfekt for one image. But how can I send more than one image, e.g. the second page of a document. </p>

<p>I´ve tried to merge the base64 strings but it do not work. </p>

<pre><code>var base64ImagesArrayConcarved = base64ImagesArray.join('')
</code></pre>",51881100,1,3,,2018-06-24 10:11:46.740 UTC,,2018-08-16 15:58:23.983 UTC,,,,,3542250,1,0,google-cloud-vision,350
Send image from OpenCV 3 to Cognitive Face API using C++ REST SDK,42984821,Send image from OpenCV 3 to Cognitive Face API using C++ REST SDK,"<p>I want to use the Microsoft Face API from an application in C++. The cpprest sdk allows me to send an url of image or binary data of image. The problem is that my image is not a file in disk but a cv::Mat in memory. I have been trying to serialize it via an stringstream, but the request method complains because only accepts some strings and istream.</p>

<p>The following code is good when opening an image from file:</p>

<pre><code>file_stream&lt;unsigned char&gt;::open_istream(filename)
 .then([=](pplx::task&lt;basic_istream&lt;unsigned char&gt;&gt; previousTask)
 {
    try
    {
       auto fileStream = previousTask.get();

       auto client = http_client{U(""https://api.projectoxford.ai/face/v0/detections"")};

       auto query = uri_builder()
          .append_query(U(""analyzesFaceLandmarks""), analyzesFaceLandmarks ? ""true"" : ""false"")
          .append_query(U(""analyzesAge""), analyzesAge ? ""true"" : ""false"")
          .append_query(U(""analyzesGender""), analyzesGender ? ""true"" : ""false"")
          .append_query(U(""analyzesHeadPose""), analyzesHeadPose ? ""true"" : ""false"")
          .append_query(U(""subscription-key""), subscriptionKey)
          .to_string();

       client
          .request(methods::POST, query, fileStream)
   ...
    }
}
</code></pre>

<p>Here a file_stream is used to open the file.
I tried serializing my Mat like this:</p>

<pre><code>    // img is the cv::Mat
    std::vector&lt;uchar&gt; buff;
    cv::imencode("".jpg"", img, buff);
    std::stringstream ssbuff;
    copy(buff.begin(), buff.end(), std::ostream_iterator&lt;unsigned char&gt;(ssbuff,""""));
</code></pre>

<p>This serialization works as I can decode if after and rebuild the image.</p>

<p>¿How can I send to server the opencv Mat image through the client?</p>",43072145,1,1,,2017-03-23 19:02:31.597 UTC,1,2017-03-28 14:21:44.237 UTC,2017-03-28 04:03:36.413 UTC,,2328443,,2328443,1,3,c++|opencv|microsoft-cognitive|opencv3.1|cpprest-sdk,571
How to record face masking with google vision API?,53475106,How to record face masking with google vision API?,"<p>I want to try face masking like <em>Snapchat</em> feature on my Android app. I've tried this face masking using Google vision, but I can't find how to record this face masking.</p>

<p>Is there any solution to record face masking? </p>",,0,2,,2018-11-26 05:17:56.640 UTC,,2018-11-28 07:00:46.237 UTC,2018-11-28 07:00:46.237 UTC,,15168,,10704528,1,0,android|augmented-reality|masking|face,33
How to intergrate Wolfram into android studio,55793092,How to intergrate Wolfram into android studio,"<p>I am designing an android app that will scan mathematical equations using google vision(OCR) and solve them using the Wolfram API. So far I am through with the OCR part but I can't find a way of integrating Wolfram into my app. It seems they have no dependencies(or so I have concluded).</p>

<p>What is the correct way of integrating this into my app? I haven't found anything on the internet since yesterday.</p>",,0,1,,2019-04-22 10:43:36.067 UTC,,2019-04-22 10:43:36.067 UTC,,,,,11118152,1,0,android|wolfram-mathematica|wolframalpha,16
RegEx for extracting specific variables and values,56094441,RegEx for extracting specific variables and values,"<p>I am using Google Vision API to extract the text (handwritten plus computer-written) from images of application forms. The response is a long string like the following. </p>

<p>The string:</p>

<pre><code>""A. Bank Challan
Bank Branch
ca
ABC muitce
Deposit ID VOSSÁETM-0055
Deposit Date 16 al 19
ate
B. Personal Information: Use CAPITAL letters and leave spaces between words.
Name: MUHAMMAD HANIE
Father's Name: MUHAMMAD Y AQOOB
Computerized NIC No. 44 603-5 284 355-3
D D M m rrrr
Gender: Male Age: (in years) 22 Date of Birth ( 4-08-1999
Domicile (District): Mirpuskhas Contact No. 0333-7078758
(Please do not mention converted No.)
Postal Address: Raheel Book Depo Naukot Taluka jhuddo Disstri mes.
Sindh.
Are You Government Servant: Yes
(If yes, please attach NOC)
No
✓
Religion: Muslim
✓
Non-Muslimo
C. Academic Information:
B
Intermediate/HSSC ENG Mirpuskhas Bise Match
Seience BISEmirpuskhas Match
2016
2014
Matric/SSC""
</code></pre>

<p>The whole response isn't useful for me, however I need to parse the response to get specific fields like Name, Father's Name, NIC No., Gender, Age, DoB, Domicile, and Contact No. </p>

<p>I am defining patterns for each of these fields using regular expression library (re) in Python. For example:</p>

<pre><code>import re
name ='Name: \w+\s\w+'
fatherName = 'Father\'s Name: \w+\s\w+\s\w+'
age ='Age: \D+\d+'

print(re.search(name,string).group())
print(re.search(fatherName, string).group())
print(re.search(age,string).group())
</code></pre>

<p>Output:</p>

<pre><code>""Name: MUHAMMAD HANIE
Father's Name: MUHAMMAD Y AQOOB
Age: (in years) 22""
</code></pre>

<p>However these are not robust patterns, and I don't know whether this approach is good or not. I also cannot extract the fields that are on same line, like Gender and Age. </p>

<p>How do I solve this problem? </p>",,1,2,,2019-05-11 21:35:24.917 UTC,,2019-05-15 20:42:18.540 UTC,2019-05-15 20:42:18.540 UTC,,6553328,,11456782,1,2,python|regex|regex-lookarounds|regex-group|regex-greedy,82
IBM Watson Visual Recognition: Results format in Java,45812258,IBM Watson Visual Recognition: Results format in Java,"<p>I'd like to know if is there a possibility to change the format of result returned bi Watson Visual Recognition API.</p>

<p>For example: </p>

<p>Instead of having this:  </p>

<p><code>wasp-nest: 0.98, bird-nest: 0.9, hornet-nest: 0.95</code>  </p>

<p>Get this: </p>

<p><code>wasp-nest: 0.98, bird-nest: 0.0, hornet-nest: 0.02</code></p>

<p>So the sum of classes results would be 1 (100%)</p>",,1,0,,2017-08-22 08:10:35.997 UTC,,2017-08-23 10:54:42.200 UTC,,,,,8498925,1,0,java|ibm-watson|visual-recognition,74
Google Vision OCR - reading morse code from image file,50571761,Google Vision OCR - reading morse code from image file,"<p>I'm trying to read in an image of morse code (dots and dashes) using Google Vision OCR but it's not picking up the symbols very well. Is there a way to make Google Vision pick up the dots and dashes better or is there a different OCR product that can do a better job of recognizing symbols?</p>

<p>Currently I am using the nodejs example as provided by Google Cloud at the moment:</p>

<pre><code>const vision = require('@google-cloud/vision');

// Creates a client
const client = new vision.ImageAnnotatorClient();

// Read a remote image as a text document
client
  .documentTextDetection('./resources/morse.png')
  .then(results =&gt; {
    const fullTextAnnotation = results[0].fullTextAnnotation;
    console.log(fullTextAnnotation.text);
  })
  .catch(err =&gt; {
    console.error('ERROR:', err);
  });
</code></pre>",,0,2,,2018-05-28 18:21:42.457 UTC,,2018-05-28 18:21:42.457 UTC,,,,,9683569,1,0,image-processing|ocr|vision|google-vision|morse-code,235
How to get square crop hint from Google Cloud Vision?,44085992,How to get square crop hint from Google Cloud Vision?,"<p>How to get square crop hint from Google Cloud Vision?</p>

<p>I'm setting the aspect ratio to 1 but the resulting hint is never an exact square.</p>

<p>Below is the modified example from <a href=""https://cloud.google.com/vision/docs/detecting-crop-hints"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/detecting-crop-hints</a> checking sample image from <a href=""https://cloud.google.com/vision/docs/images/cat.jpg"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/images/cat.jpg</a>.</p>

<pre class=""lang-java prettyprint-override""><code>public class QuickTest {

    public static void main(String[] args) throws Exception {
        String filePath = ""/path/to/cat.jpg"";
        List&lt;AnnotateImageRequest&gt; requests = new ArrayList&lt;&gt;();
        ByteString imgBytes = ByteString.readFrom(new FileInputStream(filePath));
        Image img = Image.newBuilder().setContent(imgBytes).build();
        CropHintsParams cropHintsParams = CropHintsParams.newBuilder().clearAspectRatios().addAspectRatios(1).build();
        Feature feat = Feature.newBuilder().setType(Feature.Type.CROP_HINTS).build();
        AnnotateImageRequest request =
                AnnotateImageRequest.newBuilder().addFeatures(feat).setImage(img)
                        .setImageContext(ImageContext.newBuilder().setCropHintsParams(cropHintsParams).build())
                        .build();
        requests.add(request);
        BatchAnnotateImagesResponse response =
                ImageAnnotatorClient.create().batchAnnotateImages(requests);
        List&lt;AnnotateImageResponse&gt; responses = response.getResponsesList();
        for (AnnotateImageResponse res : responses) {
            if (res.hasError()) {
                System.out.printf(""Error: %s\n"", res.getError().getMessage());
                return;
            }
            CropHintsAnnotation annotation = res.getCropHintsAnnotation();
            for (CropHint hint : annotation.getCropHintsList()) {
                System.out.println(hint.getBoundingPoly());
            }
        }
    }
}
</code></pre>

<p>The result is (zeroes are not printed):</p>

<pre><code>vertices {
}
vertices {
  x: 1100
}
vertices {
  x: 1100
  y: 1114
}
vertices {
  y: 1114
}
</code></pre>",,0,2,,2017-05-20 12:41:18.807 UTC,1,2017-05-20 13:35:54.227 UTC,2017-05-20 13:35:54.227 UTC,,405935,,405935,1,0,java|google-cloud-vision,830
Mocking Intents For scanning QR code in Android Espresso (Google Vision),56035004,Mocking Intents For scanning QR code in Android Espresso (Google Vision),"<p>I want to automate following thing in my test -</p>

<ol>
<li>Click on Pay at store.</li>
<li>It starts scanning for QR Code using Google Vision Api</li>
</ol>

<p>I wanted to mock the scanning flow mostly using Espresso Intents.</p>",,0,0,,2019-05-08 06:47:17.087 UTC,1,2019-05-08 06:47:17.087 UTC,,,,,9520594,1,-1,android-espresso|google-vision,9
How to connect a flask API with google vision API?,50907956,How to connect a flask API with google vision API?,<p>Well I'm creating a android application which uses web services as well. For that I'm planing to create a web API using flask framework to communicate with my android application. I want my API to communicate with Google Vision API to analyze text from images which will be send from the android app. How can I make both two APIs to communicate with each other?</p>,,1,0,,2018-06-18 10:59:05.960 UTC,,2018-06-18 11:15:23.060 UTC,,,,,8018641,1,0,python|flask|google-vision,145
API Rekognition Does not exist Error using amplify and React-native AWS,55463500,API Rekognition Does not exist Error using amplify and React-native AWS,"<p>I am trying to set a React-native for detect text using amazon rekognition API.</p>

<p>My guide is this tutorial <a href=""https://medium.com/@glen.bray/text-detection-with-mobile-camera-using-react-native-and-aws-rekognition-7826b3e2aeef"" rel=""nofollow noreferrer"">https://medium.com/@glen.bray/text-detection-with-mobile-camera-using-react-native-and-aws-rekognition-7826b3e2aeef</a></p>

<p>i have configured the connection with AWS using awsmobile and amplify and in both cases i had the same error: API rekognition does not exist.</p>

<p>My user has the corrects permissions and my modules and sdk are with the last version.</p>

<p>My connection API.js is the next:</p>

<pre><code>import Amplify, { API } from ""aws-amplify"";
import awsExports from ""./aws-exports"";

Amplify.configure({
  ...awsExports,
  API: {
    endpoints: [
      {
        name: ""rekognition"",
        endpoint: ""https://rekognition.us-east-2.amazonaws.com"",
        service: ""rekognition"",
        region: ""us-east-2""
      }
    ]
  }
});

async function detectText(bytes) {
  const apiName = ""rekognition"";
  const path = ""/detect-text"";
  const body = { Image: { Bytes: bytes } };

  const headers = {
    ""X-Amz-Target"": ""RekognitionService.DetectText"",
    ""Content-Type"": ""application/x-amz-json-1.1"",
    ""X-Amz-Date"": ""20190330T120000Z""
  };

  const init = {
    body: body,
    headers: headers
  };

  var response=await API.post(apiName, path, init);
  //console.log(JSON.stringify(respuesta.body));

  return response;
}

export { detectText };
</code></pre>

<p>Thank you!!</p>",,0,0,,2019-04-01 20:59:37.790 UTC,,2019-04-02 18:22:34.023 UTC,2019-04-02 18:22:34.023 UTC,,6269729,,6269729,1,0,amazon-web-services|react-native|aws-amplify|amazon-rekognition|text-recognition,94
Google Vision V1 deserialize API response,54089791,Google Vision V1 deserialize API response,"<p>I am using Google Vision via Rest API v1 with feature <code>DOCUMENT_TEXT_DETECTION</code> and  API is returning correct result:</p>

<pre><code>var response = await httpClient.PostAsync(""https://vision.googleapis.com/v1/images:annotate?key=XXXXX"",new StringContent(requestJson, Encoding.UTF8, ""application/json""));

var gVisionContent = await request.Content.ReadAsStringAsync();
</code></pre>

<p>what I want to deserialize via Json.Net to Google vision object (from <a href=""https://www.nuget.org/packages/Google.Cloud.Vision.V1/"" rel=""nofollow noreferrer"">Google.Cloud.Vision.V1</a> nuget), but when I run this <code>JsonConvert.DeserializeObject&lt;AnnotateImageResponse&gt;(gVisionContent)</code> it will return me empty results</p>

<pre><code>Context null
CropHintsAnnotation null
Error   null
FaceAnnotations {[ ]}
FullTextAnnotation  null
ImagePropertiesAnnotation   null
LabelAnnotations    {[ ]}
LandmarkAnnotations {[ ]}
LogoAnnotations {[ ]}
SafeSearchAnnotation    null
TextAnnotations {[ ]}
WebDetection    null
</code></pre>

<p>When I try to deserialize it to <code>AnotateFileResponse</code>, then I am getting this error message..</p>

<pre><code>Error converting value ""LINE_BREAK"" to type 
'Google.Cloud.Vision.V1.TextAnnotation+Types+DetectedBreak+Types+BreakType'. 
Path 'responses[0].fullTextAnnotation.pages[0] 
.blocks[0].paragraphs[0].words[1].symbols[0].property.detectedBreak.type', 
line 3850, position 52.
</code></pre>

<p>I cannot use nuget package directly for detection, because I need to call it via rest API</p>",,0,2,,2019-01-08 10:25:47.533 UTC,,2019-01-08 11:14:25.073 UTC,2019-01-08 11:14:25.073 UTC,,3471882,,3471882,1,0,c#|google-vision,62
Computer Vision: How to obtain what percentage of image contains a specific texture?,48848031,Computer Vision: How to obtain what percentage of image contains a specific texture?,"<p>I am building a app to see the progress of deforestation. Over time i would like to take a satellite image from a location and see what percentage of that image contains forest.</p>

<p>I have attempted google's vision API, it does not have this functionality.</p>

<p>Is this something that can be done in OpenCV or must I do this from scratch with semantic segmentation or something similar?</p>",,2,0,,2018-02-18 02:58:25.960 UTC,,2018-03-01 10:58:48.607 UTC,,,,,4313927,1,-1,python|computer-vision|google-cloud-platform|vision,60
Using Google Cloud Vision API,44607269,Using Google Cloud Vision API,"<p>I'm trying to use Google Cloud Vision API, but facing some issues. Let me explain the steps I took and then the issue I'm facing. I'm running this code in Windows 10.</p>

<ol>
<li>Download and install ""GoogleCloudSDKInstaller"".</li>
<li>gcloud auth application-default login to activate my login
credential</li>
<li>Used <a href=""https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/vision/cloud-client/face_detection/faces.py"" rel=""nofollow noreferrer"">this</a> code to run the face detector, for the image
in my local system.</li>
<li>cd into the folder where image exists</li>
<li><p>Then run the following code:</p>

<p>python detect_face.py ""image.jpg""</p></li>
</ol>

<p>But I'm getting this error:</p>

<blockquote>
  <p>raise EnvironmentError('Project was not passed and could not be '
  OSError: Project was not passed and could not be determined from the
  environment.</p>
</blockquote>

<p>Can anyone please tell me why I'm getting this issue?</p>",44651329,1,0,,2017-06-17 17:10:24.397 UTC,1,2017-06-21 00:41:29.227 UTC,2017-06-21 00:41:29.227 UTC,,322020,,697363,1,0,python|google-cloud-vision,261
Google cloud vision api- OCR,49665196,Google cloud vision api- OCR,<p>I want to use text-detection from image (OCR) of google cloud vision api. But i dont know how to get the subscription key from and how to authenticate and make calls in C#. Can some body tell me the step by step procedure to do that. Im very new this btw. </p>,,2,7,,2018-04-05 06:05:42.183 UTC,,2018-04-19 15:04:57.423 UTC,2018-04-05 06:33:30.667 UTC,,7376458,,7376458,1,1,c#|ocr|google-cloud-vision,3844
"Implement latest Google Cloud Vision Api Text/Logo Recognition update - LabelDetectionConfig - from Sept 28, 2018?",53081398,"Implement latest Google Cloud Vision Api Text/Logo Recognition update - LabelDetectionConfig - from Sept 28, 2018?","<p>Anyone know how to set LabelDetectionConfig in Google Cloud Vision api for PHP?</p>

<p>Apparently there is new functionality released, described here: <a href=""https://cloud.google.com/vision/docs/release-notes"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/release-notes</a></p>

<p>Improved detection models are now available for the following features:</p>

<p>Logo Detection
Text Detection (OCR)
Specify ""builtin/latest"" in the LabelDetectionConfig field to use the new models.</p>

<p>We'll support both the current model and the new model the next 90 days. After 90 days the current detection models will be deprecated and only the new detection models will be used for all logo and text (OCR) detection requests.</p>

<p>This is what my code looks like now:</p>

<pre><code>$vision = new VisionClient([
   'projectId' =&gt; XXXX
]);
$contents = get_contents($url);
$image = $vision-&gt;image($contents, ['LOGO_DETECTION']);
$result = $vision-&gt;annotate($image);
</code></pre>",53545250,1,1,,2018-10-31 10:38:50.390 UTC,,2018-11-29 18:17:06.320 UTC,,,,,3502747,1,0,php|google-api|google-cloud-platform|google-vision,105
can we get list of video formats / codecs supported by Google Cloud video intelligence API,50273951,can we get list of video formats / codecs supported by Google Cloud video intelligence API,<p>Can you please help with list of video formats and codecs that are supported by Google Cloud Video Intelligence API</p>,,2,0,,2018-05-10 13:22:05.287 UTC,,2018-05-22 08:41:28.750 UTC,,,,,997239,1,0,google-cloud-platform|video-intelligence-api,91
Google cloud vision API image request,39905841,Google cloud vision API image request,<p>I would like to get the data from Google cloud vision API and see the input can be given in the base64 and image uri format.But base64 appears to be too long and to upload the image as uri it take some extra time.Please let me know if anyone knows of any other work around for this.</p>,,1,1,,2016-10-06 21:23:39.893 UTC,,2017-10-23 21:48:43.133 UTC,2016-10-07 18:37:26.770 UTC,,2117651,,2320740,1,1,unity3d|google-cloud-vision,534
Use ML Kit with NNAPI,54557026,Use ML Kit with NNAPI,"<p>I'm trying to use the new Google machine learning sdk, ML Kit, on an Android devices that run Android 9. 
From the official site:</p>

<blockquote>
  <p>ML Kit makes it easy to apply ML techniques in your apps by bringing
  Google's ML technologies, such as the Google Cloud Vision API,
  TensorFlow Lite, and the Android Neural Networks API together in a
  single SDK. Whether you need the power of cloud-based processing, the
  real-time capabilities of mobile-optimized on-device models, or the
  flexibility of custom TensorFlow Lite models, ML Kit makes it possible
  with just a few lines of code.</p>
</blockquote>

<p>I think it means that on a device with at least Android 8.1 (according to the documentation of nnapi) the SDK can uses NNAPI. But when I run the same app on a device with Android 7.1 (where nnapi is not supported) I obtain the same performance of the device that use Android 9 (and in theory the NNAPI). How i can use ML Kit with NNAPI? I am doing something wrong?
Link to documentation of mlkit: <a href=""https://firebase.google.com/docs/ml-kit/"" rel=""nofollow noreferrer"">https://firebase.google.com/docs/ml-kit/</a></p>",54558864,1,0,,2019-02-06 15:25:06.733 UTC,,2019-02-07 10:04:25.647 UTC,2019-02-07 10:04:25.647 UTC,,404970,,9557745,1,0,android|machine-learning|nnapi,89
Google vision api vs build your own,47780934,Google vision api vs build your own,"<p>I have quite a challenging use case for image recognition.  I want to detect composition of mixed recycling e.g. Crushed cans,paper,bottles and detect any anomalies such as glass, bags, shoes etc.  </p>

<p>Trying images with the google vision api the results are mainly ""trash"", ""recycling"" ""plastic"" etc likely because the api hasn't been trained on mixed and broken material like this?.</p>

<p>For something like this would I have to go for something like tensor flow and build a neural network from my own images? I guess I wouldn't need to use google for this as tensor flow is open source?</p>

<p>Thanks.</p>",47781553,1,0,,2017-12-12 20:23:14.757 UTC,1,2017-12-12 21:08:05.730 UTC,,,,,3103335,1,1,machine-learning|tensorflow|neural-network|deep-learning|image-recognition,321
How to integrate Kinesis Video Stream with Kinesis Video Rekognition for dectecting persons,54554676,How to integrate Kinesis Video Stream with Kinesis Video Rekognition for dectecting persons,"<p>Using Raspberry pi got the live video streaming using Kinesis Video Streaming Parser library and want to process stream to Kinesis Video Rekognition for detecting persons.</p>

<p>Set the required details of ARN, got the video stream an set to Frame Viewer. Then trying to integrate Kinesis Video Stream with Rekognition.</p>",,1,0,,2019-02-06 13:21:43.327 UTC,1,2019-03-02 00:11:20.600 UTC,,,,,6555824,1,0,aws-lambda|amazon-kinesis|amazon-rekognition,35
How to fetch (extract) text from image which is in gallery and search that text - Android?,47089134,How to fetch (extract) text from image which is in gallery and search that text - Android?,"<p>I want to do that I have thousands of images on my phone and I want to fetch text from an image like below image: for example, i have above image on my phone and I want to fetch text ""Sample Source Code"" which is written in image. so how can we do that in android I have to try Google Vision API also gives sometimes correct text but sometimes not accurate. so is there any other option for this? <a href=""https://i.stack.imgur.com/sLEep.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sLEep.jpg"" alt="" ""></a></p>",,1,2,,2017-11-03 05:37:42.997 UTC,,2017-11-13 06:21:40.983 UTC,2017-11-03 06:12:05.780 UTC,,6774387,,3881656,1,1,java|android|android-image|google-vision,1739
Compare two images by using AWS Rekognition error,54015378,Compare two images by using AWS Rekognition error,"<p>I try to compare the two image which is in s3.
So I have completed the code by referring to the following:
<a href=""https://docs.aws.amazon.com/ko_kr/rekognition/latest/dg/faces-comparefaces.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/ko_kr/rekognition/latest/dg/faces-comparefaces.html</a></p>

<p>I made the IdentityPool with Role(S3 Full Access, Rekognition Full Access).</p>

<pre><code>try{

            CompareFacesRequest compareFacesRequest = new CompareFacesRequest().
                    withSourceImage(new Image().withS3Object(new S3Object().withName(photo1).withBucket(bucket))).
                    withTargetImage(new Image().withS3Object(new S3Object().withName(photo2).withBucket(bucket))).withSimilarityThreshold(80F);

            CompareFacesResult result = amazonRekognitionClient.compareFaces(compareFacesRequest);
            List&lt;CompareFacesMatch&gt; list = result.getFaceMatches();

            Log.d(TAG,""testing.."");

            if (!list.isEmpty()){

                Log.d(TAG,""face match"");

                for (CompareFacesMatch match:list){
                    Log.d(TAG,match.getFace() + """" + match.getSimilarity());
                }

            }else{

                Log.d(TAG,""face dismatch"");

            }

        }catch (Exception e){

            Log.d(TAG,""Exception occurs"");
            e.printStackTrace();

        }
</code></pre>

<p>But it makes that error.
com.amazonaws.services.rekognition.model.InvalidS3ObjectException: Unable to get object metadata from S3. Check object key, region and/or access permissions. (Service: AmazonRekognition; Status Code: 400; Error Code: InvalidS3ObjectException; Request ID: 2c4720e3-0e67-11e9-a286-7761b1c828e5)</p>

<p>I thought if I make a mistake of IAM, the app can't upload the file.
I try to upload the file with same credentialsProvider, upload success.</p>

<p>I don't think that's what happened because of permission. </p>

<p>S3 region is in Seoul, and Cognito IdentityPool region is AP_NORTHEAST_2</p>

<p>is there any information to get s3 object with Rekognition?</p>",54048917,1,0,,2019-01-03 01:48:02.983 UTC,,2019-01-05 04:11:42.747 UTC,,,,,6782402,1,0,amazon-s3|amazon-cognito|amazon-rekognition,41
Limited results with Google Cloud Vision api,47287749,Limited results with Google Cloud Vision api,"<p>I'm using Google Vision API, yet I've noticed that it is limited for the top 10 labels, and does not return results under 70% confidence.</p>

<p>Is there a setting or a way to receive results that are lower than the 70% threshold?</p>",,1,0,,2017-11-14 14:02:38.930 UTC,,2017-11-20 18:08:27.030 UTC,2017-11-15 03:36:51.167 UTC,,322020,,420581,1,0,google-cloud-platform|google-cloud-vision,250
Detect Table of image using cloud vision api,49974994,Detect Table of image using cloud vision api,"<p>I prepare some solution to detect text of the image, now I am getting bounding box of text, symbol, and language property.</p>

<p>Is there any way to getting table structure of documents using Google Vision API?</p>",,1,3,,2018-04-23 06:51:55.123 UTC,,2018-04-26 15:27:48.840 UTC,2018-04-23 12:19:55.043 UTC,,894763,,5700401,1,-1,image-processing|ibm-cloud|ibm-watson|google-cloud-vision,890
AWS Recognizing Faces in a Streaming Video Issue,52270452,AWS Recognizing Faces in a Streaming Video Issue,"<p>I am facing issue while implementing Amazon Rekognition. The error I am getting is:</p>

<ol>
<li>AWSRekognition class, createStreamProcessor API call always through the following error:</li>
</ol>

<blockquote>
  <p>Error Domain=com.amazonaws.AWSRekognitionErrorDomain Code=7  UserInfo={__type=InvalidParameterException, Logref=aadd2387-8289-44e7-b8f8-d72d52debed3, Message=Input stream limit exceeded for arn:aws:kinesisvideo:eu-west-1:549731499035:stream/FaceRecognitionDemo/1535630649185, Code=InvalidParameterException}”</p>
</blockquote>

<ol start=""2"">
<li>AWSKinesisRecorder class API submitAllRecords API call always through the following error:</li>
</ol>

<blockquote>
  <p>Error Domain=com.amazonaws.AWSKinesisErrorDomain Code=13  UserInfo={__type=ResourceNotFoundException, message=Stream FaceRecognitionDemo under account 549731499035 not found.}</p>
</blockquote>

<p>Due to these issue buffer data not submitted to kinesis video so that stream can start and start searching the face.</p>

<p>Any help appreciated?</p>",,1,0,,2018-09-11 07:07:51.253 UTC,,2018-09-21 21:38:35.353 UTC,2018-09-11 10:51:55.060 UTC,,6013019,,6013019,1,0,amazon-web-services|amazon-rekognition,118
IBM Visual Recognition Classifier status failed,37984641,IBM Visual Recognition Classifier status failed,"<p>I have the following IBM Watson Visual Recognition Python SDK for creating a simple classifier: </p>

<pre><code>with open(os.path.dirname(""/home/xxx/Desktop/Husky.zip/""), 'rb') as dogs, \ 
    open(os.path.dirname(""/home/xxx/Desktop/Husky.zip/""), 'rb') as cats:
    print(json.dumps(visual_recognition.create_classifier('Dogs Vs Cats',dogs_positive_examples=dogs,negative_examples=cats), indent=2))
</code></pre>

<p>The response with the new classifier ID and its status is as follows:</p>

<pre><code>{
  ""status"": ""training"", 
  ""name"": ""Dogs Vs Cats"", 
  ""created"": ""2016-06-23T06:30:00.115Z"", 
  ""classes"": [
    {
      ""class"": ""dogs""
    }
  ], 
  ""owner"": ""840ad7db-1e17-47bd-9961-fc43f35d2ad0"", 
  ""classifier_id"": ""DogsVsCats_250748237""
}
</code></pre>

<p>The training status shows failed. </p>

<p>print(json.dumps(visual_recognition.list_classifiers(), indent=4))</p>

<pre><code>{
    ""classifiers"": [
        {
            ""status"": ""failed"", 
            ""classifier_id"": ""DogsVsCats_250748237"", 
            ""name"": ""Dogs Vs Cats""
        }
    ]
}
</code></pre>

<p>What is the cause of this?</p>",38840285,2,1,,2016-06-23 07:14:16.450 UTC,,2016-08-09 01:45:18.390 UTC,2016-08-09 01:45:18.390 UTC,,5489858,,4453982,1,0,python-2.7|ibm-cloud|ibm-watson|visual-recognition,264
Cloud Vision API Not Returning Score for WebDetection.WebPage,45624819,Cloud Vision API Not Returning Score for WebDetection.WebPage,"<p>Does the Cloud Vision API return a score?  </p>

<p>public float getScore()
 Overall relevancy score for the web page.</p>

<p>The <a href=""http://googleapis.github.io/googleapis/java/proto-google-cloud-vision-v1/0.1.9/apidocs/com/google/cloud/vision/v1/WebDetection.WebPage.html#getScore--"" rel=""nofollow noreferrer"">documentation state</a>s that it does; however, I have not been able to get a score for any image I submit.  All queries return 0.0, which seems unlikely given the depth of the result list and human verified accuracy that the image does in fact reside on WebPage result.</p>

<p>Pleas advise. Thanks.</p>",,1,0,,2017-08-10 23:24:09.130 UTC,,2017-08-24 18:07:34.887 UTC,,,,,905031,1,0,google-cloud-vision,52
Microsoft Cognitive - Face API - Using persistedFaceId to Face to Face verification,55029634,Microsoft Cognitive - Face API - Using persistedFaceId to Face to Face verification,"<p>So, I intend to perform a face to face verification using Azure Face Api. In my case, what I need to do is verify if the face in the photo sent by the user is the same as the photo saved in database. My question is: can store faces in a faceList in a way that I can use the <strong>persistedFaceId</strong> for the faces coming from my database instead of performing a detect on the same picture just to get it's Id?</p>",55115691,1,0,,2019-03-06 18:10:20.357 UTC,,2019-03-12 06:54:24.767 UTC,,,,,10891170,1,1,azure|microsoft-cognitive|face-api,82
How to draw a dynamic line graph using json data?,48491815,How to draw a dynamic line graph using json data?,"<p>So basically i'm trying to draw a multi line graph from the results of microsoft emotion api result data.
<code>
anger: 0.00108685507
contempt: 0.00179950753
disgust: 0.0000793820145
fear: 0.00272498373
happiness: 0.0000412895
neutral: 0.9678982
sadness: 0.0132255116
surprise: 0.0131443078</code></p>

<p>This is the result data.So what i want is to have a line drawn each time a result is obtained.And the line should be drawn from a combination of these results.Each line should be drawn from 4 points as the results have 8 fields which can be paired up into 4 points. </p>",,0,4,,2018-01-28 22:02:58.667 UTC,,2018-01-28 22:02:58.667 UTC,,,,,9274031,1,0,json|graph,17
I get INVALID_ARGUMENT when I try to use Google Cloud Vision,39407269,I get INVALID_ARGUMENT when I try to use Google Cloud Vision,"<p>So I just want to detect text or labels from an image using the google cloud vision API. But When I run this code I always get: <code>com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request</code></p>

<p>But I don't know why...here is the full json output what I get:</p>

<pre><code>{
  ""code"": 400,
  ""errors"": [
    {
      ""domain"": ""global"",
      ""message"": ""Request Admission Denied."",
      ""reason"": ""badRequest""
    }
  ],
  ""message"": ""Request Admission Denied."",
  ""status"": ""INVALID_ARGUMENT""
}
</code></pre>

<p>My test code is here:</p>

<pre><code>import com.google.api.client.http.HttpTransport;
import com.google.api.client.http.javanet.NetHttpTransport;
import com.google.api.client.json.JsonFactory;
import com.google.api.client.json.gson.GsonFactory;
import com.google.api.services.vision.v1.Vision;
import com.google.api.services.vision.v1.VisionRequestInitializer;
import com.google.api.services.vision.v1.model.*;

import javax.imageio.ImageIO;
import java.awt.image.BufferedImage;
import java.awt.image.DataBufferByte;
import java.awt.image.WritableRaster;
import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;


public class GoogleDetection extends Detector {

    private static final String CLOUD_VISION_API_KEY = ""MY_API_KEY"";

    @Override
    String detect(String filePath) {
        HttpTransport httpTransport = new NetHttpTransport();
        JsonFactory jsonFactory = GsonFactory.getDefaultInstance();

        Vision.Builder builder = new Vision.Builder(httpTransport, jsonFactory, null);
        builder.setVisionRequestInitializer(new
                VisionRequestInitializer(CLOUD_VISION_API_KEY));
        Vision vision = builder.build();

        BatchAnnotateImagesRequest batchAnnotateImagesRequest =
                new BatchAnnotateImagesRequest();
        batchAnnotateImagesRequest.setRequests(new ArrayList&lt;AnnotateImageRequest&gt;() {{
            AnnotateImageRequest annotateImageRequest = new AnnotateImageRequest();

            // Image to bytes
            Image base64EncodedImage = new Image();
            BufferedImage bufferedImage = null;
            File imgPath = new File(filePath);
            try {
                bufferedImage = ImageIO.read(imgPath);
            } catch (IOException e) {
                e.printStackTrace();
            }
            WritableRaster raster = bufferedImage.getRaster();
            DataBufferByte data = (DataBufferByte) raster.getDataBuffer();

            byte[] imageBytes = data.getData();

            // Base64 encode the JPEG
            base64EncodedImage.encodeContent(imageBytes);
            annotateImageRequest.setImage(base64EncodedImage);


            // add the features we want
            annotateImageRequest.setFeatures(new ArrayList&lt;Feature&gt;() {{
                Feature labelDetection = new Feature();
                labelDetection.setType(""LABEL_DETECTION"");
                labelDetection.setMaxResults(10);
                add(labelDetection);
            }});

            // Add the list of one thing to the request
            add(annotateImageRequest);
        }});

        Vision.Images.Annotate annotateRequest;
        BatchAnnotateImagesResponse response = null;
        try {
            annotateRequest = vision.images().annotate(batchAnnotateImagesRequest);
            // Due to a bug: requests to Vision API containing large images fail when GZipped.
            annotateRequest.setDisableGZipContent(true);

            response = annotateRequest.execute();
            System.out.println(response.toString());
        } catch (IOException e) {
            e.printStackTrace();
        }

        return convertResponseToString(response);
    }

    private String convertResponseToString(BatchAnnotateImagesResponse response) {
        String message = ""I found these things:\n\n"";

        List&lt;EntityAnnotation&gt; labels = response.getResponses().get(0).getLabelAnnotations();
        if (labels != null) {
            for (EntityAnnotation label : labels) {
                message += String.format(""%.3f: %s"", label.getScore(), label.getDescription());
                message += ""\n"";
            }
        } else {
            message += ""nothing"";
        }

        return message;
    }
}
</code></pre>

<p>So the question is.. what is wrong with this code?</p>",,1,0,,2016-09-09 08:36:59.770 UTC,,2017-02-11 19:24:04.823 UTC,,,,,6649170,1,0,java|ocr|google-cloud-vision,376
Microsoft ProjectOxford Face API Error: System.UriFormatException,53918980,Microsoft ProjectOxford Face API Error: System.UriFormatException,"<p>I was following <a href=""https://docs.microsoft.com/en-us/azure/cognitive-services/face/face-api-how-to-topics/howtoidentifyfacesinimage"" rel=""nofollow noreferrer"">this documentation</a> for using the Microsoft Face API to identify faces in an image in Visual Studio when I got the following error printed in the Console:</p>

<blockquote>
  <p>Error adding Person to Group Exception of type 'Microsoft.ProjectOxford.Face.FaceAPIException' was thrown.</p>
</blockquote>

<p>The exception is printed when the following function to add a person to an existing person group is called:</p>

<pre><code>public async void AddPersonToGroup(string personGroupId, string name, string pathImage){
    try{
        await faceServiceClient.GetPersonGroupAsync(personGroupId);
        CreatePersonResult person = await faceServiceClient.CreatePersonAsync(personGroupId, name);

        foreach (var imgPath in Directory.GetFiles(pathImage, ""*.jpg"")) {
            using (Stream s = File.OpenRead(imgPath)) {
                await faceServiceClient.AddPersonFaceAsync(personGroupId, person.PersonId, s);
            }
        }
    } catch (Exception ex){
        //Below is where the error was printed.
        Console.WriteLine(""Error adding Person to Group "" + ex.Message);
    }
}
</code></pre>

<p>This is how I am calling <code>AddPersonToGroup</code> in the main method:</p>

<pre><code>new Program().AddPersonToGroup(""actor"", ""Tom Cruise"", @""C:\Users\ishaa\Documents\Face_Pictures\Tom_Cruise\"");
</code></pre>

<hr>

<p>I tried searching up this error in Google and came across <a href=""https://stackoverflow.com/questions/47885650/face-api-detectasync-error"">this SO question</a>, but that answer did not work for me. <em>(Their answer was to pass in the subscription key and endpoint for the <code>FaceServiceClient</code> constructor.)</em></p>

<p><strong>Would anyone be able to provide any insight into why this error is occurring?</strong> I have been unable to figure out what is causing it, but I believe it may have to do with
<code>await faceServiceClient.GetPersonGroupAsync(personGroupId);</code>. I also read that it may be due to the Cognitive Services pricing plan I have chosen. However, the free one which I am using allows for 20 transactions per minute and I am only trying to add 9 pictures for 3 different people. </p>",53950838,1,4,,2018-12-25 02:46:47.523 UTC,0,2018-12-28 01:34:10.667 UTC,2018-12-25 03:07:20.680 UTC,,10589041,,10589041,1,0,c#|visual-studio|exception|microsoft-cognitive|face-api,123
Stored Video Analysis takes too long,55947906,Stored Video Analysis takes too long,"<p>I am using the amazon rekognition API to analyse my video to find and Search faces.
A one minute video has been on processing for nearly an hour now. But have not received any result .
Is this normal ?</p>",,1,0,,2019-05-02 07:52:51.437 UTC,,2019-05-02 09:06:29.270 UTC,,,,,2856292,1,0,amazon-rekognition,11
Google Vision API: Cannot read property of undefined,53519748,Google Vision API: Cannot read property of undefined,"<p>I have problem with my properties from my json response body.</p>

<p>The reponse I get from the call is from Google Vision API.
The property 'description' is undefined but will show up when logged in the console sometimes this will works.</p>

<pre><code> this.http.post(this.apiUri, this.bodyAPI).subscribe((response : any) =&gt; 

{
  //console.log(response.responses[0].labelAnnotations[1].description);
  //console.log(this.photoTasks[this.activeTask].task_solution);
    if (response.responses[0].labelAnnotations[0].description? == ""magenta""){
      console.log(""great, "" + response.responses[0].labelAnnotations[0].topicality * 100 + ""% accuracy"");
    }
    else if (response.responses[0].labelAnnotations[1].description == ""magenta""){
      console.log(""great, "" + response.responses[0].labelAnnotations[1].topicality * 100 + ""% accuracy"");
    }
    else if (response.responses[0].labelAnnotations[2].description == this.photoTasks[this.activeTask].task_solution){
      console.log(""great, "" + response.responses[0].labelAnnotations[2].topicality * 100 + ""% accuracy"");
    }
    else if (response.responses[0].labelAnnotations[3].description == this.photoTasks[this.activeTask].task_solution){
      console.log(""great, "" + response.responses[0].labelAnnotations[3].topicality * 100 + ""% accuracy"");
    }
    // else if (response.responses[0].labelAnnotations[4].description == this.photoTasks[this.activeTask].task_solution){
    //   console.log(""great, "" + response.responses[0].labelAnnotations[4].topicality * 100 + ""% accuracy"");
    // }
    else{
      console.log(""you should try again..."");
    }

}  
  //console.log(response.responses[0].labelAnnotations);
) ;
}
</code></pre>

<p>This is the error:</p>

<pre><code>core.js:1673 ERROR TypeError: Cannot read property 'description' of undefined
at SafeSubscriber._next (cameraintent.component.ts:94)
</code></pre>",,1,1,,2018-11-28 12:42:13.513 UTC,,2018-12-20 12:46:18.963 UTC,2018-12-20 12:46:18.963 UTC,,7757976,,10716814,1,0,angular|rest|google-vision,42
What method does Microsoft Face API use for face comparison?,45720763,What method does Microsoft Face API use for face comparison?,"<p>I've been working on a facial detection and recognition program for a few days now in OpenCV using Eigen/Fisher/LBPH FaceRecognizers that will compare the faces in two photos using the 3 listed recognizers and return a confidence value that the faces are the same person or not.</p>

<p>While I've been able to get everything working, the results and recognition rates have not been inspiring, especially when you look at a service like Microsoft Face API (which I cannot use due to privacy concerns) at this url: <a href=""https://azure.microsoft.com/en-ca/services/cognitive-services/face/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-ca/services/cognitive-services/face/</a></p>

<p>Does anyone here have any idea what method(s) Microsoft is using in their Face verification on the above URL? It's <em>exactly</em> what I need (my tests have shown it to be extremely accurate for my scenario), aside from the fact that it's an API and not an SDK.</p>",,0,3,,2017-08-16 18:36:53.807 UTC,,2017-08-16 18:36:53.807 UTC,,,,,5998776,1,0,opencv|computer-vision|emgucv|face-recognition|azure-cognitive-services,576
Google Vision API does not recognize single digits,49386572,Google Vision API does not recognize single digits,"<p>I have a project that make use of Google Vision API DOCUMENT_TEXT_DETECTION in order to extract text from document images.</p>

<p>Often the API has troubles in recognizing single digits, as you can see in this image:</p>

<p><a href=""https://i.stack.imgur.com/qlWgG.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/qlWgG.jpg"" alt=""enter image description here""></a></p>

<p>I suppose that the problem could be related to some algorithm of noise removal, that recognizes isolated single digits as noise. Is there a way to improve Vision response in these situations? (for example managing noise threshold or others parameters)</p>

<p>At other times Vision confuses digits with letters:</p>

<p><a href=""https://i.stack.imgur.com/jUORu.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/jUORu.jpg"" alt=""enter image description here""></a></p>

<p>But if I specify as parameter languageHints = 'en' or 'mt' these digits are ignored by the ocr. Is there a way to force the recognition of digits or latin characters?</p>",,1,5,,2018-03-20 14:12:43.957 UTC,3,2019-05-28 17:23:49.097 UTC,,,,,7219743,1,10,google-cloud-platform|ocr|google-cloud-vision|text-recognition,853
Error in Watson Visual Recognition service: Cannot execute learning task,55956173,Error in Watson Visual Recognition service: Cannot execute learning task,"<p>I am repeatedly getting the error:""Error in Watson Visual Recognition service: Cannot execute learning task. : this plan instance can have only 2 custom classifier(s), and 2 already exist."" It will not allow me to train my model. Can anyone help? Thank you in advance!</p>",,1,0,,2019-05-02 15:56:09.387 UTC,,2019-05-02 17:22:03.537 UTC,,,,,11442697,1,0,ibm-watson|visual-recognition,23
"How to handle overlapping suffixes, prefix for s3 event notification",51918848,"How to handle overlapping suffixes, prefix for s3 event notification","<p>I have two AWS lambda functions which are processing all images uploaded to an S3 bucket(One is for creating thumbnail and another is for image moderation[rekognition]).</p>

<p>While I am doing it, I found it invalid to add event notifications with overlapping prefix and suffix.</p>

<p>For example, let's assume that I want to set the two event notification like the below.</p>

<pre><code>event type: PUT | prefix: root | suffix: .jpg | send to: lambda-A
event type: PUT | prefix: root | suffix: .jpg | send to: lambda-B

# Error message: Cannot have overlapping suffixes in two rules if the prefixes are overlapping for the same event type.
</code></pre>

<p>If it is not available I think there must be a kind of pattern commonly used in this case(for instance, making a proxy lambda to call the two lambdas passing the same event notification.)</p>

<p>What is the best way to handle the case?</p>",51919142,1,0,,2018-08-19 14:49:42.043 UTC,,2018-08-19 15:25:23.150 UTC,,,,,3523935,1,0,amazon-web-services|amazon-s3|aws-lambda,409
Pulling the 'ExternalImageId' data when running search_faces_by_image,52252580,Pulling the 'ExternalImageId' data when running search_faces_by_image,"<p>I'm fairly new to AWS and for the past week, been following all the helpful documentation on the site.</p>

<p>I am currently stuck on bring unable to pull the External Image Id data from a Reko collection after a 'search face by image', I just need to be able to put that data into a variable or to print it, does anybody know how I could do that?</p>

<p>Basically, this is my code:</p>

<pre><code>import boto3
</code></pre>

<p>if <strong>name</strong> == ""<strong>main</strong>"":</p>

<pre><code>bucket = 'bucketname'
collectionId = 'collectionname'
fileName = 'test.jpg'
threshold = 90
maxFaces = 2

admin = 'test'

targetFile = ""%sTarget.jpg"" % admin
imageTarget = open(targetFile, 'rb')

client = boto3.client('rekognition')
response = client.search_faces_by_image(CollectionId=collectionId,
                                        Image={'Bytes': imageTarget.read()},
                                        FaceMatchThreshold=threshold,
                                        MaxFaces=maxFaces)

faceMatches = response['FaceMatches']
print ('Matching faces')
for match in faceMatches:
    print ('FaceId:' + match['Face']['FaceId'])
    print ('Similarity: ' + ""{:.2f}"".format(match['Similarity']) + ""%"")
</code></pre>

<p>at the end of it, I receive:</p>

<pre><code>Matching faces
FaceId:8081ad90-b3bf-47e0-9745-dfb5a530a1a7
Similarity: 96.12%

Process finished with exit code 0
</code></pre>

<p>What I need is the External Image Id instead of the FaceId.</p>

<p>Thanks!</p>",,0,4,,2018-09-10 06:59:34.217 UTC,,2018-09-10 09:18:09.530 UTC,2018-09-10 09:18:09.530 UTC,,10340634,,10340634,1,0,python|amazon-web-services,23
Gem conflict error,46327110,Gem conflict error,"<p>I have automation tests that I run through RubyMine fails due to conflict error. Although it runs completely fine through the command line.
Following is the error:</p>

<pre><code>Unable to activate aws-sdk-v1-1.67.0, because json-2.0.2 conflicts with json (~&gt; 1.4) (Gem::ConflictError)
</code></pre>

<p>Also I am new to ruby so following is the list of gems:</p>

<pre><code>actioncable (5.1.4)
actionmailer (5.1.4)
actionpack (5.1.4)
actionview (5.1.4)
activejob (5.1.4)
activemodel (5.1.4)
activerecord (5.1.4)
activesupport (5.1.4)
addressable (2.5.2)
appium_lib (9.6.1)
arel (8.0.0)
ast (2.3.0)
avro (1.8.2, 1.7.7)
awesome_print (1.8.0)
aws-partitions (1.21.0)
aws-sdk (1.67.0)
aws-sdk-acm (1.0.0)
aws-sdk-apigateway (1.1.0)
aws-sdk-applicationautoscaling (1.3.0)
aws-sdk-applicationdiscoveryservice (1.0.0)
aws-sdk-appstream (1.0.0)
aws-sdk-athena (1.0.0)
aws-sdk-autoscaling (1.3.0)
aws-sdk-batch (1.1.0)
aws-sdk-budgets (1.1.0)
aws-sdk-clouddirectory (1.0.0)
aws-sdk-cloudformation (1.1.0)
aws-sdk-cloudfront (1.0.0)
aws-sdk-cloudhsm (1.1.0)
aws-sdk-cloudhsmv2 (1.0.0)
aws-sdk-cloudsearch (1.0.0)
aws-sdk-cloudsearchdomain (1.0.0)
aws-sdk-cloudtrail (1.0.0)
aws-sdk-cloudwatch (1.2.0)
aws-sdk-cloudwatchevents (1.1.0)
aws-sdk-cloudwatchlogs (1.1.0)
aws-sdk-codebuild (1.2.0)
aws-sdk-codecommit (1.0.0)
aws-sdk-codedeploy (1.0.0)
aws-sdk-codepipeline (1.0.0)
aws-sdk-codestar (1.1.0)
aws-sdk-cognitoidentity (1.0.0)
aws-sdk-cognitoidentityprovider (1.0.0)
aws-sdk-cognitosync (1.0.0)
aws-sdk-configservice (1.0.0)
aws-sdk-core (3.5.0)
aws-sdk-costandusagereportservice (1.0.0)
aws-sdk-databasemigrationservice (1.1.0)
aws-sdk-datapipeline (1.0.0)
aws-sdk-dax (1.0.0)
aws-sdk-devicefarm (1.2.0)
aws-sdk-directconnect (1.0.0)
aws-sdk-directoryservice (1.0.0)
aws-sdk-dynamodb (1.2.0)
aws-sdk-dynamodbstreams (1.0.0)
aws-sdk-ec2 (1.7.0)
aws-sdk-ecr (1.1.0)
aws-sdk-ecs (1.1.0)
aws-sdk-efs (1.0.0)
aws-sdk-elasticache (1.1.0)
aws-sdk-elasticbeanstalk (1.1.0)
aws-sdk-elasticloadbalancing (1.1.0)
aws-sdk-elasticloadbalancingv2 (1.3.0)
aws-sdk-elasticsearchservice (1.0.0)
aws-sdk-elastictranscoder (1.0.0)
aws-sdk-emr (1.0.0)
aws-sdk-firehose (1.0.0)
aws-sdk-gamelift (1.1.0)
aws-sdk-glacier (1.4.0)
aws-sdk-glue (1.0.0)
aws-sdk-greengrass (1.0.0)
aws-sdk-health (1.0.0)
aws-sdk-iam (1.3.0)
aws-sdk-importexport (1.0.0)
aws-sdk-inspector (1.1.0)
aws-sdk-iot (1.0.0)
aws-sdk-iotdataplane (1.0.0)
aws-sdk-kinesis (1.0.0)
aws-sdk-kinesisanalytics (1.0.0)
aws-sdk-kms (1.2.0)
aws-sdk-lambda (1.0.0)
aws-sdk-lambdapreview (1.0.0)
aws-sdk-lex (1.1.0)
aws-sdk-lexmodelbuildingservice (1.2.0)
aws-sdk-lightsail (1.0.0)
aws-sdk-machinelearning (1.0.0)
aws-sdk-marketplacecommerceanalytics (1.0.0)
aws-sdk-marketplaceentitlementservice (1.0.0)
aws-sdk-marketplacemetering (1.0.0)
aws-sdk-migrationhub (1.0.0)
aws-sdk-mobile (1.0.0)
aws-sdk-mturk (1.0.0)
aws-sdk-opsworks (1.1.0)
aws-sdk-opsworkscm (1.0.0)
aws-sdk-organizations (1.3.0)
aws-sdk-pinpoint (1.0.0)
aws-sdk-polly (1.2.0)
aws-sdk-rds (1.2.0)
aws-sdk-redshift (1.0.0)
aws-sdk-rekognition (1.0.0)
aws-sdk-resourcegroupstaggingapi (1.0.0)
aws-sdk-resources (3.1.0)
aws-sdk-route53 (1.2.0)
aws-sdk-route53domains (1.0.0)
aws-sdk-s3 (1.4.0)
aws-sdk-servicecatalog (1.1.0)
aws-sdk-ses (1.2.0)
aws-sdk-shield (1.0.0)
aws-sdk-simpledb (1.0.0)
aws-sdk-sms (1.0.0)
aws-sdk-snowball (1.1.0)
aws-sdk-sns (1.1.0)
aws-sdk-sqs (1.1.0)
aws-sdk-ssm (1.1.0)
aws-sdk-states (1.0.0)
aws-sdk-storagegateway (1.1.0)
aws-sdk-support (1.0.0)
aws-sdk-swf (1.0.0)
aws-sdk-v1 (1.67.0)
aws-sdk-waf (1.1.0)
aws-sdk-wafregional (1.1.0)
aws-sdk-workdocs (1.0.0)
aws-sdk-workspaces (1.0.0)
aws-sdk-xray (1.0.0)
aws-sigv2 (1.0.1)
aws-sigv4 (1.0.2)
backports (3.8.0)
bigdecimal (1.3.2, default: 1.3.0)
binding_of_caller (0.7.2)
builder (3.2.3)
bundler (1.16.0.pre.2, 1.15.4)
capybara (2.15.1)
childprocess (0.7.1)
cliver (0.3.2)
coderay (1.1.2)
common-tools (0.0.6)
concurrent-ruby (1.0.5)
cucumber (2.4.0)
cucumber-core (2.0.0, 1.5.0)
cucumber-wire (0.0.1)
debug_inspector (0.0.3)
did_you_mean (1.1.2, 1.1.0)
diff-lcs (1.3)
domain_name (0.5.20170404)
erubi (1.6.1)
ezlog (0.0.2)
ffi (1.9.18)
gherkin (4.1.3)
globalid (0.4.0)
http-cookie (1.0.3)
i18n (0.8.6)
io-console (default: 0.4.6)
jmespath (1.3.1)
**json (default: 2.0.2, 1.8.6)**
kafka-handler (0.1.51)
loofah (2.0.3)
mail (2.6.6)
method_source (0.8.2)
mime-types (3.1)
mime-types-data (3.2016.0521)
mini_mime (0.1.4)
mini_portile2 (2.3.0, 2.2.0)
minitest (5.10.3, 5.10.1)
mobot (2013.08.1377443804, 0.2.10)
multi_json (1.12.2)
multi_test (0.1.2)
mustermann (1.0.1)
mysql2 (0.4.9)
net-telnet (0.1.1)
netrc (0.11.0)
nio4r (2.1.0)
nokogiri (1.8.1, 1.8.0)
nori (2.6.0)
openssl (default: 2.0.3)
parallel (1.12.0)
parser (2.4.0.0)
poltergeist (1.16.0)
poseidon (0.0.5)
poseidon_cluster (0.3.3, 0.3.0)
power_assert (1.1.0, 0.4.1)
powerpack (0.1.1)
pry (0.10.4)
pry-nav (0.2.4)
pry-stack_explorer (0.4.9.2)
psych (default: 2.2.2)
public_suffix (3.0.0)
rack (2.0.3)
rack-protection (2.0.0)
rack-test (0.7.0)
rails (5.1.4)
rails-dom-testing (2.0.3)
rails-html-sanitizer (1.0.3)
railties (5.1.4)
rainbow (2.2.2)
rake (12.1.0, 12.0.0)
rdoc (5.1.0, default: 5.0.0)
rest-client (2.0.2)
rspec (3.6.0)
rspec-core (3.6.0)
rspec-expectations (3.6.0)
rspec-mocks (3.6.0)
rspec-support (3.6.0)
rubocop (0.50.0, 0.49.1)
ruby-progressbar (1.8.3)
rubygems-update (2.6.13)
rubyzip (1.2.1)
s3-handler (0.2.5)
selenium-webdriver (3.5.2)
sinatra (2.0.0)
slop (4.5.0, 3.6.0)
sprockets (3.7.1)
sprockets-rails (3.2.1)
test-unit (3.2.5, 3.2.3)
thor (0.20.0)
thread_safe (0.3.6)
tilt (2.0.8)
tomlrb (1.2.4)
tzinfo (1.2.3)
unf (0.1.4)
unf_ext (0.0.7.4)
unicode-display_width (1.3.0)
watir-webdriver (0.9.9)
websocket-driver (0.7.0, 0.6.5)
websocket-extensions (0.1.2)
xmlrpc (0.3.0, 0.2.1)
xpath (2.1.0)
zk (1.9.6)
zookeeper (1.4.11)
</code></pre>

<p>My concern is how to remove default tag from the JSON version 2.0.2. Can anyone help me resolve this issue or suggest ways to remove default gem JSON.</p>",,0,0,,2017-09-20 16:19:19.880 UTC,1,2017-09-21 05:02:11.950 UTC,2017-09-21 05:02:11.950 UTC,,3190347,,1947573,1,3,android|ruby-on-rails|rubygems|bundler|android-testing,571
NodeJS google-cloud/vision: methods does not perform anything,51563897,NodeJS google-cloud/vision: methods does not perform anything,"<p>I'm trying to use Google Cloud Vision in a NodeJS app. Following the <a href=""https://github.com/googleapis/nodejs-vision#using-the-client-library"" rel=""nofollow noreferrer"">client library example</a>:</p>

<pre><code>console.log('started')
// Performs label detection on the image file
client
  .labelDetection('./mydocument.jpg')
  .then(results =&gt; {
    console.log('checked')
    const labels = results[0].labelAnnotations

    console.log('Labels:')
    labels.forEach(label =&gt; console.log(label.description))
  }, err =&gt; {
    console.error('ERROR:', err)
  })
  .catch(err =&gt; {
    console.error('ERROR:', err)
  })
</code></pre>

<p>Only the 'started' is printed in the console. It does not enter in either success or failure functions. Looking at the Google's Dashboard, it shows the API being consumed (there is a real time graph that updates when my nodejs app runs).
It seems the endpoint does not return anything and there is no timeout. But I can't find anything in docs, Stack Overflow or GitHub issues. Any clue? </p>",,2,4,,2018-07-27 18:51:08.520 UTC,,2018-10-23 19:49:58.550 UTC,2018-07-27 18:57:46.983 UTC,,9337071,,640053,1,2,javascript|node.js|google-api|google-cloud-vision,91
google cloud vision api quickstart error opening file,51195006,google cloud vision api quickstart error opening file,"<p>I am following the following Google Cloud Vision quickstart: <br/>
<a href=""https://cloud.google.com/vision/docs/quickstart"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/quickstart</a> 
<br/>This is using the API Explorer, and I get </p>

<blockquote>
  <p>Error Opening File</p>
</blockquote>

<p>I have created a bucket named vision2018, and checked Share Publicly for the file.
My portion of the request related to the file is:</p>

<pre><code>""image"":
{
""source"":
{
""imageUri"":""gs://vision2018/demo-image.jpg""
}
}
</code></pre>

<p>The response I get is:</p>

<pre><code>{
 ""responses"": [
  {
   ""error"": {
    ""code"": 5,
    ""message"": ""Error opening file: gs://vision2018/demo-image.jpg\"".""
   }
  }
 ]
}

}
</code></pre>

<p>What do I need to specify in order to access files in my GCP storage?</p>

<p>Alternatively, I read other Stack Overflows that talk about GOOGLE_APPLICATION_CREDENTIALS, Simple API Key, and ""Create Service account key and download the key in JSON format"", ...  but these seem to be giving commands in the shell, which this quickstart doesn't even open.
Is there initial setup assumed prior to the quickstart?</p>

<p>I am not ready to call the api from code</p>",51206364,3,0,,2018-07-05 15:24:28.333 UTC,,2019-04-19 23:52:55.083 UTC,2018-07-05 17:16:59.730 UTC,,4358339,,10037876,1,0,google-cloud-vision|google-apis-explorer,739
Use Azure custom-vision trained model with tensorflow.js,49840929,Use Azure custom-vision trained model with tensorflow.js,"<p>I've trained a model with Azure Custom Vision and downloaded the TensorFlow files for Android 
(see: <a href=""https://docs.microsoft.com/en-au/azure/cognitive-services/custom-vision-service/export-your-model"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-au/azure/cognitive-services/custom-vision-service/export-your-model</a>). How can I use this with <a href=""https://js.tensorflow.org"" rel=""nofollow noreferrer"">tensorflow.js</a>?</p>

<p>I need a model (pb file) and weights (json file). However Azure gives me a .pb and a textfile with tags. </p>

<p>From my research I also understand that there are also different pb files, but I can't find which type Azure Custom Vision exports. </p>

<p>I found the <a href=""https://github.com/tensorflow/tfjs-converter"" rel=""nofollow noreferrer"">tfjs converter</a>. This is to convert a TensorFlow SavedModel (is the *.pb file from Azure a SavedModel?) or Keras model to a web-friendly format. However I need to fill in ""output_node_names"" (how do I get these?). I'm also not 100% sure if my pb file for Android is equal to a ""tf_saved_model"".</p>

<p>I hope someone has a tip or a starting point.</p>",,1,0,,2018-04-15 10:34:00.577 UTC,3,2019-02-25 09:59:45.240 UTC,2018-06-01 09:28:46.340 UTC,,1021819,,8051909,1,3,microsoft-cognitive|azure-cognitive-services|tensorflow.js,310
OCR PDF Files Using Google Cloud Vision?,52343909,OCR PDF Files Using Google Cloud Vision?,"<p>Are there currently any services or software tools that use Google Cloud Vision as backend for OCRing scanned PDF files?</p>

<p>If not, how would one be able to use Google Cloud Vision to turn PDFs into OCRed PDFs? As far as I know, Cloud Vision currently supports PDF files, but it will output recognized text only as a JSON file. So it seems one would need to do the additional step of placing this converted text on top of the image inside the PDF outside of Google Cloud Vision, in a separate step.</p>

<p>Background:</p>

<p>I often have to convert scanned-document PDF files into PDF files containing an OCRed text layer. So far, I've been using Software like OCRKit or ABBYY FineReader. I tested the accuracy of these solutions against the text recognition abilities of Google Cloud Vision, and the latter came out far ahead.</p>",,1,1,,2018-09-15 10:40:51.883 UTC,,2018-11-07 17:50:58.830 UTC,,,,,10367380,1,0,pdf|pdf-generation|ocr|google-cloud-vision,338
Can not access properties of VisionCloudTextRecognizerOptions class of Firebase/MLVision on iOS Project,53109098,Can not access properties of VisionCloudTextRecognizerOptions class of Firebase/MLVision on iOS Project,"<p>I was trying to create an ios app for text recognition with Google Vision text recognition.</p>

<p>I had integrated all the required pods into my project as mentioned in </p>

<pre><code>https://firebase.google.com/docs/ml-kit/ios/recognize-text
</code></pre>

<p>In order to increase the detection accuracy, I tried to access the languageHints property of VisionCloudTextRecognizerOptions class but I can not understand why i could not access that property of this class. </p>

<p>Whenever I create an instance of that class and with a variable and try to access the properties of that class it there is a <strong><em>Red error</em></strong> indication at the line where I try to access the properties with the instance variable of the class and a <strong><em>GRAY ERROR</em></strong> indication at the top of the <strong><em>ViewController Class</em></strong> </p>

<pre><code> For reference, I am adding the screenshot of both my error message and google 
 MLkit documentation and also the code. Any help for this problem will be 
 appreciated. I am using Xcode 10.1 and all pods are working well. 
</code></pre>

<p>The ERROR MESSAGE
<a href=""https://i.stack.imgur.com/EMTIz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EMTIz.png"" alt="" The ERROR MESSAGE ""></a></p>

<p>Google ML Documentation
<a href=""https://i.stack.imgur.com/kjZSR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kjZSR.png"" alt=""Google ML Documentation ""></a></p>

<p>In that documentation, they have used let and I also checked with that but every time same problem.</p>

<p>Here is the code also:</p>

<pre><code>  import UIKit
  import Firebase


  class ViewController: UIViewController, UIImagePickerControllerDelegate, 
  UINavigationControllerDelegate {


private lazy var vision = Vision.vision()
private lazy var textRecognizer = vision.onDeviceTextRecognizer()


private lazy var options = VisionCloudTextRecognizerOptions()
     options.languageHints = [""en"", ""hi""]


lazy var textRecognizerCloud = Vision.vision().cloudTextRecognizer()

@IBOutlet weak var imagePickView: UIImageView!

@IBAction func imagePick(_ sender: UIButton) { let image = UIImagePickerController()
    image.delegate = self

    image.sourceType = UIImagePickerController.SourceType.photoLibrary

    image.allowsEditing = false

    self.present(image, animated: true)
}


func imagePickerController(_ picker: UIImagePickerController, didFinishPickingMediaWithInfo info: [UIImagePickerController.InfoKey : Any])
{
    if let image = info[UIImagePickerController.InfoKey.originalImage] as? UIImage
    {
        imagePickView.image = image


    }
    else
    {
        //Error message
    }


    let visionImage = VisionImage(image: imagePickView.image!)
    textRecognizerCloud.process(visionImage) { result, error in
        guard error == nil, let result = result else {
            // ...
            return
        }

        // Recognized text
        print(""the result"")
        print(result.text)
    }
    self.dismiss(animated: true, completion: nil)

}

override func viewDidLoad() {
    super.viewDidLoad()
    // Do any additional setup after loading the view, typically from a nib.
}

}
</code></pre>",,0,0,,2018-11-01 20:45:08.030 UTC,,2018-11-01 20:45:08.030 UTC,,,,,6311080,1,0,ios|tensorflow|google-vision|text-recognition|firebase-mlkit,80
Microsoft Emotion API implementation Error,41167490,Microsoft Emotion API implementation Error,"<p>I am trying to implement Microsoft emotion api in C# using code available on github. I followed all the steps given in <a href=""https://www.microsoft.com/cognitive-services/en-us/Emotion-api/documentation/GetStarted"" rel=""nofollow noreferrer"">Microsoft Cognitive Service</a>. I have 3 errors</p>

<pre><code>Error : The tag 'VideoResultControl' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'

Error: The tag 'SampleScenarios' does not exist in XML namespace 'clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary'.

Error NuGet Package restore failed for project EmotionAPI-WPF-Samples: The specified path, file name, or both are too long. The fully qualified file name must be less than 260 characters, and the directory name must be less than 248 characters
</code></pre>

<p>I found a similar question asked <a href=""https://stackoverflow.com/questions/40258634/severitycodedescriptionprojectfilelinesuppression-state-error-the-tag-vid"">here</a> (except for the change in the third question) but no answers.</p>

<p>I tried deleting the <code>.suo</code> file to fix the last error, but no luck</p>",41169276,1,0,,2016-12-15 15:13:11.590 UTC,,2016-12-15 17:11:44.250 UTC,2017-05-23 12:02:58.590 UTC,,-1,,2126771,1,1,c#|nuget-package|microsoft-cognitive,179
Problems running node.js in browser,35951874,Problems running node.js in browser,"<p>I'm trying to run an app with node.js functions my browser. I checked in the terminal and my javascript file which includes node.js functions runs well. However when I run the html file which is connected to the javascript file the browser returns an error which says the require function is not defined. I understand that this is because the browser can't run node.js alone</p>

<p>The node.js code I'm trying to run is to access a Watson visual recognition api:</p>

<pre><code>var watson = require('./node_modules/watson-developer-cloud');

watson.visual_recognition({
    username : '49f5d504-9387-45c6-9fda-9b58a9afc209',
    password : 'IITqAn0VPaFr',
    version : 'v2-beta',
    version_date : '2015-12-02'
}).listClassifiers({}, function(err, response) {
    if (err){
        console.log(err);
    } else {
        console.log(JSON.stringify(response, null, 2));
    }
});
</code></pre>

<p>I know I have all of the required files because the file runs in the terminal. Therefore I proceeded to include:</p>

<pre><code>&lt;script src =""https://cdn.socket.io/socket.io-1.4.5.js""&gt;&lt;/script&gt;
</code></pre>

<p>in my index.html before connecting my javascript file. </p>

<p>However my javascript file still returns the same error that the require function is not defined. Am I doing something wrong? Is there any way I could run this javascript file in the browser that has node.js but specifically without using browserify (which caused me some directory problems in the past)?</p>",35951975,2,4,,2016-03-11 23:43:15.270 UTC,,2016-03-12 03:18:25.120 UTC,2016-03-12 03:18:25.120 UTC,,1946501,,4799495,1,2,javascript|html|node.js,959
Converting words table from image to text Vision API,52172303,Converting words table from image to text Vision API,"<p><a href=""https://i.stack.imgur.com/SbHjl.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SbHjl.jpg"" alt=""Example table""></a></p>

<p>So using Google's Vision API I'm trying to convert this table using Nodejs. It would be best if the result would be an array like <code>[hi: bonjour, bye: au revoir ...]</code>. Now the problem I'm facing is that I only get the words and their coordinates back from Google when I upload this image. Using some kind of hacky solution I managed to merge the words. For example: I managed to merge 'au' and 'revoir' to 'au revoir', but the solution I have is absolutely not solid. </p>

<p>Does someone have a simple solution to this problem? Im afraid I'm thinking way too difficult, but I cannot find a lot of examples online. </p>

<p>Any help would be greatly appreciated.</p>

<p>My current code: <a href=""https://pastebin.com/jY5jDrqD"" rel=""nofollow noreferrer"">https://pastebin.com/jY5jDrqD</a> (yes it's a mess and not very solid)</p>",,1,0,,2018-09-04 18:44:56.573 UTC,,2018-09-04 21:14:36.863 UTC,,,,,4183724,1,0,node.js|ocr|vision,40
Watson visual recognition API update,43910100,Watson visual recognition API update,"<p>It seems to be no more possible to associate public IMages of IBM Cloud Object Storage with Watson visual recognition. something has been changed in the type of calls between the 2 services. 
My code below use to work but know it says there is no ""images founds"" . </p>

<pre><code>import json
from os.path import join, dirname
from os import environ
import sys
import os 
import boto3
import pprint
from boto3 import client
from botocore.utils import fix_s3_host
from watson_developer_cloud import VisualRecognitionV3


param_1= ""MY S3 KEY""
param_2= ""MY S3 SECRET KEY ""
param_3= ""https://s3-api.us-geo.objectstorage.softlayer.net""  
param_4= ""MY BUCKET ""
param_5= ""MY WATSON API KEY ""

#The Name of my image I want to analyse that is currently in my bucket and is made public
objectNMAE='THIEF.jpg'

s3ressource = client(
    service_name='s3', 
    endpoint_url= param_3,
    aws_access_key_id= param_1,
    aws_secret_access_key=param_2,
    use_ssl=True,
    )
visual_recognition = VisualRecognitionV3('2016-05-20', api_key=param_5)

#The URL of my image I made public with Public ACL
urltobeanalysed=""%s/%s/%s"" % (param_3,param_4,objectNMAE)   

#For Debug, I use an image that can be viewed in a web-browser
URL2=""https://fr.wikipedia.org/wiki/Barack_Obama#/media/File:President_Barack_Obama.jpg""

print(json.dumps(visual_recognition.classify(images_url=urltobeanalysed), indent=2))
</code></pre>

<p>What is more, the image that is made public used to be displayed in my browser, now when I enter the URL, it is downloading instead..  Any Clues ? </p>",43913763,2,0,,2017-05-11 08:20:59.457 UTC,,2017-05-11 11:32:30.570 UTC,,,,,6387180,1,0,ibm-cloud|ibm-watson,69
Is there a way to specify the structure / layout of a document so that OCR processes the doc in a specific manor?,56403969,Is there a way to specify the structure / layout of a document so that OCR processes the doc in a specific manor?,"<p>Google vision document text detection does a good job of detecting symbols &amp; words but it groups the text together strictly by lines and paragraph and even then, sometimes text is logically out of place when processing a document with structured text. </p>

<p>I have already looked at the API documentation and cannot find and examples or references for providing hints (other than language) to change how it parses the document. One possible solution may be to pre-process the document and uses google's api to process the document one piece at a time but would prefer to use google's API directly without intermediate steps.</p>

<p>The code I am using was taken directly from google's vision pdf python example and is reproducible using that code without any changes:</p>

<p><a href=""https://cloud.google.com/vision/docs/pdf"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/pdf</a></p>

<hr>

<pre><code>from google.cloud import storage
import re

def async_detect_document(gcs_source_uri, gcs_destination_uri):
    """"""OCR with PDF/TIFF as source files on GCS""""""
    from google.cloud import vision
    mime_type = 'application/pdf'
    batch_size = 2
    client = vision.ImageAnnotatorClient()

    feature = vision.types.Feature(
        type=vision.enums.Feature.Type.DOCUMENT_TEXT_DETECTION)

    gcs_source = vision.types.GcsSource(uri=gcs_source_uri)
    input_config = vision.types.InputConfig(
        gcs_source=gcs_source, mime_type=mime_type)

    gcs_destination = vision.types.GcsDestination(uri=gcs_destination_uri)
    output_config = vision.types.OutputConfig(
        gcs_destination=gcs_destination, batch_size=batch_size)

    async_request = vision.types.AsyncAnnotateFileRequest(
        features=[feature], input_config=input_config,
        output_config=output_config)

    operation = client.async_batch_annotate_files(
        requests=[async_request])

    print('Waiting for the operation to finish.')
    operation.result(timeout=180)
    storage_client = storage.Client()

    match = re.match(r'gs://([^/]+)/(.+)', gcs_destination_uri)
    bucket_name = match.group(1)
    prefix = match.group(2)

    bucket = storage_client.get_bucket(bucket_name)

    # List objects with the given prefix.
    blob_list = list(bucket.list_blobs(prefix=prefix))
    print('Output files:')
    for blob in blob_list:
        print(blob.name)
</code></pre>

<p>The results need to be grouped differently, logically, looking at the document the address should be grouped together. Here'show one document is structured (top 4 lines):</p>

<pre><code>-----------------------------------------------------------------
| Active              |  2415 ST PETER STREET |  $500,000 (LP) 
| R2222222            |    Port Moody Centre  |           (SP) 
| Board: V, Attached  |       Port Moody      |
| House/Single Family |         V3G 2T5       |
-----------------------------------------------------------------
</code></pre>

<p>After using the sample provided by google and printing out the result along with page, block and paragraph numbers to hopefully show what's happening. We can see that the text at the top of the file is in the wrong place and the information shown above is spread out over multiple paragraphs and begins at block 5 where it should start at block 0:</p>

<pre><code>********* Page Number: 0*************
********* Block Number: 0*************
********* Paragraph Number: 0*************
MAIN FLOOR 
BASEMENT 
TOTAL FINISHED AREA 
UNFINISHED"" 
TOTAL AREA

[ snip ]

********* Block Number: 5*************
********* Paragraph Number: 10*************
Active 
2415 ST PETER STREET
********* Paragraph Number: 11*************
$500,000 (LP) 
R2222222 
Port Moody
********* Paragraph Number: 12*************
(SP) 
Board: V, Attached
********* Paragraph Number: 13*************
Port Moody Centre 
********* Paragraph Number: 15*************
V3G 2T5 
********* Paragraph Number: 16*************

[ SNIP ]
</code></pre>

<p>The above output was generated using this:</p>

<pre><code>def annotate(_json):
    annotation = _json['responses'][0]['fullTextAnnotation']
    line = 0
    paranum = 0
    blcknum = 0
    pgenum = 0
    for page in annotation['pages']:
        print('********* Page Number: ' + str(pgenum) + '*************')
        pgenum += 1
        for block in page['blocks']:
            print('********* Block Number: ' + str(blcknum) + '*************')
            blcknum += 1
            for paragraph in block['paragraphs']:
                print('********* Paragraph Number: ' + str(paranum) + '*************')
                paranum += 1
                for word in paragraph['words']:
                    for symbol in word['symbols']:
                        print(symbol['text'], end='')
                        try:
                            bType = symbol['property']['detectedBreak']['type']
                            if bType == 'SPACE':
                                print(' ', end='')
                            if bType == 'EOL_SURE_SPACE':
                                print(' ')
                            if bType == 'LINE_BREAK':
                                print('')
                        except KeyError:
                            print('', end='')
                line += 1
</code></pre>",,0,0,,2019-06-01 04:56:36.757 UTC,,2019-06-01 04:56:36.757 UTC,,,,,11585672,1,1,google-cloud-vision,14
Amazon Rekognition DetectText pre-conditions error,55917241,Amazon Rekognition DetectText pre-conditions error,"<p>I want to get text from image using Amazon rekognition api.</p>

<p>Here is my code:</p>

<pre><code>bucket='my-project'
photo='my-image.jpg'
client=boto3.client('rekognition', 's3-ap-southeast-1')
response=client.detect_text(Image={'S3Object':{'Bucket':bucket,'Name':photo}})
</code></pre>

<p>However, it shows an error:</p>

<pre><code>File ""C:\Users\Chan\AppData\Roaming\Python\Python37\site-packages\botocore\client.py"", line 661, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.exceptions.ClientError: An error occurred (412) when calling the DetectText operation:
&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;Error&gt;
  &lt;Code&gt;PreconditionFailed&lt;/Code&gt;
  &lt;Message&gt;At least one of the pre-conditions you specified did not hold&lt;/Message&gt;
  &lt;Condition&gt;Bucket POST must be of the enclosure-type multipart/form-data&lt;/Condition&gt;
  &lt;RequestId&gt;BAED284813213F69&lt;/RequestId&gt;
  &lt;HostId&gt;grt34dS34dgecadsfrrwer424234dxfgKyO1/sdf423sdfshoprtld=&lt;/HostId&gt;
&lt;/Error&gt;
</code></pre>

<p>How to solve the problem?</p>",,0,1,,2019-04-30 08:53:23.780 UTC,,2019-05-07 05:28:03.667 UTC,2019-05-07 05:28:03.667 UTC,,174777,,9087866,1,0,python|amazon-web-services|amazon-s3|amazon-rekognition,19
Image cannot be set with ImageSource.FromStream,55251014,Image cannot be set with ImageSource.FromStream,"<p>I'm having the wierdest problem. I try to set the Source property of an Image object on the constructor using Image.FromStream method and I'm not able to make ir work. It just does not display the image.</p>

<pre><code>    public ImageRecognitionPage (Stream image)
    {
        InitializeComponent ();

        imgPhoto.Source = ImageSource.FromStream(() =&gt;
        {
            return image;
        });

        _imagen = ReadFully(image);
    }
</code></pre>

<p>The stream is correct because I send _imagen to AWS Rekognition later and it works well. The thing is just that the image is not being showed.</p>

<p>I wasted too much time on this incredibly simple problem. I hope someone can help me out.</p>",,2,1,,2019-03-19 22:39:42.787 UTC,,2019-03-20 09:04:38.317 UTC,,,,,637840,1,0,c#|xamarin.forms,41
Amazon Rekognition Image caption,52746720,Amazon Rekognition Image caption,"<p>In Azure Cognitive Image processing the returned json have a ""caption"" field which summarizes the content of the image. However, I didn't find anything similar in AWS.</p>

<p>In Amazon Rekognition for image processing how do I get the caption for an image?</p>",,1,0,,2018-10-10 18:35:31.590 UTC,,2018-10-11 02:05:27.560 UTC,2018-10-11 02:05:27.560 UTC,,174777,,4852910,1,0,amazon-web-services|aws-sdk|amazon-rekognition,46
Rekognition: Empty Result,53269405,Rekognition: Empty Result,"<p><strong>Hey Stackers.</strong></p>

<p>I'm trying to use Rekognition via the AWS PHP SDK. I do, however have a problem with it. After a long time trying to figure out what's wrong and I still haven't figured it out.  I'm making a request as follows;</p>

<pre><code>    $returnData = new \stdClass();

    $this-&gt;rekognition = new RekognitionClient([
        'version' =&gt; 'latest',
        'region' =&gt; 'eu-west-1',
        'credentials' =&gt; [
            'key' =&gt; 'XXXX',
            'secret' =&gt; 'XXXX'
        ]
    ]);

    try {
        $this-&gt;basePrint = $basePrint;
        $this-&gt;newPrint = $newPrint;

        $faceAnalysis = $this-&gt;rekognition-&gt;compareFaces([
            'SourceImage' =&gt; [
                'Bytes' =&gt; base64_decode($this-&gt;basePrint),
            ],
            'TargetImage' =&gt; [
                'Bytes' =&gt; base64_decode($this-&gt;newPrint),
            ],
            'Attributes' =&gt; ['all']
        ]);

        $returnData-&gt;state = ""success"";
        $returnData-&gt;matchResult = $faceAnalysis;


    } catch (RekognitionException $e){

        $returnData-&gt;state = ""error"";
        $returnData-&gt;AwsErrorCode = $e-&gt;getAwsErrorCode();
        $returnData-&gt;AwsErrorMessage = $e-&gt;getAwsErrorMessage();
        $returnData-&gt;OriginalPrint = $this-&gt;basePrint;
        $returnData-&gt;NewPrint = $this-&gt;newPrint;


    }

return $returnData;
</code></pre>

<p>That's all fine. No excepton is thrown. However, the result of <code>$faceAnalysis</code> is empty. It is null. Without any error thrown. I looked it up in the documentation, and I can't find anything that would result is this behaviour. </p>

<p>Am I doing something wrong or am I missing something?</p>",,0,3,,2018-11-12 20:11:52.877 UTC,,2018-11-12 20:11:52.877 UTC,,,,,2470626,1,0,amazon-web-services|amazon-rekognition,36
Junit testcase for amazon Rekognition,52833231,Junit testcase for amazon Rekognition,"<p>How can i write a test case in Junit for amazon Rekognition.</p>

<pre><code>public class SearchFaceMatchingImageCollection {
    public static final String collectionId = ""MyCollection"";
    public static final String bucket = ""bucket"";
    public static final String photo = ""input.jpg"";
    public static void main(String[] args) throws Exception {
        AmazonRekognition rekognitionClient = AmazonRekognitionClientBuilder.defaultClient();
        ObjectMapper objectMapper = new ObjectMapper();

        // Get an image object from S3 bucket.
        Image image=new Image()
                      .withS3Object(new S3Object()
                      .withBucket(bucket)
                      .withName(photo));

      // Search collection for faces similar to the largest face in the image.
         SearchFacesByImageRequest searchFacesByImageRequest = new SearchFacesByImageRequest()
                .withCollectionId(collectionId)
                .withImage(image)
                .withFaceMatchThreshold(70F)
                .withMaxFaces(2);
         SearchFacesByImageResult searchFacesByImageResult = 
           rekognitionClient.searchFacesByImage(searchFacesByImageRequest);

         System.out.println(""Faces matching largest face in image from"" + photo);
         List &lt; FaceMatch &gt; faceImageMatches = searchFacesByImageResult.getFaceMatches();
         for (FaceMatch face: faceImageMatches) {
             System.out.println(objectMapper.writerWithDefaultPrettyPrinter()
              .writeValueAsString(face));
             System.out.println();
  }
}
}
</code></pre>

<p>For the above program I wanted to write a Junit testcase. Kindly help me with the same </p>",,0,3,,2018-10-16 10:18:33.513 UTC,,2018-10-16 12:27:55.187 UTC,2018-10-16 12:27:55.187 UTC,,2413201,,10418362,1,0,junit|amazon-rds,21
"Face detection ,Identify real human or human from playing video",51076368,"Face detection ,Identify real human or human from playing video","<p>I am using <strong>Google Vision</strong> for <strong>facial detection</strong>, everything works fine but we have to know that if we are actually detecting actual <strong>Human</strong>, One may try to play the <strong>video containing a Human</strong> so we want to know if it is from <strong>video/Image</strong> or an original <strong>human being</strong> ?
Is there any way to <strong>achieve</strong> it, I have search throughout for 2 days and did not get any lead on this, <strong>please guide</strong> me through this <strong>thanks</strong>. </p>",,0,3,,2018-06-28 06:48:16.507 UTC,,2018-06-28 06:48:16.507 UTC,,,,,7750922,1,0,face-detection|google-vision,32
Watson Visual Recognition API,39252746,Watson Visual Recognition API,"<p>I am trying to use the Watson Visual Recognition API as an OCR component, however while it is doing a good job on the computerized text, I want to expand it more to recognize ""Nicely-handwritten"" text.</p>

<p>Is it possible to use the custom classifiers to train the API? and if yes and someone did try it already, is it effective?</p>",39257047,2,0,,2016-08-31 14:45:49.097 UTC,,2016-08-31 18:56:26.657 UTC,,,,,1262667,1,3,ibm-watson|visual-recognition,679
Image quality issues with generateThumbnail,42420031,Image quality issues with generateThumbnail,"<p>Testing the <code>generateThumbnail</code> call of the Azure Computer Vision API from PHP. I have been able to get it to operate, but the images being saved locally are very, very poor quality. Highly pixelated, very blurry, etc. They look nothing like the examples presented at <a href=""https://www.microsoft.com/cognitive-services/en-us/computer-vision-api/documentation#Thumbnails"" rel=""nofollow noreferrer"">https://www.microsoft.com/cognitive-services/en-us/computer-vision-api/documentation#Thumbnails</a></p>

<p>Is this an issue with the image processing on the server side, or possibly a degradation issue occurring locally during the file save process? I'm having trouble determining where to start on this one.</p>

<p>This seems to be the same follow-up question asked here:
<a href=""https://stackoverflow.com/questions/37985715/generate-thumbnail-in-php-posting-to-azure-computer-vision-api"">Generate thumbnail in php, posting to Azure Computer Vision API</a></p>

<p>Source image dimensions are 542x1714. Trying to create 115x115 thumbnail.</p>

<p>Code at the moment.  Have tried it with smartCropping set to both True and False.</p>

<pre><code>        $posturl = 'https://westus.api.cognitive.microsoft.com/vision/v1.0/generateThumbnail';
        $posturl = add_query_arg( array( 'width' =&gt; $max_w, 'height' =&gt; $max_h, 'smartCropping' =&gt; true), $posturl);

        $request = wp_remote_post( $posturl, array( 'headers' =&gt; array( 'Content-Type' =&gt; 'application/octet-stream', 'Ocp-Apim-Subscription-Key' =&gt; 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' ), 'body' =&gt; file_get_contents( $this-&gt;file ) ) );

        if ( is_wp_error( $request ) ) {
            return null;
        } else {
            $resized = @imagecreatefromstring( $request['body'] );
        }
</code></pre>",,0,4,,2017-02-23 15:34:51.507 UTC,2,2017-02-23 16:04:19.707 UTC,2017-05-23 12:00:08.933 UTC,,-1,,4927318,1,0,php|azure|computer-vision|microsoft-cognitive,85
Google Vison Text Recognition,48473858,Google Vison Text Recognition,"<p>I followed tutorial on codelabs developer google for Google vision api, it's worked fine for me
There is a method called <strong>ONTAP</strong>, when the user clic the camera screen TexttoSpeech, speak the text loud.
Here is the method:</p>

<pre><code> private boolean onTap(float rawX, float rawY) {
        // TODO: Speak the text when the user taps on screen.
        OcrGraphic graphic = mGraphicOverlay.getGraphicAtLocation(rawX, rawY);
        TextBlock text = null;
        if (graphic != null) {
            text = graphic.getTextBlock();
            if (text != null &amp;&amp; text.getValue() != null) {
                Log.d(TAG, ""text data is being spoken! "" + text.getValue());
                // Speak the string.
                tts.speak(text.getValue(), TextToSpeech.QUEUE_ADD, null, ""DEFAULT"");
            }


            else {
                Log.d(TAG, ""text data is null"");
            }
        }
        else {
            Log.d(TAG,""no text detected"");
        }
        return text != null;
    }
</code></pre>

<p>NOW what i want to do is when the camera detect the sentence string:  <strong>I LOVE YOU</strong> I want it to make in action in a TOAST for example to say:
<strong>Ok this sentence has been detected.</strong> 
I tried this its not working:</p>

<pre><code>private boolean onTap(float rawX, float rawY) {
        // TODO: Speak the text when the user taps on screen.
        OcrGraphic graphic = mGraphicOverlay.getGraphicAtLocation(rawX, rawY);
        TextBlock text = null;
        if (graphic != null) {
            text = graphic.getTextBlock();
            if (text != null &amp;&amp; text.getValue() != null) {
                Log.d(TAG, ""text data is being spoken! "" + text.getValue());
                // Speak the string.
                tts.speak(text.getValue(), TextToSpeech.QUEUE_ADD, null, ""DEFAULT"");
            }
            else if (text.getValue()==""I love you""){

                // Speak the string.
                Toast.makeText(this, ""Ok this sentence has been detected"", Toast.LENGTH_LONG).show();
            }

            else {
                Log.d(TAG, ""text data is null"");
            }
        }
        else {
            Log.d(TAG,""no text detected"");
        }
        return text != null;
    }
</code></pre>

<p>Please somebody helps me. Thanks you.</p>",,0,4,,2018-01-27 08:04:43.327 UTC,,2018-01-27 08:06:57.637 UTC,2018-01-27 08:06:57.637 UTC,,22656,,7181623,1,0,java|text|speech|vision,19
Does google-api-services-translate and google-cloud-translate do the same thing?,44096947,Does google-api-services-translate and google-cloud-translate do the same thing?,"<p>Android Dev with <a href=""https://cloud.google.com/translate/"" rel=""nofollow noreferrer"">cloud translate</a></p>

<p>I see that they are all libraries of the translator. The cloud-vision has two libraries as well but in the Android <a href=""https://github.com/GoogleCloudPlatform/cloud-vision/blob/master/android/README.md"" rel=""nofollow noreferrer"">Sample</a> we use the <a href=""https://mvnrepository.com/artifact/com.google.apis/google-api-services-vision"" rel=""nofollow noreferrer"">cloud-vision of google-api-services</a> different from <a href=""https://mvnrepository.com/artifact/com.google.cloud/google-cloud-vision"" rel=""nofollow noreferrer"">cloud-service</a>. Does the translator-API do the same like vision-api?</p>

<p>Latest versions of libraries:</p>

<p>google-api-services-translate: <a href=""https://mvnrepository.com/artifact/com.google.apis/google-api-services-translate/v2-rev49-1.22.0"" rel=""nofollow noreferrer"">https://mvnrepository.com/artifact/com.google.apis/google-api-services-translate/v2-rev49-1.22.0</a></p>

<p>google-cloud-translate:<a href=""https://mvnrepository.com/artifact/com.google.cloud/google-cloud-translate/0.18.0-beta"" rel=""nofollow noreferrer"">https://mvnrepository.com/artifact/com.google.cloud/google-cloud-translate/0.18.0-beta</a></p>",44111798,1,0,,2017-05-21 12:36:55.820 UTC,,2017-05-22 11:32:16.867 UTC,,,,,1835650,1,2,android|google-app-engine|google-cloud-platform|google-translate|google-cloud-vision,419
Google Vision Api supports PDF nad TIFF text detection but can it work with PDf containing images as well?,51876363,Google Vision Api supports PDF nad TIFF text detection but can it work with PDf containing images as well?,"<p>I am trying with a pdf containing images as well with google vision API but it throws the following error :</p>

<blockquote>
  <p>4:35:12.207 pm info    dialogflowFirebaseFulfillment Dialogflow Request
  headers:
  {""host"":""us-central1-detecttext-5a0c3.cloudfunctions.net"",""user-agent"":""Apache-HttpClient/4.5.4
  (Java/1.8.0_181)"",""transfer-encoding"":""chunked"",""accept"":""text/plain,
  <em>/</em>"",""accept-charset"":""big5, big5-hkscs, cesu-8, euc-jp, euc-kr, gb18030, gb2312, gbk, ibm-thai, ibm00858, ibm01140, ibm01141,
  ibm01142, ibm01143, ibm01144, ibm01145, ibm01146, ibm01147, ibm01148,
  ibm01149, ibm037, ibm1026, ibm1047, ibm273, ibm277, ibm278, ibm280,
  ibm284, ibm285, ibm290, ibm297, ibm420, ibm424, ibm437, ibm500,
  ibm775, ibm850, ibm852, ibm855, ibm857, ibm860, ibm861, ibm862,
  ibm863, ibm864, ibm865, ibm866, ibm868, ibm869, ibm870, ibm871,
  ibm918, iso-2022-cn, iso-2022-jp, iso-2022-jp-2, iso-2022-kr,
  iso-8859-1, iso-8859-13, iso-8859-15, iso-8859-2, iso-8859-3,
  iso-8859-4, iso-8859-5, iso-8859-6, iso-8859-7, iso-8859-8,
  iso-8859-9, jis_x0201, jis_x0212-1990, koi8-r, koi8-u, shift_jis,
  tis-620, us-ascii, utf-16, utf-16be, utf-16le, utf-32, utf-32be,
  utf-32le, utf-8, windows-1250, windows-1251, windows-1252,
  windows-1253, windows-1254, windows-1255, windows-1256, windows-1257,
  windows-1258, windows-31j, x-big5-hkscs-2001, x-big5-solaris,
  x-compound_text, x-euc-jp-linux, x-euc-tw, x-eucjp-open, x-ibm1006,
  x-ibm1025, x-ibm1046, x-ibm1097, x-ibm1098, x-ibm1112, x-ibm1122,
  x-ibm1123, x-ibm1124, x-ibm1166, x-ibm1364, x-ibm1381, x-ibm1383,
  x-ibm300, x-ibm33722, x-ibm737, x-ibm833, x-ibm834, x-ibm856,
  x-ibm874, x-ibm875, x-ibm921, x-ibm922, x-ibm930, x-ibm933, x-ibm935,
  x-ibm937, x-ibm939, x-ibm942, x-ibm942c, x-ibm943, x-ibm943c,
  x-ibm948, x-ibm949, x-ibm949c, x-ibm950, x-ibm964, x-ibm970,
  x-iscii91, x-iso-2022-cn-cns, x-iso-2022-cn-gb, x-iso-8859-11,
  x-jis0208, x-jisautodetect, x-johab, x-macarabic, x-maccentraleurope,
  x-maccroatian, x-maccyrillic, x-macdingbat, x-macgreek, x-machebrew,
  x-maciceland, x-macroman, x-macromania, x-macsymbol, x-macthai,
  x-macturkish, x-macukraine, x-ms932_0213, x-ms950-hkscs,
  x-ms950-hkscs-xp, x-mswin-936, x-pck, x-sjis_0213, x-utf-16le-bom,
  x-utf-32be-bom, x-utf-32le-bom, x-windows-50220, x-windows-50221,
  x-windows-874, x-windows-949, x-windows-950,
  x-windows-iso2022jp"",""content-type"":""application/json;
  charset=UTF-8"",""function-execution-id"":""dvrpphf9f855"",""x-appengine-api-ticket"":""4b7e84f29e9ce22b"",""x-appengine-city"":""?"",""x-appengine-citylatlong"":""0.000000,0.000000"",""x-appengine-country"":""US"",""x-appengine-https"":""on"",""x-appengine-region"":""?"",""x-appengine-user-ip"":""35.193.50.245"",""x-cloud-trace-context"":""9d163f59b7fc5d0049692efae5269b4c/11159965978299906802;o=1"",""x-forwarded-for"":""35.193.50.245,
  35.193.50.245"",""x-forwarded-proto"":""https"",""accept-encoding"":""gzip""} 4:35:12.045 pm outlined_flag  dialogflowFirebaseFulfillment Function
  execution started 4:32:49.480 pm warning<br>
  dialogflowFirebaseFulfillment  ERROR: { Error: Error in extracting
  images from PDF file gs://detecttext-5a0c3.appspot.com/NFM-11099M1.pdf
      at GoogleError.Error (native)
      at new GoogleError (/user_code/node_modules/@google-cloud/vision/node_modules/google-gax/build/src/GoogleError.js:46:42)
      at Operation._unpackResponse (/user_code/node_modules/@google-cloud/vision/node_modules/google-gax/build/src/longrunning.js:228:29)
      at /user_code/node_modules/@google-cloud/vision/node_modules/google-gax/build/src/longrunning.js:214:18
  code: 13 }</p>
</blockquote>",,2,1,,2018-08-16 11:49:28.327 UTC,,2018-09-18 14:40:58.817 UTC,2018-08-16 11:50:18.657 UTC,,372091,,10234115,1,0,api|pdf|google-cloud-platform|vision,238
How to detect a cropped-off head with Google Vision API?,50782221,How to detect a cropped-off head with Google Vision API?,"<p>How can the Google Vision API be used to detect if a head is completely inside an image or partly cut off by the image frame?</p>

<p><strong>3 examples:</strong></p>

<ul>
<li><a href=""https://i.imgur.com/PerZZcO.png"" rel=""nofollow noreferrer"">Image 1</a> shows a complete head</li>
<li><a href=""https://i.imgur.com/wRpNHAD.png"" rel=""nofollow noreferrer"">Image 2</a> shows a cut off head where the full ""face"" is visible</li>
<li><a href=""https://i.imgur.com/YapStMY.png"" rel=""nofollow noreferrer"">Image 3</a> shows a cut off head where also the face is cut off</li>
</ul>

<p><a href=""https://i.stack.imgur.com/PnVRt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PnVRt.png"" alt=""enter image description here""></a></p>

<p>To narrow down the question, the following cases should be detected:</p>

<ul>
<li>there is a completely visible head in the image</li>
<li>there is a partly visible head in the image where parts of the head are outside the image bounds</li>
</ul>

<p>The following is out of scope for this question:</p>

<ul>
<li>heads that are spatially or scenically inside the image bounds but fully or partly covered by other objects</li>
<li>there are no parts of a head visible in the image, e.g. if there is only a neck visible it can't be assumed that there is or is not a head attached to it</li>
<li>the effectiveness or efficiency of the API in detecting faces that are fully or partly visible, file that under caveats</li>
</ul>

<p>I have checked the documentation but it doesn't say anything about head crop-off detection.</p>

<p><em>I am not asking for code but whether / how the API can be used for the described purpose. Hence neither the question contains any code nor is an answer expected to contain any code. If you are looking for code examples for API calls, take a look at the plenty example calls in the API docs.</em></p>

<p><em>There was a <a href=""https://meta.stackoverflow.com/questions/369270/are-questions-about-api-usage-too-broad"">meta discussion</a> about this question.</em></p>

<p><a href=""https://www.pexels.com/photo/man-in-black-framed-eyeglasses-and-blue-button-up-shirt-939817/"" rel=""nofollow noreferrer"">Image credit</a> </p>",50782330,1,0,,2018-06-10 09:19:29.550 UTC,1,2018-06-21 09:37:02.977 UTC,2018-06-21 09:37:02.977 UTC,,472495,,1870795,1,1,google-cloud-vision,297
Error 403 because Google Cloud Vision client points to wrong project,52048829,Error 403 because Google Cloud Vision client points to wrong project,"<p>I'm trying to work through the Google Cloud Vision <a href=""https://cloud.google.com/vision/docs/libraries"" rel=""nofollow noreferrer"">Pyhon example</a> but I'm getting an authentication error.</p>

<p>This is not my only Google Cloud project, and my GOOGLE_APPLICATION_CREDENTIALS environment variable is set to the path to my bigquery project. I thought I could override this by using this statement:</p>

<pre><code>client = vision.ImageAnnotatorClient.from_service_account_json(key_path)
</code></pre>

<p>where <code>key_path</code> is the path of the json key file associated with my (Cloud Vision API-enabled) vision project. However, I'm getting the 403 error from this</p>

<pre><code>response = client.label_detection(image=image)
</code></pre>

<p>Apparently, even though I specified the key file path for the ImageAnnotatorClient, it still looks at my bigquery project's credentials and spits the dummy because there is no vision API enabled for it.</p>

<p>Do I really have to change the environment variable every time I change the project?</p>",,1,0,,2018-08-28 01:20:39.343 UTC,,2018-08-29 09:34:16.160 UTC,,,,,5405830,1,0,google-cloud-vision|google-python-api,88
Can I use face landmarks returned from microsoft face API to track the face in real time,42117805,Can I use face landmarks returned from microsoft face API to track the face in real time,"<p>My target is to use Microsoft face API cognitive service to detect the faces in a frame , then using the landmarks returned for each face I would track it using optical flow for example!.</p>

<p>My question is about the accuracy .. is this approach would work properly , or there are some other logical constrains exists behind tracking face using its landmarks?</p>",42150992,1,0,,2017-02-08 15:56:30.110 UTC,1,2017-02-10 03:27:20.007 UTC,,,,,5516665,1,0,face-detection|microsoft-cognitive,198
Google Vision iOS Example: Scanned barcode Shape(purpleColor) shows in wrong location,45095486,Google Vision iOS Example: Scanned barcode Shape(purpleColor) shows in wrong location,"<p>I have downloaded the google vision api from <a href=""https://github.com/googlesamples/ios-vision"" rel=""nofollow noreferrer"">https://github.com/googlesamples/ios-vision</a>. And I tried Barcode detector example, When I try to scan a linear and 2D barcodes, the scanned area(purple shape) shows in wrong location on preview layer.</p>

<p>Note: This issue only occurs when I hold the device horizontally at the top of barcode.</p>

<p>Herewith I have attached the screenshot which reflects this issue.</p>

<p><a href=""https://i.stack.imgur.com/jd64C.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jd64C.jpg"" alt=""Screenshot""></a></p>

<p>Thank you!</p>",,1,1,,2017-07-14 05:43:52.427 UTC,1,2018-05-08 12:59:32.207 UTC,2017-07-14 07:28:15.533 UTC,,3420996,,3420996,1,2,ios|barcode-scanner|google-vision,184
List of languages supported in Google Cloud Vision OCR detection?,38227082,List of languages supported in Google Cloud Vision OCR detection?,"<p>Working on some modules using Google Cloud Vision API for text detection and was wondering if anyone has list of languages/text it can detect.
Personal experience with Italian, French, English, Chinese, Spanish works. What about the ones like Hindi, Urdu etc?</p>

<p>Thanks and appreciate your help!
Suman </p>",,1,0,,2016-07-06 14:51:05.040 UTC,,2017-02-18 14:41:46.453 UTC,2016-07-08 04:33:14.047 UTC,,6114925,,6114925,1,1,android|ocr|google-cloud-platform|google-cloud-vision,1379
Why Microsoft Cognitive doesn't return every OCR field?,52308804,Why Microsoft Cognitive doesn't return every OCR field?,"<p>I'm trying to read MRZ zone from passports with Microsoft Cognitive Vision but is impossible. It never returns that field, when (I guess) is the easiest field of all...</p>

<p>An example:</p>

<p><a href=""https://i.stack.imgur.com/19tq4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/19tq4.jpg"" alt=""Ukrainian Passport""></a></p>

<p>Does anyone knows why it doesn't return that field? Has Cognitive a limit of fields? Do I need to include any param to increase the number of fields to return? Is there any valid alternative that will return that field (I've tried Amazon Rekognition but only returns 50 fields)</p>",,1,0,,2018-09-13 07:39:06.817 UTC,,2018-09-13 10:56:50.840 UTC,,,,,911420,1,0,ocr|microsoft-cognitive,1870
How do we retrieve status of VideoAnnotation request in Google Cloud Video Intelligence API?,49842698,How do we retrieve status of VideoAnnotation request in Google Cloud Video Intelligence API?,"<p>I am using following nodejs code to retrieve operation instance of video annotation request</p>

<pre><code>var vidOptions = {
        ""inputUri"": gCloudURL,
        ""features"": [ 'LABEL_DETECTION' ]
        };
    client.annotateVideo(vidOptions).then(function(results){
        //console.log(results[0].name);
        //return JSON.stringify(results[0].name);
        const operation = results[0];
        res.json(operation);
        //console.log('Waiting for operation to complete...');
        //return operation.promise();
        })
</code></pre>

<p>But once I have operation name which function / library i need to call to get operation status and results. Videos might be lnong duration. Hence I dont want to rely on promise. Rather in 1st Call I initiate video annotation. In 2nd call I run a for loop and keep checking if videoannotation operation is finished?</p>

<p>BTW AWS rekognition rocks when it comes to this, they allow integration with SNS so you automatically recieve an event when video processing finishes and you avoid all overhead of polling from client side. Doesnt GCP has similar feature?</p>",,1,0,,2018-04-15 13:50:17.123 UTC,,2018-05-07 15:11:38.940 UTC,,,,,997239,1,0,node.js|amazon-web-services|google-app-engine|google-cloud-platform|google-api-nodejs-client,37
Can I use Google Mobile Vision API in PyCharm?,47940052,Can I use Google Mobile Vision API in PyCharm?,"<p>I am exploring the APIs provided by <code>Google</code>. Firstly, I was experimenting with <code>Google Cloud Vision API</code> with Python in PyCharm in order to try to perform Optical Character Recognition with various texts. </p>

<p>So I wrote a basic program in Python in PyCharm which was calling this API, I gave to it as an input an image which included text e.g. the <code>image/photo</code> of an ice-cream bucket and then takes the text written on this bucket as an output.</p>

<p>Now I want to test the barcode scanner of <code>Google Mobile Vision API</code>. So ideally I would like to call the <code>Google Mobile Vision API</code> in a python program in PyCharm which calls this API, give as an input an <code>image/photo</code> of a barcode and take as an output the details saved in this barcode.</p>

<p>My question is if this can be (easily) done with <code>PyCharm</code> or if I should download Android Studio to do this simple task?</p>

<p>In other words, can I call easily a mobile API in an IDE which is not for mobile app development like Android Studio but in an IDE for desktop applications like Pycharm?</p>

<p>It may be a very basic question but I do not know if I missing something important.</p>",47940628,1,0,,2017-12-22 10:36:00.280 UTC,,2017-12-22 11:18:54.933 UTC,2017-12-22 10:40:35.007 UTC,,3505534,,9024698,1,1,android|python|api,241
Google Cloud Functions Text Extraction from image using Vision API,51045843,Google Cloud Functions Text Extraction from image using Vision API,"<p>I am following the tutorial to extract text from images at:</p>

<p><a href=""https://cloud.google.com/functions/docs/tutorials/ocr?authuser=1"" rel=""nofollow noreferrer"">https://cloud.google.com/functions/docs/tutorials/ocr?authuser=1</a></p>

<p>But I do not wish to translate the text, I wish to detect and save the text.</p>

<p>The tutorial implements 3 functions:</p>

<pre><code>gcloud beta functions deploy ocr-extract --trigger-bucket [YOUR_IMAGE_BUCKET_NAME] --entry-point processImage

gcloud beta functions deploy ocr-translate --trigger-topic [YOUR_TRANSLATE_TOPIC_NAME] --entry-point translateText

gcloud beta functions deploy ocr-save --trigger-topic [YOUR_RESULT_TOPIC_NAME] --entry-point saveResult
</code></pre>

<p>I just wish to detect text and save the text but I could not remove the translation portion of the code below:</p>

<pre><code>/**
 * Detects the text in an image using the Google Vision API.
 *
 * @param {string} bucketName Cloud Storage bucket name.
 * @param {string} filename Cloud Storage file name.
 * @returns {Promise}
 */
function detectText (bucketName, filename) {
  let text;

  console.log(`Looking for text in image ${filename}`);
  return vision.textDetection({ source: { imageUri: `gs://${bucketName}/${filename}` } })
    .then(([detections]) =&gt; {
      const annotation = detections.textAnnotations[0];
      text = annotation ? annotation.description : '';
      console.log(`Extracted text from image (${text.length} chars)`);
      return translate.detect(text);
    })
    .then(([detection]) =&gt; {
      if (Array.isArray(detection)) {
        detection = detection[0];
      }
      console.log(`Detected language ""${detection.language}"" for ${filename}`);

      // Submit a message to the bus for each language we're going to translate to
      const tasks = config.TO_LANG.map((lang) =&gt; {
        let topicName = config.TRANSLATE_TOPIC;
        if (detection.language === lang) {
          topicName = config.RESULT_TOPIC;
        }
        const messageData = {
          text: text,
          filename: filename,
          lang: lang,
          from: detection.language
        };

        return publishResult(topicName, messageData);
      });

      return Promise.all(tasks);
    });
}
</code></pre>

<p>After that, I just wish to save the detectec text to a file, as the code below shows:</p>

<pre><code>/**
 * Saves the data packet to a file in GCS. Triggered from a message on a Pub/Sub
 * topic.
 *
 * @param {object} event The Cloud Functions event.
 * @param {object} event.data The Cloud Pub/Sub Message object.
 * @param {string} event.data.data The ""data"" property of the Cloud Pub/Sub
 * Message. This property will be a base64-encoded string that you must decode.
     */
exports.saveResult = (event) =&gt; {
  const pubsubMessage = event.data;
  const jsonStr = Buffer.from(pubsubMessage.data, 'base64').toString();
  const payload = JSON.parse(jsonStr);

  return Promise.resolve()
    .then(() =&gt; {
      if (!payload.text) {
        throw new Error('Text not provided. Make sure you have a ""text"" property in your request');
      }
      if (!payload.filename) {
        throw new Error('Filename not provided. Make sure you have a ""filename"" property in your request');
      }
      if (!payload.lang) {
        throw new Error('Language not provided. Make sure you have a ""lang"" property in your request');
      }

      console.log(`Received request to save file ${payload.filename}`);

      const bucketName = config.RESULT_BUCKET;
      const filename = renameImageForSave(payload.filename, payload.lang);
      const file = storage.bucket(bucketName).file(filename);

      console.log(`Saving result to ${filename} in bucket ${bucketName}`);

      return file.save(payload.text);
    })
    .then(() =&gt; {
      console.log(`File saved.`);
    });
};
</code></pre>",51981922,1,6,,2018-06-26 14:52:54.140 UTC,1,2018-08-23 08:58:18.710 UTC,2018-07-31 09:00:45.550 UTC,,9384667,,8833943,1,0,node.js|google-cloud-functions|google-vision,521
Image Classification in Nativescript via API,55979551,Image Classification in Nativescript via API,"<p>I will be building an app to detect cat diseases (in their eyes) using Nativescript-Vue. </p>

<p>I'm searching other image classifications project and plugins in Native Script but there where little to no results. I explored google vision, Microsoft custom vision, clarify, but there were no plugins ready for Native Script.</p>

<p>I'm not so good yet to convert react native implementation and its possible counterpart to Native Script. Has anyone tried it?</p>",,1,0,,2019-05-04 04:51:31.013 UTC,,2019-05-04 07:40:07.790 UTC,2019-05-04 07:40:07.790 UTC,,10970473,,10245760,1,0,machine-learning|nativescript,18
How to compile Google Vision API library?,49386259,How to compile Google Vision API library?,"<p>I am trying to implement Google Vision API, but after adding libraries I am getting the following error:</p>

<p>Error:Execution failed for task ':app:transformClassesWithMultidexlistForDebug'.</p>

<blockquote>
  <p>java.io.IOException: Can't write [/Users/anshul/AndroidStudioProjects/CisiveDemo/app/build/intermediates/multi-dex/debug/componentClasses.jar] (Can't read [/Users/anshul/.gradle/caches/modules-2/files-2.1/com.google.guava/guava-jdk5/17.0/463f8378feba44df7ba7cd9272d01837dad62b36/guava-jdk5-17.0.jar(;;;;;;**.class)] (Duplicate zip entry [guava-jdk5-17.0.jar:com/google/common/annotations/Beta.class]))</p>
</blockquote>

<pre><code>apply plugin: 'com.android.application'

android {
    compileSdkVersion 27
    defaultConfig {
        applicationId ""com.demo.cisive""
        minSdkVersion 15
        targetSdkVersion 27
        versionCode 1
        versionName ""1.0""
        multiDexEnabled true
        testInstrumentationRunner ""android.support.test.runner.AndroidJUnitRunner""
    }
    buildTypes {
        release {
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
        }
    }
    configurations.all {
        resolutionStrategy.force 'com.google.code.findbugs:jsr305:1.3.9'
    }
}

dependencies {
    implementation fileTree(dir: 'libs', include: ['*.jar'])
    implementation 'com.android.support:appcompat-v7:27.1.0'
    implementation 'com.android.support.constraint:constraint-layout:1.0.2'
    testImplementation 'junit:junit:4.12'
    implementation 'com.android.support:design:27.1.0'
    implementation 'com.github.delight-im:Android-AdvancedWebView:v3.0.0'
    implementation 'com.basgeekball:awesome-validation:1.3'
    implementation 'com.google.android.gms:play-services-auth:11.8.0'
    implementation 'com.google.android.gms:play-services-base:11.8.0'
    androidTestImplementation 'com.android.support.test:runner:1.0.1'
    androidTestImplementation 'com.android.support.test.espresso:espresso-core:3.0.1'


    implementation 'com.google.api-client:google-api-client-android:1.23.0'
    implementation 'com.google.http-client:google-http-client-gson:1.23.0'
    implementation 'com.google.apis:google-api-services-vision:v1-rev369-1.23.0'
}
</code></pre>

<p>Dependencies</p>

<pre><code>+--- com.android.support:appcompat-v7:27.1.0
|    +--- com.android.support:support-annotations:27.1.0
|    +--- com.android.support:support-core-utils:27.1.0
|    |    +--- com.android.support:support-annotations:27.1.0
|    |    \--- com.android.support:support-compat:27.1.0
|    |         +--- com.android.support:support-annotations:27.1.0
|    |         \--- android.arch.lifecycle:runtime:1.1.0
|    |              +--- android.arch.lifecycle:common:1.1.0
|    |              \--- android.arch.core:common:1.1.0
|    +--- com.android.support:support-fragment:27.1.0
|    |    +--- com.android.support:support-compat:27.1.0 (*)
|    |    +--- com.android.support:support-core-ui:27.1.0
|    |    |    +--- com.android.support:support-annotations:27.1.0
|    |    |    +--- com.android.support:support-compat:27.1.0 (*)
|    |    |    \--- com.android.support:support-core-utils:27.1.0 (*)
|    |    +--- com.android.support:support-core-utils:27.1.0 (*)
|    |    +--- com.android.support:support-annotations:27.1.0
|    |    +--- android.arch.lifecycle:livedata-core:1.1.0
|    |    |    +--- android.arch.lifecycle:common:1.1.0
|    |    |    +--- android.arch.core:common:1.1.0
|    |    |    \--- android.arch.core:runtime:1.1.0
|    |    |         \--- android.arch.core:common:1.1.0
|    |    \--- android.arch.lifecycle:viewmodel:1.1.0
|    +--- com.android.support:support-vector-drawable:27.1.0
|    |    +--- com.android.support:support-annotations:27.1.0
|    |    \--- com.android.support:support-compat:27.1.0 (*)
|    \--- com.android.support:animated-vector-drawable:27.1.0
|         +--- com.android.support:support-vector-drawable:27.1.0 (*)
|         \--- com.android.support:support-core-ui:27.1.0 (*)
+--- com.android.support.constraint:constraint-layout:1.0.2
|    \--- com.android.support.constraint:constraint-layout-solver:1.0.2
+--- com.android.support:design:27.1.0
|    +--- com.android.support:support-v4:27.1.0
|    |    +--- com.android.support:support-compat:27.1.0 (*)
|    |    +--- com.android.support:support-media-compat:27.1.0
|    |    |    +--- com.android.support:support-annotations:27.1.0
|    |    |    \--- com.android.support:support-compat:27.1.0 (*)
|    |    +--- com.android.support:support-core-utils:27.1.0 (*)
|    |    +--- com.android.support:support-core-ui:27.1.0 (*)
|    |    \--- com.android.support:support-fragment:27.1.0 (*)
|    +--- com.android.support:appcompat-v7:27.1.0 (*)
|    +--- com.android.support:recyclerview-v7:27.1.0
|    |    +--- com.android.support:support-annotations:27.1.0
|    |    +--- com.android.support:support-compat:27.1.0 (*)
|    |    \--- com.android.support:support-core-ui:27.1.0 (*)
|    \--- com.android.support:transition:27.1.0
|         +--- com.android.support:support-annotations:27.1.0
|         \--- com.android.support:support-compat:27.1.0 (*)
+--- com.github.delight-im:Android-AdvancedWebView:v3.0.0
+--- com.basgeekball:awesome-validation:1.3
|    \--- com.google.guava:guava:19.0
+--- com.google.android.gms:play-services-auth:11.8.0
|    +--- com.google.android.gms:play-services-auth-api-phone:11.8.0
|    |    +--- com.google.android.gms:play-services-base:11.8.0
|    |    |    +--- com.google.android.gms:play-services-basement:11.8.0
|    |    |    |    +--- com.android.support:support-v4:25.2.0 -&gt; 27.1.0 (*)
|    |    |    |    \--- com.google.android.gms:play-services-basement-license:11.8.0
|    |    |    +--- com.google.android.gms:play-services-tasks:11.8.0
|    |    |    |    +--- com.google.android.gms:play-services-basement:11.8.0 (*)
|    |    |    |    \--- com.google.android.gms:play-services-tasks-license:11.8.0
|    |    |    \--- com.google.android.gms:play-services-base-license:11.8.0
|    |    +--- com.google.android.gms:play-services-basement:11.8.0 (*)
|    |    +--- com.google.android.gms:play-services-tasks:11.8.0 (*)
|    |    \--- com.google.android.gms:play-services-auth-api-phone-license:11.8.0
|    +--- com.google.android.gms:play-services-auth-base:11.8.0
|    |    +--- com.google.android.gms:play-services-base:11.8.0 (*)
|    |    +--- com.google.android.gms:play-services-basement:11.8.0 (*)
|    |    +--- com.google.android.gms:play-services-tasks:11.8.0 (*)
|    |    \--- com.google.android.gms:play-services-auth-base-license:11.8.0
|    +--- com.google.android.gms:play-services-base:11.8.0 (*)
|    +--- com.google.android.gms:play-services-basement:11.8.0 (*)
|    \--- com.google.android.gms:play-services-tasks:11.8.0 (*)
+--- com.google.android.gms:play-services-base:11.8.0 (*)
+--- com.google.api-client:google-api-client-android:1.23.0
|    +--- com.google.api-client:google-api-client:1.23.0
|    |    +--- com.google.oauth-client:google-oauth-client:1.23.0
|    |    |    +--- com.google.http-client:google-http-client:1.23.0
|    |    |    |    +--- com.google.code.findbugs:jsr305:1.3.9
|    |    |    |    \--- org.apache.httpcomponents:httpclient:4.0.1
|    |    |    |         +--- org.apache.httpcomponents:httpcore:4.0.1
|    |    |    |         +--- commons-logging:commons-logging:1.1.1
|    |    |    |         \--- commons-codec:commons-codec:1.3
|    |    |    \--- com.google.code.findbugs:jsr305:1.3.9
|    |    +--- com.google.http-client:google-http-client-jackson2:1.23.0
|    |    |    +--- com.google.http-client:google-http-client:1.23.0 (*)
|    |    |    \--- com.fasterxml.jackson.core:jackson-core:2.1.3
|    |    \--- com.google.guava:guava-jdk5:17.0
|    \--- com.google.http-client:google-http-client-android:1.23.0
|         \--- com.google.http-client:google-http-client:1.23.0 (*)
+--- com.google.http-client:google-http-client-gson:1.23.0
|    +--- com.google.http-client:google-http-client:1.23.0 (*)
|    \--- com.google.code.gson:gson:2.1
+--- com.google.apis:google-api-services-vision:v1-rev369-1.23.0
|    \--- com.google.api-client:google-api-client:1.23.0 (*)
\--- com.android.support:multidex:1.0.2
</code></pre>",,1,8,,2018-03-20 13:58:50.280 UTC,,2018-03-20 18:48:31.187 UTC,2018-03-20 18:48:31.187 UTC,,2649012,,5164601,1,1,android|android-library,183
How can I avoid Http Error Code 429 from Google Vision API?,39982559,How can I avoid Http Error Code 429 from Google Vision API?,"<p>I've been using Google Vision API to perform OCR tasks in some documents using Python.</p>

<p>It begins working perfectly, until I start receiving Http Error Code 429, which means I am doing too many requests in a short amount of time. Then, I decided to put a sleep between each request, of which time increases as the number of Http Error Code 429 increases. However, after some time, the error message keeps coming. Since the messages keeps arriving, the sleeping time keeps increasing until it reaches a point that it sleeps for so long that I lose connection.</p>

<p>The weirdest thing is that if I receive such error message many times in a row and, immediately, finish the process and start it again, the requests start to work again in the first try.</p>

<p>In other words, it seems that no matter the sleeping time I put I will start receiving such messages at some point and the only way to put it work again is restarting the process (which makes no sens at all).</p>

<p>How can I avoid having such error message without having to restart the process? Can anyone help me?</p>

<p>Thanks a lot!</p>

<p>EDIT:</p>

<p>This is the code of the request (part of it).</p>

<pre><code>    from apiclient import discovery
    from oauth2client.client import GoogleCredentials
    # The other imports are omitted

    DISCOVERY_URL = 'https://{api}.googleapis.com/$discovery/rest?version={apiVersion}'  # noqa
    credentials = GoogleCredentials.get_application_default()
    self.vision = discovery.build(
        'vision', 'v1', credentials=credentials,
        discoveryServiceUrl=DISCOVERY_URL)

    batch_request = []

    for image in images:
        batch_request.append({
            'image': {
                'content': base64.b64encode(image).decode('UTF-8')
            },
            'features': [{
                'type': 'TEXT_DETECTION',
            }]
        })

    request = self.vision.images().annotate(
        body={'requests': batch_request})
</code></pre>",,1,5,,2016-10-11 16:52:14.777 UTC,,2018-09-28 08:41:44.807 UTC,2018-09-28 08:41:44.807 UTC,,441757,,7002574,1,0,python|ocr|http-error|google-cloud-vision|http-status-code-429,1035
Google Vision: Run Multiple Types of Detection on a Single Image,45134020,Google Vision: Run Multiple Types of Detection on a Single Image,"<p>Google Vision Post requests usually look like this:</p>

<pre><code>{
  ""requests"":[
    {
      ""image"":{
        ""content"": ""image base64 string...""
      },
      ""features"":[
        {
          ""type"":""LABEL_DETECTION"",
          ""maxResults"":1
        }
      ]
    }
  ]
}
</code></pre>

<p>As far as I know, this only supports one 'type'.
However, I want Google to analyze for two types: <code>LABEL_DETECTION</code> and <code>FACE_DETECTION</code>.</p>

<p>Is it possible to ask for both in one request?
I'm currently sending two seperate requests, which is kind of inefficient.</p>",,1,0,,2017-07-16 22:56:14.457 UTC,,2017-10-05 13:12:37.850 UTC,,,,,2251258,1,1,google-vision,223
Iinvalid subscription key when using Microsoft Emotion API for Video in API Testing Console and in Python 2.7,46760602,Iinvalid subscription key when using Microsoft Emotion API for Video in API Testing Console and in Python 2.7,"<p>I only manage to use the Emotion API subscription key for pictures but never for videos. It makes no difference whether I use the API Testing Console or try to call the Emotion API by Pathon 2.7. In both cases I get a response status 202 Accepted, however when opening the Operation-Location it says </p>

<pre><code>{ ""error"": { ""code"": ""Unauthorized"", ""message"": ""Access denied due to 
invalid subscription key. Make sure you are subscribed to an API you are 
trying to call and provide the right key."" } }
</code></pre>

<p>On the Emotion API explanatory page it says that Response 202 means that </p>

<blockquote>
  <p>The service has accepted the request and will start the process later.
  In the response, there is a ""Operation-Location"" header. Client side should further query the operation status from the URL specified in this header.</p>
</blockquote>

<p>Then there is <a href=""https://i.stack.imgur.com/XJynV.png"" rel=""nofollow noreferrer"">Response 401</a>, which is exactly what my Operation-Location contains. I do not understand why I'm getting a response 202 which looks like response 401.</p>

<p>I have tried to call the API with Python using at least three code versions that I found on the Internet that 
all amount to the same, I found the code here : 
       <a href=""https://stackoverflow.com/questions/40198227/microsoft-emotion-api-for-"">Microsoft Emotion API for Python - upload video from memory</a>
     python-upload-video-from-memory</p>

<pre><code>   import httplib
   import urllib
   import base64
   import json
   import pandas as pd
   import numpy as np
   import requests

   _url = 'https://api.projectoxford.ai/emotion/v1.0/recognizeInVideo'
   _key = '**********************'
   _maxNumRetries = 10

   paramsPost = urllib.urlencode({'outputStyle' : 'perFrame', \
                           'file':'C:/path/to/file/file.mp4'})
   headersPost = dict()
   headersPost['Ocp-Apim-Subscription-Key'] = _key
   headersPost['content-type'] = 'application/octet-stream'
   jsonGet = {}
   headersGet = dict()
   headersGet['Ocp-Apim-Subscription-Key'] = _key
   paramsGet = urllib.urlencode({})

   responsePost = requests.request('post', _url + ""?"" + paramsPost, \
   data=open('C:/path/to/file/file.mp4','rb').read(), \
   headers = headersPost)

   print responsePost.status_code

   videoIDLocation = responsePost.headers['Operation-Location']
   print videoIDLocation
</code></pre>

<p>Note that changing <code>_url = 'https://api.projectoxford.ai/emotion/v1.0/recognizeInVideo'</code> to  <code>_url = 
      'https://westus.api.cognitive.microsoft.com/emotion/v1.0/recognizeInVideo'</code> doesn't help.</p>

<p>However, afterwards I wait and run every half an hour:</p>

<pre><code>   getResponse = requests.request('get', videoIDLocation, json = jsonGet,\
   data = None, headers = headersGet, params = paramsGet)

   print json.loads(getResponse.text)['status']
</code></pre>

<p>The outcome has been 'Running' for hours and my video is only about half an hour long.</p>

<p>Here is what my Testing Console looks like <a href=""https://i.stack.imgur.com/3eFeG.png"" rel=""nofollow noreferrer"">Testing Console for Emotion API, Emotion Recognition in Video</a>
Here I used another video that is about 5 minutes long and available on the internet. I found the video in a different usage example </p>

<pre><code> https://benheubl.github.io/data%20analysis/fr/
</code></pre>

<p>that uses a very similar code, which again gets me a response status 202 Accepted and when opening the Operation-Location the subscription key is wrong</p>

<p>Here the code:</p>

<pre><code>import httplib
import urllib
import base64
import json
import pandas as pd
import numpy as np
import requests



# you have to sign up for an API key, which has some allowances. Check the 
API documentation for further details:
_url = 'https://api.projectoxford.ai/emotion/v1.0/recognizeinvideo'
_key = '*********************' #Here you have to paste your 
primary key
_maxNumRetries = 10

# URL direction: I hosted this on my domain
urlVideo = 'http://datacandy.co.uk/blog2.mp4'

# Computer Vision parameters
paramsPost = { 'outputStyle' : 'perFrame'}

headersPost = dict()
headersPost['Ocp-Apim-Subscription-Key'] = _key
headersPost['Content-Type'] = 'application/json'

jsonPost = { 'url': urlVideo }

responsePost = requests.request( 'post', _url, json = jsonPost, data = None, 
headers = headersPost, params = paramsPost )
if responsePost.status_code == 202: # everything went well!
  videoIDLocation = responsePost.headers['Operation-Location']
  print videoIDLocation
</code></pre>

<p>There are further examples on the internet and they all seem to work but replicating any of them never worked for me. Does anyone have any idea what could be wrong?</p>",46772755,3,0,,2017-10-15 22:21:22.370 UTC,,2017-10-16 15:43:39.650 UTC,2017-10-16 06:14:07.910 UTC,,8780892,,8780892,1,1,python|api|key|subscription|microsoft-cognitive,273
How to improve the results of the image sentiment analysis?,35883234,How to improve the results of the image sentiment analysis?,"<p>I created a sample app to test the image sentiment analysis from Google Cloud Vision API, but I'm not getting good results. The app is running on App Engine: <a href=""https://feel-vision.appspot.com"" rel=""nofollow"">https://feel-vision.appspot.com</a></p>

<p>The likeliness of sorrow, anger and surprise are really hard to move from the ""Very unlikely"" threshold, no matter how sad, angry or surprised I try to be at the photo.</p>

<p>The likeliness of joy is something that moves easily by just making a fake smile.</p>

<p>Is there something I can do to improve the sentiment analysis results?</p>

<p>Currently I'm calling the API this way (using the Java client):</p>

<pre><code>//dataUrl is the JPEG base64 encoded image sent by the client
AnnotateImageRequest req = new AnnotateImageRequest()
                .setImage(new Image().setContent(dataUrl))
                .setFeatures(Arrays.asList(
                        new Feature().setType(""LABEL_DETECTION""),
                        new Feature().setType(""FACE_DETECTION"")));

Annotate annotate = vision.images().annotate(new BatchAnnotateImagesRequest().setRequests(Arrays.asList(req)));
BatchAnnotateImagesResponse batchResponse = annotate.execute();
//process the response
</code></pre>",,0,2,,2016-03-09 04:58:53.417 UTC,,2016-03-09 04:58:53.417 UTC,,,,,1013327,1,0,google-cloud-platform|google-cloud-vision,156
"""line crossing detection"", high fps streaming video",54794136,"""line crossing detection"", high fps streaming video","<p>I'm slowly getting to understand machine learning, but still looking into more of the ""as a service"" options (sagemaker, lobe.ai, google cloud vision, etc).</p>

<p>Can someone provide some insight as to the easiest way to proceed, if I'm looking to take a high-fps overhead video of a race, and have it flag the finish-line (ie photo finish) upon crossing, and kick that particular frame out for further analysis?  I'm using a yi action cam, 480p 240fps, and at this initial stage all sorts of preprocessing could be used to limit the data being analyzed (all I care about is ""has this line been crossed?"").  Overhead fixed camera, similar/near-identical setup every time.  At the end of the day, I'll want the analysis to be local and not cloud-based.  And I'm fine with ""teaching"" it based on a couple thousand examples, if someone can direct me.</p>",,0,0,,2019-02-20 19:43:30.743 UTC,,2019-02-20 19:43:30.743 UTC,,,,,11091998,1,0,video|line|analysis,13
"Google Vision API Invalid JSON payload received. Unknown name \""images\"" at 'requests[0]'",43758896,"Google Vision API Invalid JSON payload received. Unknown name \""images\"" at 'requests[0]'","<p>I want to use <a href=""https://cloud.google.com/vision/docs/detecting-text#vision-text-detection-protocol"" rel=""nofollow noreferrer"">Google vision API</a> using Postman or any other rest api tool.</p>

<ol>
<li><p>So as per documentation I convert my image to base64-encoded-image-content using Java 8 encoding as below :</p>

<pre><code>byte[] data = Files.readAllBytes(path);

// getting base64 encoded string bytes

byte[] bytesEncoded = Base64.getEncoder().encode(data);

String encodedData = ""{ \""content\"": \"""" + new String(bytesEncoded) + ""\"" }"";
</code></pre></li>
<li><p>Now in postman I choose Post option and then use the URL as <a href=""https://vision.googleapis.com/v1/images:annotate?key=keyVal"" rel=""nofollow noreferrer"">https://vision.googleapis.com/v1/images:annotate?key=keyVal</a>  </p></li>
<li><p>And then in the body part I choose raw option and paste the below content </p></li>
</ol>

<p><code>{ ""requests"": [ { ""images"": { ""content"": ""iVBORw0KGgoAAAANSUhEUgAAALkAAABeCAIAAADwjiIxAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAO+SURBVHhe7dxRcuMgEEXRrMsL0nq8Gm8mi8nQIESDWvaLNU6hqnt+RgbRuOgXO/OhfP0AGrICFVmBiqxARVagIitQkRWoyApUZAUqsgIVWYGKrEBFVqAiK1CRFajIClRkBSqyAhVZgYqsQEVWoCIrUJEVqMgKVGQFKrICFVmBiqxARVagIitQkRWoyApUZAUqsgLVxFl5LF/V8ljHzrB6t/t3vv6+38Ki6Z56S78A82bFmrl187H875a9zkr3BmBmzso+IH2Lu8amK/scqB9DudWmFbFpW7xNmWGPWrLeuzq1r9+wFskl7mWd22du034HlQMeWvmkZ9uhr51pL2oN3/++UJNLPtyi4sy+7UOxG2z3XsbUv9uWE3WJedazbbx7YSXWF+5yKNTsdixO7du0O8Lp2U2dlcJOeG3OqZ75/vSFmlIy7+inT+2bq23KoH8vl3GBrLgunOqZ709fqKklbd7dcGLfeFc/ehmTZiWd6tqOxA52fdXO2M59GxZ6tu9a26BJ99Rht+uZfd2udrleu9HrmPx328J3NR+4WR7pUu/Z0J9a3pdOWskk31NXnNs3y//zKYNu+jou8R2EKZAVqMgKVGQFKrICFVmBiqxARVagIitQkRWoyApUZAUqsgIVWYGKrEBFVqAiK1CRFajIClRkBSqyAhVZgYqsQEVWoCIrUJEVqMgKVPNmpT5xnNQnf7cng5v2UHCZ3D+fvB8sXH33dHI1Povc5Jk2Pjz+vAqfVS43dG8mqvNe8T8wbVban0Oyo4uOpxtPL27LMvzxg3Aws6MfRtNQHcmNCbZcd7R/dm/uxfKypv+LUWGdd4r/kQt8B9nptAOuulF7sTyGG8PBIjVhX9EJF9XBfjIqFS433URY50Txj5s/K/bDtT8bG60/XXZ6dr3rxG5wlcdu24f6/qfUV6/aWDf7/NZB/2bCOu8X/7iJs2KHctBLf2A1FF0nwsGNra5jNj/sEC3pW7S9tVQnZU5YXoxTYZ13i3/cRb6D+hNzB2aXdW4bDgedru/jDTa560ZUpehqJeHy6rjMrk72q+Ifd4GshN2sJ2jXI/f10vgCXcHuxUEzxp41/czB8o0euex3xT9u1qykU62nZAfsjmx46YSdeDXYOpALR8043tG373C5E76ZJIzBb4t/3LSfK/l0Vv6Qnvx0hZ04aI8rv6bACvfcRFfBrW3jh8uLcdoWhnXeKf5HrvAdhDmQFajIClRkBSqyAhVZgYqsQEVWoCIrUJEVqMgKVGQFKrICFVmBiqxA8/PzDwWjo8ARxuPBAAAAAElFTkSuQmCC"" }, ""features"": [ { ""type"": ""TEXT_DETECTION"" } ] } ] }
</code></p>

<p>But I am getting an error :
<code>""Invalid JSON payload received. Unknown name \""images\"" at 'requests[0]': Cannot find field.""</code></p>",,1,0,,2017-05-03 11:33:57.330 UTC,,2017-08-29 09:54:43.247 UTC,2017-08-29 09:54:43.247 UTC,,741249,,7574163,1,3,google-cloud-platform|postman|google-vision,1654
Grouping text extracted as full words from the Google Vision API,44762298,Grouping text extracted as full words from the Google Vision API,"<p>I am trying to reproduce the output of the ""Document Text Detection"" sample UI uploader through the Google Vision API. However, the output I am getting from the <a href=""https://cloud.google.com/vision/docs/detecting-fulltext"" rel=""nofollow noreferrer"">sample code</a>  is only providing individual characters as an output, when I require words to be grouped together.</p>

<p>Is there a feature within the library that allows grouping by ""words"" instead from the DOCUMENT_TEXT_DETECT endpoint or or <code>image.detect_full_text()</code> function in Python?</p>

<p>I am not looking for full text extraction as my .jpg files are not visually structured in a way that the <code>image.detect_text()</code> function satisfies.</p>

<p>Google's Sample Code:</p>

<pre><code>def detect_document(path):
    """"""Detects document features in an image.""""""
    vision_client = vision.Client()

    with io.open(path, 'rb') as image_file:
        content = image_file.read()

    image = vision_client.image(content=content)

    document = image.detect_full_text()

    for page in document.pages:
        for block in page.blocks:
            block_words = []
            for paragraph in block.paragraphs:
                block_words.extend(paragraph.words)

            block_symbols = []
            for word in block_words:
                block_symbols.extend(word.symbols)

            block_text = ''
            for symbol in block_symbols:
                block_text = block_text + symbol.text

            print('Block Content: {}'.format(block_text))
            print('Block Bounds:\n {}'.format(block.bounding_box))
</code></pre>

<p>Sample output of the off the shelf sample provided by Google:</p>

<pre><code>property {
  detected_languages {
    language_code: ""mt""
  }
}
bounding_box {
  vertices {
    x: 1193
    y: 1664
  }
  vertices {
    x: 1206
    y: 1664
  }
  vertices {
    x: 1206
    y: 1673
  }
  vertices {
    x: 1193
    y: 1673
  }
}
symbols {
  property {
    detected_languages {
      language_code: ""en""
    }
  }
  bounding_box {
    vertices {
      x: 1193
      y: 1664
    }
    vertices {
      x: 1198
      y: 1664
    }
    vertices {
      x: 1198
      y: 1673
    }
    vertices {
      x: 1193
      y: 1673
    }
  }
  text: ""P""
}
symbols {
  property {
    detected_languages {
      language_code: ""en""
    }
    detected_break {
      type: LINE_BREAK
    }
  }
  bounding_box {
    vertices {
      x: 1200
      y: 1664
    }
    vertices {
      x: 1206
      y: 1664
    }
    vertices {
      x: 1206
      y: 1673
    }
    vertices {
      x: 1200
      y: 1673
    }
  }
  text: ""M""
}


block_words
Out[47]: 
[property {
   detected_languages {
     language_code: ""en""
   }
 }
 bounding_box {
   vertices {
     x: 1166
     y: 1664
   }
   vertices {
     x: 1168
     y: 1664
   }
   vertices {
     x: 1168
     y: 1673
   }
   vertices {
     x: 1166
     y: 1673
   }
 }
 symbols {
   property {
     detected_languages {
       language_code: ""en""
     }
   }
   bounding_box {
     vertices {
       x: 1166
       y: 1664
     }
     vertices {
       x: 1168
       y: 1664
     }
     vertices {
       x: 1168
       y: 1673
     }
     vertices {
       x: 1166
       y: 1673
     }
   }
   text: ""2""
 }
</code></pre>",,2,0,,2017-06-26 14:41:12.510 UTC,,2018-02-11 12:09:37.390 UTC,2017-08-26 17:21:01.827 UTC,,2917381,,2917381,1,2,python|image-recognition|google-vision,723
Empty description in Web Entities of Google Vision API,50164690,Empty description in Web Entities of Google Vision API,"<p>If you try Google Vision API with follwoing demo-image.jpg
<img src=""https://i.stack.imgur.com/MRPdL.jpg"" alt=""""> shown in <a href=""https://cloud.google.com/vision/docs/quickstart"" rel=""nofollow noreferrer"">QuickStart</a>, you will get a record with empty description and score of 0.7024 in “<a href=""https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate#WebEntity"" rel=""nofollow noreferrer"">Web Entities</a>”. Why!?</p>

<p><a href=""https://i.stack.imgur.com/jrH0V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jrH0V.png"" alt=""enter image description here""></a></p>",50177250,1,0,,2018-05-03 22:33:34.990 UTC,,2018-05-04 21:21:24.133 UTC,2018-05-04 21:21:24.133 UTC,,8212141,,9738153,1,0,google-cloud-vision|google-vision,169
Syntax for calling Watson API in python,48375623,Syntax for calling Watson API in python,"<p>What is the syntax for calling the watson visual recognition api in python?
I've looked around a lot but have not been able to find a proper syntax for the call of the api. What parameters have to be defined within the call of the api?
Thanks for the help.</p>",48847429,1,1,,2018-01-22 06:14:50.740 UTC,,2018-02-18 00:55:18.390 UTC,,,,,7778132,1,-1,python|python-3.x|ibm-watson,158
How to limit the text returned from making a TEXT_DETECTION call to Google Vision?,42663690,How to limit the text returned from making a TEXT_DETECTION call to Google Vision?,"<p>I am currently experimenting with levaraging Google Vision API for OCR. When I upload a image, I see the resulting JSON payload returned to me is rather large. I see two major buckets in the response:
1) ""textAnnotations""
2) ""fullTextAnnotation""</p>

<p>I am only interested in the JSON returned by ""textAnnotations"" and I dont care about the fullTextAnnotation bucket. Essentially I am only interested in the individual words and their corresponding bounding boxes, I dont need any more granular OCR data. The response seems to parse out paragraphs, symbols, and individual characters as well but I dont need ANY OF THAT.</p>

<p>Is there anyway to filter google vision's result set by sending some flag or parameter in the request? Surely there must be because this JSON being returned is very large.</p>",,1,2,,2017-03-08 05:31:32.850 UTC,,2017-08-26 01:41:23.933 UTC,,,,,2220458,1,1,ios|google-cloud-platform|ocr|google-cloud-vision,439
How to convert the following list to json file in python?,49657006,How to convert the following list to json file in python?,"<p>Hi I am new in python and I would love to convert the following data into a json file. I obtained this data set from Google Vision API. However, I have no idea how to approach this problem. Please help. Any type of help will be much appreciated.</p>

<p>The data set:</p>

<pre><code>[mid: ""/m/02wbm""
description: ""food""
score: 0.8716073632240295
topicality: 0.8716073632240295
, mid: ""/m/01ykh""
description: ""cuisine""
score: 0.848469614982605
topicality: 0.848469614982605
, mid: ""/m/01f5gx""
description: ""eating""
score: 0.8267097473144531
topicality: 0.8267097473144531
, mid: ""/m/01_bhs""
description: ""fast food""
score: 0.6969127655029297
topicality: 0.6969127655029297
, mid: ""/m/02q08p0""
description: ""dish""
score: 0.6883306503295898
topicality: 0.6883306503295898
, mid: ""/m/0h55b""
description: ""junk food""
score: 0.6554489135742188
topicality: 0.6554489135742188
, mid: ""/m/0krfg""
description: ""meal""
score: 0.6400452256202698
topicality: 0.6400452256202698
, mid: ""/m/0cp_p""
description: ""taste""
score: 0.5145916938781738
topicality: 0.5145916938781738
, mid: ""/m/0jfd5""
description: ""lunch""
score: 0.5000766515731812
topicality: 0.5000766515731812
]
</code></pre>

<p>The desired format is this:</p>

<pre><code>{
mid: ""/m/02wbm""
description: ""food""
score: 0.8716073632240295
topicality: 0.8716073632240295
},
{
mid: ""/m/01ykh""
description: ""cuisine""
score: 0.848469614982605
topicality: 0.848469614982605
},
{
mid: ""/m/01f5gx""
description: ""eating""
score: 0.8267097473144531
topicality: 0.8267097473144531
},
{
mid: ""/m/01_bhs""
description: ""fast food""
score: 0.6969127655029297
topicality: 0.6969127655029297
},
{
mid: ""/m/02q08p0""
description: ""dish""
score: 0.6883306503295898
topicality: 0.6883306503295898
},
{
mid: ""/m/0h55b""
description: ""junk food""
score: 0.6554489135742188
topicality: 0.6554489135742188
},
{
mid: ""/m/0krfg""
description: ""meal""
score: 0.6400452256202698
topicality: 0.6400452256202698
},
{
mid: ""/m/0cp_p""
description: ""taste""
score: 0.5145916938781738
topicality: 0.5145916938781738
},
{
mid: ""/m/0jfd5""
description: ""lunch""
score: 0.5000766515731812
topicality: 0.5000766515731812
}
</code></pre>

<p>It looks like I have four types of data within this set and these four types of data are repeated. Each four is split with commas.</p>

<p>Thank you so much.</p>",,1,4,,2018-04-04 17:27:50.907 UTC,,2018-04-04 18:06:11.920 UTC,2018-04-04 18:06:11.920 UTC,,9509811,,9509811,1,-1,json|python-3.x,34
How to fix the Image reference?,55886635,How to fix the Image reference?,"<p>I'm using the Google Cloud Vision API with Python 3, but i'm getting the error
""Cannot find reference 'Image' in types.py"" when i use:</p>

<pre><code>image = vision.types.Image(content=content)
</code></pre>

<p>I made the correct imports and the documentation tells me to use this function to get an image. Anyone can help me?</p>

<p>Code:</p>

<pre><code>import io
import os
from google.cloud import vision
from google.cloud.vision import types

os.environ[""GOOGLE_APPLICATION_CREDENTIALS""] = ""C:/Keys/key.json""

client = vision.ImageAnnotatorClient

path = os.path.join(os.path.dirname(__file__), ""image.jpg"")

with io.open(path, ""rb"") as image_file:
 content = image_file.read()

image = types.Image(content=content)
</code></pre>

<p>Error Message: </p>

<p><a href=""https://i.stack.imgur.com/zNqSY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zNqSY.png"" alt=""error message""></a></p>

<p>Google Cloud Vision API version: 0.36.0</p>",,1,2,,2019-04-28 02:57:33.817 UTC,,2019-04-28 13:35:09.560 UTC,2019-04-28 13:35:09.560 UTC,,2745495,,9477148,1,0,python|google-cloud-vision,38
Google's VisionAPI example FaceTracker Camera permission,43532713,Google's VisionAPI example FaceTracker Camera permission,"<p>I'm trying to run <a href=""https://github.com/googlesamples/android-vision/tree/master/visionSamples/FaceTracker"" rel=""nofollow noreferrer"">Google Vision FaceTracker</a> but I have an error on one line of code in <code>CameraSourcePreview</code>. </p>

<p><strong>This is the error</strong> - 
Call requires permission which may be rejected by user: code should explicitly check to see if permission is available (with <code>checkPermission</code>) or explicitly handle a potential <code>SecurityException</code></p>

<p>And this is the function-</p>

<pre><code>private void startIfReady() throws IOException {
    if (mStartRequested &amp;&amp; mSurfaceAvailable) {

        mCameraSource.start(mSurfaceView.getHolder()); //Error

        //...other code

        mStartRequested = false;
    }
}
</code></pre>",,1,6,,2017-04-21 02:01:07.030 UTC,,2017-04-21 10:30:57.570 UTC,,,,,7868376,1,2,android|android-camera|google-vision,137
Google Vision API - Invalid JSON payload received. Unable to parse number,35823073,Google Vision API - Invalid JSON payload received. Unable to parse number,"<p>I was working with google Vision API.</p>

<p>When I curl in command line it gives me status 200 OK with the following command:</p>

<pre><code>curl -v -k -s -H ""Content-Type: application/json"" https://vision.googleapis.com/v1/images:annotate?key=API_KEY --data-binary @base64.json
</code></pre>

<p>But when I use it with PHP, I get an return message:</p>

<p>{ ""error"": { ""code"": 400, ""message"": ""Invalid JSON payload received. Unable to parse number.\n--------------------\n^"", ""status"": ""INVALID_ARGUMENT"" } }</p>

<pre><code>try {
$post = array(
    'file' =&gt; '@base64.json'
);

$ch = curl_init('https://vision.googleapis.com/v1/images:annotate?key=API_KEY');
    curl_setopt($ch, CURLOPT_POST, 1);
    curl_setopt($ch, CURLOPT_POSTFIELDS, $post);
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
    curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, false);
    $content = curl_exec($ch);

    if (FALSE === $content)
        throw new Exception(curl_error($ch), curl_errno($ch));

    curl_close($ch);  // Seems like good practice
    return $content;
} catch (Exception $e) {
    trigger_error(sprintf(
        'Curl failed with error #%d: %s',
        $e-&gt;getCode(), $e-&gt;getMessage()),
        E_USER_ERROR);
}
</code></pre>

<p>I was following this example:</p>

<p><a href=""https://cloud.google.com/vision/docs/getting-started"" rel=""nofollow"">https://cloud.google.com/vision/docs/getting-started</a></p>",35823114,2,1,,2016-03-06 04:21:52.477 UTC,,2017-05-04 12:30:56.440 UTC,,,,,2656354,1,0,php|json|curl|google-vision,3676
"Can AWS rekognition analyze faces (emotion, etc.) in a video stream?",54802917,"Can AWS rekognition analyze faces (emotion, etc.) in a video stream?","<p>I thought that I could by at of today from the docs it looks like I can't (<a href=""https://docs.aws.amazon.com/rekognition/latest/dg/streaming-video.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/rekognition/latest/dg/streaming-video.html</a>). Seems like for video stream only face detection is supported, not analysis. Analysis says it only works for stored media (<a href=""https://docs.aws.amazon.com/rekognition/latest/dg/faces.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/rekognition/latest/dg/faces.html</a>). Can someone confirm this?</p>

<p>If so, wonder what's a good way to ""hack"" video stream analysis on AWS? does it make sense to use a lambda function to read video from kineses, chop it into chunks, write to S3, and then let a face analyzer (rekognition) periodically poll S3 to analyze the faces? we kinda really need the sentiment analysis for video stream...</p>

<p>many thanks!!</p>",,0,0,,2019-02-21 08:56:52.543 UTC,1,2019-02-21 08:56:52.543 UTC,,,,,3984890,1,0,amazon-web-services|amazon-kinesis|amazon-rekognition,37
opencv-python camera read erroes,51702859,opencv-python camera read erroes,"<p>I'm getting an errors</p>

<blockquote>
  <p>VIDEOIO ERROR: V4L2: Pixel format of incoming image is unsupported by OpenCV</p>
  
  <p>Unable to stop the stream: Device or resource busy</p>
  
  <p>VIDEOIO ERROR: V4L: can't open camera by index 0</p>
</blockquote>

<p>when trying to run a code</p>

<pre><code>import time
import datetime
import cv2
import boto3

rekog = boto3.client('rekognition')
video = cv2.VideoCapture(0)


class VideoCamera(object):
    def __init__(self):
        # Using OpenCV to capture from device 0. If you have trouble capturing
        # from a webcam, comment the line below out and use a video file
        # instead.
        # self.video = cv2.VideoCapture(0)
        # If you decide to use video.mp4, you must have this file in the folder
        # as the main.py.
        # self.video = cv2.VideoCapture('video.mp4')
        print('camera loaded')

    def __del__(self):
        self.video.release()

    def get_frame(self):
        # success, image = self.video.read()
        success, image = video.read()
        overlay = image.copy()
        overlayTxt = image.copy()

        h, w = image.shape[:2]
        regImg = cv2.resize(image, (int(0.2 * w), int(0.2 * h)))
        _, newjpeg = cv2.imencode('.jpg', regImg)
        imgbytes = newjpeg.tobytes()
        t0 = time.time()
        resp = rekog.detect_labels(Image={'Bytes': imgbytes})

        cv2.rectangle(overlay, (10, 10), (300, 50 + 50 * len(resp['Labels'])), (0, 0, 0), -1)
        alpha = 0.3
        cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0, image)

        cnt = 1
        now = str(datetime.datetime.now())
        cv2.putText(image, now, (20, 40 * cnt), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
        cnt = cnt + 1
        for label in resp['Labels']:
            outTxt = label['Name'] + ' (' + str(int(label['Confidence'])) + '%)'
            cv2.putText(image, outTxt, (20, 40 * cnt), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 1)
            cnt = cnt + 1

        # We are using Motion JPEG, but OpenCV defaults to capture raw images,
        # so we must encode it into JPEG in order to correctly display the
        # video stream.
        ret, jpeg = cv2.imencode('.jpg', image)
        t0 = time.time()
        resp = rekog.detect_labels(Image={'Bytes': imgbytes})
        print(""{} ---&gt; {}"".format((time.time() - t0), resp[""Labels""]))

        return jpeg.tobytes()
</code></pre>

<p>camera is working, tested it in an other application.</p>

<p>what are the reasons of those errors?</p>",,1,0,,2018-08-06 07:48:40.887 UTC,,2018-10-20 07:03:07.433 UTC,,,,,10185451,1,0,python|opencv,432
Missing elements for a Google Cloud Video Annotation request,51712998,Missing elements for a Google Cloud Video Annotation request,"<p>I am trying to run annotation on a video using the Google Cloud Video Intelligence API. Annotation requests with just one feature request (i.e., one of ""LABEL_DETECTION"", ""SHOT_CHANGE_DETECTION"" or ""EXPLICIT_CONTENT_DETECTION""), things work fine. However, when I request an annotation with two or more features at the same time, the response does not always return all the request feature fields. For example, here is a request I ran recently using the <a href=""https://developers.google.com/apis-explorer/#p/videointelligence/v1/videointelligence.videos.annotate?fields=done%252Cerror%252Cmetadata%252Cname%252Cresponse&amp;_h=3&amp;resource=%257B%250A%20%20%2522features%2522%253A%20%250A%20%20%255B%2522EXPLICIT_CONTENT_DETECTION%2522%252C%2522LABEL_DETECTION%2522%252C%2522SHOT_CHANGE_DETECTION%2522%250A%20%20%255D%252C%250A%20%20%2522inputUri%2522%253A%20%2522gs%253A%252F%252Fgccl_dd_01%252FVideo1%2522%250A%257D&amp;"" rel=""nofollow noreferrer"">API explorer</a>:</p>

<pre><code>{
 ""features"": [
   ""EXPLICIT_CONTENT_DETECTION"",
   ""LABEL_DETECTION"",
   ""SHOT_CHANGE_DETECTION""
 ],
 ""inputUri"": ""gs://gccl_dd_01/Video1""
}
</code></pre>

<p>The operation Id I got back is this: ""us-east1.11264560501473964275"". When I run a GET with this Id, I have the following response:</p>

<pre><code>200

{
 ""name"": ""us-east1.11264560501473964275"",
 ""metadata"": {
  ""@type"": ""type.googleapis.com/google.cloud.videointelligence.v1.AnnotateVideoProgress"",
  ""annotationProgress"": [
   {
    ""inputUri"": ""/gccl_dd_01/Video1"",
    ""progressPercent"": 100,
    ""startTime"": ""2018-08-06T17:13:58.129978Z"",
    ""updateTime"": ""2018-08-06T17:18:01.274877Z""
   },
   {
    ""inputUri"": ""/gccl_dd_01/Video1"",
    ""progressPercent"": 100,
    ""startTime"": ""2018-08-06T17:13:58.129978Z"",
    ""updateTime"": ""2018-08-06T17:14:39.074505Z""
   },
   {
    ""inputUri"": ""/gccl_dd_01/Video1"",
    ""progressPercent"": 100,
    ""startTime"": ""2018-08-06T17:13:58.129978Z"",
    ""updateTime"": ""2018-08-06T17:16:23.230536Z""
   }
  ]
 },
 ""done"": true,
 ""response"": {
  ""@type"": ""type.googleapis.com/google.cloud.videointelligence.v1.AnnotateVideoResponse"",
  ""annotationResults"": [
   {
    ""inputUri"": ""/gccl_dd_01/Video1"",
    ""segmentLabelAnnotations"": [
     ...
    ],
    ""shotLabelAnnotations"": [
     ...
    ],
    ""shotAnnotations"": [
     ...
    ]
   }
  ]
 }
}
</code></pre>

<p>The done parameter for the response is set to true, but it does not have any field containing the annotations for Explicit Content.</p>

<p>This issue seems to be occurring at random to my novice eyes. The APIs will return a response with all parameters on some occasions and be missing one on others. I am wondering if there is anything I am missing here or something on my end that is causing this?</p>",,1,0,,2018-08-06 17:40:31.050 UTC,,2018-08-07 14:28:16.300 UTC,2018-08-06 19:49:20.077 UTC,,10167828,,10167828,1,0,google-api|video-intelligence-api,48
boto3 not able to access given region name while taking region provided by AWS Lambda,49293605,boto3 not able to access given region name while taking region provided by AWS Lambda,"<p>I have boto client like this</p>

<pre><code>client = boto3.client('rekognition', region_name=""us-east-1"")
</code></pre>

<p>I am using this client to detect text from image and deployed code in AWS region where Rekognition api is not available but provided the region-name where it is available in client. On executing/Testing the lambda function, it is giving</p>

<pre><code>errorMessage"": ""Could not connect to the endpoint URL: \""https://rekognition.ap-south-1.amazonaws.com/""
</code></pre>

<p>Why it is picking ap-south-1 as i provided in client-""us-east-1""</p>

<p>client = boto3.client('rekognition', region_name=""us-east-1"")</p>

<p>But when I run the code locally with region-name:- ap-south-1 and in client</p>

<pre><code>client = boto3.client('rekognition', region_name=""us-east-1"")
</code></pre>

<p>its running wonderfully</p>

<p>but not running on AWS lambda</p>

<p>While successfully running when both the regions are same(us-east-1)</p>

<p>So great if anyone can provide any suggestion, Required Help soon!!!!!!!</p>",,1,4,,2018-03-15 07:21:33.653 UTC,,2018-03-16 06:40:43.060 UTC,2018-03-16 06:40:43.060 UTC,,6164697,,6164697,1,0,amazon-web-services|amazon-s3|aws-lambda|amazon-rekognition|aws-lambda-edge,217
google-cloud-vision for internal photo repository,48428894,google-cloud-vision for internal photo repository,<p>Is it possible to use the google-cloud-vision API to match photos with an internal photo directory or sharepoint? The purpose is to find the best match between a specific photo and the existing photos in the repository.</p>,,1,0,,2018-01-24 18:01:30.097 UTC,,2018-01-25 23:09:30.247 UTC,2018-01-25 07:44:04.620 UTC,,322020,,9263631,1,0,google-cloud-vision,55
How to stop scanning and store data from Google's Vision API?,32558923,How to stop scanning and store data from Google's Vision API?,"<p>I'm using Google's Vision API BarcodeScanner on my project. I would like to interrupt scanning once a code has been scanned and store the content in another activity. How can i do that ? There are so many classes and 'interconnections' :x</p>

<p>Thanks !</p>",,2,0,,2015-09-14 07:04:28.763 UTC,1,2018-05-04 09:36:27.017 UTC,,,,,4722182,1,3,android-intent|android-studio|qr-code|barcode-scanner|google-vision,4363
How to fix 'google.cloud.vision' has no attribute 'Client' in python3?,51189021,How to fix 'google.cloud.vision' has no attribute 'Client' in python3?,"<p>I am running a simple code that utilizes google cloud vision api but it keeps on throwing this error. I have tries upgrading my packages shown here: <a href=""https://stackoverflow.com/questions/47362736/how-to-fix-attributeerror-module-object-has-no-attribute-client-when-runnin"">How to fix AttributeError: &#39;module&#39; object has no attribute &#39;Client&#39; when running python in Google Cloud Interactive Shell</a> but the error persists.</p>

<p>PS:am using a virtual environment(virtualenv)</p>",51193192,1,0,,2018-07-05 10:29:36.407 UTC,,2018-07-05 13:56:15.447 UTC,,,,,5127707,1,0,python|google-cloud-vision,1522
Android Out of Memory with bitmap,47048947,Android Out of Memory with bitmap,"<p>I'm creating a app that gets a image from camera (using CameraKit library), process the image and do a OCR Read using Google Vision Api, And get this error: </p>

<blockquote>
  <p>FATAL EXCEPTION: main
                                                                        Process: com.<strong><em>.</em></strong>, PID: 1938
                                                                        java.lang.OutOfMemoryError: Failed to allocate a 63701004 byte
  allocation with 16777216 free bytes and 60MB until OOM
                                                                            at dalvik.system.VMRuntime.newNonMovableArray(Native Method)
                                                                            at android.graphics.Bitmap.nativeCreate(Native Method)
                                                                            at android.graphics.Bitmap.createBitmap(Bitmap.java:905)
                                                                            at android.graphics.Bitmap.createBitmap(Bitmap.java:882)
                                                                            at android.graphics.Bitmap.createBitmap(Bitmap.java:849)
                                                                            at
  com.****.****.Reader.ReaderResultActivity.createContrast(ReaderResultActivity.java:123)
                                                                            at
  com.*****.****.Reader.ReaderResultActivity.onCreate(ReaderResultActivity.java:47)
                                                                            at android.app.Activity.performCreate(Activity.java:6672)
                                                                            at
  android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1140)
                                                                            at
  android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2612)
                                                                            at
  android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2724)
                                                                            at android.app.ActivityThread.-wrap12(ActivityThread.java)
                                                                            at
  android.app.ActivityThread$H.handleMessage(ActivityThread.java:1473)
                                                                            at android.os.Handler.dispatchMessage(Handler.java:102)
                                                                            at android.os.Looper.loop(Looper.java:154)
                                                                            at android.app.ActivityThread.main(ActivityThread.java:6123)
                                                                            at java.lang.reflect.Method.invoke(Native Method)
                                                                            at
  com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:867)
                                                                            at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:757)</p>
</blockquote>

<p>ReaderResultActivity Code:</p>

<pre><code>@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.activity_reader_result);

    ImageView img1 = (ImageView)findViewById(R.id.imageView2);
    ImageView img2 = (ImageView)findViewById(R.id.imageView3);
    ImageView img3 = (ImageView)findViewById(R.id.imageView4);
    TextView scanResults = (TextView)findViewById(R.id.textView);

    //Get bitmap from a static class.
    Bitmap bitmap = Reader.img;

    Bitmap grayScale = toGrayscale(bitmap);
    Bitmap blackWhiteImage = createContrast(grayScale, 50);
    Bitmap invertColor = invertColor(blackWhiteImage);

    //Show process steps
    img1.setImageBitmap(grayScale);
    img2.setImageBitmap(blackWhiteImage);
    img3.setImageBitmap(invertColor);

    TextRecognizer detector = new TextRecognizer.Builder(getApplicationContext()).build();

    try {

        if (detector.isOperational()) {
            Frame frame = new Frame.Builder().setBitmap(invertColor).build();
            SparseArray&lt;TextBlock&gt; textBlocks = detector.detect(frame);
            String blocks = """";
            String lines = """";
            String words = """";
            for (int index = 0; index &lt; textBlocks.size(); index++) {
                //extract scanned text blocks here
                TextBlock tBlock = textBlocks.valueAt(index);
                blocks = blocks + tBlock.getValue() + ""\n"" + ""\n"";
                for (Text line : tBlock.getComponents()) {
                    //extract scanned text lines here
                    lines = lines + line.getValue() + ""\n"";
                    for (Text element : line.getComponents()) {
                        //extract scanned text words here
                        words = words + element.getValue() + "", "";
                    }
                }
            }
            if (textBlocks.size() == 0) {
                scanResults.setText(""Scan Failed: Found nothing to scan"");
            } else {
                lines = lines.replaceAll(""o"", ""0"");
                lines = lines.replaceAll(""A"", ""1"");

                scanResults.setText(lines + ""\n"");
            }
        } else {
            scanResults.setText(""Could not set up the detector!"");
        }
    } catch (Exception e) {
        Toast.makeText(this, ""Failed to load Image"", Toast.LENGTH_SHORT)
                .show();
        Log.e(""312"", e.toString());
    }
}

private Bitmap processImage(Bitmap bitmap){
    Bitmap grayScale = toGrayscale(bitmap);
    Bitmap blackWhiteImage = createContrast(grayScale, 50);
    Bitmap invertColor = invertColor(blackWhiteImage);

    return invertColor;
}
public Bitmap toGrayscale(Bitmap bmpOriginal) {
    int width, height;
    height = bmpOriginal.getHeight();
    width = bmpOriginal.getWidth();

    Bitmap bmpGrayscale = Bitmap.createBitmap(width, height, bmpOriginal.getConfig());
    Canvas c = new Canvas(bmpGrayscale);
    Paint paint = new Paint();
    ColorMatrix cm = new ColorMatrix();
    cm.setSaturation(0);
    ColorMatrixColorFilter f = new ColorMatrixColorFilter(cm);
    paint.setColorFilter(f);
    c.drawBitmap(bmpOriginal, 0, 0, paint);
    return bmpGrayscale;
}

public static Bitmap createContrast(Bitmap src, double value) {
    // image size
    int width = src.getWidth();
    int height = src.getHeight();
    // create output bitmap
    Bitmap bmOut = Bitmap.createBitmap(width, height, src.getConfig());
    // color information
    int A, R, G, B;
    int pixel;
    // get contrast value
    double contrast = Math.pow((100 + value) / 100, 2);

    // scan through all pixels
    for(int x = 0; x &lt; width; ++x) {
        for(int y = 0; y &lt; height; ++y) {
            // get pixel color
            pixel = src.getPixel(x, y);
            A = Color.alpha(pixel);
            // apply filter contrast for every channel R, G, B
            R = Color.red(pixel);
            R = (int)(((((R / 255.0) - 0.5) * contrast) + 0.5) * 255.0);
            if(R &lt; 0) { R = 0; }
            else if(R &gt; 255) { R = 255; }

            G = Color.red(pixel);
            G = (int)(((((G / 255.0) - 0.5) * contrast) + 0.5) * 255.0);
            if(G &lt; 0) { G = 0; }
            else if(G &gt; 255) { G = 255; }

            B = Color.red(pixel);
            B = (int)(((((B / 255.0) - 0.5) * contrast) + 0.5) * 255.0);
            if(B &lt; 0) { B = 0; }
            else if(B &gt; 255) { B = 255; }

            // set new pixel color to output bitmap
            bmOut.setPixel(x, y, Color.argb(A, R, G, B));
        }
    }

    return bmOut;
}

Bitmap invertColor(Bitmap src){
    Bitmap copy = src.copy(src.getConfig(), true);

    for (int x = 0; x &lt; copy.getWidth(); ++x) {
        for (int y = 0; y &lt; copy.getHeight(); ++y) {
            int color = copy.getPixel(x, y);
            int r = Color.red(color);
            int g = Color.green(color);
            int b = Color.blue(color);
            int avg = (r + g + b) / 3;
            int newColor = Color.argb(255, 255 - avg, 255 - avg, 255 - avg);
            copy.setPixel(x, y, newColor);
        }
    }


    return copy;
}
</code></pre>

<p>Already try to do this in Manifest</p>

<blockquote>
  <p>android:largeHeap=""true""</p>
</blockquote>

<p>But the application just stop running when is on:</p>

<blockquote>
  <p>ReaderResultActivity.createContrast(ReaderResultActivity.java:123)</p>
</blockquote>

<p>The same line that appears on error without the ""largeHeap"" tag.
Just dont know what to do, but i think that has something with all those ""Bitmap.CreateBitmap"" in every process function.
But without doing this, in OCR reading, appear a error saying that the bitmap has a wrong format.</p>",,1,2,,2017-11-01 05:47:31.690 UTC,1,2017-11-01 06:00:54.613 UTC,,,,,1779394,1,0,java|android|bitmap|out-of-memory|ocr,535
Handle OCR / Computer Vision result to match a receipt structure,40837023,Handle OCR / Computer Vision result to match a receipt structure,"<p>I'm using Microsoft Computer Vision to read receipts.</p>

<p>The result I get is ordered into regions that are grouped by columns, e.g. quantities, product names, amount are in three different regions.</p>

<p>I would to prefer if the whole list of products is one region and that each line is a product.</p>

<p>Is there any way to configure the Computer Vision to accomplish this, or maybe more likely are there any good techniques or libraries that one can use in a post-processing of the result since the positions of all the words are available.</p>

<p>Bellow is the image of the receipt and the result from the computer vision.</p>

<p><a href=""https://i.stack.imgur.com/da8SP.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/da8SP.jpg"" alt=""receipt""></a></p>

<pre><code>{
  ""language"": ""sv"",
  ""textAngle"": 2.0999999999999632,
  ""orientation"": ""Up"",
  ""regions"": [
    {
      ""boundingBox"": ""1012,450,660,326"",
      ""lines"": [
        {
          ""boundingBox"": ""1362,450,76,30"",
          ""words"": [
            {
              ""boundingBox"": ""1362,450,76,30"",
              ""text"": ""JULA""
            }
          ]
        },
        {
          ""boundingBox"": ""1207,486,465,49"",
          ""words"": [
            {
              ""boundingBox"": ""1207,502,172,33"",
              ""text"": ""Ekslinsan""
            },
            {
              ""boundingBox"": ""1400,497,51,30"",
              ""text"": ""3B,""
            },
            {
              ""boundingBox"": ""1479,491,95,33"",
              ""text"": ""25467""
            },
            {
              ""boundingBox"": ""1595,486,77,32"",
              ""text"": ""VALA""
            }
          ]
        },
        {
          ""boundingBox"": ""1304,539,265,38"",
          ""words"": [
            {
              ""boundingBox"": ""1304,539,265,38"",
              ""text"": ""SE5S6944785601""
            }
          ]
        },
        {
          ""boundingBox"": ""1245,584,369,44"",
          ""words"": [
            {
              ""boundingBox"": ""1245,594,148,34"",
              ""text"": ""Telefon:""
            },
            {
              ""boundingBox"": ""1421,584,193,37"",
              ""text"": ""042-324040""
            }
          ]
        },
        {
          ""boundingBox"": ""1012,695,269,35"",
          ""words"": [
            {
              ""boundingBox"": ""1012,702,75,28"",
              ""text"": ""Kund""
            },
            {
              ""boundingBox"": ""1109,695,172,33"",
              ""text"": ""072202787""
            }
          ]
        },
        {
          ""boundingBox"": ""1109,738,289,38"",
          ""words"": [
            {
              ""boundingBox"": ""1109,744,133,32"",
              ""text"": ""LILLVIK""
            },
            {
              ""boundingBox"": ""1265,738,133,32"",
              ""text"": ""ANDREAS""
            }
          ]
        }
      ]
    },
    {
      ""boundingBox"": ""1085,845,14,516"",
      ""lines"": [
        {
          ""boundingBox"": ""1090,845,9,29"",
          ""words"": [
            {
              ""boundingBox"": ""1090,845,9,29"",
              ""text"": ""1""
            }
          ]
        },
        {
          ""boundingBox"": ""1087,1037,9,28"",
          ""words"": [
            {
              ""boundingBox"": ""1087,1037,9,28"",
              ""text"": ""1""
            }
          ]
        },
        {
          ""boundingBox"": ""1086,1133,9,27"",
          ""words"": [
            {
              ""boundingBox"": ""1086,1133,9,27"",
              ""text"": ""I""
            }
          ]
        },
        {
          ""boundingBox"": ""1085,1332,9,29"",
          ""words"": [
            {
              ""boundingBox"": ""1085,1332,9,29"",
              ""text"": ""1""
            }
          ]
        }
      ]
    },
    {
      ""boundingBox"": ""1122,839,454,573"",
      ""lines"": [
        {
          ""boundingBox"": ""1128,839,173,33"",
          ""words"": [
            {
              ""boundingBox"": ""1128,843,36,29"",
              ""text"": ""ST""
            },
            {
              ""boundingBox"": ""1186,839,115,30"",
              ""text"": ""661107""
            }
          ]
        },
        {
          ""boundingBox"": ""1127,879,389,41"",
          ""words"": [
            {
              ""boundingBox"": ""1127,887,232,33"",
              ""text"": ""VERKTYGSLÅDR""
            },
            {
              ""boundingBox"": ""1382,883,36,28"",
              ""text"": ""JC""
            },
            {
              ""boundingBox"": ""1441,882,16,26"",
              ""text"": ""5""
            },
            {
              ""boundingBox"": ""1481,879,35,28"",
              ""text"": ""ÅR""
            }
          ]
        },
        {
          ""boundingBox"": ""1126,935,173,34"",
          ""words"": [
            {
              ""boundingBox"": ""1126,940,36,29"",
              ""text"": ""ST""
            },
            {
              ""boundingBox"": ""1187,935,112,32"",
              ""text"": ""181460""
            }
          ]
        },
        {
          ""boundingBox"": ""1126,967,450,50"",
          ""words"": [
            {
              ""boundingBox"": ""1126,987,75,30"",
              ""text"": ""BORR""
            },
            {
              ""boundingBox"": ""1224,977,193,35"",
              ""text"": ""GLAS/KRKEL""
            },
            {
              ""boundingBox"": ""1440,974,16,27"",
              ""text"": ""ø""
            },
            {
              ""boundingBox"": ""1482,971,34,27"",
              ""text"": ""10""
            },
            {
              ""boundingBox"": ""1539,967,37,28"",
              ""text"": ""MM""
            }
          ]
        },
        {
          ""boundingBox"": ""1125,1027,173,37"",
          ""words"": [
            {
              ""boundingBox"": ""1125,1036,36,28"",
              ""text"": ""ST""
            },
            {
              ""boundingBox"": ""1185,1027,113,34"",
              ""text"": ""181740""
            }
          ]
        },
        {
          ""boundingBox"": ""1124,1062,432,49"",
          ""words"": [
            {
              ""boundingBox"": ""1124,1071,252,40"",
              ""text"": ""UNIVERSALBORR""
            },
            {
              ""boundingBox"": ""1400,1066,96,32"",
              ""text"": ""8X120""
            },
            {
              ""boundingBox"": ""1519,1062,37,30"",
              ""text"": ""MM""
            }
          ]
        },
        {
          ""boundingBox"": ""1123,1125,175,34"",
          ""words"": [
            {
              ""boundingBox"": ""1123,1129,36,30"",
              ""text"": ""ST""
            },
            {
              ""boundingBox"": ""1183,1125,115,32"",
              ""text"": ""181738""
            }
          ]
        },
        {
          ""boundingBox"": ""1122,1164,416,44"",
          ""words"": [
            {
              ""boundingBox"": ""1122,1170,255,38"",
              ""text"": ""UNIVERSRLBORR""
            },
            {
              ""boundingBox"": ""1501,1164,37,31"",
              ""text"": ""MM""
            }
          ]
        },
        {
          ""boundingBox"": ""1123,1225,170,33"",
          ""words"": [
            {
              ""boundingBox"": ""1123,1228,36,30"",
              ""text"": ""ST""
            },
            {
              ""boundingBox"": ""1183,1225,110,32"",
              ""text"": ""316401""
            }
          ]
        },
        {
          ""boundingBox"": ""1123,1270,355,39"",
          ""words"": [
            {
              ""boundingBox"": ""1123,1275,216,34"",
              ""text"": ""LÅSCYLINDER""
            },
            {
              ""boundingBox"": ""1362,1270,116,33"",
              ""text"": ""2-PACK""
            }
          ]
        },
        {
          ""boundingBox"": ""1123,1327,177,34"",
          ""words"": [
            {
              ""boundingBox"": ""1123,1330,37,31"",
              ""text"": ""ST""
            },
            {
              ""boundingBox"": ""1183,1327,117,32"",
              ""text"": ""396026""
            }
          ]
        },
        {
          ""boundingBox"": ""1124,1373,356,39"",
          ""words"": [
            {
              ""boundingBox"": ""1124,1377,216,35"",
              ""text"": ""LÅSCYLINDER""
            },
            {
              ""boundingBox"": ""1363,1373,117,33"",
              ""text"": ""2-PRCK""
            }
          ]
        }
      ]
    },
    {
      ""boundingBox"": ""1644,820,118,524"",
      ""lines"": [
        {
          ""boundingBox"": ""1658,820,96,31"",
          ""words"": [
            {
              ""boundingBox"": ""1658,820,96,31"",
              ""text"": ""79,00""
            }
          ]
        },
        {
          ""boundingBox"": ""1659,912,97,31"",
          ""words"": [
            {
              ""boundingBox"": ""1659,916,50,27"",
              ""text"": ""44,""
            },
            {
              ""boundingBox"": ""1719,912,37,28"",
              ""text"": ""90""
            }
          ]
        },
        {
          ""boundingBox"": ""1659,1004,98,32"",
          ""words"": [
            {
              ""boundingBox"": ""1659,1007,51,29"",
              ""text"": ""69,""
            },
            {
              ""boundingBox"": ""1720,1004,37,28"",
              ""text"": ""90""
            }
          ]
        },
        {
          ""boundingBox"": ""1661,1103,97,35"",
          ""words"": [
            {
              ""boundingBox"": ""1661,1103,97,35"",
              ""text"": ""49,90""
            }
          ]
        },
        {
          ""boundingBox"": ""1644,1309,118,35"",
          ""words"": [
            {
              ""boundingBox"": ""1644,1309,118,35"",
              ""text"": ""299,00""
            }
          ]
        }
      ]
    },
    {
      ""boundingBox"": ""1064,1469,620,45"",
      ""lines"": [
        {
          ""boundingBox"": ""1064,1469,620,45"",
          ""words"": [
            {
              ""boundingBox"": ""1064,1481,237,33"",
              ""text"": ""-Rabattcheck""
            },
            {
              ""boundingBox"": ""1324,1486,51,24"",
              ""text"": ""nr:""
            },
            {
              ""boundingBox"": ""1384,1469,300,38"",
              ""text"": ""935011035567095""
            }
          ]
        }
      ]
    },
    {
      ""boundingBox"": ""1123,1584,159,82"",
      ""lines"": [
        {
          ""boundingBox"": ""1123,1584,159,33"",
          ""words"": [
            {
              ""boundingBox"": ""1123,1584,159,33"",
              ""text"": ""DELSUMMA""
            }
          ]
        },
        {
          ""boundingBox"": ""1143,1635,116,31"",
          ""words"": [
            {
              ""boundingBox"": ""1143,1635,116,31"",
              ""text"": ""Rabatt""
            }
          ]
        }
      ]
    },
    {
      ""boundingBox"": ""1609,1570,180,189"",
      ""lines"": [
        {
          ""boundingBox"": ""1609,1570,160,36"",
          ""words"": [
            {
              ""boundingBox"": ""1609,1575,11,31"",
              ""text"": ""|""
            },
            {
              ""boundingBox"": ""1648,1570,121,34"",
              ""text"": ""041,70""
            }
          ]
        },
        {
          ""boundingBox"": ""1690,1621,99,34"",
          ""words"": [
            {
              ""boundingBox"": ""1690,1621,99,34"",
              ""text"": ""50,00""
            }
          ]
        },
        {
          ""boundingBox"": ""1651,1725,120,34"",
          ""words"": [
            {
              ""boundingBox"": ""1651,1727,53,32"",
              ""text"": ""991""
            },
            {
              ""boundingBox"": ""1715,1746,9,13"",
              ""text"": "",""
            },
            {
              ""boundingBox"": ""1732,1725,39,32"",
              ""text"": ""70""
            }
          ]
        }
      ]
    },
    {
      ""boundingBox"": ""992,1737,310,1226"",
      ""lines"": [
        {
          ""boundingBox"": ""1123,1737,179,35"",
          ""words"": [
            {
              ""boundingBox"": ""1123,1737,179,35"",
              ""text"": ""SLUTSUMMA""
            }
          ]
        },
        {
          ""boundingBox"": ""1036,2756,227,35"",
          ""words"": [
            {
              ""boundingBox"": ""1036,2756,227,35"",
              ""text"": ""Totalbelopp""
            }
          ]
        },
        {
          ""boundingBox"": ""1140,2811,124,37"",
          ""words"": [
            {
              ""boundingBox"": ""1140,2811,53,35"",
              ""text"": ""991""
            },
            {
              ""boundingBox"": ""1207,2833,8,15"",
              ""text"": ""/""
            },
            {
              ""boundingBox"": ""1225,2811,39,34"",
              ""text"": ""70""
            }
          ]
        },
        {
          ""boundingBox"": ""992,2927,271,36"",
          ""words"": [
            {
              ""boundingBox"": ""992,2928,159,35"",
              ""text"": ""Säljare:""
            },
            {
              ""boundingBox"": ""1182,2927,81,33"",
              ""text"": ""7688""
            }
          ]
        }
      ]
    },
    {
      ""boundingBox"": ""1330,2754,145,92"",
      ""lines"": [
        {
          ""boundingBox"": ""1330,2754,144,34"",
          ""words"": [
            {
              ""boundingBox"": ""1330,2754,39,33"",
              ""text"": ""Ex""
            },
            {
              ""boundingBox"": ""1394,2754,80,34"",
              ""text"": ""Moms""
            }
          ]
        },
        {
          ""boundingBox"": ""1352,2809,123,37"",
          ""words"": [
            {
              ""boundingBox"": ""1352,2809,123,37"",
              ""text"": ""793,36""
            }
          ]
        }
      ]
    },
    {
      ""boundingBox"": ""1563,2752,126,92"",
      ""lines"": [
        {
          ""boundingBox"": ""1563,2752,125,33"",
          ""words"": [
            {
              ""boundingBox"": ""1563,2752,82,33"",
              ""text"": ""Moms""
            },
            {
              ""boundingBox"": ""1670,2755,18,27"",
              ""text"": ""%""
            }
          ]
        },
        {
          ""boundingBox"": ""1586,2808,103,36"",
          ""words"": [
            {
              ""boundingBox"": ""1586,2808,103,36"",
              ""text"": ""25,00""
            }
          ]
        }
      ]
    },
    {
      ""boundingBox"": ""1780,2751,123,93"",
      ""lines"": [
        {
          ""boundingBox"": ""1820,2751,83,33"",
          ""words"": [
            {
              ""boundingBox"": ""1820,2751,83,33"",
              ""text"": ""Moms""
            }
          ]
        },
        {
          ""boundingBox"": ""1780,2807,123,37"",
          ""words"": [
            {
              ""boundingBox"": ""1780,2807,123,37"",
              ""text"": ""198,34""
            }
          ]
        }
      ]
    },
    {
      ""boundingBox"": ""985,2924,966,573"",
      ""lines"": [
        {
          ""boundingBox"": ""1523,2924,83,33"",
          ""words"": [
            {
              ""boundingBox"": ""1523,2924,83,33"",
              ""text"": ""7618""
            }
          ]
        },
        {
          ""boundingBox"": ""1288,2926,167,33"",
          ""words"": [
            {
              ""boundingBox"": ""1288,2939,17,7"",
              ""text"": ""-""
            },
            {
              ""boundingBox"": ""1330,2926,125,33"",
              ""text"": ""Sabina""
            }
          ]
        },
        {
          ""boundingBox"": ""1182,2981,468,36"",
          ""words"": [
            {
              ""boundingBox"": ""1182,2983,38,34"",
              ""text"": ""24""
            },
            {
              ""boundingBox"": ""1245,2982,146,34"",
              ""text"": ""oktober""
            },
            {
              ""boundingBox"": ""1416,2982,82,34"",
              ""text"": ""2016""
            },
            {
              ""boundingBox"": ""1547,2982,10,33"",
              ""text"": ""1""
            },
            {
              ""boundingBox"": ""1571,2981,79,34"",
              ""text"": ""7:20""
            }
          ]
        },
        {
          ""boundingBox"": ""991,2985,103,33"",
          ""words"": [
            {
              ""boundingBox"": ""991,2985,103,33"",
              ""text"": ""Datum""
            }
          ]
        },
        {
          ""boundingBox"": ""1161,3040,403,34"",
          ""words"": [
            {
              ""boundingBox"": ""1161,3040,96,34"",
              ""text"": ""44601""
            },
            {
              ""boundingBox"": ""1288,3040,140,34"",
              ""text"": ""Kvitto:""
            },
            {
              ""boundingBox"": ""1460,3040,104,34"",
              ""text"": ""51756""
            }
          ]
        },
        {
          ""boundingBox"": ""990,3042,103,33"",
          ""words"": [
            {
              ""boundingBox"": ""990,3042,103,33"",
              ""text"": ""Kassa""
            }
          ]
        },
        {
          ""boundingBox"": ""1096,3157,728,40"",
          ""words"": [
            {
              ""boundingBox"": ""1096,3159,105,38"",
              ""text"": ""Spara""
            },
            {
              ""boundingBox"": ""1225,3157,163,39"",
              ""text"": ""kvittot,""
            },
            {
              ""boundingBox"": ""1418,3157,127,39"",
              ""text"": ""gäller""
            },
            {
              ""boundingBox"": ""1570,3169,63,26"",
              ""text"": ""som""
            },
            {
              ""boundingBox"": ""1657,3158,167,39"",
              ""text"": ""garanti.""
            }
          ]
        },
        {
          ""boundingBox"": ""1268,3217,388,39"",
          ""words"": [
            {
              ""boundingBox"": ""1268,3217,103,39"",
              ""text"": ""Öppet""
            },
            {
              ""boundingBox"": ""1397,3218,62,38"",
              ""text"": ""köp""
            },
            {
              ""boundingBox"": ""1484,3218,41,37"",
              ""text"": ""30""
            },
            {
              ""boundingBox"": ""1550,3218,106,38"",
              ""text"": ""dager""
            }
          ]
        },
        {
          ""boundingBox"": ""1290,3276,317,39"",
          ""words"": [
            {
              ""boundingBox"": ""1290,3276,192,38"",
              ""text"": ""VÄLKOMMEN""
            },
            {
              ""boundingBox"": ""1506,3278,101,37"",
              ""text"": ""ÅTER!""
            }
          ]
        },
        {
          ""boundingBox"": ""1116,3335,719,42"",
          ""words"": [
            {
              ""boundingBox"": ""1116,3337,41,36"",
              ""text"": ""Om""
            },
            {
              ""boundingBox"": ""1182,3335,82,38"",
              ""text"": ""ditt""
            },
            {
              ""boundingBox"": ""1290,3346,84,28"",
              ""text"": ""namn""
            },
            {
              ""boundingBox"": ""1398,3337,63,38"",
              ""text"": ""och""
            },
            {
              ""boundingBox"": ""1485,3349,261,28"",
              ""text"": ""personnummer""
            },
            {
              ""boundingBox"": ""1771,3338,64,37"",
              ""text"": ""har""
            }
          ]
        },
        {
          ""boundingBox"": ""1032,3395,894,42"",
          ""words"": [
            {
              ""boundingBox"": ""1032,3397,146,36"",
              ""text"": ""lämnats""
            },
            {
              ""boundingBox"": ""1204,3395,62,38"",
              ""text"": ""för""
            },
            {
              ""boundingBox"": ""1290,3395,61,38"",
              ""text"": ""att""
            },
            {
              ""boundingBox"": ""1377,3399,194,36"",
              ""text"": ""genomföra""
            },
            {
              ""boundingBox"": ""1596,3399,61,36"",
              ""text"": ""ett""
            },
            {
              ""boundingBox"": ""1685,3399,241,38"",
              ""text"": ""JulaPro-köp""
            }
          ]
        },
        {
          ""boundingBox"": ""985,3455,966,42"",
          ""words"": [
            {
              ""boundingBox"": ""985,3456,193,37"",
              ""text"": ""behandlar""
            },
            {
              ""boundingBox"": ""1203,3455,85,37"",
              ""text"": ""Jula""
            },
            {
              ""boundingBox"": ""1312,3456,84,37"",
              ""text"": ""dina""
            },
            {
              ""boundingBox"": ""1421,3458,195,39"",
              ""text"": ""uppgifter""
            },
            {
              ""boundingBox"": ""1645,3462,12,33"",
              ""text"": ""i""
            },
            {
              ""boundingBox"": ""1686,3458,173,38"",
              ""text"": ""enlighet""
            },
            {
              ""boundingBox"": ""1886,3461,65,36"",
              ""text"": ""med""
            }
          ]
        }
      ]
    }
  ]
}
</code></pre>",,2,2,,2016-11-28 04:06:02.490 UTC,3,2018-10-17 23:01:09.630 UTC,2016-12-03 04:57:56.283 UTC,,3881090,,3881090,1,8,c#|computer-vision|ocr|microsoft-cognitive|post-processing,2555
"Google Vision API : POST returns 200 OK, but no data",47574353,"Google Vision API : POST returns 200 OK, but no data","<p>We have been struggling with posting images for recognition through the Google Vision API for some days now..</p>

<p>We are serializing the JSON object using Newtonsoft. It seems that Google Vision is handling this escaped JSON-string differently (not valid?), than when it's not escaped (valid JSON format).</p>

<p>.. cause when removing the escaped characters, and posting it directly to the Google API using Postman it works.</p>

<p><strong>POST :</strong> </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>public async Task&lt;IActionResult&gt; Post([FromBody] ICollection&lt;IFormFile&gt; files)
        {
            var API_KEY = ""xxxxxxxxxxxxxx"";
            var AUTH_URL = ""https://vision.googleapis.com/v1/images:annotate?key="" + API_KEY;


    
            string filePath = System.IO.Path.GetTempFileName();
            foreach (var formFile in files)
            {
                if (formFile.Length &gt; 0)
                {
                    using (var stream = new MemoryStream())
                    {
                        await formFile.CopyToAsync(stream);
                        var base64Image = Extensions.ConvertToBase64(stream);
                        
                        var DetectionTypes = new List&lt;string&gt;() { ""LABEL_DETECTION"", ""TEXT_DETECTION"" };
                        var googleReq = new GoogleRequest();
                        var requests = googleReq.Requests;
                        requests = new List&lt;Request&gt;(){(new Request())};
                        requests[0].Features = new List&lt;Feature&gt;(){new Feature(""LABEL_DETECTION""), new Feature(""TEXT_DETECTION"")};
                        requests[0].Image = new ugle.Image(base64Image);
                        googleReq.Requests = requests;
                        
                        var json = JsonConvert.SerializeObject(googleReq);
                        var request = new StringContent(json);
                        request.Headers.ContentType = new MediaTypeHeaderValue(""application/json"");
                        var result = await httpclient.PostAsync(new Uri(AUTH_URL), request);
                        /*
                        var json = JsonConvert.SerializeObject(googleReq);
                        var content = new StringContent(json, Encoding.UTF8, ""application/json"");
                        var result = await httpclient.PostAsync(AUTH_URL, content);
                        */
                        return Ok( new{result=result, Request= json});
                    }

                }
            }

            return Ok(new { count = files.Count, filePath });
            //return Ok(new PictureInfo(null, null, null, BarcodeNumbers: null));

        }</code></pre>
</div>
</div>
</p>

<p><strong>Escaped JSON (not working)</strong></p>

<pre><code>""{\""requests\"":[{\""image\"":{\""content\"":\""/9j/4QAuxW1f... etc.etc.. base64 image content.../Z\""},\""features\"":[{\""type\"":\""LABEL_DETECTION\""},{\""type\"":\""TEXT_DETECTION\""}]}]}""
</code></pre>

<p><strong>Non-Escaped JSON (working) :</strong></p>

<pre><code>{""requests"":[{""image"":{""content"":""/9j/W1f/Z""},""features"":[{""type"":""LABEL_DETECTION""},{""type"":""TEXT_DETECTION""}]}]}
</code></pre>

<p><strong>Edited : Screenshot of json (escaped json)</strong>
<a href=""https://i.stack.imgur.com/MCh69.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MCh69.jpg"" alt=""enter image description here""></a></p>",,0,8,,2017-11-30 13:12:17.210 UTC,1,2017-11-30 13:58:28.610 UTC,2017-11-30 13:58:28.610 UTC,,1287699,,1287699,1,1,c#|json|asp.net-web-api2|asp.net-core-webapi|google-vision,214
How to convert a file-path URI to a readable https URL,49848920,How to convert a file-path URI to a readable https URL,"<p>Is there a way to convert a captured file path uri as such 
<code>file:///var/mobile/Containers/Data/Application/1E1E919E-6C21-4CC7-B9C2-5B4B3BC84B0F/Library/Caches/ExponentExperienceData/%2540chuks93%252Fihu-main/Camera/F3B8EBCC-BB09-4603-AF7E-FD3CA792C237.jpg</code> to say a temporary https URL?
Im working with the Microsoft azure face API and I'm running into errors that suggest that only way to run the API is with network URLs like <code>https</code>.</p>

<p>Anyone know of a workaround solution for this?</p>

<pre><code>export default  {

processImage: (image) =&gt; {
// Replace the subscriptionKey string value with your valid subscription key.
var subscriptionKey = ""********************"";



var uriBase = ""https://westus.api.cognitive.microsoft.com/face/v1.0/detect?returnFaceId=true&amp;returnFaceLandmarks=false&amp;returnFaceAttributes=age,gender,headPose,smile,facialHair,glasses,emotion,hair,makeup,occlusion,accessories,blur,exposure,noise"";

// Display the image.
    var sourceImageUrl = image;
console.log(typeof sourceImageUrl)
console.log(""here"", sourceImageUrl);

// Perform the REST API call.
return fetch(uriBase, {

        method: 'POST',
    headers: {
        ""Content-type"": ""application/json"",
        ""Ocp-Apim-Subscription-Key"": subscriptionKey
    },

    body: JSON.stringify({ uri: sourceImageUrl })
    })
    .then((data) =&gt; data.json())
    .then(function (data){
        console.log(""hello"", data);
    })


    .catch(function (error) {
        console.log(error);
    });

}
</code></pre>

<p>}</p>",,0,4,,2018-04-16 02:37:48.793 UTC,,2018-04-16 20:04:39.043 UTC,2018-04-16 20:04:39.043 UTC,,8817135,,8817135,1,0,javascript|azure|https|filepath,243
aws-java-sdk get temporary credentials for non aws environment,43586009,aws-java-sdk get temporary credentials for non aws environment,"<p>I have access to aws account with username <code>logingUserId</code>. I want to create access profile in my CI server so that I can test my applications against the AWS tools like kinesis, dynamodb etc.</p>

<p>I wrote a method to generate access key, secret key and session token(using <code>AssumeRoleRequest</code>). It does not seem to be working.</p>

<pre><code>  it(""provides temporary access to AWS"") {
    val assumeRoleRequest = new AssumeRoleRequest

    assumeRoleRequest.setRoleArn(""arn:aws:iam::"" + accountId + "":role/"" + roleName)
    assumeRoleRequest.setRoleSessionName(""test-session"")
    assumeRoleRequest.setExternalId(loginUserId)

    val tokenService = new AWSSecurityTokenServiceClient() // 
    tokenService.setEndpoint(""sts-endpoint.amazonaws.com"")
    tokenService.assumeRole(assumeRoleRequest)

    val tokenRequestEvent = new GetSessionTokenRequest()
    tokenRequestEvent.setDurationSeconds(7200) // optional

    val tokenResponseEvent =
      tokenService.getSessionToken(tokenRequestEvent)

    val creds = tokenResponseEvent.getCredentials

    println(creds.getAccessKeyId) //write to ~/.aws/credentials
    println(creds.getSecretAccessKey) //write to ~/.aws/credentials
    println(creds.getSessionToken) //write to ~/.aws/credentials
    println(creds.getExpiration)
  }
</code></pre>

<p><strong>Error - Unable to load AWS credentials from any provider in the chain</strong></p>

<pre><code>/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java -Didea.launcher.port=7541 ""-Didea.launcher.bin.path=/Applications/IntelliJ IDEA.app/Contents/bin"" -Dfile.encoding=UTF-8 -classpath ""/Users/as18/Library/Application Support/IntelliJIdea2016.2/Scala/lib/scala-plugin-runners.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/lib/tools.jar:/Users/as18/possibilities/programming/s2/whats-in-stream-v2/target/test-classes:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk/1.11.109/aws-java-sdk-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-pinpoint/1.11.109/aws-java-sdk-pinpoint-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/jmespath-java/1.11.109/jmespath-java-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-xray/1.11.109/aws-java-sdk-xray-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-opsworkscm/1.11.109/aws-java-sdk-opsworkscm-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-support/1.11.109/aws-java-sdk-support-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-simpledb/1.11.109/aws-java-sdk-simpledb-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-servicecatalog/1.11.109/aws-java-sdk-servicecatalog-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-servermigration/1.11.109/aws-java-sdk-servermigration-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-simpleworkflow/1.11.109/aws-java-sdk-simpleworkflow-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-storagegateway/1.11.109/aws-java-sdk-storagegateway-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-route53/1.11.109/aws-java-sdk-route53-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-s3/1.11.109/aws-java-sdk-s3-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-importexport/1.11.109/aws-java-sdk-importexport-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-sts/1.11.109/aws-java-sdk-sts-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-sqs/1.11.109/aws-java-sdk-sqs-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-rds/1.11.109/aws-java-sdk-rds-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-redshift/1.11.109/aws-java-sdk-redshift-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-elasticbeanstalk/1.11.109/aws-java-sdk-elasticbeanstalk-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-glacier/1.11.109/aws-java-sdk-glacier-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-iam/1.11.109/aws-java-sdk-iam-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-datapipeline/1.11.109/aws-java-sdk-datapipeline-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-elasticloadbalancing/1.11.109/aws-java-sdk-elasticloadbalancing-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-elasticloadbalancingv2/1.11.109/aws-java-sdk-elasticloadbalancingv2-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-emr/1.11.109/aws-java-sdk-emr-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-elasticache/1.11.109/aws-java-sdk-elasticache-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-elastictranscoder/1.11.109/aws-java-sdk-elastictranscoder-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-ec2/1.11.109/aws-java-sdk-ec2-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-dynamodb/1.11.109/aws-java-sdk-dynamodb-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-sns/1.11.109/aws-java-sdk-sns-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-budgets/1.11.109/aws-java-sdk-budgets-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-cloudtrail/1.11.109/aws-java-sdk-cloudtrail-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-cloudwatch/1.11.109/aws-java-sdk-cloudwatch-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-logs/1.11.109/aws-java-sdk-logs-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-events/1.11.109/aws-java-sdk-events-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-cognitoidentity/1.11.109/aws-java-sdk-cognitoidentity-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-cognitosync/1.11.109/aws-java-sdk-cognitosync-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-directconnect/1.11.109/aws-java-sdk-directconnect-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-cloudformation/1.11.109/aws-java-sdk-cloudformation-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-cloudfront/1.11.109/aws-java-sdk-cloudfront-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-clouddirectory/1.11.109/aws-java-sdk-clouddirectory-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-kinesis/1.11.109/aws-java-sdk-kinesis-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-opsworks/1.11.109/aws-java-sdk-opsworks-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-ses/1.11.109/aws-java-sdk-ses-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-autoscaling/1.11.109/aws-java-sdk-autoscaling-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-cloudsearch/1.11.109/aws-java-sdk-cloudsearch-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-cloudwatchmetrics/1.11.109/aws-java-sdk-cloudwatchmetrics-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-codedeploy/1.11.109/aws-java-sdk-codedeploy-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-codepipeline/1.11.109/aws-java-sdk-codepipeline-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-kms/1.11.109/aws-java-sdk-kms-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-config/1.11.109/aws-java-sdk-config-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-lambda/1.11.109/aws-java-sdk-lambda-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-ecs/1.11.109/aws-java-sdk-ecs-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-ecr/1.11.109/aws-java-sdk-ecr-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-cloudhsm/1.11.109/aws-java-sdk-cloudhsm-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-ssm/1.11.109/aws-java-sdk-ssm-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-workspaces/1.11.109/aws-java-sdk-workspaces-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-machinelearning/1.11.109/aws-java-sdk-machinelearning-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-directory/1.11.109/aws-java-sdk-directory-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-efs/1.11.109/aws-java-sdk-efs-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-codecommit/1.11.109/aws-java-sdk-codecommit-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-devicefarm/1.11.109/aws-java-sdk-devicefarm-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-elasticsearch/1.11.109/aws-java-sdk-elasticsearch-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-waf/1.11.109/aws-java-sdk-waf-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-marketplacecommerceanalytics/1.11.109/aws-java-sdk-marketplacecommerceanalytics-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-inspector/1.11.109/aws-java-sdk-inspector-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-iot/1.11.109/aws-java-sdk-iot-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-api-gateway/1.11.109/aws-java-sdk-api-gateway-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-acm/1.11.109/aws-java-sdk-acm-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-gamelift/1.11.109/aws-java-sdk-gamelift-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-dms/1.11.109/aws-java-sdk-dms-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-marketplacemeteringservice/1.11.109/aws-java-sdk-marketplacemeteringservice-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-cognitoidp/1.11.109/aws-java-sdk-cognitoidp-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-discovery/1.11.109/aws-java-sdk-discovery-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-applicationautoscaling/1.11.109/aws-java-sdk-applicationautoscaling-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-snowball/1.11.109/aws-java-sdk-snowball-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-rekognition/1.11.109/aws-java-sdk-rekognition-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-polly/1.11.109/aws-java-sdk-polly-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-lightsail/1.11.109/aws-java-sdk-lightsail-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-stepfunctions/1.11.109/aws-java-sdk-stepfunctions-1.11.109.jar:/Users/as18/.m2/repository/com/jayway/jsonpath/json-path/2.2.0/json-path-2.2.0.jar:/Users/as18/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-health/1.11.109/aws-java-sdk-health-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-costandusagereport/1.11.109/aws-java-sdk-costandusagereport-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-codebuild/1.11.109/aws-java-sdk-codebuild-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-appstream/1.11.109/aws-java-sdk-appstream-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-shield/1.11.109/aws-java-sdk-shield-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-batch/1.11.109/aws-java-sdk-batch-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-lex/1.11.109/aws-java-sdk-lex-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-mechanicalturkrequester/1.11.109/aws-java-sdk-mechanicalturkrequester-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-organizations/1.11.109/aws-java-sdk-organizations-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-workdocs/1.11.109/aws-java-sdk-workdocs-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-core/1.11.109/aws-java-sdk-core-1.11.109.jar:/Users/as18/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/Users/as18/.m2/repository/org/apache/httpcomponents/httpclient/4.5.2/httpclient-4.5.2.jar:/Users/as18/.m2/repository/org/apache/httpcomponents/httpcore/4.4.4/httpcore-4.4.4.jar:/Users/as18/.m2/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/Users/as18/.m2/repository/software/amazon/ion/ion-java/1.0.2/ion-java-1.0.2.jar:/Users/as18/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.6/jackson-databind-2.6.6.jar:/Users/as18/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.0/jackson-annotations-2.6.0.jar:/Users/as18/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.6/jackson-core-2.6.6.jar:/Users/as18/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.6.6/jackson-dataformat-cbor-2.6.6.jar:/Users/as18/.m2/repository/joda-time/joda-time/2.8.1/joda-time-2.8.1.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-models/1.11.109/aws-java-sdk-models-1.11.109.jar:/Users/as18/.m2/repository/com/amazonaws/aws-java-sdk-swf-libraries/1.11.22/aws-java-sdk-swf-libraries-1.11.22.jar:/Users/as18/.m2/repository/org/scalatest/scalatest_2.11/3.0.1/scalatest_2.11-3.0.1.jar:/Users/as18/.m2/repository/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/Users/as18/.m2/repository/org/scalactic/scalactic_2.11/3.0.1/scalactic_2.11-3.0.1.jar:/Users/as18/.m2/repository/org/scala-lang/scala-reflect/2.11.8/scala-reflect-2.11.8.jar:/Users/as18/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/Users/as18/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/usr/local/scala-2.11.8/lib/scala-actors-2.11.0.jar:/usr/local/scala-2.11.8/lib/scala-actors-migration_2.11-1.1.0.jar:/usr/local/scala-2.11.8/lib/scala-library.jar:/usr/local/scala-2.11.8/lib/scala-parser-combinators_2.11-1.0.4.jar:/usr/local/scala-2.11.8/lib/scala-reflect.jar:/usr/local/scala-2.11.8/lib/scala-swing_2.11-1.0.2.jar:/usr/local/scala-2.11.8/lib/scala-xml_2.11-1.0.4.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar"" com.intellij.rt.execution.application.AppMain org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner -s creds.Test -testName ""provides temporary access to AWS"" -showProgressMessages true -C org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestReporter
Testing started at 3:20 AM ...

Unable to load AWS credentials from any provider in the chain
com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain
    at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:131)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1119)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:759)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:723)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:716)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)
    at com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.doInvoke(AWSSecurityTokenServiceClient.java:1271)
    at com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.invoke(AWSSecurityTokenServiceClient.java:1247)
    at com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.executeAssumeRole(AWSSecurityTokenServiceClient.java:454)
    at com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.assumeRole(AWSSecurityTokenServiceClient.java:431)
    at creds.Test$$anonfun$1.apply$mcV$sp(Test.scala:24)
    at creds.Test$$anonfun$1.apply(Test.scala:15)
    at creds.Test$$anonfun$1.apply(Test.scala:15)
    at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
    at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
    at org.scalatest.Transformer.apply(Transformer.scala:22)
    at org.scalatest.Transformer.apply(Transformer.scala:20)
    at org.scalatest.FunSpecLike$$anon$1.apply(FunSpecLike.scala:454)
    at org.scalatest.TestSuite$class.withFixture(TestSuite.scala:196)
    at org.scalatest.FunSpec.withFixture(FunSpec.scala:1630)
    at org.scalatest.FunSpecLike$class.invokeWithFixture$1(FunSpecLike.scala:451)
    at org.scalatest.FunSpecLike$$anonfun$runTest$1.apply(FunSpecLike.scala:464)
    at org.scalatest.FunSpecLike$$anonfun$runTest$1.apply(FunSpecLike.scala:464)
    at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
    at org.scalatest.FunSpecLike$class.runTest(FunSpecLike.scala:464)
    at org.scalatest.FunSpec.runTest(FunSpec.scala:1630)
    at org.scalatest.FunSpecLike$$anonfun$runTests$1.apply(FunSpecLike.scala:497)
    at org.scalatest.FunSpecLike$$anonfun$runTests$1.apply(FunSpecLike.scala:497)
    at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
    at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
    at scala.collection.immutable.List.foreach(List.scala:381)
    at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
    at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)
    at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
    at org.scalatest.FunSpecLike$class.runTests(FunSpecLike.scala:497)
    at org.scalatest.FunSpec.runTests(FunSpec.scala:1630)
    at org.scalatest.Suite$class.run(Suite.scala:1147)
    at org.scalatest.FunSpec.org$scalatest$FunSpecLike$$super$run(FunSpec.scala:1630)
    at org.scalatest.FunSpecLike$$anonfun$run$1.apply(FunSpecLike.scala:501)
    at org.scalatest.FunSpecLike$$anonfun$run$1.apply(FunSpecLike.scala:501)
    at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
    at org.scalatest.FunSpecLike$class.run(FunSpecLike.scala:501)
    at org.scalatest.FunSpec.run(FunSpec.scala:1630)
    at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
    at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
    at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)
    at scala.collection.immutable.List.foreach(List.scala:381)
    at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
    at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
    at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
    at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
    at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
    at org.scalatest.tools.Runner$.run(Runner.scala:850)
    at org.scalatest.tools.Runner.run(Runner.scala)
    at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:138)
    at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
</code></pre>

<p>Tried using <a href=""http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_control-access_assumerole.html"" rel=""nofollow noreferrer""><code>AssumeRoleWithWebIdentityRequest</code></a> too, which makes more sense than <code>AssumeRoleRequest</code>. But throws same Unable to load creds error.</p>

<pre><code>  it(""provides temporary access to AWS"") {
    val identityRequest = new AssumeRoleWithWebIdentityRequest()
    identityRequest.setRoleArn(""arn:aws:iam::"" + accountId + "":role/"" + roleName)
    //identityRequest.setWebIdentityToken(loginUserId) //I dont know what is it
    identityRequest.setRoleSessionName(loginUserId)

    val tokenService = new AWSSecurityTokenServiceClient()
    tokenService.setEndpoint(""sts-endpoint.amazonaws.com"")
    val creds = tokenService.assumeRoleWithWebIdentity(identityRequest).getCredentials

    println(creds.getAccessKeyId)
    println(creds.getSecretAccessKey)
    println(creds.getSessionToken)
    println(creds.getExpiration)
  }
</code></pre>

<p>The request its sending is </p>

<pre><code>POST null / Parameters: ({""Action"":[""AssumeRoleWithWebIdentity""],""Version"":[""2011-06-15""],""RoleArn"":[""arn:aws:iam::accountId:role/roleName""],""RoleSessionName"":[""loginUserId""]}
</code></pre>

<p>where resourcePath is <code>null</code>, dont know why?</p>

<p>I'm using <code>aws-java-sdk 1.11</code></p>

<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;
        &lt;artifactId&gt;aws-java-sdk&lt;/artifactId&gt;
        &lt;version&gt;1.11.109&lt;/version&gt;
        &lt;scope&gt;compile&lt;/scope&gt;
    &lt;/dependency&gt;
</code></pre>

<p><a href=""http://docs.aws.amazon.com/cli/latest/reference/sts/assume-role.html"" rel=""nofollow noreferrer"">On terminal</a>, asks for profile which I don't have. All I have is username and password to aws account.</p>

<pre><code>$ aws sts assume-role --role-arn arn:aws:iam::someAccount:role/rolenNameForMe --role-session-name ""RoleSession1"" &gt; assume-role-output.txt
Unable to locate credentials. You can configure credentials by running ""aws configure"".
</code></pre>

<p>When I check the UI users page, I have restricted access</p>

<pre><code>User: arn:aws:sts::accountId:assumed-role/roleName/loginUserId is not authorized to perform: iam:ListUsers on resource: arn:aws:iam::accountId:user/
</code></pre>",,1,0,,2017-04-24 10:43:54.763 UTC,,2017-05-17 20:35:00.457 UTC,2017-05-17 20:35:00.457 UTC,,298054,,432903,1,0,java|amazon-web-services|aws-sdk|aws-appstream,1036
failed to run Microsoft face api in C#,41895608,failed to run Microsoft face api in C#,"<p>I am trying to implement <strong>Microsoft Face API in C#</strong> using code available on GitHub. </p>

<p>I followed all the steps given in :</p>

<p><a href=""https://github.com/Microsoft/Cognitive-face-windows"" rel=""nofollow noreferrer"">Microsoft Face API: Windows Client Library &amp; Sample</a></p>

<p>I have some errors like:</p>

<p><strong>1-</strong> The name ""<code>SampleScenarios</code>"" does not exist in the namespace <code>""clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary""</code>.   </p>

<p><strong>2-</strong> The type '<code>sampleUtil:SampleScenarios</code>' was not found. Verify that you are not missing an assembly reference and that all referenced assemblies have been built. </p>

<p><strong>3-</strong> The tag '<code>SampleScenarios</code>' does not exist in XML namespace '<code>clr-namespace:SampleUserControlLibrary;assembly=SampleUserControlLibrary</code>'. Line 8 Position 10.    </p>

<p>In Solution Explorer, ""<code>SampleUserControlLibrary (Load fail)</code>"" appears: that means no user controls libraries are loaded.</p>",,0,2,,2017-01-27 13:54:53.110 UTC,,2017-01-30 07:53:02.330 UTC,2017-01-30 07:53:02.330 UTC,,1202025,,7478951,1,0,vb.net-to-c#,92
Python gTTS not accepting strings within variables,40668684,Python gTTS not accepting strings within variables,"<p>I am writing a python script to scan a photo which contains text with google vision OCR, then use Google gTTS to speak the text. Here is the code:</p>

<pre><code>#BookrBasic

from os import system
from time import sleep
from pygame import mixer
from gtts import gTTS
import subprocess


def tts(speech):
tts = gTTS(text=speech, lang='en')
tts.save(""/tmp/text.mp3"")
subprocess.Popen(['mpg123', '-q', '/tmp/text.mp3']).wait()

def ocr(file):
out = system('python3 ~/bookrbasic/ocr.py &lt;KEY GOES HERE&gt; ' + file)
return out

text = ocr(""~/bookrbasic/photos/canada4.jpg"")
tts(text)
</code></pre>

<p>This is the error I recieve:</p>

<pre><code>Traceback (most recent call last):
  File ""BookrBasic.py"", line 20, in &lt;module&gt;
    tts(text)
  File ""BookrBasic.py"", line 11, in tts
    tts = gTTS(text=speech, lang='en')
  File ""/usr/local/lib/python2.7/dist-packages/gtts/tts.py"", line 72, in     __init__
    raise Exception('No text to speak')
Exception: No text to speak
</code></pre>

<p>Does anyone know what the issue is here?</p>

<p>Thanks in advance.</p>",,1,0,,2016-11-18 03:02:39.230 UTC,,2016-11-18 22:50:11.403 UTC,2016-11-18 22:50:11.403 UTC,,486919,,6506426,1,0,python,311
Google Vision API Document_Text_Detection,53701338,Google Vision API Document_Text_Detection,"<p>I am trying to develop C# Google Vision API function.</p>

<p>the code is supposed to compile into dll and it should run to do the following steps.</p>

<ol>
<li>get the image from the image Path.</li>
<li>send the image to Google vision api</li>
<li>Call the document text detection function</li>
<li>get the return value (text string values)</li>
<li>Done</li>
</ol>

<p>When I run the dll, However, it keeps giving me an throw exception error. I am assuming that the problem is on the google credential but not sure...</p>

<p>Could somebody help me out with this? I don't even know that the var credential = GoogleCredential.FromFile(Credential_Path); would be the right way to call the json file...</p>

<pre><code>using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

using Google.Cloud.Vision.V1;
using Google.Apis.Auth.OAuth2;
using Image = Google.Cloud.Vision.V1.Image;


namespace DLL_TEST_NetFramework4._6._1version
{
    public class Class1
    {
        public string doc_text_dection(string GVA_File_Path, string Credential_Path)
        {
            var credential = GoogleCredential.FromFile(Credential_Path);
            //Load the image file into memory
            var image = Image.FromFile(GVA_File_Path);    

            // Instantiates a client
            ImageAnnotatorClient client = ImageAnnotatorClient.Create();

            TextAnnotation text = client.DetectDocumentText(image);
            //Console.WriteLine($""Text: {text.Text}"");

            return $""Text: {text.Text}"";
            //return ""test image..."";
        }
    }
}
</code></pre>",,3,0,,2018-12-10 07:41:01.740 UTC,,2019-04-23 02:04:51.920 UTC,2018-12-10 08:37:03.597 UTC,,703163,,4834002,1,1,c#|.net|google-authentication|.net-framework-version|google-vision,180
How to properly set up Google cloud vision on my localhost in PHP?,50018491,How to properly set up Google cloud vision on my localhost in PHP?,"<p>I want to use Google cloud Vision for detecting image properties. I have created an account with Google Cloud and found the exact solution on one of their code snippet here (<a href=""https://cloud.google.com/vision/docs/detecting-properties#vision-image-property-detection-gcs-php"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/detecting-properties#vision-image-property-detection-gcs-php</a>). </p>

<p>I copied and adjust it to what I want to achieve. I installed their package using composer <code>google/cloud-vision</code>. </p>

<p>So here is my code: </p>

<pre><code>&lt;?php 

namespace Google\Cloud\Samples\Vision;

use Google\Cloud\Vision\VisionClient;

 $projectId = 'YOUR_PROJECT_ID';
 $path = 'event1.jpg'; 

function detect_image_property($projectId, $path)
{
    $vision = new VisionClient([
        'projectId' =&gt; $projectId,
    ]);
    $image = $vision-&gt;image(file_get_contents($path), [
        'IMAGE_PROPERTIES'
    ]);
    $result = $vision-&gt;annotate($image);
    print(""Properties:\n"");
    foreach ($result-&gt;imageProperties()-&gt;colors() as $color) {
        $rgb = $color['color'];
        printf(""red:%s\n"", $rgb['red']);
        printf(""green:%s\n"", $rgb['green']);
        printf(""blue:%s\n\n"", $rgb['blue']);
    }
}

detect_image_property($projectId, $path); 


?&gt; 
</code></pre>

<p>So when I run my code it throws this error: </p>

<p><code>Fatal error: Uncaught Error: Class 'Google\Cloud\Vision\VisionClient' not found in C:\xampp\htdocs\vision\index.php:12 Stack trace: #0 C:\xampp\htdocs\vision\index.php(28): Google\Cloud\Samples\Vision\detect_image_property('YOUR_PROJECT_ID', 'event1.jpg') #1 {main} thrown in C:\xampp\htdocs\vision\index.php on line 12</code></p>

<p>Now am wondering what is the next step for me, also what will be my<br>
 <code>$projectId = 'YOUR_PROJECT_ID'</code>     </p>

<p>*Please, if this question needs more explanation let me know in the comment instead of downvoting. </p>

<p>Thanks.   </p>",,1,0,,2018-04-25 09:20:24.963 UTC,,2018-04-25 09:32:28.427 UTC,,,,,9675880,1,0,php|google-cloud-platform|google-cloud-vision,673
How to grant access to react-native-camera after don't allow is pressed first?,54224218,How to grant access to react-native-camera after don't allow is pressed first?,"<p>At first, when you run the app for the first time and go to the camera it launches the Permission modal on android or ios with the don't allow and allow options.</p>

<p>The package used is react-native-camera. It has a property called notAuthorizedView that you can return any view you want. What I want to do is to enable the camera or grant access to it in the notAuthorizedView which appears when not allowing the camera.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>export default class MyCamera extends React.Component {
    constructor(props) {
        super(props);
        this.state = {
            uri:''
        };
    }

      render() {
        return (
          &lt;View style={styles.container}&gt;
        &lt;RNCamera
          ref={ref =&gt; {
            this.camera = ref;
          }}
          style={styles.preview}
          type={RNCamera.Constants.Type.back}
          flashMode={RNCamera.Constants.FlashMode.on}
          notAuthorizedView={
            &lt;View&gt;
              &lt;Text&gt;YOU ARE NOT AUTHORIZED TO USE THE CAMERA&lt;/Text&gt;
              &lt;Button onPress={()=&gt;{Alert.alert('SET CAMERA STATUS TO READY')}}/&gt;
            &lt;/View&gt;
          }
          permissionDialogTitle={'Permission to use camera'}
          permissionDialogMessage={'We need your permission to use your camera phone'}
          onGoogleVisionBarcodesDetected={({ barcodes }) =&gt; {
            console.log(barcodes);
          }}
        /&gt;
        &lt;View style={{ flex: 0, flexDirection: 'row', justifyContent: 'center' }}&gt;
          &lt;TouchableOpacity onPress={this.takePicture.bind(this)} style={styles.capture}&gt;
            &lt;Text style={{ fontSize: 14 }}&gt; SNAP &lt;/Text&gt;
          &lt;/TouchableOpacity&gt;
        &lt;/View&gt;
      &lt;/View&gt;
        );
      }


     goToConcern = () =&gt; {
        this.props.navigation.navigate('Concern', {uriPhoto: this.state.uri})
     };

     

  takePicture = async function() {
    if (this.camera) {
      const options = { quality: 0.5, base64: true };
      const data = await this.camera.takePictureAsync(options)
      console.log(data.uri);
      console.log(data);
      this.setState({uri:data.uri})
    }
  };
}</code></pre>
</div>
</div>
</p>",54224841,2,0,,2019-01-16 19:43:05.307 UTC,,2019-01-16 20:35:05.470 UTC,,,,,6353460,1,0,android|ios|reactjs|react-native|react-native-camera,725
Watson visual recognition run error,35952647,Watson visual recognition run error,"<p>I'm trying to set up a visual recognition app using the Watson visual recognition api. To do this I started by downloading watson-developer-cloud and I put it in my node_modules folder, which is next to my index.html and api_request.js.</p>

<p>This is my api_request.js file:</p>

<pre><code> var watson = require('./node_modules/watson-developer-cloud');

 var visual_recognition = watson.visual_recognition({
   username: '*********',
   password: '*********',
   version: 'v2-beta',
   version_date: '2015-12-02'
 });
 visual_recognition.listClassifiers({},
    function(err, response) {
     if (err){
        console.log(err);
  }
     else {
        console.log(JSON.stringify(response, null, 2));
  }
    }
 );
</code></pre>

<p>It is taken directly from the visual recognition api documentation. I ran this file in the terminal and it provided the desired output which is a list of visual recognition classifiers. However as it has node.js functions I decided to use browserify to allow it to run in the browser. I installed browserify and built bundle.js out of api_request.js in the same directory as the api_request.js and index.html file. </p>

<p>Once index.html was linked to bundle.js I opened it in the browser and it didn't have any issues with node.js functions.</p>

<p>However an error occurred when a file that was in watson-developer-cloud couldn't find another file that was inside watson-developer-cloud. To be specific index.js couldn't find v2-beta (I didn't edit the watson-developer-cloud files). What I find strange is that when I ran api_request.js in the terminal none of the watson-developer-cloud files had any problems, but once I used browserify, bundle.js logged the error that index.js couldn't find v2-beta. </p>

<pre><code>""build"": ""browserify api_request.js -o bundle.js""
</code></pre>

<p>^that is the script I used to build bundle.js. The only thing I can think could be causing this error is browserify. Is there something else that could be causing this?</p>",36020024,1,0,,2016-03-12 01:30:14.167 UTC,,2016-03-15 19:08:32.243 UTC,,,,,4799495,1,2,node.js|browserify|ibm-watson|visual-recognition,362
aws-php-sdk: Is it possible to search or compare face using php,46733224,aws-php-sdk: Is it possible to search or compare face using php,"<p>I have uploaded images using php on S3, now i want to compare/match given image in my S3 collection, I googled but not getting answer, if it is possible to use AWS face rekognition using PHP to search in S3 collection.</p>",,3,0,,2017-10-13 15:25:08.760 UTC,,2017-11-01 17:32:21.367 UTC,,,,,3560746,1,-1,amazon-s3|amazon-ec2|aws-sdk|aws-php-sdk,462
How do I send a Google API POST request using Jmeter?,44517510,How do I send a Google API POST request using Jmeter?,"<p>I have never used Jmeter before. I have been trying to use Jmeter to send an HTTP request to Google Vision API - but it's returning a FORBIDDEN (403) error. My request as well as required response is in JSON format.<br>
I have attached below the:<br>
a) HTTP Request<br>
b) Response Error  </p>

<p>Other than this, in HTTP Header Manager I have set:<br>
Content-Type: application/json</p>

<p>What is wrong with the attached request?<br>
<a href=""https://i.stack.imgur.com/zNz2y.png"" rel=""nofollow noreferrer"">Request image.. </a><br>
<a href=""https://i.stack.imgur.com/AapNj.png"" rel=""nofollow noreferrer"">Response error image</a>   </p>",44520393,1,0,,2017-06-13 09:29:18.933 UTC,,2017-06-13 11:34:55.320 UTC,2017-06-13 09:37:31.550 UTC,,8124713,,8124713,1,0,jmeter|httprequest|google-cloud-vision,226
7 segment OCR using Google Cloud Vision,36408010,7 segment OCR using Google Cloud Vision,"<p>I am trying to use Google Cloud Vision with TEXT_DETECTION to try and do OCR on a seven segment display, but am getting pretty lousy results, mostly because it seems to think its a different language. The typical locale it seems to associate it with is ""zh"" or ""ja"".</p>

<p>Is there a specific hint that I can give Cloud Vision which might produce better results?</p>

<p>For example, this image below --
<a href=""https://i.stack.imgur.com/UZXN8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UZXN8.png"" alt=""enter image description here""></a></p>

<p>produces this output --</p>

<pre><code>""locale"" : ""ja"",
...
...
""description"" : ""ココ\n""
</code></pre>

<p>I have also tried to preprocess the image by increasing contrast, gaussian blur and even erode it to fill in the spaces between the segments, but without much luck.</p>

<p>Any help/pointers would be appreciated.</p>",,1,1,,2016-04-04 16:27:55.277 UTC,,2016-11-04 00:13:52.257 UTC,,,,,723920,1,2,google-cloud-vision,541
Google Vision Rest API in Xamarin,40156561,Google Vision Rest API in Xamarin,"<p>I'm trying to call the Google Vision REST API from a Xamarin.Forms app. I have the following code:-</p>

<pre class=""lang-cs prettyprint-override""><code> private async void SendToGoogle(MediaFile file)
    {
        using (HttpClient client = new HttpClient())
        {
            string uri = ""https://vision.googleapis.com/v1/images:annotate?key=API_KEY"";
            HttpResponseMessage response;

            var stream = file.GetStream();
            using (var content = new StreamContent(stream))
            {
                content.Headers.ContentType = new MediaTypeHeaderValue(""application/json"");
                content.Headers.ContentLength = stream.Length;
                response = await client.PostAsync(uri, content);

                var resp = await response.Content.ReadAsStringAsync();
            }
        }
    }
</code></pre>

<p>But this is returning me a <code>400</code> error message:</p>

<blockquote>
  <p>Invalid JSON payload received. Expected a value.</p>
</blockquote>",,0,2,,2016-10-20 13:57:18.183 UTC,,2016-10-21 13:46:11.210 UTC,2016-10-21 13:46:11.210 UTC,,322020,,2704244,1,0,rest|xamarin|dotnet-httpclient|google-cloud-vision,769
References from Python Bigquery Client don't work,47537278,References from Python Bigquery Client don't work,"<p>Im having troubles running the following code:</p>

<pre><code>from google.cloud import bigquery
client = bigquery.Client.from_service_account_json(BQJSONKEY,project = BQPROJECT)
dataset = client.dataset(BQDATASET)
assert not dataset.exists() 
</code></pre>

<p>The following error pop up:
<code>'DatasetReference' object has no attribute 'exists'</code></p>

<p>Similarly when i do:</p>

<p><code>table = dataset.table(BQTABLE)</code></p>

<p>i get: <code>'TableReference' object has no attribute 'exists'</code></p>

<p>However, according to the docs it should work:
<a href=""https://googlecloudplatform.github.io/google-cloud-python/stable/bigquery/usage.html#datasets"" rel=""nofollow noreferrer"">https://googlecloudplatform.github.io/google-cloud-python/stable/bigquery/usage.html#datasets</a></p>

<p>here is my <code>pip freeze</code> (the part with google-cloud):</p>

<pre><code>gapic-google-cloud-datastore-v1==0.15.3
gapic-google-cloud-error-reporting-v1beta1==0.15.3
gapic-google-cloud-logging-v2==0.91.3
gevent==1.2.2
glob2==0.5
gmpy2==2.0.8
google-api-core==0.1.1
google-auth==1.2.1
google-cloud==0.30.0
google-cloud-bigquery==0.28.0
google-cloud-bigtable==0.28.1
google-cloud-core==0.28.0
google-cloud-datastore==1.4.0
google-cloud-dns==0.28.0
google-cloud-error-reporting==0.28.0
google-cloud-firestore==0.28.0
google-cloud-language==1.0.0
google-cloud-logging==1.4.0
google-cloud-monitoring==0.28.0
google-cloud-pubsub==0.29.1
google-cloud-resource-manager==0.28.0
google-cloud-runtimeconfig==0.28.0
google-cloud-spanner==0.29.0
google-cloud-speech==0.30.0
google-cloud-storage==1.6.0
google-cloud-trace==0.16.0
google-cloud-translate==1.3.0
google-cloud-videointelligence==0.28.0
google-cloud-vision==0.28.0
google-gax==0.15.16
google-resumable-media==0.3.1
googleapis-common-protos==1.5.3
</code></pre>

<p>I wonder how can i fix it and make it work?</p>",,2,0,,2017-11-28 17:11:07.517 UTC,,2017-11-28 17:29:34.870 UTC,2017-11-28 17:23:50.263 UTC,,4933628,,4933628,1,1,python|python-2.7|google-bigquery|service-accounts,351
Why I am getting 'The requested URL not found on this server' while using API?,51973564,Why I am getting 'The requested URL not found on this server' while using API?,"<p>I am creating an app in app inventor for that I need to detection emotions. So I used API (Google Vision API) to make my work easier. But got sucked in the screen when I access the url <code>https://vision.googleapis.com/v1/images:annotate?key=YOUR_KEY</code>.</p>

<p>Here is the screen shot</p>

<p><img src=""https://i.stack.imgur.com/ZCBuj.png"" alt=""at here""></p>",,0,0,,2018-08-22 19:06:26.103 UTC,,2018-08-22 22:54:34.533 UTC,2018-08-22 22:54:34.533 UTC,,472495,,9126324,1,0,api|url|app-inventor,27
Google Cloud Vision API - Python,36570132,Google Cloud Vision API - Python,"<p>I can't seem to find where to add the API key or where I need to locate to the google credentials file in my google cloud vision code:</p>

<pre><code>    import argparse
    import base64
    import httplib2
    import validators
    import requests

    from apiclient.discovery import build
    from oauth2client.client import GoogleCredentials


    def main(photo_file):
      '''Run a label request on a single image'''

      API_DISCOVERY_FILE = 'https://vision.googleapis.com/$discovery/rest?version=v1'
      http = httplib2.Http()

      credentials = GoogleCredentials.get_application_default().create_scoped(
          ['https://www.googleapis.com/auth/cloud-platform'])
      credentials.authorize(http)

      service = build('vision', 'v1', http, discoveryServiceUrl=API_DISCOVERY_FILE)

    if __name__ == '__main__':
      parser = argparse.ArgumentParser()
      parser.add_argument(
        'image_file', help='The image you\'d like to label.')
      args = parser.parse_args()
      main(args.image_file)

    photo_file = ""image_of_bottle.jpg""
    main(photo_file)
</code></pre>

<p>Does anyone know where I can add the API key or locate to the credentials file? </p>

<p>EDIT: Added changes recommended by Eray Balkanli and I added my image file in the call. I'm not sure if I did it correctly:</p>

<pre><code>import argparse
import base64
import httplib2
import validators
import requests

from apiclient.discovery import build
from oauth2client.client import GoogleCredentials


def main(photo_file,developerkey):
  '''Run a label request on a single image'''

  API_DISCOVERY_FILE = 'https://vision.googleapis.com/$discovery/rest?version=v1'
  http = httplib2.Http()

  credentials = GoogleCredentials.get_application_default().create_scoped(
      ['https://www.googleapis.com/auth/cloud-platform'])
  credentials.authorize(http)

  service = build('vision', 'v1', http, discoveryServiceUrl=API_DISCOVERY_FILE,developerkey=INSERT API KEY)

if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument(
    'image_file', help='The image you\'d like to label.')
  args = parser.parse_args()
  main(args.image_file)

photo_file = ""image_file.jpg""
main(photo_file,developerkey)
</code></pre>

<p>I received the following error:</p>

<pre><code>usage: googleimagetest_v.4.py [-h] image_file
googleimagetest_v.4.py: error: too few arguments
</code></pre>

<p>Does anyone know how I can solve this error? </p>",36673077,1,2,,2016-04-12 10:10:23.747 UTC,2,2017-07-24 21:32:47.673 UTC,2016-04-19 21:26:04.170 UTC,,881229,,5080273,1,3,python|google-cloud-vision,1479
ServiceAccountJwtAccessCredentials exception trying to call Google Vision API,55685353,ServiceAccountJwtAccessCredentials exception trying to call Google Vision API,"<p>I'm trying to evaluate Google vision endpoint. my pom is configured like below</p>

<pre><code>&lt;dependency&gt;
  &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;
  &lt;artifactId&gt;google-cloud-vision&lt;/artifactId&gt;
  &lt;version&gt;1.69.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>There are no other google dependency added. I see below conflict within the vision dependency itself.
<a href=""https://i.stack.imgur.com/ld0Zm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ld0Zm.png"" alt=""enter image description here""></a></p>

<p>When I run the code I'm getting below error.</p>

<pre><code>GoogleAuthLibraryCallCredentials#createJwtHelperOrNull#229 - Failed to create JWT helper. This is unexpected
java.lang.NoSuchMethodException: com.google.auth.oauth2.ServiceAccountJwtAccessCredentials.&lt;init&gt;(java.lang.String, java.lang.String, java.security.PrivateKey, java.lang.String)
at java.lang.Class.getConstructor0(Class.java:3082) ~[?:1.8.0_202]
at java.lang.Class.getConstructor(Class.java:1825) ~[?:1.8.0_202]
</code></pre>

<p>I believe this has something to do with mismatched versions. but got no idea which one to use and how to fix dependency issues within the same jar.</p>",55690632,1,0,,2019-04-15 08:37:58.923 UTC,,2019-04-15 13:47:56.823 UTC,,,,,4161590,1,0,java|maven|google-vision,94
Azure Face API - someone can use another person's photo for Identification,55170430,Azure Face API - someone can use another person's photo for Identification,"<p>I am developing a system to identify persons, I am using an app (to take photos) and then send the photo to a Web Asp.Net Core API (It use Microsoft Azure Face API). But the system is not secure!. Because someone using a Photo of other people can validate to another person !. The system is for validate a person! If someone use a photo then the system is not secure!
Some idea about What can I do to check that the person is a person and not is a photo of a another person?</p>",,1,1,,2019-03-14 19:18:23.903 UTC,,2019-03-26 18:15:14.377 UTC,2019-03-26 18:15:14.377 UTC,,8161670,,11205065,1,0,azure|security|microsoft-cognitive|identification|face-api,50
"Can I configure Google Vision to recognize vulgar fraction symbols (e.g. ½, ¼, etc...)?",52119949,"Can I configure Google Vision to recognize vulgar fraction symbols (e.g. ½, ¼, etc...)?","<p>When I run <a href=""https://i.stack.imgur.com/6gJ0e.png"" rel=""nofollow noreferrer"">this image</a> containing various vulgar fraction symbols through the <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">Google Vision demo</a>, it recognizes all of the characters correctly except for those symbols. The same is true when I consume the API whether it be with TEXT_DETECTION or DOCUMENT_TEXT_DETECTION. Is there some way I can configure Google Vision to accurately recognize these symbols?</p>",52154981,1,0,,2018-08-31 17:05:15.460 UTC,,2018-09-03 19:02:21.150 UTC,,,,,7416199,1,0,google-cloud-vision,62
sed recognition response to DynamoDB table using Lambda-python,54824911,sed recognition response to DynamoDB table using Lambda-python,"<p>I am using Lambda to detect faces and would like to send the response to a Dynamotable. 
This is the code I am using:</p>

<pre><code>rekognition = boto3.client('rekognition', region_name='us-east-1')
dynamodb = boto3.client('dynamodb', region_name='us-east-1')



 # --------------- Helper Functions to call Rekognition APIs ------------------


def detect_faces(bucket, key):
    response = rekognition.detect_faces(Image={""S3Object"": {""Bucket"": bucket, 
    ""Name"": key}}, Attributes=['ALL'])
    TableName = 'table_test'
    for face in response['FaceDetails']:
        table_response = dynamodb.put_item(TableName=TableName, Item='{0} - {1}%')


     return response
</code></pre>

<p>My problem is in this line:</p>

<pre><code> for face in response['FaceDetails']:
        table_response = dynamodb.put_item(TableName=TableName, Item= {'key:{'S':'value'}, {'S':'Value')
</code></pre>

<p>I am able to see the result in the console.
I don't want to add specific item(s) to the table- I need the whole response to be transferred to the table. </p>

<p>Do do this: 
1. What to add as a key and partition key in the table?
2. How to transfer the whole response to the table </p>

<p>i have been stuck in this for three days now and can't figure out any result. Please help!</p>

<pre><code>           ******************* EDIT *******************
</code></pre>

<p>I tried this code:</p>

<pre><code>rekognition = boto3.client('rekognition', region_name='us-east-1')




 # --------------- Helper Functions to call Rekognition APIs ------------------


 def detect_faces(bucket, key):
     response = rekognition.detect_faces(Image={""S3Object"": {""Bucket"": bucket, 
     ""Name"": key}}, Attributes=['ALL'])
     TableName = 'table_test'
     for face in response['FaceDetails']:
         face_id = str(uuid.uuid4())
         Age = face[""AgeRange""]
         Gender = face[""Gender""]
         print('Generating new DynamoDB record, with ID: ' + face_id)
         print('Input Age: ' + Age)
         print('Input Gender: ' + Gender)
         dynamodb = boto3.resource('dynamodb')
         table = dynamodb.Table(os.environ['test_table'])
         table.put_item(
         Item={
            'id' : face_id,
            'Age' : Age,
            'Gender' : Gender
         }
     )


     return response
</code></pre>

<p>It gave me two of errors:</p>

<pre><code>1. Error processing object xxx.jpg
2. cannot concatenate 'str' and 'dict' objects
</code></pre>

<p>Can you pleaaaaase help!</p>",54825400,1,0,,2019-02-22 10:21:51.750 UTC,,2019-02-22 10:53:43.383 UTC,2019-02-22 10:47:25.820 UTC,,7000874,,7000874,1,1,python|python-2.7|amazon-web-services|amazon-s3|aws-lambda,19
Node.js - Problem to extract text from PDF file using Google Cloud Vision API,53381742,Node.js - Problem to extract text from PDF file using Google Cloud Vision API,"<p>I'm new to cloud environments and programming in general, and I'm struggling to use the Google Vision API to extract text from a PDF file located in a remote bucket.</p>

<p>I've found it really difficult to get meaningful content related to this subject in the docs and even in Stack Overflow. The closest I got to solving this problem was with this question: </p>

<p><a href=""https://stackoverflow.com/questions/36728347/cloud-vision-api-pdf-ocr"">Cloud Vision API - PDF OCR</a></p>

<p>But it did not work for me for the reasons described below, which is why I'm asking a question of my own.</p>

<p>Here is the problem:</p>

<p>I am making the following post request to the specified url</p>

<pre><code>    https://vision.googleapis.com/v1/files:asyncBatchAnnotate?key=MY_API_KEY

    ""requests"": [
    {
        ""inputConfig"": {
            ""gcsSource"": {
                ""uri"": ""gs://BUCKET_NAME/FILE_NAME.pdf""
            },
            ""mimeType"": ""application/pdf""
        },
        ""features"": [
            {
                ""type"": ""DOCUMENT_TEXT_DETECTION""
            }
        ],
        ""outputConfig"": {
            ""gcsDestination"": {
                ""uri"": ""gs://BUCKET_NAME/output/""
            },
            ""batchSize"": 1
        }
    }
]
</code></pre>

<p>The POST request is successful, and after that, according to what I found, I have to make a get request to check if the document text detection is done, using the response I received from my previous post request. If it is done, it's supposed to write a response in a file inside my Bucket (Which is why I configured an 'output' in the json above)</p>

<p>However, when I make a get request on the url</p>

<pre><code>    https://vision.googleapis.com/v1/operations/RESPONSE?key=API_KEY
</code></pre>

<p>I get the following error: </p>

<pre><code>    ""error"": {
        ""code"": 7,
        ""message"": ""Error writing final output to: gs://BUCKET_NAME/output/filename.json""
    }
</code></pre>

<p>Even if there is a way to solve this problem to write the final output, I wonder if that's the best way to extract data from a pdf, it looks very weird to make a post and a get, specially considering that when you're extracting data from an image using the same API, you only have to make one request</p>

<p>Thanks for the help.</p>",53389773,1,0,,2018-11-19 19:55:07.013 UTC,0,2018-11-20 09:20:44.053 UTC,,,,,9219089,1,1,node.js|google-cloud-platform|google-vision|pdf-extraction,281
How to recognise digits using Firebase ML Kit custom model,55139181,How to recognise digits using Firebase ML Kit custom model,"<p>My android app have a need to read the text in a image and I am using the Firebase ML Kit for that purpose, it works well for the text which is in standard Roman script but it completely fails to read the digits in the below font family.</p>

<p><a href=""https://i.stack.imgur.com/TLHrK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TLHrK.png"" alt=""enter image description here""></a></p>

<p>Especially for the digits 0, 1 &amp; 6 Firebase result gives O, i, l, L &amp; b, since it gives alternate letters for the same digit in different test I cannot apply regex to replace those letters with digits.</p>

<p>I have also tried the Firebase ML Kit cloud text recognition (Google Cloud Vision) for getting those digits as it is and it do have the same problem.</p>

<p>So my only option left is <strong>Firebase ML Kit Custom Model</strong> and I want to know on the possibility of reading text using <strong>ML Kit Custom Model</strong> which is based on <strong>TensorFlowLite</strong> model, I am asking this because all the android <em>ML Kit Custom Model</em> examples I saw is only used for detecting objects and didn't see any sample for reading text using MLKit custom model anywhere.</p>

<p><strong>Basically what I want is the same behaviour like Firebase offline/cloud text recognition but with a custom tensorflow lite model, is that possible?</strong> Please describe</p>",,0,4,,2019-03-13 10:07:20.827 UTC,,2019-03-15 10:55:45.903 UTC,2019-03-15 10:55:45.903 UTC,,6694035,,6694035,1,0,android|firebase|tensorflow|tensorflow-lite|firebase-mlkit,122
Use Google service account securely with Heroku,49664844,Use Google service account securely with Heroku,"<p>I'm trying to use Google Cloud Vision API with Node and run the application on Heroku. Something very close to this example:</p>

<p><a href=""https://github.com/googleapis/nodejs-vision"" rel=""nofollow noreferrer"">https://github.com/googleapis/nodejs-vision</a></p>

<p>However, the Google API wants to authenticate by reading a file containing the service account, and location of the file is read using an environment variable. Is there a way to either securely store this file using Heroku, or somehow utilize Heroku Config Vars?</p>",,1,0,,2018-04-05 05:36:42.673 UTC,,2018-04-19 11:59:01.177 UTC,,,,,76535,1,2,node.js|heroku|google-cloud-platform,727
Could not proxy request /upload,49657702,Could not proxy request /upload,"<p>I am working with React and Express.js to create a file upload service to the Google Vision API. I have create a simple file upload to that posts to '/upload' upon submit. Upon upload, I keep hitting this error:</p>

<pre><code>Proxy error: Could not proxy request /upload from localhost:3000 to http://localhost:3000/ (ECONNRESET).
</code></pre>

<p>Here is my react app.js that constitutes the simple image upload form.</p>

<p><strong>App.js</strong></p>

<pre><code>import React, { Component } from 'react';
import {
  Button,
  Col,
  Form,
  FormGroup,
  Label,
  Input,
  FormText
} from 'reactstrap';
import './App.css';

class App extends Component {

  render() {
    return (
      &lt;div className=""App""&gt;
        &lt;header className=""App-header""&gt;
          &lt;img src='https://media1.tenor.com/images/aa12acad78c918bb62fa41cf7af8cf75/tenor.gif?itemid=5087595' className=""App-logo"" alt=""logo"" /&gt;
          &lt;h1 className=""App-title""&gt;Welcome to Readr&lt;/h1&gt;
        &lt;/header&gt;
        &lt;Form action='/upload' method=""POST""&gt;
          &lt;FormGroup row&gt;
            &lt;Label for=""exampleFile"" sm={2}&gt;File&lt;/Label&gt;
            &lt;Col sm={10}&gt;
              &lt;Input type='file' name='image' /&gt;
              &lt;FormText color=""muted""&gt;
              &lt;/FormText&gt;
            &lt;/Col&gt;
          &lt;/FormGroup&gt;
          &lt;FormGroup row&gt;
            &lt;Label for=""exampleSelect"" sm={2}&gt;What are we trying to see today in the image?&lt;/Label&gt;
            &lt;Col sm={10}&gt;
              &lt;Input type=""select"" name=""select"" id=""exampleSelect""&gt;
                &lt;option&gt;Labels&lt;/option&gt;
                &lt;option&gt;Faces&lt;/option&gt;
                &lt;option&gt;Landmarks&lt;/option&gt;
                &lt;option&gt;Text&lt;/option&gt;
                &lt;option&gt;Logos&lt;/option&gt;
              &lt;/Input&gt;
            &lt;/Col&gt;
          &lt;/FormGroup&gt;
          &lt;FormGroup check row&gt;
            &lt;Col sm={{ size: 10, offset: 2 }}&gt;
              &lt;Button type=""submit""&gt;Submit&lt;/Button&gt;
            &lt;/Col&gt;
          &lt;/FormGroup&gt;
        &lt;/Form&gt;
      &lt;/div&gt;
    );
  }
}

export default App;
</code></pre>

<p><strong>Server.js</strong></p>

<pre><code>'use strict';
// Middleware
const express = require('express');
const fs = require('fs');
const util = require('util');
const mime = require('mime-types');
const multer = require('multer');
const upload = multer({ dest: 'uploads/',
 rename: function (fieldname, filename) {
   return filename;
 },
});
const Image = require('./data/db.js');
const path = require('path');

// Imports the Google Cloud client library
const vision = require('@google-cloud/vision');
// Creates a client
const client = new vision.ImageAnnotatorClient();

let app = express();

// Simple upload form

app.get('/', function(req, res) {
  res.sendFile(path.join(__dirname + '/client/index.html'));
});

// Get the uploaded image
// Image is uploaded to req.file.path
app.post('/upload', upload.single('image'), function(req, res, next) {

  // Choose what the Vision API should detect
  // Choices are: faces, landmarks, labels, logos, properties, safeSearch, texts
  var types = ['labels'];

  // Send the image to the Cloud Vision API
  client
  .labelDetection(req.file.path)
  .then(results =&gt; {
    // Pull all labels from POST request
    const labels = [];
    results[0].labelAnnotations.forEach(function(element) {
      labels.push(element.description);
    })
    res.writeHead(200, {
      'Content-Type': 'text/html'
    });

    // Create new Image Record
    let image = new Image ({});
    image.data = fs.readFileSync(req.file.path);
    image.contentType = 'image/png';
    image.labels = labels;
    image.save((err) =&gt; {
      if (err) {
        console.log('Error:' , err);
      }
    })

    res.write('&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;body&gt;');

    // Base64 the image so we can display it on the page
    res.write('&lt;img width=600 src=""' + base64Image(req.file.path) + '""&gt;&lt;br&gt;');

    // Write out the JSON output of the Vision API
    res.write(JSON.stringify(labels, null, 4));
    // Delete file (optional)
    fs.unlinkSync(req.file.path);

    res.end('&lt;/body&gt;&lt;/html&gt;');
  })

  // ERROR from Cloud Vision API
  .catch(err =&gt; {
    console.log(err);
    res.end('Cloud Vision Error:' , err);
  });
});

app.listen(8080);
console.log('Server listening on 8080');

// Turn into Base64, an easy encoding for small images
function base64Image(src) {
  var data = fs.readFileSync(src).toString('base64');
  return util.format('data:%s;base64,%s', mime.lookup(src), data);
}
</code></pre>

<p><strong>package.json</strong></p>

<pre><code>   {
  ""name"": ""my-app"",
  ""version"": ""0.1.0"",
  ""private"": true,
  ""dependencies"": {
    ""bootstrap"": ""^4.0.0"",
    ""react"": ""^16.3.0"",
    ""react-dom"": ""^16.3.0"",
    ""react-scripts"": ""1.1.1"",
    ""reactstrap"": ""^5.0.0-beta.3"",
    ""@google-cloud/vision"": ""^0.18.0"",
    ""babel"": ""^6.23.0"",
    ""express"": ""^4.13.4"",
    ""mime"": ""^1.3.4"",
    ""mime-lookup"": ""0.0.2"",
    ""mime-types"": ""^2.1.18"",
    ""mongodb"": ""^3.0.5"",
    ""mongoose"": ""^5.0.12"",
    ""multer"": ""^1.1.0""
  },
  ""scripts"": {
    ""start"": ""react-scripts start"",
    ""server"": ""GOOGLE_APPLICATION_CREDENTIALS='key.json' nodemon ./public/sample.js --ignore client"",
    ""build"": ""react-scripts build"",
    ""test"": ""react-scripts test --env=jsdom"",
    ""eject"": ""react-scripts eject""
  },
  ""proxy"": ""http://localhost:3000/""
}
</code></pre>

<p>I am new to using a proxy for a react and express.js app. Can someone help clarify what exactly is causing this proxy error? </p>",,1,0,,2018-04-04 18:10:31.607 UTC,,2018-05-29 12:34:16.860 UTC,,,,,4041447,1,0,express|reactstrap,802
ClassNotFound GoogleApiAvailability when proguard is used,45024118,ClassNotFound GoogleApiAvailability when proguard is used,"<p>I am creating an android aar in which I am using google's vision API. To check if Play Services are available or not I have added check using <code>GoogleApiAvailability.getInstance().isGooglePlayServicesAvailable(context)</code>.</p>

<p>For excluding this from obfuscation I have added </p>

<pre><code>-keep class com.google.android.gms.vision.**{*;}
-keep class com.google.android.gms.common.**{*;}
</code></pre>

<p>I am getting this error when obfuscated:</p>

<pre><code>07-11 00:36:50.970 26184-26184/com.example.anujakothekar.myapplication 

I/zygote: Rejecting re-init on previously-failed class java.lang.Class&lt;com.example.textscan.scanner.camera.b&gt;: java.lang.NoClassDefFoundError: Failed resolution of: Lcom/google/android/gms/vision/Detector$Processor;
07-11 00:36:50.970 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at java.lang.Object java.lang.Class.newInstance() (Class.java:-2)
07-11 00:36:50.970 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at android.app.Activity android.app.Instrumentation.newActivity(java.lang.ClassLoader, java.lang.String, android.content.Intent) (Instrumentation.java:1173)
07-11 00:36:50.970 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at android.app.Activity android.app.ActivityThread.performLaunchActivity(android.app.ActivityThread$ActivityClientRecord, android.content.Intent) (ActivityThread.java:2708)
07-11 00:36:50.970 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at void android.app.ActivityThread.handleLaunchActivity(android.app.ActivityThread$ActivityClientRecord, android.content.Intent, java.lang.String) (ActivityThread.java:2892)
07-11 00:36:50.970 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at void android.app.ActivityThread.-wrap11(android.app.ActivityThread, android.app.ActivityThread$ActivityClientRecord, android.content.Intent, java.lang.String) (ActivityThread.java:-1)
07-11 00:36:50.970 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at void android.app.ActivityThread$H.handleMessage(android.os.Message) (ActivityThread.java:1593)
07-11 00:36:50.970 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at void android.os.Handler.dispatchMessage(android.os.Message) (Handler.java:105)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at void android.os.Looper.loop() (Looper.java:164)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at void android.app.ActivityThread.main(java.lang.String[]) (ActivityThread.java:6540)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at java.lang.Object java.lang.reflect.Method.invoke(java.lang.Object, java.lang.Object[]) (Method.java:-2)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at void com.android.internal.os.Zygote$MethodAndArgsCaller.run() (Zygote.java:240)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at void com.android.internal.os.ZygoteInit.main(java.lang.String[]) (ZygoteInit.java:767)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote: Caused by: java.lang.ClassNotFoundException: Didn't find class ""com.google.android.gms.vision.Detector$Processor"" on path: DexPathList[[zip file ""/data/app/com.example.anujakothekar.myapplication-tunZSOorubmtNQccOZwWPw==/base.apk"", zip file ""/data/app/com.example.anujakothekar.myapplication-tunZSOorubmtNQccOZwWPw==/split_lib_dependencies_apk.apk"", zip file ""/data/app/com.example.anujakothekar.myapplication-tunZSOorubmtNQccOZwWPw==/split_lib_slice_0_apk.apk"", zip file ""/data/app/com.example.anujakothekar.myapplication-tunZSOorubmtNQccOZwWPw==/split_lib_slice_1_apk.apk"", zip file ""/data/app/com.example.anujakothekar.myapplication-tunZSOorubmtNQccOZwWPw==/split_lib_slice_2_apk.apk"", zip file ""/data/app/com.example.anujakothekar.myapplication-tunZSOorubmtNQccOZwWPw==/split_lib_slice_3_apk.apk"", zip file ""/data/app/com.example.anujakothekar.myapplication-tunZSOorubmtNQccOZwWPw==/split_lib_slice_4_apk.apk"", zip file ""/data/app/com.example.anujakothekar.myapplication-tunZSOorubmtNQccOZwWPw==/split_lib_slice_5_apk.a
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at java.lang.Class dalvik.system.BaseDexClassLoader.findClass(java.lang.String) (BaseDexClassLoader.java:93)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at java.lang.Class java.lang.ClassLoader.loadClass(java.lang.String, boolean) (ClassLoader.java:379)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at java.lang.Class java.lang.ClassLoader.loadClass(java.lang.String) (ClassLoader.java:312)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at java.lang.Object java.lang.Class.newInstance() (Class.java:-2)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at android.app.Activity android.app.Instrumentation.newActivity(java.lang.ClassLoader, java.lang.String, android.content.Intent) (Instrumentation.java:1173)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at android.app.Activity android.app.ActivityThread.performLaunchActivity(android.app.ActivityThread$ActivityClientRecord, android.content.Intent) (ActivityThread.java:2708)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at void android.app.ActivityThread.handleLaunchActivity(android.app.ActivityThread$ActivityClientRecord, android.content.Intent, java.lang.String) (ActivityThread.java:2892)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at void android.app.ActivityThread.-wrap11(android.app.ActivityThread, android.app.ActivityThread$ActivityClientRecord, android.content.Intent, java.lang.String) (ActivityThread.java:-1)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at void android.app.ActivityThread$H.handleMessage(android.os.Message) (ActivityThread.java:1593)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at void android.os.Handler.dispatchMessage(android.os.Message) (Handler.java:105)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at void android.os.Looper.loop() (Looper.java:164)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at void android.app.ActivityThread.main(java.lang.String[]) (ActivityThread.java:6540)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at java.lang.Object java.lang.reflect.Method.invoke(java.lang.Object, java.lang.Object[]) (Method.java:-2)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at void com.android.internal.os.Zygote$MethodAndArgsCaller.run() (Zygote.java:240)
07-11 00:36:50.972 26184-26184/com.example.anujakothekar.myapplication I/zygote:     at void com.android.internal.os.ZygoteInit.main(java.lang.String[]) (ZygoteInit.java:767)
</code></pre>

<p><strong>Update:</strong>
When I am using playservices dependency in project where this aar is also imported, then my code is working perfectly.
Is there any way, to avoid adding playservices dependency in project and just use it from aar?</p>",45026061,1,0,,2017-07-11 01:50:16.197 UTC,,2017-07-11 05:29:51.630 UTC,2017-07-11 04:00:22.620 UTC,,6270950,,6270950,1,0,android|proguard|google-vision,598
scalafx custom binding race condition,51731727,scalafx custom binding race condition,"<p>I'm trying to use Amazon Rekognition to draw boxes around detected text in an image.</p>

<p>Here's a stripped-down JavaFX app that is supposed to do that:</p>

<pre><code>object TextRekognizeGui extends JFXApp {
  lazy val rekognition: Option[AmazonRekognition] =
    RekogClient.get(config.AwsConfig.rekogEndpoint, config.AwsConfig.region)
  private val config = MainConfig.loadConfig()

  stage = new PrimaryStage {
    scene = new Scene {
      var imgFile: Option[File] = None

      content = new HBox {
        val imgFile: ObjectProperty[Option[File]] = ObjectProperty(None)
        val img: ObjectProperty[Option[Image]] = ObjectProperty(None)
        val lines: ObjectProperty[Seq[TextDetection]] = ObjectProperty(Seq.empty)
        // These two print statements are always executed when we press the appropriate button.
        img.onChange((_, _, newValue) =&gt; println(""New image""))
        lines.onChange((_, _, newValue) =&gt; println(s""${newValue.length} new text detections""))
        children = Seq(
          new VBox {
            children = Seq(
              new OpenButton(img, imgFile, lines, stage),
              new RekogButton(img, imgFile, lines, rekognition)
            )
          },
          new ResultPane(675, 425, img, lines))
      }
    }
  }

  stage.show
}

class ResultPane(width: Double, height: Double, img: ObjectProperty[Option[Image]],
                 textDetections: ObjectProperty[Seq[TextDetection]]) extends AnchorPane { private val emptyPane: Node = Rectangle(0, 0, width, height)

  // This print statement, and the one below, are often not executed even when their dependencies change.
  private val backdrop = Bindings.createObjectBinding[Node](
    () =&gt; {println(""new backdrop""); img.value.map(new MainImageView(_)).getOrElse(emptyPane)},
    img
  )

  private val outlines = Bindings.createObjectBinding[Seq[Node]](
    () =&gt; {println(""new outlines""); textDetections.value map getTextOutline},
    textDetections
  )
</code></pre>

<p>This looks to me as though I followed the ScalaFX documentation's <a href=""http://www.scalafx.org/docs/properties/"" rel=""nofollow noreferrer"">directions</a> for creating a custom binding. When I click my ""RekogButton"", I expect to see the message "" new text detections"" along with the message ""new outlines"", but more often than not I see only the "" new text detections"" message.</p>

<p>The behavior appears to be the same throughout the lifetime of the program, but it only manifests on some runs. The same problem happens with the <code>img</code> -> <code>background</code> binding, but it manifests less often.</p>

<p>Why do my properties sometimes fail to call the bindings when they change?</p>

<p><strong>edit 1</strong>
JavaFX stores the listeners associated with bindings as a <a href=""https://www.baeldung.com/java-weak-reference"" rel=""nofollow noreferrer"">weak reference</a>, meaning they can still be garbage-collected unless another object holds a strong or soft reference to them. So it seems my app fails if the garbage collector happens to run between app initialization and the time the outlines are drawn.</p>

<p>This is odd, because I seem to have a strong reference to the binding -- the ""delegate"" field of the value ""outlines"" in the result pane. Looking into it more.</p>",,0,1,,2018-08-07 16:45:38.063 UTC,,2018-08-08 15:17:09.337 UTC,2018-08-08 15:17:09.337 UTC,,423301,,423301,1,1,scala|javafx|scalafx,28
Cloud Vision Sample app failed to make API request,36602892,Cloud Vision Sample app failed to make API request,"<p>I am having trouble getting the Google Vision Sample App to have a successful API request.</p>

<p>I made sure the billing, API-key, were correct.  I even tried using a browser key and service key, but had no luck.  </p>

<p>The error coming back is:  </p>

<blockquote>
  <p>failed to make API request because of other IOException Unable to
  resolve host ""vision.googleapis.com"": No address associated with
  hostname</p>
</blockquote>

<p>If you have any ideas, I would surely appreciate it.</p>",,1,0,,2016-04-13 15:20:14.220 UTC,,2016-04-14 14:56:39.507 UTC,2016-04-14 14:56:39.507 UTC,,5231007,,6199455,1,1,google-cloud-vision,404
"Google cloud vision api-OCR , C#,Deadline Exceeded Exception",49669981,"Google cloud vision api-OCR , C#,Deadline Exceeded Exception","<p>I am trying to use google cloud vision OCR API to read text from image. </p>

<pre><code>using System;
using Google.Cloud.Vision.V1;
namespace blablabla
{
    class Program
    {
        static void Main(string[] args)
        {
            string filePath = @""D:\Manisha\Pictures\1.png"";
            var image = Image.FromFile(filePath);
            var client = ImageAnnotatorClient.Create();
            var response = client.DetectText(image);
            foreach (var annotation in response)
            {
                if (annotation.Description != null)
                    Console.WriteLine(annotation.Description);
            }
            Console.ReadLine();
        }
    }
}
</code></pre>

<p>var response = client.DetectText(image); This lines gives exception : Status(StatusCode=DeadlineExceeded, Detail=""Deadline Exceeded"")</p>",,0,6,,2018-04-05 10:22:55.917 UTC,,2018-04-06 06:23:19.413 UTC,2018-04-06 06:23:19.413 UTC,,7376458,,7376458,1,0,c#|exception|ocr|google-cloud-vision,321
How to detect more than 10 faces in the google vision apis,46095355,How to detect more than 10 faces in the google vision apis,"<p>Hi i am new to google vision apis. I want to detect the faces on the Image ,i am using the node.js. the local image containing more than 10 faces. but vision api returning only 10 faces Detection. Is there any way to detect all the faces using this Vision api. please refer <a href=""https://cloud.google.com/vision/docs/detecting-faces#vision-face-detection-nodejs"" rel=""nofollow noreferrer"">vision node api</a>.
and you can take this image as ref <a href=""https://i.stack.imgur.com/rqCHs.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rqCHs.jpg"" alt=""enter image description here""></a></p>

<p>Here is my code</p>

<pre><code>function findFaceontheImage(req, res, next) {
        var vision = Vision();
        var inputfile = 'NASA_Astronaut_Group_15.jpg';
        var outputFile = 'out.png';
        vision.faceDetection({source: {filename: inputfile}})
            .then(function (results) {

            const faces = results[0].faceAnnotations;
            console.log('Faces:');

            req.body['faces']=results;
            var numFaces = faces.length;
            console.log('Found ' + numFaces + (numFaces === 1 ? ' face' : ' faces'));

            highlightFaces(inputfile, faces, outputFile, Canvas, function (err) {
                if (err) {
                    next()
                }
                console.log(""Finished!"");
                next()
            });


        })
        .catch(function (err) {
            console.error('ERROR:', err);
        });

}

function highlightFaces(inputFile, faces, outputFile, Canvas, callback) {
    fs.readFile(inputFile, function (err, image) {
        if (err) {
            return callback(err);
        }

        var Image = Canvas.Image;
        // Open the original image into a canvas
        var img = new Image();
        img.src = image;
        var canvas = new Canvas(img.width, img.height);
        var context = canvas.getContext(""2d"");
        context.drawImage(img, 0, 0, img.width, img.height);

        // Now draw boxes around all the faces
        context.strokeStyle = ""rgba(0,255,0,0.8)"";
        context.lineWidth = ""5"";

        faces.forEach(function (face) {
            context.beginPath();
            var origX = 0;
            var origY = 0;
            face.boundingPoly.vertices.forEach(function (bounds, i) {
                if (i === 0) {
                    origX = bounds.x;
                    origY = bounds.y;
                }
                context.lineTo(bounds.x, bounds.y);
            });
            context.lineTo(origX, origY);
            context.stroke();
        });

        // Write the result to a file
        console.log(""Writing to file "" + outputFile);
        var writeStream = fs.createWriteStream(outputFile);
        var pngStream = canvas.pngStream();

        pngStream.on(""data"", function (chunk) {
            writeStream.write(chunk);
        });
        pngStream.on(""error"", console.log);
        pngStream.on(""end"", callback);
    });
}
</code></pre>",,1,1,,2017-09-07 11:35:58.927 UTC,1,2017-09-08 07:49:19.637 UTC,2017-09-07 22:28:46.767 UTC,,5231007,,4431509,1,0,node.js|google-cloud-platform|face-detection|google-cloud-vision,108
Google Vision API Response Parsing in java (ClassCastException),48683242,Google Vision API Response Parsing in java (ClassCastException),"<p><strong>ClassCastException while parsing Google Vision API Response</strong> </p>

<p><strong>REQUEST</strong></p>

<pre><code>visionApiRequest {""requests"":[{""features"":[{""maxResults"":2,""type"":""LOGO_DETECTION""}],""image"":{""source"":{""imageUri"":""https://www.tenfold.com/wp-content/uploads/2017/05/icon-sap-hybris.png""}}}]}
</code></pre>

<p><strong>RESPONSE</strong></p>

<pre><code>visionApiResponse {  ""responses"": [    {      ""logoAnnotations"": [        {          ""mid"": ""/m/0gwz218"",          ""description"": ""Hybris"",          ""score"": 0.17361198,          ""boundingPoly"": {            ""vertices"": [              {                ""x"": 65,                ""y"": 58              },              {                ""x"": 114,                ""y"": 58              },              {                ""x"": 114,                ""y"": 120              },              {                ""x"": 65,                ""y"": 120              }            ]          }        }      ]    }  ]}
</code></pre>

<p>Parsing code snippet:</p>

<pre><code>String resp = """";
    while (httpResponseScanner.hasNext()) {
        final String line = httpResponseScanner.nextLine();
        resp += line;
        //System.out.println(line); // alternatively, print the line of response
    }
    System.out.println(""visionApiResponse ""+resp);


    BatchAnnotateImagesResponse annotateImagesResponse = new ObjectMapper().readValue(resp, BatchAnnotateImagesResponse.class);

    List&lt;AnnotateImageResponse&gt; responses = annotateImagesResponse.getResponses();

    for (AnnotateImageResponse res : responses) {

        System.out.println(res);
    }
</code></pre>

<p>Exception at line :  for (AnnotateImageResponse res : responses) {</p>

<p><strong>Exception in thread ""main"" java.lang.ClassCastException: java.util.LinkedHashMap cannot be cast to com.google.api.services.vision.v1.model.AnnotateImageResponse</strong></p>

<p>Dependency : google-api-services-vision-v1-rev370-1.23.0.jar</p>

<p>How to handle this ?</p>",,1,0,,2018-02-08 10:28:43.450 UTC,,2018-02-09 13:24:59.477 UTC,,,,,3783734,1,0,java|google-api|objectmapper|jsonparser|vision-api,265
InvalidS3ObjectException when calling the IndexFaces operation,56051643,InvalidS3ObjectException when calling the IndexFaces operation,"<p>I am attempting the following tutorial </p>

<p><a href=""https://aws.amazon.com/blogs/machine-learning/easily-perform-facial-analysis-on-live-feeds-by-creating-a-serverless-video-analytics-environment-with-amazon-rekognition-video-and-amazon-kinesis-video-streams/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/easily-perform-facial-analysis-on-live-feeds-by-creating-a-serverless-video-analytics-environment-with-amazon-rekognition-video-and-amazon-kinesis-video-streams/</a></p>

<p>So I replaced some part with my own bucket and key(file) name:</p>

<pre><code>aws rekognition index-faces --image ""{\""S3Object\"":{\""Bucket\"":\""testbucket\"",\""Name\"":\""testfile.png\""}}"" --collection-id=rekVideoBlog --detection-attributes=ALL --external-image-id=Andy --region us-west-2
</code></pre>

<p>(assume testbucket is my bucket name and testfile is the file I uploaded and made public). Is this correct?</p>

<p>I have made sure to set the bucket and object public etc but I keep getting an error:</p>

<pre><code>An error occurred (InvalidS3ObjectException) when calling the IndexFaces operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.
</code></pre>

<p>I also tried to access my bucket using:</p>

<p><code>aws s3 ls s3://testbucket</code></p>

<p>and I am able to display the content fine</p>",,0,0,,2019-05-09 03:10:13.313 UTC,,2019-05-09 03:10:13.313 UTC,,,,,11057247,1,0,amazon-web-services|amazon-rekognition,25
"Solution to identify ""Similar Product Images""?",45033467,"Solution to identify ""Similar Product Images""?","<p>I want to build a cloud based solution in which I would give a pool of images; and then ask for ""find similar image to a particular image from this pool of images""  !! Pool of images can be like ""all t-shirt"" images. Hence, similar images mean ""t-shirt with similar design/color/sleeves"" etc.</p>

<p>Tagging solution won't work as they are at very high level. </p>

<p>AWS Rekognition gives ""facial similarities"" .. but not ""product similarities"" .. it does not work like for images of dresses..</p>

<p>I am open to use any cloud providers; but all are providing ""tags"" of the image which won't help me. </p>

<p>One solution could be that I use some ML framework like MXNet/Tensorflow, create my own models, train them and then use.. But is there any other ready made solution on any of cloud providers ?</p>",45036482,5,0,,2017-07-11 11:37:20.790 UTC,,2017-09-15 10:26:49.130 UTC,2017-07-11 13:10:38.780 UTC,,1312478,,1312478,1,4,azure|amazon-web-services|ibm-cloud|google-compute-engine|amazon-rekognition,620
How to restore text file encoding back to UTF-8 after SED command,48896074,How to restore text file encoding back to UTF-8 after SED command,"<p>I received an utf-8 encoded Indian language text file through Google Cloud Vision (OCR). I did some processing on the file using <code>sed</code> and now the file shows strange characters.</p>

<p><code>file -bi 100.txt</code> shows <code>text/plain; charset=unknown-8bit</code> (after sed)</p>

<p>Original file shows this:</p>

<blockquote>
  <p>1 YYHI6808794 ಹೆನರು ಮಿಲ್ಕಾ ಗಂಡನ ಹೆಸರು ನರೇಶ ದಿವಾಕರ ಮನೆ ನಂ 1 ವಯನ್ನು 40
  ಲಿಂಗ ಹೆಣ್ಣು</p>
</blockquote>

<p>Processed file shows this:</p>

<blockquote>
  <p>1 YYHI6808794 à²¹à³à²¨à²°à³ à²®à²¿à²²à³à²à²¾ à²à²à²¡à²¨
  à²¹à³à²¸à²°à³ à²¨à²°à³à²¶ à²¦à²¿à²µà²¾à²à²° à²®à²¨à³ à²¨à² 1
  à²µà²¯à²¨à³à²¨à³ 40 à²²à²¿à²à² à²¹à³à²£à³à²£à³</p>
</blockquote>

<p>This is the command I ran:</p>

<p><code>sed 's/[]*""._,()•&amp;[-]//g; s/^[ L\/]*//; s/  */ /g; s/ಹೆಣ್ಣು/&amp;\n/; s/ಗಂಡು /&amp;\n/;/^$/d;/!/d' oldfile.txt &gt; newfile.txt</code></p>

<p>Is there any way to restore it back to original encoding?</p>",,0,5,,2018-02-20 23:50:02.277 UTC,,2018-02-21 18:24:03.257 UTC,2018-02-21 18:24:03.257 UTC,,8852495,,8852495,1,1,sed|encoding,79
Microsoft Face API (Android)- Add Person to Person Group,53952217,Microsoft Face API (Android)- Add Person to Person Group,"<p>I know that this may seem to be a rather broad question, but I have been unable to figure out <strong>how to create a Person in a Person Group using the Microsoft Face API in Android Studio</strong>. </p>

<p>I have tried the following code to make a <code>CreatePersonResult</code> object in Android:</p>

<pre><code>try {
    CreatePersonResult person1 = faceServiceClient.createPerson(personGroupId, ""Bob"", ""My Friends"");
    Toast.makeText(getApplicationContext(), ""Created Person called Bob"", Toast.LENGTH_LONG).show();
 } catch (Exception e) {
    Toast.makeText(getApplicationContext(), ""Creation failed: "" + e.getMessage(), Toast.LENGTH_LONG).show();
 }
</code></pre>

<p>The above code outputs: <strong>""Creation failed: null""</strong> which means that <strong>the <code>Exception</code> was <code>null</code></strong> for some reason.</p>

<p>In Visual Studio, to create a <code>Person</code> I simply have to do the following:</p>

<pre><code>CreatePersonResult person1 = await faceServiceClient.CreatePersonAsync(personGroupId, ""Bob"");
</code></pre>

<hr>

<p>Does anyone know how I can create the <code>Person</code> in a Person Group in Android? I have been unable to figure out how to do this in Android, but found plenty of tutorials for Visual Studio.</p>",,0,0,,2018-12-28 00:04:43.127 UTC,0,2019-01-25 13:31:31.597 UTC,2019-01-25 13:31:31.597 UTC,,10589041,,10589041,1,1,android|azure|exception|microsoft-cognitive|face-api,141
"Firestore returns a Dictionary<String, Any> but I need the values as a String",53643788,"Firestore returns a Dictionary<String, Any> but I need the values as a String","<p>I am using Firebase to get Google Cloud Vision Optical Character Recognition on an image then putting that information into a Firestore database, however when I pull the data from Firestore it is of type Dictionary. I need the values to be in a String so I can manipulate them however I can't seem to cast something of type Any to a String. I can put the values into an array but it is still an array of Any type. Here is the relevant code snippet:</p>

<pre><code>    db.collection(""imagedata"").document(puzzletest.name!).addSnapshotListener { documentSnapshot, error in
        guard let document = documentSnapshot else{
            print(""error"")
            return
        }
        guard let data = document.data() else{
            print(""empty"")
            return
        }
        let arrayofres = Array(data.values)

        print(type(of:arrayofres))
    }
</code></pre>

<p>Here is the data I am trying to query:</p>

<p><strong>Image of database</strong>:</p>

<p><img src=""https://i.stack.imgur.com/PPkMu.png"" alt=""Screen Shot""></p>

<p>Any guidance would be appreciated.</p>",53644609,1,2,,2018-12-06 02:36:10.707 UTC,,2018-12-08 19:56:56.277 UTC,2018-12-08 19:56:56.277 UTC,,243192,,6947516,1,1,swift|firebase|google-cloud-firestore,103
AWS SQS error : SignatureDoesNotMatch HTTP error: 403,51977903,AWS SQS error : SignatureDoesNotMatch HTTP error: 403,"<p>I'm working on a face tracking thought video <code>AWS Rekognition</code> (<code>StartFaceSearch</code>). It will return the job id. I have setup everything <code>SQS</code> <code>SNS</code> parts. <strong>Here my problem</strong> is i can't able to get a message from SQS using <code>receiveMessage()</code>. Even <code>listQueues()</code> also not working. It gives this below error. </p>

<blockquote>
  <p>""ListQueues"" on ""<a href=""https://sqs.us-east-1.amazonaws.com"" rel=""nofollow noreferrer"">https://sqs.us-east-1.amazonaws.com</a>""; AWS HTTP error: Client error: <code>POST https://sqs.us-east-1.amazonaws.com</code> resulted in a <code>403 Forbidden</code> response:
  &lt; ErrorResponse xmlns=""http://queue.amazonaws.com/doc/2012-11-05/"">Sender&lt; Code >S (truncated...)
  SignatureDoesNotMatch (client): The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.</p>
  
  <p>The Canonical String for this request should have been
  'POST
  / ....... &lt; continue.............</p>
</blockquote>

<p>The code as follow. AWS PHP SDK ver 3.64.11.</p>

<pre><code>$queueUrl = ""https://sqs.us-east-1.amazonaws.com/xxxxxxx/facetrackingreceive"";
        // Get the client from the builder by namespace
        $sqs_client = new SqsClient(array(
            'credentials' =&gt; array(
                'key'    =&gt; 'xxxxxxxxxx',
                'secret' =&gt; 'xxxxxxxxxxxxxxxxxxxxxxx',
            ),
            'version' =&gt; '2012-11-05',
            'region'  =&gt; 'us-east-1',
            'http' =&gt; [ 'verify' =&gt; false ],

        ));

        $result = $sqs_client-&gt;listQueues();

        foreach ($result-&gt;get('QueueUrls') as $queueUrl) {
            echo ""$queueUrl\n"";
        }
</code></pre>

<p>Thanks in advance!</p>",52003396,1,4,,2018-08-23 03:22:19.677 UTC,1,2018-08-24 11:22:11.793 UTC,2018-08-24 05:39:29.150 UTC,,8368982,,8368982,1,0,php|laravel|amazon-web-services|amazon-sqs|aws-php-sdk,313
OCR numbers recognition (Google Vision),49717845,OCR numbers recognition (Google Vision),"<p>I have to create a sudoku solver, so I create with google vision, a number recognition to retrieve numbers from the grid.This numbers recognition trim the grid to analyse each cell but the recognition doesn't work.. I think the problem comes from TextRecognizer who has trouble recognizing a single character.</p>

<p>Can you help me please?</p>

<p>Thanks.</p>

<pre><code>    btnProcess.setOnClickListener(new View.OnClickListener() {
        @Override
        public void onClick(final View v) {
            new Thread(new Runnable() {
                public void run() {
                    final StringBuilder stringBuilder = new StringBuilder();
                    TextRecognizer textRecognizer=new TextRecognizer.Builder(getApplicationContext()).build();
                    if(!textRecognizer.isOperational()){
                        Log.e(""Error"",""Detector not available"");
                    }
                    else {
                        int largeur = (bitmap.getWidth()) / 9;
                        int hauteur = (bitmap.getHeight()) / 9;
                        Bitmap cellule = null;
                        for (int y = 0; y&lt; 9; y++) {
                            for (int x = 0; x &lt; 9; x++) {
                                cellule = Bitmap.createBitmap(bitmap,x*largeur,y*hauteur,largeur,hauteur);
                                Frame frame = new Frame.Builder().setBitmap(cellule).build();
                                cellule.recycle();
                                SparseArray&lt;TextBlock&gt; items = textRecognizer.detect(frame);
                                if (items.size()==0){
                                    stringBuilder.append(""0"" + "" "");
                                }
                                else{
                                    TextBlock item=items.valueAt(0);
                                    stringBuilder.append(item.getValue() + "" "");
                                }

                            }
                            stringBuilder.append(""\n"");
                        }
                    }
                    runOnUiThread(new Runnable() {
                        public void run() {
                            txtResult.setText(stringBuilder.toString());
                        }
                    });
                }
            }).start();

        };
    });
</code></pre>",,1,0,,2018-04-08 12:31:45.160 UTC,,2019-02-18 18:17:45.950 UTC,2018-04-08 19:12:02.830 UTC,,9582793,,9582793,1,2,java|android|android-studio|google-vision|android-applicationinfo,365
Using Azure Computer Vision API with mathematical expressions,45395459,Using Azure Computer Vision API with mathematical expressions,"<p>i'm kind of new to the Azure Computer Vision API but i'm interested to use this for parsing a lot of mathematical documents.</p>

<p>I wanted to use <a href=""http://mathparser.org/"" rel=""nofollow noreferrer"" title=""mXparser"">mxParser</a> for evaluating the output string from the Vision API but currently only text recocgnition is supported.
Does somebody know if the API is usable for this kind of scenario in any way (or will be in the future)?</p>

<p>If this is not a good use-case for  this kind of AI API what would you recommend to use for generating a usable string expression from handwritten mathematical documents ?</p>",45405120,1,0,,2017-07-30 01:09:25.037 UTC,,2017-07-30 22:01:01.810 UTC,,,,,8387849,1,1,c#|azure|.net-core|microsoft-cognitive,113
Google Cloud Vision API on emulator works 6x faster rather than on real device,42657315,Google Cloud Vision API on emulator works 6x faster rather than on real device,"<p>I am in process of integration Google Cloud Vision API to my app.
I noticed that Text Detection, Label Detection works on real device 6x slower rather than on Android emulator.</p>

<p>The reason of issue is NOT low internet speed in real device. Because another app part, which also makes http request, gives one performance for real device and emulator. </p>

<p>The issue is reproducible at official Google <a href=""https://github.com/GoogleCloudPlatform/cloud-vision/tree/master/android"" rel=""nofollow noreferrer"">Cloud Vision Android Sample</a></p>

<p>Tested on 2 different real devices and 2 emulators. </p>

<p>Any ideas why it happens? </p>

<p><strong>Code from official sample:</strong></p>

<pre><code>private void callCloudVision(final Bitmap bitmap) throws IOException {
    // Switch text to loading
    mImageDetails.setText(R.string.loading_message);

    // Do the real work in an async task, because we need to use the network anyway
    new AsyncTask&lt;Object, Void, String&gt;() {
        @Override
        protected String doInBackground(Object... params) {
            try {
                HttpTransport httpTransport = AndroidHttp.newCompatibleTransport();
                JsonFactory jsonFactory = GsonFactory.getDefaultInstance();

                Vision.Builder builder = new Vision.Builder(httpTransport, jsonFactory, null);
                builder.setVisionRequestInitializer(new
                        VisionRequestInitializer(CLOUD_VISION_API_KEY));
                Vision vision = builder.build();

                BatchAnnotateImagesRequest batchAnnotateImagesRequest =
                        new BatchAnnotateImagesRequest();
                batchAnnotateImagesRequest.setRequests(new ArrayList&lt;AnnotateImageRequest&gt;() {{
                    AnnotateImageRequest annotateImageRequest = new AnnotateImageRequest();

                    //https://cloud.google.com/vision/docs/languages
                    ImageContext imageContext = new ImageContext();
                    List&lt;String&gt; languages = new ArrayList&lt;&gt;();
                    languages.add(""pl"");
                    imageContext.setLanguageHints(languages);
                    annotateImageRequest.setImageContext(imageContext);

                    // Add the image
                    Image base64EncodedImage = new Image();
                    // Convert the bitmap to a JPEG
                    // Just in case it's a format that Android understands but Cloud Vision
                    ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();
                    bitmap.compress(Bitmap.CompressFormat.JPEG, 90, byteArrayOutputStream);
                    byte[] imageBytes = byteArrayOutputStream.toByteArray();

                    // Base64 encode the JPEG
                    base64EncodedImage.encodeContent(imageBytes);
                    annotateImageRequest.setImage(base64EncodedImage);

                    // add the features we want
                    annotateImageRequest.setFeatures(new ArrayList&lt;Feature&gt;() {{
                        Feature labelDetection = new Feature();
                        labelDetection.setType(""LABEL_DETECTION"");
                        labelDetection.setMaxResults(10);
                        add(labelDetection);
                    }});

                    // Add the list of one thing to the request
                    add(annotateImageRequest);
                }});

                Vision.Images.Annotate annotateRequest =
                        vision.images().annotate(batchAnnotateImagesRequest);
                // Due to a bug: requests to Vision API containing large images fail when GZipped.
                annotateRequest.setDisableGZipContent(true);
                Log.d(TAG, ""created Cloud Vision request object, sending request"");

                BatchAnnotateImagesResponse response = annotateRequest.execute();
                return convertResponseToString(response);

            } catch (GoogleJsonResponseException e) {
                Log.d(TAG, ""failed to make API request because "" + e.getContent());
            } catch (IOException e) {
                Log.d(TAG, ""failed to make API request because of other IOException "" +
                        e.getMessage());
            }
            return ""Cloud Vision API request failed. Check logs for details."";
        }

        protected void onPostExecute(String result) {
            mImageDetails.setText(result);
        }
    }.execute();
}
</code></pre>

<p>Time results for processing one image after 3 tests: <br>
<strong>real device</strong><br>
48409608471<br>
49837573418<br>
48430621961<br></p>

<p><strong>emulator</strong><br>
7725682517<br>
7967006977<br>
5478519794<br></p>

<p>I used simple</p>

<pre><code>long startTime = System.nanoTime();
long endTime = System.nanoTime();
long duration = (endTime - startTime); 
</code></pre>",,0,6,,2017-03-07 20:05:02.697 UTC,,2017-03-09 10:43:18.557 UTC,2017-03-09 10:43:18.557 UTC,,3627736,,3627736,1,1,android|google-vision,466
Will google vision/natural language api start charging when the number of free requests are exceeded?,48625509,Will google vision/natural language api start charging when the number of free requests are exceeded?,"<p>I am preparing my first batch of requests to google vision/natural language apis. I plan on sending enough requests to exceed the free quota. I do still have my $300 in free credits in my account. So my question is: when my script is running and passes the last free request, will google then simply start deducting from my balance and allow the script to continue running seamlessly, or will it stop the script and ask me for some user input?</p>

<p>Thanks </p>",,1,0,,2018-02-05 15:07:40.170 UTC,,2018-02-05 16:17:39.520 UTC,,,,,8679724,1,0,google-api|google-vision|google-language-api,22
(Microsoft Azure custom vision service: Object Detection) How to find bounding box info of training data?,56293609,(Microsoft Azure custom vision service: Object Detection) How to find bounding box info of training data?,"<p>I am using Microsoft custom vision service in object detection to extract the wanted objects. And I would like to make a regression test to compare the results. However, I cannot find a place to export the training picture with the bounding box that user defined by GUI.</p>

<p>The model training is done within the custom vision platform provided by Microsoft (<a href=""https://www.customvision.ai/"" rel=""nofollow noreferrer"">https://www.customvision.ai/</a>). Within this platform we can add the images and then tag the objects. I have tried to export the model, but I am not sure where to find the info of training pictures along with their tag(s) and bounding box(es).</p>

<p>I expect that in this platform, user can export the not only the trained model but also the training data (images with tags and bounding boxes.) But I was not able to find them.</p>",,2,0,,2019-05-24 13:24:33.270 UTC,1,2019-05-29 09:01:26.663 UTC,,,,,6387826,1,1,object-detection|microsoft-cognitive|azure-cognitive-services|microsoft-custom-vision,38
IBM Bluemix Watson visual recognition authorization issue,34643033,IBM Bluemix Watson visual recognition authorization issue,"<p>under my IBM Bluemix account, I have registered a Watson Visual Recognition service.
My intention is to call the service from Bizagi BPMS as REST service.
Bizagi brings an ""unauthorized"" error.</p>

<p>The URL for the REST Service is
<a href=""https://gateway.watsonplatform.net/visual-recognition-beta/api/v2/classify"" rel=""nofollow"">https://gateway.watsonplatform.net/visual-recognition-beta/api/v2/classify</a></p>

<p>The Service URL is
<a href=""https://gateway.watsonplatform.net/visual-recognition-beta/api/?username=xxxxxxxxxxxxxxxxxxx&amp;password=yyyyyyyyyyy"" rel=""nofollow"">https://gateway.watsonplatform.net/visual-recognition-beta/api/?username=xxxxxxxxxxxxxxxxxxx&amp;password=yyyyyyyyyyy</a> 
(x and y are the credentials from the service instance in Bluemix.
When entering the Service URL directly into the browser, I can enter the authentication credentials in a popup window, but the response is 
""Error 404: SRVE0190E: File not found: / ""</p>",,1,0,,2016-01-06 21:23:25.743 UTC,,2016-01-06 21:33:11.107 UTC,2016-01-06 21:33:11.107 UTC,,456564,,5754413,1,1,ibm-cloud|ibm-watson|visual-recognition,326
Google Vision API - To draw graphic layout on Camera Preview bounding the QR code,35121089,Google Vision API - To draw graphic layout on Camera Preview bounding the QR code,<p>I am integrating Google vision API into my existing android application.  the app does recognises the QR codes but i need to implement the UI feature where the user is shown a graphic outline over the bar code . </p>,,1,1,,2016-02-01 00:01:15.570 UTC,3,2016-02-02 15:36:21.863 UTC,,,,,2458337,1,5,android|google-vision,2676
AttributeError: module 'google.cloud.vision' has no attribute 'Client',53486685,AttributeError: module 'google.cloud.vision' has no attribute 'Client',"<p>**help me this a simple script to implement google vision API in python . </p>

<h2>i installed all requirements i need but still see that error**</h2>

<h2>AttributeError: module 'google.cloud.vision' has no attribute 'Client'</h2>

<blockquote>
  <p>Traceback (most recent call last):   File
  ""C:/Users/TAKWA/Desktop/vision/44.py"", line 4, in 
      vision_client = vision.Client() AttributeError: module 'google.cloud.vision' has no attribute 'Client'</p>
</blockquote>

<hr>

<pre><code>import io
from google.cloud import vision

vision_client = vision.Client('my-key')
file_name = '1.jpg'

with io.open(file_name, 'rb') as image_file:
    content = image_file.read()
    image = vision_client.image(
        content=content, )

labels = image.detect_labels()
for label in labels:
    print(label.description)
</code></pre>",,1,0,,2018-11-26 18:03:35.470 UTC,1,2018-11-26 19:09:04.323 UTC,,,,user9815201,,1,0,python|python-3.x|google-cloud-platform|vision-api,138
How to get complete product information from Bar code display value in android?,45379312,How to get complete product information from Bar code display value in android?,"<p>I used Google vision Barcode API for scanning the bar code.
From that, I can able to get display value from the bar code. But, how can I get complete product information after scanning the bar code?</p>

<p>Which means, After scanning I'm getting bar code value as 036000291452.
But I need to get product information from it as well. </p>

<p>Similar to below:</p>

<p>Barcode Formats: UPC 036000291452
Artist: Haindling
Label: Polydor</p>

<p>Please help! </p>",,1,0,,2017-07-28 17:32:49.427 UTC,,2017-07-28 17:35:36.437 UTC,,,,,7472416,1,2,android|barcode|barcode-scanner|google-vision,5879
How to group blocks that are part of a bigger sentences in Google Cloud Vision API?,52383178,How to group blocks that are part of a bigger sentences in Google Cloud Vision API?,"<p>I am using <code>Google Cloud Vision API</code> on <code>Python</code> to detect text values in hoarding boards that are usually found above a shop/store. So far I have been able to detect individual words and their bounding polygons' coordinates. Is there a way to group the detected words based on their relative positions and sizes?</p>

<p>For example, the name of the store is usually written in same size and the words are aligned. Does the API provide some functions that group those words which probably are parts of a bigger sentence (the store name, or the address, etc.)?</p>

<p>If the API does not provide such functions, what would be a good approach to group them? Following is an example of an image what I have done so far:</p>

<p><a href=""https://i.stack.imgur.com/sT6mg.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sT6mg.jpg"" alt=""shop&#39;s banner""></a>
Vision API output excerpt:</p>

<pre><code>description: ""SHOP""
bounding_poly {
  vertices {
    x: 4713
    y: 737
  }
  vertices {
    x: 5538
    y: 737
  }
  vertices {
    x: 5538
    y: 1086
  }
  vertices {
    x: 4713
    y: 1086
  }
}
, description: ""OVOns""
bounding_poly {
  vertices {
    x: 6662
    y: 1385
  }
  vertices {
    x: 6745
    y: 1385
  }
  vertices {
    x: 6745
    y: 1402
  }
  vertices {
    x: 6662
    y: 1402
  }
}
</code></pre>",52389731,1,0,,2018-09-18 09:20:52.460 UTC,,2018-09-18 15:12:54.013 UTC,,,,,2572287,1,0,ocr|google-cloud-vision,169
AWS Rekognition InvalidImageFormatException when jpeg source is mjpeg stream,41228649,AWS Rekognition InvalidImageFormatException when jpeg source is mjpeg stream,"<p>I'm pulling jpg frames out of a mjpg stream. These are valid jpg files and work in any image tool I've tried; however, Rekognition will not accept them either when sending it as Bytes, or when I move them to S3 and try that route.</p>

<p>I've made a few versions (<a href=""https://dl.dropboxusercontent.com/u/16969955/test-images.zip"" rel=""nofollow noreferrer"">attached here</a>), all from the same source jpg (I would include them inline but I don't want image optimization code to alter them)</p>

<p><code>test.jpg</code> - original frame</p>

<p><code>test-photoshop.jpg</code> - opened in Photoshop, ""save for web""d</p>

<p><code>test-imageoptim.jpg</code> - run through ImageOptim (which I believe compresses with jpegtran)</p>

<p>Looking at these in a hex editor, the only difference I can't see is more exif data (using exiftool). When I run exiftool on the original, it still reports back all the basic details of the frame.</p>

<p>I'm assuming this is a bug with Rekognition, or there is some specific exif bit it's looking for that my mjpeg stream extraction is omitting. Maybe someone has information on why pulling jpeg frames from mjpeg isn't possible by just attaching the right start and end frame bytes.</p>",41246806,3,2,,2016-12-19 18:17:31.927 UTC,,2016-12-20 16:19:59.347 UTC,,,,,1177599,1,2,amazon-web-services|jpeg|mjpeg|amazon-rekognition,925
How to change the recognition language,43793934,How to change the recognition language,"<p>Using Google vision from <a href=""https://github.com/googlecloudplatform/google-cloud-python"" rel=""nofollow noreferrer"">here</a> I was successfully able to create a <code>client</code> and an <code>image</code> using <code>vision.Client()</code> and <code>client.image(content=data)</code> respectively. And then send my image using <code>image.detect_text()</code>, attempting to read the digits within the image. however Google-vision has been inaccurate and I heard, from <a href=""https://stackoverflow.com/questions/39540741/google-cloud-vision-numbers-and-numerals-ocr"">this</a> question, that by setting the language to another (non-latin) language would help with this.</p>

<p>But that is where I am stuck, I'm not sure where to set the <code>languageHints</code>, and yes I have seen <a href=""https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate#AnnotateImageRequest"" rel=""nofollow noreferrer"">this</a> link to the documentation of the <code>AnnotateImageRequest</code>, but I am still confused as to where this comes in.</p>",,1,0,,2017-05-04 22:50:00.123 UTC,1,2018-01-18 13:04:43.220 UTC,2017-05-23 12:34:18.457 UTC,,-1,,6137812,1,1,python|ocr|google-cloud-vision,386
Customize charset in Google Cloud Vision,54231123,Customize charset in Google Cloud Vision,<p>I am working with text detection feature of Google Cloud Vision API and after taking a look to the documentation I am not able to find any way to customize the desired charset used to perform OCR. This is perfectly possible in Tesseract thus it permits avoiding misspellings when trying to scan a limited set of characters. It seems that the only possibility here is to use 'languageHints' in order to select a language beforehand. Does anybody know about some way of establishing a whitelist of desired characters?</p>,,0,0,,2019-01-17 07:40:25.110 UTC,,2019-01-17 07:40:25.110 UTC,,,,,10923854,1,0,ocr,15
404 error using Python with Computer Vision API analyzing web-based images,50808403,404 error using Python with Computer Vision API analyzing web-based images,"<p>I have no problems using an Azure Computer Vision API service in C# using the same region/key but Python is giving me fits. I can't analyze anything from the web. Here is the snippet from a sample:</p>

<pre><code>_region = 'westus' #Here you enter the region of your subscription
_url = 'https://{}.api.cognitive.microsoft.com/vision/v1.0'.format(_region)
_key = ""&lt;my API Key&gt;""
_maxNumRetries = 10

# URL direction to image
urlImage = ""https://oxfordportal.blob.core.windows.net/vision/Analysis/1-1.jpg""

# Computer Vision parameters
params = { 'visualFeatures' : 'Color,Categories'} 

headers = dict()
headers['Ocp-Apim-Subscription-Key'] = _key
headers['Content-Type'] = 'application/json' 

json = { 'url': urlImage } 
data = None

result = processRequest( json, data, headers, params )
</code></pre>

<p>I have tried numerous images with variations of the above, but I never get anything but a 404 - Resource not found error. Where am I going wrong? Is it a problem with the service or the URL? TIA</p>",50808471,1,0,,2018-06-12 02:19:49.777 UTC,,2018-06-12 02:31:40.763 UTC,,,,,6020411,1,0,python|computer-vision,83
limit size image for ocr scan,54039956,limit size image for ocr scan,"<p>I have a app client where the user upload a photo via native cam or canvas (PNG). In both cases the uploaded image is too large, about 6MB.
I have to pass this image to Google Vision for our scanning.</p>

<p>I have to limit the photo always at 5 MB. This because I support other services, for example Amazon recognition, where the limit is 5MB.</p>

<p>How I can to use imagemagick for this problem ?
I want use PNG because is probably better for the ocr, but I don’t want a static resolution, but I want only limit the size at little less of 5MB.</p>",,0,2,,2019-01-04 13:32:47.997 UTC,,2019-01-04 13:32:47.997 UTC,,,,,6003047,1,1,javascript|node.js|imagemagick|ocr,19
"API call within script does not return expected value, script continues",42707597,"API call within script does not return expected value, script continues","<p>I have api calls within scripts. The scripts then uses the information from the api calls. I think the problem is that <em>sometimes</em> the script moves on without waiting for a response from the api call. Here's the reason why I think this is the case:</p>

<p>Example 1: I have a script (cs.py) that gets the currency conversion rate with an api call and does some other stuff. Sometimes when I run this script I get an error that looks like the following.<br>
<code>aud_to_usd = requests.get('<a href=""http://api.fixer.io/latest?base=AUD&amp;symbols=USD"" rel=""nofollow noreferrer"">http://api.fixer.io/latest?base=AUD&amp;symbols=USD</a>').json()['rates']['USD']
...
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
</code></p>

<p>Example 2: I have a script that takes invoices and either extracts text using tika (if text is searchable) or extracts text using the google vision ocr api. The end result being text that is formatted and output into a spreadsheet. <em>Sometimes</em> only the text from the tika call appears in the spreadsheet, the info from the google api call just doesn't come through.</p>

<p>If I run the scripts incrementally in ipython (e.g., pasting each api call into the terminal individually and waiting for a response) the scripts work. Also (as hinted by my use of the word ""sometimes"") just running the entire script works, sometimes.</p>

<p>This seems like it would be a common issue. Am I right that the api call is just taking too long and the script is moving on? Is there a way to make the script wait until a response is provided? For example, a try-except that tries the api call, waits some amount of time and re-tries the call if there is no response?</p>

<p>(SO is telling me not to the use tag 'api' but I cannot think of a more appropriate tag...)</p>",42707741,1,0,,2017-03-09 23:38:32.323 UTC,,2017-03-10 16:50:48.673 UTC,2017-03-10 16:50:48.673 UTC,,4592270,,4592270,1,0,python|api|python-requests,63
"Error ""Request Entity Too Large"" when trying to train Visual Recognition model",54757580,"Error ""Request Entity Too Large"" when trying to train Visual Recognition model","<p>I'm receiving this error when trying to train my first custom model:</p>

<blockquote>
  <p>Unauthorized: Access is denied due to invalid credentials. Ensure that the Watson Visual Recognition service has not been deleted, and that the quota limit for your plan has not been reached.</p>
</blockquote>

<p>I haven't done much at this point. I also am not accessing Visual Recognition through anything else, I'm just trying to get this model trained. I'm not sure what credentials or quota limit would have to do with this. Anyone else have any experience with this issue?</p>

<p>Back story: I had originally uploaded 73 zip files (all >10 files inside, each ~5MB, 1342 images total), but I was catching errors when trying to train, including a ""request too large"". So I declassified 70 of them, and now I'm just trying to train this model with 3 categories. Now I'm getting the ""Unauthorized"" message I originally mentioned. I had given some time in between pressing the train button (hours) to maybe prevent any backlog of requests from my part.</p>",,1,1,,2019-02-19 01:18:43.717 UTC,,2019-02-19 19:46:17.717 UTC,2019-02-19 01:19:16.160 UTC,,1226963,,6481591,1,0,ibm-watson|coreml|visual-recognition|watson-studio,41
netty-tcnative unavailable with Google Cloud Vision API,54634576,netty-tcnative unavailable with Google Cloud Vision API,"<p>To start, I'm quite inexperienced with APIs in general. I'm trying to do a simple Java app that calls the Google Cloud Vision Api but I keep running into the same issue that I can't really find any information on whatsoever. </p>

<p>I've cloned down <a href=""https://github.com/GoogleCloudPlatform/java-docs-samples/tree/master/vision/cloud-client"" rel=""nofollow noreferrer"">this repository</a> with code samples straight from Google. I've built the project using <code>mvn clean package</code> and it all works fine. However, when I'm to try it (using the exact commands stated in the README), it doesn't work at all. </p>

<p>First I get an <code>INFO</code> message in the log stating:</p>

<p><code>netty-tcnative unavailable (this may be normal)</code></p>

<p>After that follows:</p>

<pre><code>IllegalArgumentException: Failed to load any of the given libraries: [netty_tcnative_windows_x86_32, netty_tcnative_x86_32, netty_tcnative]
</code></pre>

<p>This error message really doesn't make any sense to me at all. I haven't done anything with netty whatsoever, neither have I been instructed to do anything with it (install dependencies or so). </p>

<p>I got my environment variable <code>GOOGLE_APPLICATION_CREDENTIALS</code> pointing to my JSON with my API credentials inside it. I really don't know what to do here, extremely thankful for any pointers. </p>",54882788,1,0,,2019-02-11 16:08:49.253 UTC,1,2019-02-26 09:56:07.297 UTC,2019-02-11 16:34:39.493 UTC,,9908251,,6502832,1,2,tomcat|google-cloud-platform|netty|google-cloud-vision,107
Google AutoML - detect form fields on an image,54367776,Google AutoML - detect form fields on an image,"<p>Is there a way to get coordinates from an form field on an image (scanned image), by using Google vision?</p>

<ul>
<li><p>With the (LocalizedObjectAnnotation) can Google detect only objects and creatures</p></li>
<li><p>Google OCR (fullTextAnnotation) detects only text</p></li>
</ul>

<p>Scenario:</p>

<p>I got an scanned formular. From this scan i would get all form field-positions (input-fields).</p>

<p>It don't work with one or both google method's ""LocalizedObjectAnnotation"" and ""fullTextAnnotation"". Because one detect only objects / creatures and the other one only text. So both can't find the input-field in the image.</p>

<p>Has anyone an idee how i get the coordinates for the input-fields?</p>",,0,0,,2019-01-25 14:58:13.743 UTC,,2019-01-25 14:58:13.743 UTC,,,,,7993505,1,1,google-vision|vision,21
Send multiple frame to AWS rekognition,50741636,Send multiple frame to AWS rekognition,"<p>I'm trying to send pictures to the aws rekognition from my webcam to detect the activity of the person sitting in front of it using python.</p>

<p>To do so I take a picture every 5 seconds and I send it to the aws.
But when I do so it seems that he's always sending back information about the first frame that I sent</p>

<pre><code>cap = cv2.VideoCapture(0)

while 1:
   ret, img = cap.read()
   client=boto3.client('rekognition')

   print(""hello"")
   ret, fileImg=cv2.imencode('.png',img)
   response = client.detect_labels(Image={'Bytes':fileImg.tobytes()})
   print('Detected labels for Camera Capture')    
   for label in response['Labels']:
       print (label['Name'] + ' : ' + str(label['Confidence']))

   sleep(5)
</code></pre>

<p>Here is the result i get from that call:</p>

<pre><code>Detected labels for Camera Capture
Human : 99.1103897095
People : 99.1103744507
Person : 99.1103897095
Face : 56.5527687073
Crypt : 51.1719360352
hello
Detected labels for Camera Capture
Human : 99.0247421265
People : 99.0247344971
Person : 99.0247421265
Face : 57.7796173096
Lighting : 51.8473701477
Crypt : 51.08152771
hello
Detected labels for Camera Capture
Human : 99.0808181763
People : 99.0808105469
Person : 99.0808181763
Face : 56.4268836975
Lighting : 54.6302490234
Crypt : 50.8622779846
hello
</code></pre>

<p>Knowing during the time of the call the image has changed a lot and should (at least I think) show me other results.</p>

<p>I'm new to aws so I might be missing a point also it's my first post on stackoverflow so I might have forget to write something</p>",,1,0,,2018-06-07 12:48:29.140 UTC,0,2018-06-07 13:26:19.610 UTC,2018-06-07 13:26:19.610 UTC,,174777,,9734638,1,1,python|amazon-web-services|opencv|webcam|amazon-rekognition,261
Getting wrong text sequence when image scanned by offline google mobile vision API,53591219,Getting wrong text sequence when image scanned by offline google mobile vision API,"<pre><code> public StringBuilder scanImage(Bitmap bp)
    {
        StringBuilder sb=null;
        TextRecognizer tcx = new 
        TextRecognizer.Builder(getApplicationContext()).build();
        if (!tcx.isOperational())
        {
            Toast.makeText(getApplicationContext(), ""could not get text"", Toast.LENGTH_SHORT).show();

        } else
        {
            Frame fame = new Frame.Builder().setBitmap(bp).build();
            SparseArray&lt;TextBlock&gt; items = tcx.detect(fame);
             sb = new StringBuilder();

            for (int i = 0; i &lt; items.size(); ++i)
            {
                TextBlock mytext = items.valueAt(i);
                sb.append(mytext.getValue());
                sb.append(""\n"");

            }


        }
            return sb;

    }
</code></pre>

<p>this is my code. I'm using mobile google vision API. I'm just passing image bitmap  for scan but this method returns scanned text in wrong sequence.please tell me how to get text in proper sequence. Thank you in advance</p>",53591326,1,0,,2018-12-03 09:54:39.973 UTC,,2018-12-03 14:06:00.123 UTC,2018-12-03 14:06:00.123 UTC,,2649012,,9006896,1,1,java|android|api,33
Understanding DetectedBreak in google OCR full text annotations,52579907,Understanding DetectedBreak in google OCR full text annotations,"<p>I am trying to convert the full-text annotations of google vision OCR result to line level and word level which is in <code>Block</code>,<code>Paragraph</code>,<code>Word</code> and <code>Symbol</code> hierarchy.</p>

<p>However, when converting <code>symbols</code> to <code>word</code> text and <code>word</code> to <code>line</code> text, I need to understand the DetectedBreak property.</p>

<p>I went through <a href=""http://googleapis.github.io/googleapis/java/grpc-google-cloud-vision-v1/0.1.5/apidocs/com/google/cloud/vision/v1/TextAnnotation.DetectedBreak.BreakType.html"" rel=""nofollow noreferrer"">This documentation</a>.But I did not understand few of the them.  </p>

<p>Can somebody explain what do the following Breaks mean? I only understood <code>LINE_BREAK</code> and <code>SPACE</code>.</p>

<ol>
<li>EOL_SURE_SPACE</li>
<li>HYPHEN</li>
<li>LINE_BREAK</li>
<li>SPACE</li>
<li>SURE_SPACE</li>
<li>UNKNOWN</li>
</ol>

<p>Can they be replaced by either a newline char or space ?</p>",53121930,1,0,,2018-09-30 16:31:43.350 UTC,,2018-11-02 15:56:23.850 UTC,,,,,8283737,1,0,ocr|google-cloud-vision|google-vision|text-segmentation,241
"Bitmap ""images"" pass to another activity (Out of memory)",51926971,"Bitmap ""images"" pass to another activity (Out of memory)","<p>please read the full question before marking it as duplicate or down-vote it.</p>

<p>i am developing an app what can slice through a picture and run google vision to recognize text in each chunk or slice of picture and run OCR to detect that the circle bubble is filled or not in the chunk. but when i am slicing the Bitmap image in an array and pass it to other activity for the process it crashes for over use of memory. I know i can compress it but i tried that already (though i did not wanted to compress it since i need to run google vision and may not able to extract text accurately) but it did not work since there are 46 slices of image. How can i do so without uploading on cloud fetch it again for process since it might take long. any alternative solution is very welcome as well. i am stuck on this for quite a while.</p>

<pre><code>import android.content.Intent;.....

public class ProcessesdResult extends AppCompatActivity {

TextView tvProcessedText;
Button btnImageSlice;
Bitmap image;
int chunkNumbers =46;

@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.activity_processesd_result);

    Intent intenttakeattendance = getIntent();
    String fname = intenttakeattendance.getStringExtra(""fname"");

    String root = Environment.getExternalStorageDirectory().toString();
    File myDir = new File(root);

    String photoPath = myDir+""/sams_images/""+ fname;
    BitmapFactory.Options options = new BitmapFactory.Options();
    options.inPreferredConfig = Bitmap.Config.ARGB_8888;
    image = BitmapFactory.decodeFile(photoPath, options);


    btnImageSlice=findViewById(R.id.btnimageslice);
    btnImageSlice.setOnClickListener(new View.OnClickListener() {
        @Override
        public void onClick(View v) {
            splitImage(image, chunkNumbers) ;
        }
    });

}


    private void splitImage(Bitmap image, int chunkNumbers) {

        //For the number of rows and columns of the grid to be displayed
        int rows =23;
        int cols =2;

        //For height and width of the small image chunks
        int chunkHeight,chunkWidth;

        //To store all the small image chunks in bitmap format in this list
        ArrayList&lt;Bitmap&gt; chunkedImages = new ArrayList&lt;Bitmap&gt;(chunkNumbers);

        //Getting the scaled bitmap of the source image


        Bitmap scaledBitmap = Bitmap.createScaledBitmap(image, image.getWidth(), image.getHeight(), true);

        chunkHeight = image.getHeight()/rows;
        chunkWidth = image.getWidth()/cols;

        //xCoord and yCoord are the pixel positions of the image chunks
        int yCoord = 0;
        for(int x=0; x&lt;rows; x++){
            int xCoord = 0;
            for(int y=0; y&lt;cols; y++){
                chunkedImages.add(Bitmap.createBitmap(scaledBitmap, xCoord, yCoord, chunkWidth, chunkHeight));
                xCoord += chunkWidth;
            }
            yCoord += chunkHeight;
        }

        //Start a new activity to show these chunks into a grid
        Intent intent = new Intent(ProcessesdResult.this, ChunkedImageActivity.class);
        intent.putParcelableArrayListExtra(""image chunks"", chunkedImages);
        startActivity(intent);
    }


}
</code></pre>

<p>This is the image type i want to slice in pieces
<img src=""https://i.stack.imgur.com/Zg7Ai.jpg"" alt=""This is the image type i want to slice in pieces""></p>",51927168,3,0,,2018-08-20 08:38:48.023 UTC,,2018-08-24 05:53:41.280 UTC,2018-08-20 08:44:47.723 UTC,,8466617,,7215342,1,-1,java|android|image-processing|bit-manipulation|android-bitmap,60
Can Google Vision API detects the outline of face in an image?,53275683,Can Google Vision API detects the outline of face in an image?,"<p>I want to draw lines around face (including forehead) and cut that face out from the image. Can I use Google Vision API to realize my goal? I have tested Google Vision API to detect face in some images, and it only returns the bounding poly (the rectangle area) around the face, the landmarks and face expression. It cannot detects the coordinates of outline around face. How to do that with Vision API? If Vision API cannot do it, than what library should I use?</p>",,1,1,,2018-11-13 07:13:10.723 UTC,,2018-11-13 21:27:10.947 UTC,2018-11-13 07:42:16.847 UTC,,8774121,,8774121,1,0,artificial-intelligence|google-cloud-vision,56
DuplicateFileException: Duplicate files copied in APK project.properties,44276413,DuplicateFileException: Duplicate files copied in APK project.properties,"<p>Error:Execution failed for task ':app:transformResourcesWithMergeJavaResForDebug'.</p>

<blockquote>
  <p>com.android.build.api.transform.TransformException: com.android.builder.packaging.DuplicateFileException: Duplicate files copied in APK project.properties
      File1: C:\Users\venka.gradle\caches\modules-2\files-2.1\com.google.cloud\google-cloud-vision\0.17.2-beta\2b8e0731b29035e73cd4e5d40cf993ec912f2913\google-cloud-vision-0.17.2-beta.jar
      File2: C:\Users\venka.gradle\caches\modules-2\files-2.1\com.google.cloud\google-cloud-core\1.0.1\2b85bbbee4913a5fed34359d148c405dd8c98aac\google-cloud-core-1.0.1.jar</p>
</blockquote>",,0,2,,2017-05-31 05:32:37.953 UTC,,2017-05-31 05:32:37.953 UTC,,,,,8089902,1,0,android|gradle|google-vision,41
Portion of code not executed in method,45546546,Portion of code not executed in method,"<p>I have an ImageAnalyses Controller where I'd like to execute some code just after <strong>ImageAnalysis</strong> is instantiated but before @image_analysis is saved. Although the controller is successfully creating an instance of ImageAnalysis it's not executing the intermediate code below. </p>

<p><strong>My controller:</strong> </p>

<pre><code>#image_analyses_controller.rb

def create
 @image_analysis = ImageAnalysis.new(image_analysis_params)


 # Start of not executed code
 @client = Aws::Rekognition::Client.new
 @image_analysis.gallery.attachments do |attachment|
   resp = @client.detect_labels(
           image:
              { s3_object: {
                bucket: ""my-bucket"",
                name: attachment.content.path,
              },
            }
         )

   high_labels = resp.labels.select { |label| label.confidence &gt; 80 }

   high_labels.each do |label|
    ImageLabel.create(
      name: label.name,
      image_url: attachment.content.path,
      image_analysis_id: @image_analysis.id
    )
   end
 end
 # End of not executed code

 respond_to do |format|
  if @image_analysis.save
    format.html { redirect_to @image_analysis, notice: 'Image analysis was successfully created.' }
    format.json { render :show, status: :created, location: @image_analysis }
  else
    format.html { render :new }
    format.json { render json: @image_analysis.errors, status: :unprocessable_entity }
  end
 end
end
</code></pre>

<p>Interestingly no exceptions are raised and the server log only registers the creation of the ImageAnalysis object with nothing that points me to an error. </p>

<p>I've tried to pass that chunk of code to a method in the model and calling it from the controller with the same results. Could you advise on why this may be happening?</p>",45546830,1,1,,2017-08-07 12:07:57.863 UTC,,2017-08-07 14:04:11.600 UTC,2017-08-07 14:04:11.600 UTC,,1798644,,6733104,1,0,ruby-on-rails|ruby,31
Setting image detection area in Google Cloud Vision API OCR,52647919,Setting image detection area in Google Cloud Vision API OCR,"<p>I have used Google Cloud Vision API for document text detection, but I could not figure out if it lets us define a particular area of image from which to extract text. 
For example if my image has 3 columns of text and I want to provide top-left coordinates, width and height of a particular column on which I want to perform OCR. Is it possible?
Also is there any other way to not get jumbled up text when we have 3 columns of text in image? </p>",,2,0,,2018-10-04 13:25:33.567 UTC,,2019-03-16 12:44:06.500 UTC,,,,,8852495,1,2,ocr|google-cloud-vision,480
Guava 23.5 Conflict with hbase-testing-util 1.2,48140339,Guava 23.5 Conflict with hbase-testing-util 1.2,"<p>I am using guava 23-5 in my application and <code>hbase-testing-util</code> 1.2.0. This is causing a conflict in my application and throwing the below exception whenever I am trying to use <code>startMiniCluster()</code> . Can some one let me know how can I get around this?</p>

<pre><code>java.lang.NoSuchMethodError: com.google.common.base.Objects.toStringHelper(Ljava/lang/Object;)Lcom/google/common/base/Objects$ToStringHelper;

    at org.apache.hadoop.metrics2.lib.MetricsRegistry.toString(MetricsRegistry.java:406)
    at java.lang.String.valueOf(String.java:2994)
    at java.lang.StringBuilder.append(StringBuilder.java:131)
    at org.apache.hadoop.ipc.metrics.RpcMetrics.&lt;init&gt;(RpcMetrics.java:74)
    at org.apache.hadoop.ipc.metrics.RpcMetrics.create(RpcMetrics.java:80)
    at org.apache.hadoop.ipc.Server.&lt;init&gt;(Server.java:2252)
    at org.apache.hadoop.ipc.RPC$Server.&lt;init&gt;(RPC.java:1042)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.&lt;init&gt;(ProtobufRpcEngine.java:535)
    at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:510)
    at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:887)
    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.&lt;init&gt;(NameNodeRpcServer.java:341)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:695)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:672)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:838)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:817)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1538)
    at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1114)
    at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:985)
    at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:814)
    at org.apache.hadoop.hdfs.MiniDFSCluster.&lt;init&gt;(MiniDFSCluster.java:745)
    at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniDFSCluster(HBaseTestingUtility.java:585)
    at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:987)
    at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:868)
    at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:862)
    at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:806)
    at com.vnera.storage.metrics.HBaseTestTableFactory.&lt;init&gt;(HBaseTestTableFactory.java:24)
    at com.vnera.storage.metrics.HBaseTests.testCounterTimetamp(HBaseTests.java:70)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
    at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
    at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
    at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
</code></pre>

<p>Versions</p>

<pre><code>&lt;dependency&gt;
   &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;
   &lt;artifactId&gt;hbase-testing-util&lt;/artifactId&gt;
   &lt;version&gt;1.2.0-cdh5.7.0&lt;/version&gt;
&lt;/dependency&gt;


&lt;dependency&gt;
   &lt;groupId&gt;com.google.guava&lt;/groupId&gt;
   &lt;artifactId&gt;guava&lt;/artifactId&gt;
   &lt;version&gt;23.5-jre&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p><strong>EDIT</strong>
As mentioned <a href=""https://www.elastic.co/blog/to-shade-or-not-to-shade"" rel=""nofollow noreferrer"">here</a> I tried to shade the Hbase dependency in a new module named <code>shadedcdh</code>. <code>pom.xml</code></p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;project xmlns=""http://maven.apache.org/POM/4.0.0""
         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
    &lt;parent&gt;
        &lt;artifactId&gt;main&lt;/artifactId&gt;
        &lt;groupId&gt;com.vnera&lt;/groupId&gt;
        &lt;version&gt;0.001-SNAPSHOT&lt;/version&gt;
    &lt;/parent&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;artifactId&gt;shaded-cdh&lt;/artifactId&gt;
    &lt;packaging&gt;jar&lt;/packaging&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;
            &lt;artifactId&gt;hbase-testing-util&lt;/artifactId&gt;
            &lt;version&gt;1.2.0-cdh5.7.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
            &lt;version&gt;${hadoop.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
            &lt;version&gt;${hadoop.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;
            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;
            &lt;!-- This must stay in sync with hbase version we deploy. --&gt;
            &lt;version&gt;1.2.0-cdh5.7.0&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.4.1&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;shade&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;relocations&gt;
                                &lt;relocation&gt;
                                    &lt;pattern&gt;com.google.common&lt;/pattern&gt;
                                    &lt;shadedPattern&gt;shaded.com.google.common&lt;/shadedPattern&gt;
                                &lt;/relocation&gt;
                            &lt;/relocations&gt;
                            &lt;transformers&gt;
                                &lt;transformer implementation=""org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"" /&gt;
                            &lt;/transformers&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

    &lt;repositories&gt;
        &lt;repository&gt;
            &lt;id&gt;cloudera&lt;/id&gt;
            &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;
        &lt;/repository&gt;
    &lt;/repositories&gt;

&lt;/project&gt;
</code></pre>

<p>Then excluded hbase &amp; hadoop dependencies from the module and added <code>shadedcdh</code> as dependency. The dependency tree of maven looks like below</p>

<pre><code>[INFO] Scanning for projects...
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] Building metrics 0.001-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-dependency-plugin:2.10:tree (default-cli) @ arkin-storage-metrics ---
[INFO] com.vnera:arkin-storage-metrics:jar:0.001-SNAPSHOT
[INFO] +- com.vnera:storage-common:jar:0.001-SNAPSHOT:compile
[INFO] |  +- net.spy:spymemcached:jar:2.11.4:compile
[INFO] |  \- org.xerial.snappy:snappy-java:jar:1.1.2.6:compile
[INFO] +- com.vnera:core-model:jar:0.001-SNAPSHOT:compile
[INFO] |  +- com.vnera:meta-model:jar:0.001-SNAPSHOT:compile
[INFO] |  +- com.vnera:denorm-model:jar:0.001-SNAPSHOT:compile
[INFO] |  +- com.vnera:flow-model:jar:0.001-SNAPSHOT:compile
[INFO] |  +- org.reflections:reflections:jar:0.9.9-RC1:compile
[INFO] |  |  \- dom4j:dom4j:jar:1.6.1:compile
[INFO] |  |     \- xml-apis:xml-apis:jar:1.0.b2:compile
[INFO] |  +- javax.validation:validation-api:jar:1.1.0.Final:compile
[INFO] |  +- com.fasterxml.jackson.core:jackson-core:jar:2.6.3:compile
[INFO] |  +- com.fasterxml.jackson.core:jackson-databind:jar:2.6.3:compile
[INFO] |  +- com.fasterxml.jackson.core:jackson-annotations:jar:2.6.3:compile
[INFO] |  +- commons-validator:commons-validator:jar:1.6:compile
[INFO] |  |  \- commons-digester:commons-digester:jar:1.8.1:compile
[INFO] |  +- commons-collections:commons-collections:jar:3.2.2:compile
[INFO] |  \- org.unitils:unitils-core:jar:3.3:compile
[INFO] |     \- ognl:ognl:jar:2.6.9:compile
[INFO] +- com.vnera:data-model:jar:0.001-SNAPSHOT:compile
[INFO] |  +- com.vnera:base-model:jar:0.001-SNAPSHOT:compile
[INFO] |  +- commons-io:commons-io:jar:2.4:compile
[INFO] |  \- org.ardverk:patricia-trie:jar:0.7-SNAPSHOT:compile
[INFO] +- com.vnera:shaded-cdh:jar:0.001-SNAPSHOT:compile
[INFO] +- org.apache.httpcomponents:httpclient:jar:4.5.2:compile
[INFO] |  \- commons-logging:commons-logging:jar:1.2:compile
[INFO] +- com.jcraft:jsch:jar:0.1.54:compile
[INFO] +- commons-codec:commons-codec:jar:1.10:compile
[INFO] +- com.vnera:common:jar:0.001-SNAPSHOT:compile
[INFO] |  +- com.koloboke:koloboke-api-jdk8:jar:1.0.0:compile
[INFO] |  +- com.koloboke:koloboke-impl-jdk8:jar:1.0.0:runtime
[INFO] |  |  \- com.koloboke:koloboke-impl-common-jdk8:jar:1.0.0:runtime
[INFO] |  +- it.unimi.dsi:fastutil:jar:8.1.0:compile
[INFO] |  +- com.amazonaws:aws-java-sdk:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-pinpoint:jar:1.11.77:compile
[INFO] |  |  |  \- com.amazonaws:jmespath-java:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-xray:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-opsworkscm:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-support:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-simpledb:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-servicecatalog:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-servermigration:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-simpleworkflow:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-storagegateway:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-route53:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-s3:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-importexport:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-sts:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-sqs:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-rds:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-redshift:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-elasticbeanstalk:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-glacier:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-iam:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-datapipeline:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-elasticloadbalancing:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-elasticloadbalancingv2:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-emr:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-elasticache:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-elastictranscoder:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-ec2:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-dynamodb:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-sns:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-budgets:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-cloudtrail:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-cloudwatch:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-logs:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-events:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-cognitoidentity:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-cognitosync:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-directconnect:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-cloudformation:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-cloudfront:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-kinesis:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-opsworks:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-ses:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-autoscaling:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-cloudsearch:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-cloudwatchmetrics:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-codedeploy:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-codepipeline:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-kms:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-config:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-lambda:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-ecs:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-ecr:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-cloudhsm:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-ssm:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-workspaces:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-machinelearning:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-directory:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-efs:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-codecommit:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-devicefarm:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-elasticsearch:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-waf:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-marketplacecommerceanalytics:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-inspector:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-iot:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-api-gateway:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-acm:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-gamelift:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-dms:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-marketplacemeteringservice:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-cognitoidp:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-discovery:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-applicationautoscaling:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-snowball:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-rekognition:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-polly:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-lightsail:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-stepfunctions:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-health:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-codebuild:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-appstream:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-shield:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-batch:jar:1.11.77:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-core:jar:1.11.77:compile
[INFO] |  |  |  \- software.amazon.ion:ion-java:jar:1.0.1:compile
[INFO] |  |  +- com.amazonaws:aws-java-sdk-models:jar:1.11.77:compile
[INFO] |  |  \- com.amazonaws:aws-java-sdk-swf-libraries:jar:1.11.22:compile
[INFO] |  +- org.springframework:spring-core:jar:4.3.7.RELEASE:compile
[INFO] |  +- com.vnera:utility:jar:0.001-SNAPSHOT:compile
[INFO] |  +- joda-time:joda-time:jar:2.9.4:compile
[INFO] |  +- org.slf4j:slf4j-api:jar:1.7.5:compile
[INFO] |  +- org.slf4j:slf4j-log4j12:jar:1.7.5:compile
[INFO] |  +- log4j:log4j:jar:1.2.17:compile
[INFO] |  +- commons-beanutils:commons-beanutils:jar:1.9.3:compile
[INFO] |  +- commons-configuration:commons-configuration:jar:1.6:compile
[INFO] |  |  \- commons-beanutils:commons-beanutils-core:jar:1.8.0:compile
[INFO] |  +- com.codahale.metrics:metrics-core:jar:3.0.1:compile
[INFO] |  +- com.codahale.metrics:metrics-healthchecks:jar:3.0.1:compile
[INFO] |  +- com.ning:async-http-client:jar:1.9.38:compile
[INFO] |  +- com.esotericsoftware:kryo-shaded:jar:3.0.0:compile
[INFO] |  |  +- com.esotericsoftware:minlog:jar:1.3.0:compile
[INFO] |  |  \- org.objenesis:objenesis:jar:2.1:compile
[INFO] |  +- com.codahale.metrics:metrics-graphite:jar:3.0.1:compile
[INFO] |  +- com.librato.metrics:metrics-librato:jar:4.1.2.4:compile
[INFO] |  |  +- com.librato.metrics:librato-java:jar:1.0.13:compile
[INFO] |  |  \- io.dropwizard.metrics:metrics-core:jar:3.1.2:compile
[INFO] |  +- org.mockito:mockito-all:jar:1.10.19:compile
[INFO] |  +- org.powermock:powermock-module-junit4:jar:1.6.4:compile
[INFO] |  |  \- org.powermock:powermock-module-junit4-common:jar:1.6.4:compile
[INFO] |  |     +- org.powermock:powermock-core:jar:1.6.4:compile
[INFO] |  |     \- org.powermock:powermock-reflect:jar:1.6.4:compile
[INFO] |  +- org.powermock:powermock-api-mockito:jar:1.6.4:compile
[INFO] |  |  +- org.mockito:mockito-core:jar:1.10.19:compile
[INFO] |  |  \- org.powermock:powermock-api-support:jar:1.6.4:compile
[INFO] |  +- com.vmw.vli:licensecheck:jar:1.4-RELEASE:compile
[INFO] |  +- org.bouncycastle:bcprov-jdk15on:jar:1.57:compile
[INFO] |  +- org.bouncycastle:bcpkix-jdk15on:jar:1.57:compile
[INFO] |  +- commons-logging:commons-logging-api:jar:1.1:compile
[INFO] |  +- org.coursera:metrics-datadog:jar:1.1.13:compile
[INFO] |  |  +- org.apache.httpcomponents:fluent-hc:jar:4.3.6:compile
[INFO] |  |  \- com.datadoghq:java-dogstatsd-client:jar:2.3:compile
[INFO] |  +- com.vnera:reg-common:jar:0.001-SNAPSHOT:compile
[INFO] |  \- com.vnera:rpc-saasinterface:jar:0.001-SNAPSHOT:compile
[INFO] +- org.elasticsearch:elasticsearch:jar:2.3.1:compile
[INFO] |  +- org.apache.lucene:lucene-core:jar:5.5.0:compile
[INFO] |  +- org.apache.lucene:lucene-backward-codecs:jar:5.5.0:compile
[INFO] |  +- org.apache.lucene:lucene-analyzers-common:jar:5.5.0:compile
[INFO] |  +- org.apache.lucene:lucene-queries:jar:5.5.0:compile
[INFO] |  +- org.apache.lucene:lucene-memory:jar:5.5.0:compile
[INFO] |  +- org.apache.lucene:lucene-highlighter:jar:5.5.0:compile
[INFO] |  +- org.apache.lucene:lucene-queryparser:jar:5.5.0:compile
[INFO] |  |  \- org.apache.lucene:lucene-sandbox:jar:5.5.0:compile
[INFO] |  +- org.apache.lucene:lucene-suggest:jar:5.5.0:compile
[INFO] |  |  \- org.apache.lucene:lucene-misc:jar:5.5.0:compile
[INFO] |  +- org.apache.lucene:lucene-join:jar:5.5.0:compile
[INFO] |  |  \- org.apache.lucene:lucene-grouping:jar:5.5.0:compile
[INFO] |  +- org.apache.lucene:lucene-spatial:jar:5.5.0:compile
[INFO] |  |  +- org.apache.lucene:lucene-spatial3d:jar:5.5.0:compile
[INFO] |  |  \- com.spatial4j:spatial4j:jar:0.5:compile
[INFO] |  +- org.elasticsearch:securesm:jar:1.0:compile
[INFO] |  +- com.carrotsearch:hppc:jar:0.7.1:compile
[INFO] |  +- org.joda:joda-convert:jar:1.2:compile
[INFO] |  +- com.fasterxml.jackson.dataformat:jackson-dataformat-smile:jar:2.6.2:compile
[INFO] |  +- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.6.2:compile
[INFO] |  |  \- org.yaml:snakeyaml:jar:1.15:compile
[INFO] |  +- com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:jar:2.6.2:compile
[INFO] |  +- io.netty:netty:jar:3.10.5.Final:compile
[INFO] |  +- com.ning:compress-lzf:jar:1.0.2:compile
[INFO] |  +- com.tdunning:t-digest:jar:3.0:compile
[INFO] |  +- org.hdrhistogram:HdrHistogram:jar:2.1.6:compile
[INFO] |  +- commons-cli:commons-cli:jar:1.3.1:compile
[INFO] |  \- com.twitter:jsr166e:jar:1.1.0:compile
[INFO] +- com.opencsv:opencsv:jar:3.7:compile
[INFO] +- org.codehaus.groovy:groovy-all:jar:2.4.4:compile
[INFO] +- org.codehaus.jettison:jettison:jar:1.3.8:compile
[INFO] |  \- stax:stax-api:jar:1.0.1:compile
[INFO] +- com.jayway.jsonpath:json-path:jar:0.9.1:compile
[INFO] |  \- net.minidev:json-smart:jar:1.2:compile
[INFO] +- com.google.guava:guava:jar:23.5-jre:compile
[INFO] |  +- org.checkerframework:checker-qual:jar:2.0.0:compile
[INFO] |  +- com.google.errorprone:error_prone_annotations:jar:2.0.18:compile
[INFO] |  +- com.google.j2objc:j2objc-annotations:jar:1.1:compile
[INFO] |  \- org.codehaus.mojo:animal-sniffer-annotations:jar:1.14:compile
[INFO] +- com.google.code.findbugs:jsr305:jar:2.0.3:provided
[INFO] +- org.apache.commons:commons-lang3:jar:3.4:compile
[INFO] +- org.glassfish.jersey.core:jersey-client:jar:2.23.2:compile
[INFO] |  +- javax.ws.rs:javax.ws.rs-api:jar:2.0.1:compile
[INFO] |  +- org.glassfish.jersey.core:jersey-common:jar:2.23.2:compile
[INFO] |  |  +- javax.annotation:javax.annotation-api:jar:1.2:compile
[INFO] |  |  +- org.glassfish.jersey.bundles.repackaged:jersey-guava:jar:2.23.2:compile
[INFO] |  |  \- org.glassfish.hk2:osgi-resource-locator:jar:1.0.1:compile
[INFO] |  +- org.glassfish.hk2:hk2-api:jar:2.5.0-b05:compile
[INFO] |  |  +- org.glassfish.hk2:hk2-utils:jar:2.5.0-b05:compile
[INFO] |  |  \- org.glassfish.hk2.external:aopalliance-repackaged:jar:2.5.0-b05:compile
[INFO] |  +- org.glassfish.hk2.external:javax.inject:jar:2.5.0-b05:compile
[INFO] |  \- org.glassfish.hk2:hk2-locator:jar:2.5.0-b05:compile
[INFO] |     \- org.javassist:javassist:jar:3.20.0-GA:compile
[INFO] +- commons-net:commons-net:jar:3.1:compile
[INFO] +- commons-lang:commons-lang:jar:2.6:compile
[INFO] +- com.google.code.gson:gson:jar:2.2.4:compile
[INFO] +- org.apache.commons:commons-jexl:jar:2.1.1:compile
[INFO] +- com.google.inject:guice:jar:3.0:compile
[INFO] +- com.google.protobuf:protobuf-java:jar:2.5.0:compile
[INFO] +- com.googlecode.protobuf-java-format:protobuf-java-format:jar:1.2:compile
[INFO] +- org.apache.thrift:libthrift:jar:0.9.1:compile
[INFO] +- org.apache.httpcomponents:httpcore:jar:4.4.5:compile
[INFO] +- com.typesafe.akka:akka-actor_2.11:jar:2.5.8:compile
[INFO] |  +- com.typesafe:config:jar:1.3.2:compile
[INFO] |  \- org.scala-lang.modules:scala-java8-compat_2.11:jar:0.7.0:compile
[INFO] +- org.quartz-scheduler:quartz:jar:2.2.1:compile
[INFO] |  \- c3p0:c3p0:jar:0.9.1.1:compile
[INFO] +- com.github.rholder:guava-retrying:jar:1.0.5:compile
[INFO] +- org.scala-lang:scala-library:jar:2.11.8:compile
[INFO] +- junit:junit:jar:4.12:test
[INFO] |  \- org.hamcrest:hamcrest-core:jar:1.3:compile
[INFO] \- org.assertj:assertj-core:jar:3.5.2:test
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3.091 s
[INFO] Finished at: 2018-01-20T11:56:42+05:30
[INFO] Final Memory: 27M/445M
[INFO] ------------------------------------------------------------------------
</code></pre>

<p>But I am still getting the same error.</p>",,1,0,,2018-01-07 18:38:04.327 UTC,,2018-01-20 06:33:11.770 UTC,2018-01-20 06:33:11.770 UTC,,785523,,785523,1,0,java|maven|hbase|guava|maven-shade-plugin,349
ERROR: Could not find com.google.gms.google-services:4.2.0,55416111,ERROR: Could not find com.google.gms.google-services:4.2.0,"<p>I am trying to add Firebase crashlytics to an existing project (not developed by me). I keep getting the error </p>

<pre><code>ERROR: Could not find com.google.gms.google-services:4.2.0:.
Required by:
    project :
Search in build.gradle files
</code></pre>

<p>I have gone through all (or most ) of the solution that asks to add maven { url '<a href=""https://maven.google.com"" rel=""nofollow noreferrer"">https://maven.google.com</a>' } and none of them work. I am getting the above error for all the solution.</p>

<p>Please not other projects that i have developed are working fine with same build.gradle but not sure why this project is giving me nightmare.</p>

<blockquote>
  <p>project level build.gradle</p>
</blockquote>

<pre><code>// Top-level build file where you can add configuration options common to all sub-projects/modules.

buildscript {

    repositories {
        google()
        jcenter()
        maven {
            url 'https://maven.fabric.io/public'
        }

    }
    dependencies {
        classpath 'com.android.tools.build:gradle:3.3.2'
        // Check for v3.1.2 or higher
        classpath 'com.google.gms.google-services:4.2.0'

        // Add dependency
        classpath 'io.fabric.tools:gradle:1.28.0'

        // NOTE: Do not place your application dependencies here; they belong
        // in the individual module build.gradle files
    }
}

allprojects {
    repositories {
        google()
        jcenter()
        maven { url 'https://jitpack.io' }
        //maven { url 'https://maven.google.com' }

    }
}

task clean(type: Delete) {
    delete rootProject.buildDir
}
</code></pre>

<blockquote>
  <p>app level build.gradle</p>
</blockquote>

<pre><code>apply plugin: 'com.android.application'
apply plugin: 'io.fabric'

android {
    compileSdkVersion 27
    defaultConfig {
        applicationId ""com.example.architgoyal.startup_prototype""
        minSdkVersion 15
        targetSdkVersion 27
        versionCode 8
        versionName ""1.6.1""
        testInstrumentationRunner ""android.support.test.runner.AndroidJUnitRunner""
    }
    buildTypes {
        debug {
            buildConfigField ""java.util.Date"", ""BUILD_TIME"", ""new java.util.Date("" + System.currentTimeMillis() + ""L)""
        }
        release {
            buildConfigField ""java.util.Date"", ""BUILD_TIME"", ""new java.util.Date("" + System.currentTimeMillis() + ""L)""
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
        }
    }
}

dependencies {
    implementation fileTree(dir: 'libs', include: ['*.jar'])
    implementation 'com.android.support:appcompat-v7:27.1.1'
    implementation 'com.android.support.constraint:constraint-layout:1.1.3'
    //implementation 'com.google.firebase:firebase-core:16.0.8'

    // Add dependency
    implementation 'com.crashlytics.sdk.android:crashlytics:2.9.9'
    // barcode reader library
    implementation 'info.androidhive:barcode-reader:1.1.5'
    // google vision library
    implementation 'com.google.android.gms:play-services-vision:11.0.2'
    implementation 'com.android.support:design:27.1.1'
    implementation 'com.github.GoodieBag:Pinview:v1.3'
    implementation 'com.android.support:support-v4:27.1.1'
    implementation 'me.dm7.barcodescanner:zxing:1.8.4'
    implementation 'com.android.volley:volley:1.0.0'
    implementation 'com.squareup.picasso:picasso:2.5.2'
    annotationProcessor 'com.jakewharton:butterknife:6.1.0'
    implementation 'com.jakewharton:butterknife:6.1.0'
    implementation 'com.jaredrummler:material-spinner:1.3.1'
    testImplementation 'junit:junit:4.12'
    androidTestImplementation 'com.android.support.test:runner:1.0.2'
    androidTestImplementation 'com.android.support.test.espresso:espresso-core:3.0.2'
}

apply plugin: 'com.google.gms.google-services'
</code></pre>",55419045,2,2,,2019-03-29 11:07:34.310 UTC,1,2019-04-26 09:19:31.260 UTC,,,,,2803557,1,4,android|firebase|google-play-services|crashlytics-android,490
Invalid image encoding when using serverless,50995238,Invalid image encoding when using serverless,"<p>I've managed to create a standalone express app that receives an image using multer and then sends a request to the aws rekognition api and tries to guess which celebrity it is. </p>

<p>When I wrap it up with the <code>serverless-http</code> package to make it compatible with the <code>serverless</code> framework, the api response says that the image has an <code>Invalid Image Encoding</code>.</p>

<p>Based on results of logging in the incoming buffer (shown at the bottom), the <code>serverless-http</code> isn't converting the incoming data for multer to read correctly but I'm not sure why?</p>

<p>Here is the code and configuration files:</p>

<h3>Http Handler Code</h3>

<pre><code>require(""dotenv"").config();
const serverless = require(""serverless-http"");
const express = require(""express"");
const app = express();
const AWS = require(""aws-sdk"");
const rekognition = new AWS.Rekognition({ region: ""eu-west-1"" });
const imageParser = require(""multer"")().single(""image"");

app.post(""/"", imageParser, async (req, res) =&gt; {
  try {
    if (!req.file) throw new Error(""Missing image"");
    console.log(req.file.buffer) // Log output below
    const celeb = { Image: { Bytes: req.file.buffer } };
    const results = await rekognition.recognizeCelebrities(celeb).promise();
    res.json(results);
  } catch (e) {
    console.error(e);
    res.send(e.message);
  }
});

if (require.main === module) {
  // This will not get run if you use sls offline start
  app.listen(3000, err =&gt;
    console.log(!err ? ""Listening..."" : ""Failed "" + err.message)
  );
}

module.exports.handler = serverless(app);
</code></pre>

<h3>serverless.yml</h3>

<pre><code>plugins:
  - serverless-offline
service: guessCeleb

provider:
  name: aws
  runtime: nodejs8.10
  stage: dev
  region: eu-west-1
  iamRoleStatements:
    - Effect: ""Allow""
      Action:
        - ""rekognition:*""
      Resource: ""*""   

functions:
  guessImage:
    handler: index.handler
    events:
      - http:
          path: /
          method: post
</code></pre>

<p>I'm using postman to send a multipart form request:</p>

<p><a href=""https://i.stack.imgur.com/CzRtr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CzRtr.png"" alt=""Postman Request""></a></p>

<h2>Serverless + Express</h2>

<p><code>&lt;Buffer c3 bf c3 98 c3 bf c3 a0 00 10 4a 46 49 46 00 01 02 00 00 01 00 0100 00 c3 bf c3 9b 00 43 00 08 06 06 07 06 05 08 07 07 07 09 09 08 0a 0c 14 0d 0c 0b... &gt;</code></p>

<h2>Just Express App (works)</h2>

<p><code>&lt;Buffer ff d8 ff e0 00 10 4a 46 49 46 00 01 02 00 00 01 00 01 00 00 ff db 00 43 00 08 06 06 07 06 05 08 07 07 07 09 09 08 0a 0c 14 0d 0c 0b 0b 0c 19 12 13 0f ... &gt;</code></p>",,0,1,,2018-06-22 20:34:23.423 UTC,,2018-06-22 20:34:23.423 UTC,,,,,6427965,1,0,node.js|express|multer|serverless-framework|amazon-rekognition,156
Amazon Textract vs Amazon Rekognition DetectText,56008341,Amazon Textract vs Amazon Rekognition DetectText,"<p>How do I decide when to use Amazon Textract vs Amazon Rekognition's <code>TextDetect</code> method?</p>

<p>My usecase is click picture from mobile and convert image data into text and store into AWS RDS.</p>

<ul>
<li><p><a href=""https://aws.amazon.com/blogs/aws/amazon-rekognition-image-detection-and-recognition-powered-by-deep-learning/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/aws/amazon-rekognition-image-detection-and-recognition-powered-by-deep-learning/</a></p></li>
<li><p><a href=""https://aws.amazon.com/textract/"" rel=""nofollow noreferrer"">https://aws.amazon.com/textract/</a></p></li>
</ul>",,2,0,,2019-05-06 15:34:20.687 UTC,,2019-05-06 21:31:32.440 UTC,2019-05-06 21:31:32.440 UTC,,174777,,4812170,1,2,amazon-web-services|amazon-rekognition|amazon-textract,132
Google Vision privacy: image deletion,43578360,Google Vision privacy: image deletion,"<p>I'm planning to use Google Vision for document recognition.
For example, I will upload driver license and I should get all text data and verify that it is driver license and not the cover of a magazine.</p>

<p>The question is: does Google Vision has API for deletion of uploaded images?
Does Google Vision fit my case if I have some security requirements?</p>",,4,3,,2017-04-24 01:19:34.130 UTC,,2018-11-09 15:02:53.590 UTC,,,,,3239169,1,3,computer-vision|ocr|vision|google-vision,658
Create separate strings from multiple lines of text in Python,47543228,Create separate strings from multiple lines of text in Python,"<p>So I used the Google Vision API to detect the text from an image. The image has a question and 3 multiple choice answers. The Google API returns the correct text, but I need the question and answers to be made into separate strings so I can use the question and each individual answer separately. This wouldn't be hard with just 1 image, but I need the program to be able to separate them no matter how many words are in the question (which <strong>always</strong> ends in with a '?')</p>

<p>Since the question always ends with '?' my idea was to read through the results, and stop when it reaches a '?' and then from 0-'?' and store it as something like questionResult. </p>

<p>Then for the answer they are all on separate lines so there has to be someway to separate them? These also need to be their own strings/variables.</p>

<p>Clearly I don't know very much on this topic, and i'm not sure what format the Google API results are in, so any help is appreciated. Any help on formatting this post is also appreciated.</p>

<p><strong>Here is my current code</strong></p>

<pre><code>import io
import os

# Imports the Google Cloud client library
from google.cloud import vision
from google.cloud.vision import types

# Instantiates a client 
vision_client = vision.Client('""MY_API_KEY.json')

# The name of the image file to annotate 
file_name = os.path.join(
    os.path.dirname(__file__),
    'hqtest.png') # Your image path from current directory

# Loads the image into memory
with io.open(file_name, 'rb') as image_file:
    content = image_file.read()
    image = vision_client.image(
        content=content)

# Performs label detection on the image file
texts = image.detect_text()

# Prints results
print (texts[0].description)
</code></pre>

<p><strong>Results of running the code</strong></p>

<p><code>C:\Users\Maxwell\Desktop\vision&gt;python test.py
Which artist is famous for
his ""Blue Period""?
J. M. W. Turner
Pablo Picasso
Prince Charles</code></p>

<p><strong>What I need the result to be</strong></p>

<pre><code>questionResult = 'Which artist is famous for his ""Blue Period""?'

answerOne = ""J. M. W. Turner""
answerTwo = ""Pablo Picasso""
answerThree = ""Prince Charles""
</code></pre>",,1,0,,2017-11-29 00:36:40.920 UTC,0,2017-11-29 03:30:00.550 UTC,,,,,9021691,1,0,python|string|google-vision,81
How to use flashlight/torch with Google Vision barcode reader API?,37908660,How to use flashlight/torch with Google Vision barcode reader API?,"<p>I have a similar question to <a href=""https://stackoverflow.com/questions/32654087/how-turn-on-flashlight-using-barcode-detection-in-google-play-services"">this question</a> and <a href=""https://stackoverflow.com/questions/35811411/accessing-autofocus-flash-with-google-vision-barcode-reader"">this question</a>, neither of which have an accepted solution.</p>

<p>I'm basically using the Google Vision barcode API but there appears no obvious way to control the flashlight.</p>

<p><a href=""https://stackoverflow.com/a/32659445/1617737"">This answer</a> suggests using <a href=""https://github.com/googlesamples/android-vision/blob/master/visionSamples/barcode-reader/app/src/main/java/com/google/android/gms/samples/vision/barcodereader/ui/camera/CameraSource.java"" rel=""nofollow noreferrer"">this code</a>, but (having tried and failed) I'm not sure how to integrate it into my app.</p>

<p>Here is the code for my activity, which basically starts the camera/barcode scanner and also uses a menu item from my <code>BaseActivity</code> which I want to use to be able to toggle the flashlight:</p>

<pre><code>import android.content.Intent;
import android.os.Bundle;
import android.util.Log;
import android.util.SparseArray;
import android.view.Menu;
import android.view.MenuItem;
import android.view.SurfaceHolder;
import android.view.SurfaceView;
import com.google.android.gms.common.api.CommonStatusCodes;
import com.google.android.gms.vision.Detector;
import com.google.android.gms.vision.barcode.Barcode;
import com.google.android.gms.vision.barcode.BarcodeDetector;
import java.io.IOException;

public class ScanQRCodeAutoActivity extends BaseActivity {

    private static final String LOG_TAG = ScanQRCodeAutoActivity.class.getSimpleName();

    private static final int CAMERA_SOURCE_PREVIEW_WIDTH = -1;
    private static final int CAMERA_SOURCE_PREVIEW_HEIGHT = 480;
    public static final String QR_CODE_VALUE = ""QR_CODE_VALUE"";

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_scan_qr_code_auto);

        super.initFlash(savedInstanceState);

        final SurfaceView cameraSurfaceView = (SurfaceView)findViewById(R.id.cameraSurfaceView);

        BarcodeDetector.Builder barcodeDetectorBuilder = new BarcodeDetector.Builder(this);
        barcodeDetectorBuilder.setBarcodeFormats(Barcode.QR_CODE);
        BarcodeDetector barcodeDetector = barcodeDetectorBuilder.build();
        barcodeDetector.setProcessor(new Detector.Processor&lt;Barcode&gt;() {
            @Override
            public void release() {
            }

            @Override
            public void receiveDetections(Detector.Detections&lt;Barcode&gt; detections) {

                final SparseArray&lt;Barcode&gt; barcodes = detections.getDetectedItems();

                if (barcodes.size() != 0) {

                    Intent data = new Intent();
                    data.putExtra(QR_CODE_VALUE, barcodes.valueAt(0).displayValue);
                    setResult(CommonStatusCodes.SUCCESS, data);
                    finish();

                }

            }
        });

        CameraSource.Builder cameraSourceBuilder = new CameraSource.Builder(this, barcodeDetector);
        //cameraSourceBuilder.setAutoFocusEnabled(true);
        //cameraSourceBuilder.setFacing(CameraSource.CAMERA_FACING_BACK);
        if (CAMERA_SOURCE_PREVIEW_WIDTH != -1 &amp;&amp; CAMERA_SOURCE_PREVIEW_HEIGHT != -1) {
            cameraSourceBuilder.setRequestedPreviewSize(CAMERA_SOURCE_PREVIEW_WIDTH, CAMERA_SOURCE_PREVIEW_HEIGHT);
        }

        final CameraSource cameraSource = cameraSourceBuilder.build();

        cameraSurfaceView.getHolder().addCallback(new SurfaceHolder.Callback() {
            @Override
            public void surfaceCreated(SurfaceHolder holder) {

                try {
                    cameraSource.start(cameraSurfaceView.getHolder());

                }
                catch (SecurityException se) {
                    Log.e(LOG_TAG, ""Could not start camera source."", se);
                }
                catch (IOException ioe) {
                    Log.e(LOG_TAG, ""Couldn't start camera source."", ioe);
                }

            }

            @Override
            public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {
            }

            @Override
            public void surfaceDestroyed(SurfaceHolder holder) {

                cameraSource.stop();

            }
        });

    }

    @Override
    public boolean onCreateOptionsMenu(Menu menu) {
        super.onCreateOptionsMenu(menu);

        MenuItem flashItem = menu.findItem(R.id.action_flash);
        flashItem.setVisible(true);

        return true;
    }

    @Override
    protected void toggleFlash() {

        super.toggleFlash();

        // HELP!!

        invalidateOptionsMenu();
        if (isFlashOn()) {
            Utilities.makeToast(this, ""Flash turned on"");
        }
        else {
            Utilities.makeToast(this, ""Flash turned off"");
        }

    }
}
</code></pre>",,1,3,,2016-06-19 15:07:22.833 UTC,,2016-12-01 11:05:51.477 UTC,2017-05-23 12:08:43.780 UTC,,-1,,1617737,1,2,android|google-play-services|barcode-scanner|flashlight|google-vision,1726
Jetty: lang.NoSuchMethodError: org.eclipse.jetty.util.thread.ExecutionStrategy.execute()V,49028947,Jetty: lang.NoSuchMethodError: org.eclipse.jetty.util.thread.ExecutionStrategy.execute()V,"<p>I upgraded dropwizard to the latest 1.2.4 from 1.0.2. Now I am seeing the below exception in my logs</p>

<pre><code>WARN [2018-02-28 11:34:50] o.e.j.u.t.QueuedThreadPool:[?:?:?] - [dw-99] -
java.lang.NoSuchMethodError: org.eclipse.jetty.util.thread.ExecutionStrategy.execute()V
        at org.eclipse.jetty.io.ManagedSelector.run(ManagedSelector.java:147)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:708)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:626)
        at java.lang.Thread.run(Thread.java:748)
</code></pre>

<p><code>mvn dependency:tree</code> output is below</p>

<pre><code>com.van:restapilayer:jar:0.001-SNAPSHOT
+- io.dropwizard:dropwizard-jackson:jar:1.2.4:compile
|  +- io.dropwizard:dropwizard-util:jar:1.2.4:compile
|  +- com.fasterxml.jackson.core:jackson-core:jar:2.9.4:compile
|  +- com.fasterxml.jackson.core:jackson-annotations:jar:2.9.0:compile
|  +- com.fasterxml.jackson.core:jackson-databind:jar:2.9.4:compile
|  +- com.fasterxml.jackson.datatype:jackson-datatype-guava:jar:2.9.4:compile
|  +- com.fasterxml.jackson.datatype:jackson-datatype-jsr310:jar:2.9.4:compile
|  +- com.fasterxml.jackson.datatype:jackson-datatype-jdk8:jar:2.9.4:compile
|  +- com.fasterxml.jackson.module:jackson-module-parameter-names:jar:2.9.4:compile
|  +- com.fasterxml.jackson.module:jackson-module-afterburner:jar:2.9.4:compile
|  \- com.fasterxml.jackson.datatype:jackson-datatype-joda:jar:2.9.4:compile
+- io.dropwizard.modules:dropwizard-elasticsearch:jar:1.2.0-1:compile
|  \- org.elasticsearch:elasticsearch:jar:2.4.6:compile
|     +- org.apache.lucene:lucene-core:jar:5.5.4:compile
|     +- org.apache.lucene:lucene-backward-codecs:jar:5.5.4:compile
|     +- org.apache.lucene:lucene-analyzers-common:jar:5.5.4:compile
|     +- org.apache.lucene:lucene-queries:jar:5.5.4:compile
|     +- org.apache.lucene:lucene-memory:jar:5.5.4:compile
|     +- org.apache.lucene:lucene-highlighter:jar:5.5.4:compile
|     +- org.apache.lucene:lucene-queryparser:jar:5.5.4:compile
|     |  \- org.apache.lucene:lucene-sandbox:jar:5.5.4:compile
|     +- org.apache.lucene:lucene-suggest:jar:5.5.4:compile
|     |  \- org.apache.lucene:lucene-misc:jar:5.5.4:compile
|     +- org.apache.lucene:lucene-join:jar:5.5.4:compile
|     |  \- org.apache.lucene:lucene-grouping:jar:5.5.4:compile
|     +- org.apache.lucene:lucene-spatial:jar:5.5.4:compile
|     |  +- org.apache.lucene:lucene-spatial3d:jar:5.5.4:compile
|     |  \- com.spatial4j:spatial4j:jar:0.5:compile
|     +- org.elasticsearch:securesm:jar:1.0:compile
|     +- com.carrotsearch:hppc:jar:0.7.1:compile
|     +- com.fasterxml.jackson.dataformat:jackson-dataformat-smile:jar:2.8.1:compile
|     +- com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:jar:2.8.1:compile
|     +- io.netty:netty:jar:3.10.6.Final:compile
|     +- com.ning:compress-lzf:jar:1.0.2:compile
|     +- com.tdunning:t-digest:jar:3.0:compile
|     +- org.hdrhistogram:HdrHistogram:jar:2.1.6:compile
|     \- com.twitter:jsr166e:jar:1.1.0:compile
+- com.van:logback-utils:jar:0.001-SNAPSHOT:compile
|  +- ch.qos.logback:logback-classic:jar:1.2.3:compile
|  |  \- ch.qos.logback:logback-core:jar:1.2.3:compile
|  +- com.van:common:jar:0.001-SNAPSHOT:compile
|  |  +- com.koloboke:koloboke-api-jdk8:jar:1.0.0:compile
|  |  +- com.koloboke:koloboke-impl-jdk8:jar:1.0.0:runtime
|  |  |  \- com.koloboke:koloboke-impl-common-jdk8:jar:1.0.0:runtime
|  |  +- it.unimi.dsi:fastutil:jar:8.1.0:compile
|  |  +- com.amazonaws:aws-java-sdk:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-pinpoint:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-xray:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-opsworkscm:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-support:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-simpledb:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-servicecatalog:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-servermigration:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-simpleworkflow:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-storagegateway:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-route53:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-s3:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-importexport:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-sts:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-sqs:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-rds:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-redshift:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-elasticbeanstalk:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-glacier:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-iam:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-datapipeline:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-elasticloadbalancing:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-elasticloadbalancingv2:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-emr:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-elasticache:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-elastictranscoder:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-ec2:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-dynamodb:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-sns:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-budgets:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-cloudtrail:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-cloudwatch:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-logs:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-events:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-cognitoidentity:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-cognitosync:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-directconnect:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-cloudformation:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-cloudfront:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-kinesis:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-opsworks:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-autoscaling:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-cloudsearch:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-cloudwatchmetrics:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-codedeploy:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-codepipeline:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-kms:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-config:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-lambda:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-ecs:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-ecr:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-cloudhsm:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-ssm:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-workspaces:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-machinelearning:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-directory:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-efs:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-codecommit:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-devicefarm:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-elasticsearch:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-waf:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-marketplacecommerceanalytics:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-inspector:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-iot:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-api-gateway:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-acm:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-gamelift:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-dms:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-marketplacemeteringservice:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-cognitoidp:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-discovery:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-applicationautoscaling:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-snowball:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-rekognition:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-polly:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-lightsail:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-stepfunctions:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-health:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-codebuild:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-appstream:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-shield:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-batch:jar:1.11.77:compile
|  |  |  +- com.amazonaws:aws-java-sdk-models:jar:1.11.77:compile
|  |  |  \- com.amazonaws:aws-java-sdk-swf-libraries:jar:1.11.22:compile
|  |  +- com.van:utility:jar:0.001-SNAPSHOT:compile
|  |  +- com.ning:async-http-client:jar:1.9.38:compile
|  |  +- com.esotericsoftware:kryo-shaded:jar:3.0.0:compile
|  |  |  \- com.esotericsoftware:minlog:jar:1.3.0:compile
|  |  +- com.codahale.metrics:metrics-graphite:jar:3.0.1:compile
|  |  +- com.librato.metrics:metrics-librato:jar:4.1.2.4:compile
|  |  |  \- com.librato.metrics:librato-java:jar:1.0.13:compile
|  |  +- org.mockito:mockito-all:jar:1.10.19:compile
|  |  +- org.powermock:powermock-module-junit4:jar:1.6.4:compile
|  |  |  \- org.powermock:powermock-module-junit4-common:jar:1.6.4:compile
|  |  |     +- org.powermock:powermock-core:jar:1.6.4:compile
|  |  |     \- org.powermock:powermock-reflect:jar:1.6.4:compile
|  |  +- org.powermock:powermock-api-mockito:jar:1.6.4:compile
|  |  |  +- org.mockito:mockito-core:jar:1.10.19:compile
|  |  |  \- org.powermock:powermock-api-support:jar:1.6.4:compile
|  |  +- com.vmw.vli:licensecheck:jar:1.4-RELEASE:compile
|  |  +- org.bouncycastle:bcprov-jdk15on:jar:1.57:compile
|  |  +- org.bouncycastle:bcpkix-jdk15on:jar:1.57:compile
|  |  +- org.apache.hadoop:hadoop-common:jar:2.6.0-cdh5.7.1:compile
|  |  |  +- org.apache.commons:commons-math3:jar:3.1.1:compile
|  |  |  +- xmlenc:xmlenc:jar:0.52:compile
|  |  |  +- commons-httpclient:commons-httpclient:jar:3.1:compile
|  |  |  +- tomcat:jasper-compiler:jar:5.5.23:compile
|  |  |  +- tomcat:jasper-runtime:jar:5.5.23:compile
|  |  |  +- javax.servlet.jsp:jsp-api:jar:2.1:compile
|  |  |  +- commons-el:commons-el:jar:1.0:compile
|  |  |  +- net.java.dev.jets3t:jets3t:jar:0.9.0:compile
|  |  |  |  \- com.jamesmurty.utils:java-xmlbuilder:jar:0.4:compile
|  |  |  +- org.apache.avro:avro:jar:1.7.6-cdh5.7.1:compile
|  |  |  +- org.apache.curator:curator-client:jar:2.7.1:compile
|  |  |  +- org.apache.curator:curator-recipes:jar:2.7.1:compile
|  |  |  +- org.apache.htrace:htrace-core4:jar:4.0.1-incubating:compile
|  |  |  \- org.apache.commons:commons-compress:jar:1.4.1:compile
|  |  |     \- org.tukaani:xz:jar:1.0:compile
|  |  +- org.apache.hadoop:hadoop-client:jar:2.6.0-cdh5.7.1:compile
|  |  |  +- org.apache.hadoop:hadoop-hdfs:jar:2.6.0-cdh5.7.1:compile
|  |  |  |  +- xerces:xercesImpl:jar:2.9.1:compile
|  |  |  |  \- org.fusesource.leveldbjni:leveldbjni-all:jar:1.8:compile
|  |  |  +- org.apache.hadoop:hadoop-mapreduce-client-app:jar:2.6.0-cdh5.7.1:compile
|  |  |  |  +- org.apache.hadoop:hadoop-mapreduce-client-common:jar:2.6.0-cdh5.7.1:compile
|  |  |  |  |  +- org.apache.hadoop:hadoop-yarn-client:jar:2.6.0-cdh5.7.1:compile
|  |  |  |  |  \- org.apache.hadoop:hadoop-yarn-server-common:jar:2.6.0-cdh5.7.1:compile
|  |  |  |  \- org.apache.hadoop:hadoop-mapreduce-client-shuffle:jar:2.6.0-cdh5.7.1:compile
|  |  |  +- org.apache.hadoop:hadoop-yarn-api:jar:2.6.0-cdh5.7.1:compile
|  |  |  +- org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.6.0-cdh5.7.1:compile
|  |  |  |  \- org.apache.hadoop:hadoop-yarn-common:jar:2.6.0-cdh5.7.1:compile
|  |  |  |     +- javax.xml.bind:jaxb-api:jar:2.2.2:compile
|  |  |  |     |  \- javax.xml.stream:stax-api:jar:1.0-2:compile
|  |  |  |     +- org.codehaus.jackson:jackson-jaxrs:jar:1.8.8:compile
|  |  |  |     \- org.codehaus.jackson:jackson-xc:jar:1.8.8:compile
|  |  |  +- org.apache.hadoop:hadoop-mapreduce-client-jobclient:jar:2.6.0-cdh5.7.1:compile
|  |  |  \- org.apache.hadoop:hadoop-aws:jar:2.6.0-cdh5.7.1:compile
|  |  +- org.coursera:metrics-datadog:jar:1.1.13:compile
|  |  |  +- org.apache.httpcomponents:fluent-hc:jar:4.3.6:compile
|  |  |  \- com.datadoghq:java-dogstatsd-client:jar:2.3:compile
|  |  \- com.van:reg-common:jar:0.001-SNAPSHOT:compile
|  \- org.slf4j:log4j-over-slf4j:jar:1.7.25:compile
+- io.dropwizard:dropwizard-validation:jar:1.2.4:compile
|  \- org.glassfish:javax.el:jar:3.0.0:compile
+- io.dropwizard:dropwizard-core:jar:1.2.4:compile
|  +- io.dropwizard:dropwizard-configuration:jar:1.2.4:compile
|  +- io.dropwizard:dropwizard-logging:jar:1.2.4:compile
|  |  +- io.dropwizard.metrics:metrics-logback:jar:3.2.5:compile
|  |  +- org.slf4j:jul-to-slf4j:jar:1.7.25:compile
|  |  +- org.slf4j:jcl-over-slf4j:jar:1.7.25:compile
|  |  \- org.eclipse.jetty:jetty-util:jar:9.4.8.v20171121:compile
|  +- io.dropwizard:dropwizard-metrics:jar:1.2.4:compile
|  +- io.dropwizard:dropwizard-jersey:jar:1.2.4:compile
|  |  +- org.glassfish.jersey.core:jersey-server:jar:2.25.1:compile
|  |  |  \- org.glassfish.jersey.media:jersey-media-jaxb:jar:2.25.1:compile
|  |  +- org.glassfish.jersey.ext:jersey-metainf-services:jar:2.25.1:compile
|  |  +- org.glassfish.jersey.ext:jersey-bean-validation:jar:2.25.1:compile
|  |  +- io.dropwizard.metrics:metrics-jersey2:jar:3.2.5:compile
|  |  +- com.fasterxml.jackson.jaxrs:jackson-jaxrs-json-provider:jar:2.9.4:compile
|  |  |  +- com.fasterxml.jackson.jaxrs:jackson-jaxrs-base:jar:2.9.4:compile
|  |  |  \- com.fasterxml.jackson.module:jackson-module-jaxb-annotations:jar:2.9.4:compile
|  |  +- org.glassfish.jersey.containers:jersey-container-servlet:jar:2.25.1:compile
|  |  |  \- org.glassfish.jersey.containers:jersey-container-servlet-core:jar:2.25.1:compile
|  |  +- org.eclipse.jetty:jetty-server:jar:9.4.8.v20171121:compile
|  |  |  +- javax.servlet:javax.servlet-api:jar:3.1.0:compile
|  |  |  \- org.eclipse.jetty:jetty-io:jar:9.4.8.v20171121:compile
|  |  +- org.eclipse.jetty:jetty-webapp:jar:9.4.8.v20171121:compile
|  |  |  \- org.eclipse.jetty:jetty-xml:jar:9.4.8.v20171121:compile
|  |  \- org.eclipse.jetty:jetty-continuation:jar:9.4.8.v20171121:compile
|  +- io.dropwizard:dropwizard-servlets:jar:1.2.4:compile
|  |  \- io.dropwizard.metrics:metrics-annotation:jar:3.2.5:compile
|  +- io.dropwizard:dropwizard-jetty:jar:1.2.4:compile
|  |  +- io.dropwizard.metrics:metrics-jetty9:jar:3.2.5:compile
|  |  +- org.eclipse.jetty:jetty-servlet:jar:9.4.8.v20171121:compile
|  |  |  \- org.eclipse.jetty:jetty-security:jar:9.4.8.v20171121:compile
|  |  +- org.eclipse.jetty:jetty-servlets:jar:9.4.8.v20171121:compile
|  |  \- org.eclipse.jetty:jetty-http:jar:9.4.8.v20171121:compile
|  +- io.dropwizard:dropwizard-lifecycle:jar:1.2.4:compile
|  +- io.dropwizard.metrics:metrics-core:jar:3.2.5:compile
|  +- io.dropwizard.metrics:metrics-jvm:jar:3.2.5:compile
|  +- io.dropwizard.metrics:metrics-servlets:jar:3.2.5:compile
|  |  +- io.dropwizard.metrics:metrics-json:jar:3.2.5:compile
|  |  \- com.papertrail:profiler:jar:1.0.2:compile
|  +- io.dropwizard.metrics:metrics-healthchecks:jar:3.2.5:compile
|  +- io.dropwizard:dropwizard-request-logging:jar:1.2.4:compile
|  |  \- ch.qos.logback:logback-access:jar:1.2.3:compile
|  +- net.sourceforge.argparse4j:argparse4j:jar:0.7.0:compile
|  \- org.eclipse.jetty.toolchain.setuid:jetty-setuid-java:jar:1.0.3:compile
+- commons-fileupload:commons-fileupload:jar:1.3.3:compile
|  \- commons-io:commons-io:jar:2.2:compile
+- io.dropwizard:dropwizard-testing:jar:1.2.4:test
|  +- org.objenesis:objenesis:jar:2.6:compile
|  \- org.glassfish.jersey.test-framework.providers:jersey-test-framework-provider-inmemory:jar:2.25.1:test
|     \- org.glassfish.jersey.test-framework:jersey-test-framework-core:jar:2.25.1:test
+- org.apache.shiro:shiro-core:jar:1.3.0:compile
|  \- commons-beanutils:commons-beanutils:jar:1.8.3:compile
+- io.swagger:swagger-core:jar:1.5.0:compile
|  +- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.4.2:compile
|  +- io.swagger:swagger-models:jar:1.5.0:compile
|  \- javax.validation:validation-api:jar:1.1.0.Final:compile
+- io.swagger:swagger-annotations:jar:1.5.0:compile
+- org.apache.shiro:shiro-web:jar:1.2.3:compile
+- org.apache.commons:commons-lang3:jar:3.4:compile
+- com.van:contracts:jar:0.001-SNAPSHOT:compile
|  +- com.van:base-model:jar:0.001-SNAPSHOT:compile
|  +- com.van:core-model:jar:0.001-SNAPSHOT:compile
|  |  +- org.reflections:reflections:jar:0.9.9-RC1:compile
|  |  |  \- dom4j:dom4j:jar:1.6.1:compile
|  |  |     \- xml-apis:xml-apis:jar:1.0.b2:compile
|  |  \- commons-validator:commons-validator:jar:1.6:compile
|  +- com.van:vshield-model:jar:0.001-SNAPSHOT:compile
|  +- com.van:nsx-model:jar:0.001-SNAPSHOT:compile
|  +- com.van:cisco-model:jar:0.001-SNAPSHOT:compile
|  +- com.van:vcenter-model:jar:0.001-SNAPSHOT:compile
|  +- com.van:force10-model:jar:0.001-SNAPSHOT:compile
|  +- com.van:flow-model:jar:0.001-SNAPSHOT:compile
|  +- com.van:openstack-model:jar:0.001-SNAPSHOT:compile
|  +- com.van:juniper-model:jar:0.001-SNAPSHOT:compile
|  \- com.van:pan-model:jar:0.001-SNAPSHOT:compile
+- com.van:rpc-saasinterface:jar:0.001-SNAPSHOT:compile
+- com.van:storage-common:jar:0.001-SNAPSHOT:compile
|  +- com.van:data-model:jar:0.001-SNAPSHOT:compile
|  |  \- org.ardverk:patricia-trie:jar:0.7-SNAPSHOT:compile
|  +- net.spy:spymemcached:jar:2.11.4:compile
|  +- org.xerial.snappy:snappy-java:jar:1.1.2.6:compile
|  +- com.opencsv:opencsv:jar:3.7:compile
|  \- org.codehaus.groovy:groovy-all:jar:2.4.4:compile
+- org.hibernate:hibernate-validator:jar:5.2.4.Final:compile
|  +- org.jboss.logging:jboss-logging:jar:3.2.1.Final:compile
|  \- com.fasterxml:classmate:jar:1.1.0:compile
+- com.van:kin-storage-metrics:jar:0.001-SNAPSHOT:compile
|  +- org.apache.hbase:hbase-client:jar:1.2.0-cdh5.7.0:compile
|  |  +- org.apache.hbase:hbase-annotations:jar:1.2.0-cdh5.7.0:compile
|  |  +- org.apache.hbase:hbase-common:jar:1.2.0-cdh5.7.0:compile
|  |  |  \- org.apache.hadoop:hadoop-core:jar:2.6.0-mr1-cdh5.7.0:compile
|  |  |     +- org.mortbay.jetty:jetty:jar:6.1.26.cloudera.2:compile
|  |  |     +- hsqldb:hsqldb:jar:1.8.0.10:compile
|  |  |     \- org.eclipse.jdt:core:jar:3.1.1:compile
|  |  +- org.apache.hbase:hbase-protocol:jar:1.2.0-cdh5.7.0:compile
|  |  +- io.netty:netty-all:jar:4.0.23.Final:compile
|  |  +- org.apache.zookeeper:zookeeper:jar:3.4.5-cdh5.7.0:compile
|  |  |  \- org.slf4j:slf4j-log4j12:jar:1.7.5:compile
|  |  +- org.apache.htrace:htrace-core:jar:3.2.0-incubating:compile
|  |  +- org.jruby.jcodings:jcodings:jar:1.0.8:compile
|  |  +- org.jruby.joni:joni:jar:2.1.2:compile
|  |  +- com.yammer.metrics:metrics-core:jar:2.2.0:compile
|  |  +- org.apache.hadoop:hadoop-annotations:jar:2.6.0-cdh5.7.0:compile
|  |  +- org.apache.hadoop:hadoop-auth:jar:2.6.0-cdh5.7.0:compile
|  |  |  +- org.apache.directory.server:apacheds-kerberos-codec:jar:2.0.0-M15:compile
|  |  |  |  +- org.apache.directory.server:apacheds-i18n:jar:2.0.0-M15:compile
|  |  |  |  +- org.apache.directory.api:api-asn1-api:jar:1.0.0-M20:compile
|  |  |  |  \- org.apache.directory.api:api-util:jar:1.0.0-M20:compile
|  |  |  \- org.apache.curator:curator-framework:jar:2.7.1:compile
|  |  \- com.github.stephenc.findbugs:findbugs-annotations:jar:1.3.9-1:compile
|  \- com.jcraft:jsch:jar:0.1.54:compile
+- org.apache.httpcomponents:httpcore:jar:4.4.5:compile
+- com.van:storage-config:jar:0.001-SNAPSHOT:compile
|  +- org.postgresql:postgresql:jar:9.4.1208.jre7:compile
|  +- commons-dbcp:commons-dbcp:jar:1.4:compile
|  |  \- commons-pool:commons-pool:jar:1.5.4:compile
|  +- com.van:dedup:jar:0.001-SNAPSHOT:compile
|  \- org.elasticsearch.module:lang-groovy:jar:2.3.1:compile
|     \- org.codehaus.groovy:groovy:jar:indy:2.4.6:compile
+- com.van:programs:jar:0.001-SNAPSHOT:compile
|  +- com.googlecode.javaewah:JavaEWAH:jar:1.1.3:compile
|  +- com.van:model-utils:jar:0.001-SNAPSHOT:compile
|  +- com.van:event-manager:jar:0.001-SNAPSHOT:compile
|  +- com.van:storage-flowstore:jar:0.001-SNAPSHOT:compile
|  +- com.van:aws-model:jar:0.001-SNAPSHOT:compile
|  +- com.van:nsxt-model:jar:0.001-SNAPSHOT:compile
|  +- com.van:firewall-model:jar:0.001-SNAPSHOT:compile
|  +- com.van:genericdevice-model:jar:0.001-SNAPSHOT:compile
|  +- com.van:mapper:jar:0.001-SNAPSHOT:compile
|  +- ma.glasnost.orika:orika-core:jar:1.4.2:compile
|  |  +- com.thoughtworks.paranamer:paranamer:jar:2.3:compile
|  |  \- com.googlecode.concurrentlinkedhashmap:concurrentlinkedhashmap-lru:jar:1.2_jdk5:compile
|  +- org.javassist:javassist:jar:3.20.0-GA:compile
|  +- com.van:denorm-model:jar:0.001-SNAPSHOT:compile
|  +- com.van:hp-model:jar:0.001-SNAPSHOT:compile
|  +- com.vmw.vapi:vapi-runtime:jar:2.7.0:compile
|  \- com.vmw.nsx.sdk:nsx-language-bindings:jar:2.0.0:compile
+- com.van:denorm-programs:jar:0.001-SNAPSHOT:test
|  \- com.van:impact:jar:0.001-SNAPSHOT:test
+- com.van:app:jar:0.001-SNAPSHOT:compile
|  \- com.van:meta-model:jar:0.001-SNAPSHOT:compile
+- com.van:resourcemanager:jar:0.001-SNAPSHOT:compile
+- com.van:ui-model:jar:0.001-SNAPSHOT:compile
|  +- commons-collections:commons-collections:jar:3.2.2:compile
|  \- org.unitils:unitils-core:jar:3.3:compile
|     \- ognl:ognl:jar:2.6.9:compile
+- com.van:query:jar:0.001-SNAPSHOT:compile
+- com.van.grid:sdmgraph:jar:0.001-SNAPSHOT:compile
|  +- commons-configuration:commons-configuration:jar:1.6:compile
|  |  +- commons-digester:commons-digester:jar:1.8:compile
|  |  \- commons-beanutils:commons-beanutils-core:jar:1.8.0:compile
|  +- commons-cli:commons-cli:jar:1.3.1:compile
|  +- org.yaml:snakeyaml:jar:1.13:compile
|  +- commons-logging:commons-logging:jar:1.2:compile
|  \- commons-logging:commons-logging-api:jar:1.1:compile
+- com.van:programs:jar:tests:0.001-SNAPSHOT:test
+- com.van:snmp:jar:0.001-SNAPSHOT:compile
|  +- org.snmp4j:snmp4j:jar:2.4.3:compile
|  |  \- log4j:log4j:jar:1.2.14:compile
|  \- org.snmp4j:snmp4j-agent:jar:2.4.2:compile
+- com.van:components:jar:0.001-SNAPSHOT:compile
|  +- javax.mail:javax.mail-api:jar:1.5.6:compile
|  \- com.sun.mail:javax.mail:jar:1.5.6:compile
|     \- javax.activation:activation:jar:1.1:compile
+- com.amazonaws:aws-java-sdk-ses:jar:1.11.77:compile
|  +- com.amazonaws:aws-java-sdk-core:jar:1.11.77:compile
|  |  \- software.amazon.ion:ion-java:jar:1.0.1:compile
|  \- com.amazonaws:jmespath-java:jar:1.11.77:compile
+- com.van:metrics-eval:jar:0.001-SNAPSHOT:compile
+- org.freemarker:freemarker:jar:2.3.20:compile
+- com.van:remote-control:jar:0.001-SNAPSHOT:compile
+- com.codahale.metrics:metrics-core:jar:3.0.1:compile
+- com.codahale.metrics:metrics-healthchecks:jar:3.0.1:compile
+- com.icegreen:greenmail:jar:1.5.5:test
+- com.van:support-request:jar:0.001-SNAPSHOT:compile
|  \- com.van:storage-utils:jar:0.001-SNAPSHOT:compile
|     \- com.github.fge:json-patch:jar:1.9:compile
|        \- com.github.fge:jackson-coreutils:jar:1.6:compile
|           \- com.github.fge:msg-simple:jar:1.1:compile
|              \- com.github.fge:btf:jar:1.2:compile
+- com.van:dns-request:jar:0.001-SNAPSHOT:compile
+- com.van:service-health:jar:0.001-SNAPSHOT:compile
+- org.apache.httpcomponents:httpclient:jar:4.5.2:compile
+- joda-time:joda-time:jar:2.9.4:compile
+- commons-codec:commons-codec:jar:1.10:compile
+- org.slf4j:slf4j-api:jar:1.7.5:compile
+- org.jsoup:jsoup:jar:1.10.1:compile
+- com.van:syslog:jar:0.001-SNAPSHOT:compile
+- com.van.external-clients:csp:jar:0.001-SNAPSHOT:compile
|  +- org.springframework:spring-web:jar:4.3.9.RELEASE:compile
|  |  \- org.springframework:spring-aop:jar:4.3.9.RELEASE:compile
|  +- org.springframework:spring-beans:jar:4.3.9.RELEASE:compile
|  +- org.springframework:spring-context:jar:4.3.9.RELEASE:compile
|  |  \- org.springframework:spring-expression:jar:4.3.9.RELEASE:compile
|  \- com.auth0:java-jwt:jar:3.2.0:compile
+- com.van.external-clients:discovery:jar:0.001-SNAPSHOT:compile
+- dnsjava:dnsjava:jar:2.1.7:compile
+- com.univocity:univocity-parsers:jar:2.5.9:compile
+- org.springframework:spring-test:jar:4.3.9.RELEASE:test
|  \- org.springframework:spring-core:jar:4.3.9.RELEASE:compile
+- org.codehaus.jettison:jettison:jar:1.3.8:compile
|  \- stax:stax-api:jar:1.0.1:compile
+- com.jayway.jsonpath:json-path:jar:0.9.1:compile
|  \- net.minidev:json-smart:jar:1.2:compile
+- com.google.guava:guava:jar:23.5-jre:compile
|  +- org.checkerframework:checker-qual:jar:2.0.0:compile
|  +- com.google.errorprone:error_prone_annotations:jar:2.0.18:compile
|  +- com.google.j2objc:j2objc-annotations:jar:1.1:compile
|  \- org.codehaus.mojo:animal-sniffer-annotations:jar:1.14:compile
+- com.google.code.findbugs:jsr305:jar:2.0.3:provided
+- org.glassfish.jersey.core:jersey-client:jar:2.23.2:compile
|  +- javax.ws.rs:javax.ws.rs-api:jar:2.0.1:compile
|  +- org.glassfish.jersey.core:jersey-common:jar:2.23.2:compile
|  |  +- javax.annotation:javax.annotation-api:jar:1.2:compile
|  |  +- org.glassfish.jersey.bundles.repackaged:jersey-guava:jar:2.23.2:compile
|  |  \- org.glassfish.hk2:osgi-resource-locator:jar:1.0.1:compile
|  +- org.glassfish.hk2:hk2-api:jar:2.5.0-b05:compile
|  |  +- org.glassfish.hk2:hk2-utils:jar:2.5.0-b05:compile
|  |  \- org.glassfish.hk2.external:aopalliance-repackaged:jar:2.5.0-b05:compile
|  +- org.glassfish.hk2.external:javax.inject:jar:2.5.0-b05:compile
|  \- org.glassfish.hk2:hk2-locator:jar:2.5.0-b05:compile
+- commons-net:commons-net:jar:3.1:compile
+- commons-lang:commons-lang:jar:2.6:compile
+- com.google.code.gson:gson:jar:2.2.4:compile
+- org.apache.commons:commons-jexl:jar:2.1.1:compile
+- com.google.inject:guice:jar:3.0:compile
+- com.google.protobuf:protobuf-java:jar:2.5.0:compile
+- com.googlecode.protobuf-java-format:protobuf-java-format:jar:1.2:compile
+- org.apache.thrift:libthrift:jar:0.9.1:compile
+- com.typesafe.akka:akka-actor_2.11:jar:2.5.8:compile
|  +- com.typesafe:config:jar:1.3.2:compile
|  \- org.scala-lang.modules:scala-java8-compat_2.11:jar:0.7.0:compile
+- org.quartz-scheduler:quartz:jar:2.2.1:compile
|  \- c3p0:c3p0:jar:0.9.1.1:compile
+- com.github.rholder:guava-retrying:jar:1.0.5:compile
+- org.scala-lang:scala-library:jar:2.11.8:compile
+- junit:junit:jar:4.12:test
|  \- org.hamcrest:hamcrest-core:jar:1.3:compile
\- org.assertj:assertj-core:jar:3.5.2:test
</code></pre>

<p>Can someone let me know what is going wrong? How can I get around this error?</p>",49037338,1,2,,2018-02-28 11:52:44.773 UTC,,2018-02-28 19:39:46.970 UTC,,,,,785523,1,1,java|jetty|dropwizard,810
Error in Boto AWS Rekognition,41998868,Error in Boto AWS Rekognition,"<p>I am trying to compare faces using AWS Rekognitionthrough Python boto3, as instructed in the AWS documentation.</p>

<p>My API call is:</p>

<pre><code>client = boto3.client('rekognition', aws_access_key_id=key, aws_secret_access_key=secret, region_name=region )

source_bytes = open('source.jpg', 'rb')
target_bytes = open('target.jpg', 'rb')

response = client.compare_faces(
    SourceImage = {
        'Bytes':bytearray(source_bytes.read())
    },
    TargetImage = {
        'Bytes':bytearray(target_bytes.read())
    },
    SimilarityThreshold = SIMILARITY_THRESHOLD
)

source_image.close()
target_image.close()
</code></pre>

<p>But everytime I run this program,I get the following error:</p>

<pre><code>botocore.errorfactory.InvalidParameterException: An error occurred (InvalidParameterException) when calling the CompareFaces operation: Request has Invalid Parameters
</code></pre>

<p>I have specified the secret, key, region, and threshold properly. How can I clear off this error and make the request call work?</p>",,2,2,,2017-02-02 09:47:25.207 UTC,,2017-10-09 23:48:30.213 UTC,,,,,5380494,1,4,python|amazon-web-services|boto3|amazon-rekognition,1257
google cloud vision category detecting,56224197,google cloud vision category detecting,<p>I want to use google cloud vision API in my android app to detect whether the uploaded picture is mainly food or not. the problem is that the response JSON is rather big and confusing. it says a lot about the picture but doesn't say what the whole picture is of (food or something like that). I contacted the support team but didn't get an answer. </p>,,1,0,,2019-05-20 15:40:38.233 UTC,2,2019-05-21 15:44:33.700 UTC,,,,,10318601,1,1,google-cloud-platform|google-cloud-vision|vision,28
"IBM Watson Visual recognition{""code"":400,""error"":""Cannot execute learning task. : no classifier name given""}",45366479,"IBM Watson Visual recognition{""code"":400,""error"":""Cannot execute learning task. : no classifier name given""}","<p>When I try to train a classifier with two positive classes and with the API key (each class contains around 1200 images) in Watson Visual Recognition, it returns that ""no classifier name is given"" - but that I have already provided. This is the code:</p>

<pre><code>     $ curl -X POST -F ""blank_positive_examples=@C:\Users\rahansen\Desktop\Altmuligt\training\no_ocd\no_ocd.zip"" -F ""OCD_positive_examples=@C:\Users\rahansen\Desktop\Altmuligt\training\ocd\ocd.zip"" -F ""name=disease"" ""https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classifiers?api_key={X}&amp;version=2016-05-20""

     {""code"":400,""error"":""Cannot execute learning task.  : no classifier name given""}
</code></pre>

<p>What I have done so far:</p>

<ol>
<li>Removed all special characters in the file names as I thought that might be the problem: </li>
<li>Tried to give other names for the classifeir, e.g. ""name=ocd""</li>
<li>I also tried to train it on a smaller dataset, like 40 images in each positive class and then it actually works fine. So maybe the size of the dataset is the problem. However, according to Watson training guidelines, I comply with the size regulations: <a href=""https://www.ibm.com/watson/developercloud/doc/visual-recognition/customizing.html"" rel=""nofollow noreferrer"">https://www.ibm.com/watson/developercloud/doc/visual-recognition/customizing.html</a> I have a free subscription. </li>
</ol>

<p>Do anyone has any recommendations for how to solve this classifier training problem?</p>",,2,0,,2017-07-28 06:37:52.303 UTC,,2018-07-04 22:30:23.977 UTC,2018-07-04 22:30:23.977 UTC,,2891664,,4694997,1,1,ibm-cloud|classification|ibm-watson|visual-recognition,338
Is it possible to use Camera2 with Google Vision API,41711704,Is it possible to use Camera2 with Google Vision API,<p>Is it possible to detect faces using Camera2 with Google Vision API only ? I could not find a way to integrate it. </p>,,2,0,,2017-01-18 05:04:36.773 UTC,1,2017-03-22 02:30:57.390 UTC,,,,,2301721,1,7,android|android-camera2,2765
AWS Rekognition JavaScript SDK using Bytes,43494736,AWS Rekognition JavaScript SDK using Bytes,"<p>The <a href=""https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Rekognition.html"" rel=""nofollow noreferrer"">AWS Rekognition Javascript API</a> states that for  <a href=""http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Rekognition.html#compareFaces-property"" rel=""nofollow noreferrer""><code>rekognition.compareFaces(params,...)</code></a> method, the <code>SourceImage</code> and <code>TargetImage</code> can take <code>Bytes</code> or <code>S3Object</code>. I want to use the <code>Bytes</code> which can be </p>

<blockquote>
  <p>""Bytes — (Buffer, Typed Array, Blob, <strong>String</strong>)"" </p>
  
  <p>Blob of image bytes up to 5 MBs.</p>
</blockquote>

<p>When I pass the <code>Base64</code> encoded string of the images, the JS SDK is re-encoding again (i.e double encoded). Hence server responding with error saying </p>

<blockquote>
  <p>{""__type"":""InvalidImageFormatException"",""Message"":""Invalid image
  encoding""}</p>
</blockquote>

<p>Did anyone manage to use the <a href=""http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Rekognition.html#compareFaces-property"" rel=""nofollow noreferrer"">compareFaces JS SDK API</a> using base64 encoded images (not <code>S3Object</code>)? or any JavaScript examples using <code>Bytes</code> param would help.</p>",,5,2,,2017-04-19 11:40:51.860 UTC,2,2019-01-07 22:07:40.237 UTC,2019-01-05 20:31:12.710 UTC,,5623301,,184389,1,5,javascript|amazon-web-services|sdk|base64|amazon-rekognition,2102
Issue Enabling Billing,37264402,Issue Enabling Billing,"<p>I have added a credit card and associated the billing account with my project. However, when I hit the Google Vision API with credentials associated with that project, I get the ""Project XXXXX has billing disabled. Please enable it."" Does anyone know if there are any tricks to get the project to recognize that billing has been added?</p>",,1,2,,2016-05-16 22:43:13.670 UTC,,2016-05-18 02:34:24.107 UTC,2016-05-17 18:04:24.890 UTC,,5231007,,4118626,1,1,google-cloud-platform|google-cloud-vision,1365
"Generate thumbnail in php, posting to Azure Computer Vision API",37985715,"Generate thumbnail in php, posting to Azure Computer Vision API","<p>I want to use <a href=""https://dev.projectoxford.ai/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fb"" rel=""nofollow noreferrer"">Azure Computer Vision API</a> to generate thumbnails for my Wordpress site. I'm trying to make it work in php with wp_remote_post, but i don't know how to parse the parameters ? It returns a thumbnail in really bad quality and default 500x500px. Any ideas on how to resolve this issue ? </p>

<pre><code>function get_thumbnail($URL)   //* * * * Azure Computer Vision API - v1.0 * * * *
{
$posturl='https://api.projectoxford.ai/vision/v1.0/generateThumbnail'; 

$request = wp_remote_post($posturl, array(
 'headers' =&gt; array(
    'Content-Type' =&gt; 'application/json',
    'Ocp-Apim-Subscription-Key' =&gt; 'xxxxxxxxxxxxxxxxxxxxxxxxxxxx'),
 'body' =&gt; array('url' =&gt; $URL)
));

if ( is_wp_error( $request ) ) 
{
    $error_message = $request-&gt;get_error_message();
    return ""Something went wrong: $error_message"";
    } else 
    {
      return $request['body'];
    }    
}
</code></pre>

<p>EDIT 1</p>

<p>Thanks @Gary your right! Now the cropping is correct, but i got a huge problem with the quality! I'm using a trial but i see no info from Azure on downgrading the thumb quality for trial users. They are claiming to deliver high quality thumbnails, but if thats the standard it's totaly useless.
I must have overlooked something i guess?</p>

<p>Of course Gary, if i get no correct answer on my quality question i will close the thread with your answer as correct.</p>

<p><a href=""https://i.stack.imgur.com/OhIyy.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OhIyy.jpg"" alt=""Generated thumb""></a><a href=""https://i.stack.imgur.com/yIw5l.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yIw5l.jpg"" alt=""Source picture""></a></p>",38006953,1,1,,2016-06-23 08:10:14.440 UTC,,2016-06-24 10:37:06.077 UTC,2016-06-24 10:37:06.077 UTC,,3209742,,3209742,1,1,php|wordpress|azure|computer-vision|microsoft-cognitive,281
PHP import Google Cloud Platform class,48549064,PHP import Google Cloud Platform class,"<p>I'd like to use the Google Cloud Vision API with PHP.
Inside my <code>www</code> directory I executed the following command line:</p>

<pre><code>composer require google/cloud
</code></pre>

<p>So now inside <code>www</code> I have a <code>vendor/</code> directory which has the following files/directories:</p>

<p><code>autoload.php</code>, <code>bin</code>, <code>composer</code>, <code>firebase</code>, <code>google</code>, <code>grpc</code>, <code>guzzlehttp</code>, <code>monolog</code>, <code>paragonie</code>, <code>psr</code>, <code>ramsey</code>, <code>rize</code></p>

<p>I'd like now to use the Google Vision API in PHP, following this tutorial:</p>

<p><a href=""https://cloud.google.com/vision/docs/detecting-web?authuser=1#vision-web-detection-php"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/detecting-web?authuser=1#vision-web-detection-php</a></p>

<p>But I'm getting:</p>

<pre><code>Uncaught Error: Class 'Cloud\Vision\VisionClient' not found in
</code></pre>

<p>I tried to add ""vendor"" when I'm defining the namespace (as my Google files are under vendor)</p>

<pre><code>namespace vendor\Google\Cloud\Samples\Vision;
</code></pre>

<p>But it didn't help.</p>",,0,4,,2018-01-31 18:27:06.780 UTC,,2018-04-02 14:19:34.220 UTC,2018-04-02 14:19:34.220 UTC,,1460422,,1754181,1,0,php|google-app-engine|google-cloud-platform|google-vision,104
How to calculate font size of text in image,56160026,How to calculate font size of text in image,"<p>I want to know the font size of the text used in an image. Do you have that way?
I used Google vision, but it only return <code>BoundingBox</code></p>",,0,0,,2019-05-16 02:37:00.297 UTC,,2019-05-16 02:37:00.297 UTC,,,,,11462245,1,0,ocr,6
StatusCode.UNAUTHENTICATED when running Vision API demo in Python/Flask running in Docker,44804442,StatusCode.UNAUTHENTICATED when running Vision API demo in Python/Flask running in Docker,"<p>I've followed the instructions the <a href=""https://cloud.google.com/vision/docs/reference/libraries"" rel=""nofollow noreferrer"">Google Client Libraries page</a> for the Vision API to get started with the Vision API in Python (I'm running 2.7). Since my code is running in Docker (a Flask app), I've followed the instructions in the following way:</p>

<ol>
<li>Added google-cloud-vision and google-cloud libraries to my requirements.txt file.</li>
<li>Created a json account credentials file and set the location of this file as an environment variable named GOOGLE_APPLICATION_CREDENTIALS</li>
<li>ran gcloud init successfully while ""ssh'd"" into my web app's docker container</li>
<li>Copied the client library example in the link above exactly into a test view to run the code</li>
</ol>

<p>The steps above result in the error below:</p>

<pre><code>&gt; RetryError: GaxError(Exception occurred in retry method that was not
&gt; classified as transient, caused by &lt;_Rendezvous of RPC that terminated
&gt; with (StatusCode.UNAUTHENTICATED, Traceback (most recent call last):  
&gt; File ""src/python/grpcio/grpc/_cython/_cygrpc/credentials.pyx.pxi"",
&gt; line 154, in grpc._cython.cygrpc.plugin_get_metadata
&gt; (src/python/grpcio/grpc/_cython/cygrpc.c:7054)   File
&gt; ""/usr/local/lib/python2.7/site-packages/grpc/_plugin_wrapping.py"",
&gt; line 106, in __call__
&gt;     AuthMetadataPluginCallback(wrapped_cygrpc_callback))   File ""/usr/local/lib/python2.7/site-packages/google/auth/transport/grpc.py"",
&gt; line 73, in __call__
&gt;     callback(self._get_authorization_headers(context), None)   File ""/usr/local/lib/python2.7/site-packages/google/auth/transport/grpc.py"",
&gt; line 61, in _get_authorization_headers
&gt;     headers)   File ""/usr/local/lib/python2.7/site-packages/google/auth/credentials.py"",
&gt; line 121, in before_request
&gt;     self.refresh(request)   File ""/usr/local/lib/python2.7/site-packages/google/oauth2/service_account.py"",
&gt; line 310, in refresh
&gt;     request, self._token_uri, assertion)   File ""/usr/local/lib/python2.7/site-packages/google/oauth2/_client.py"",
&gt; line 143, in jwt_grant
&gt;     response_data = _token_endpoint_request(request, token_uri, body)   File
&gt; ""/usr/local/lib/python2.7/site-packages/google/oauth2/_client.py"",
&gt; line 98, in _token_endpoint_request
&gt;     body = urllib.parse.urlencode(body) AttributeError: 'Module_six_moves_urllib_parse' object has no attribute 'urlencode'
&gt; )&gt;)
</code></pre>

<p>I'm thinking the problem has to do with my crendtials since it says StatusCode.UNAUTHENTICATED, but I haven't been able to get this fixed. Could anyone help? Thanks!</p>",44819041,1,2,,2017-06-28 13:51:15.920 UTC,,2017-06-29 07:39:35.350 UTC,2017-06-28 14:27:29.823 UTC,,5651931,,5651931,1,0,python|docker|google-cloud-platform|google-vision,245
aws VS2017 serverless app syntax in serverless.template to add filter to s3 notification,50685445,aws VS2017 serverless app syntax in serverless.template to add filter to s3 notification,"<p>I have created a VS2017 C# app using the AWS Serverless Application template with the ""Simple S3 Function"" blueprint.  The CloudFormation serverless.template file contains a spec for my handler function with an event spec to respond to ""s3.ObjectCreated:*"" events.  I am trying to add a filter specification to that event spec to only respond to events with the ""Source/"" prefix.  Here is my code:</p>

<pre><code>{
  ""AWSTemplateFormatVersion"" : ""2010-09-09"",
  ""Transform"" : ""AWS::Serverless-2016-10-31"",
  ""Description"" : ""Template that creates a S3 bucket and a Lambda function that will be invoked when new objects are upload to the bucket."",
  ""Parameters"" : {
    ""BucketName"" : {
        ""Type"" : ""String"",
        ""Description"" : ""Name of S3 bucket to be created. The Lambda function will be invoked when new objects are upload to the bucket. If left blank a name will be generated."",
        ""MinLength"" : ""0""
    }
  },

  ""Conditions"" : {
    ""BucketNameGenerated"" : {""Fn::Equals"" : [{""Ref"" : ""BucketName""}, """"]}
  },


  ""Resources"" : {

    ""Bucket"" : {
        ""Type"" : ""AWS::S3::Bucket"",
        ""Properties"" : {
            ""BucketName"" : { ""Fn::If"" : [""BucketNameGenerated"", {""Ref"" : ""AWS::NoValue"" }, { ""Ref"" : ""BucketName"" } ] }
        }
    },

    ""S3Function"" : {
      ""Type"" : ""AWS::Serverless::Function"",
      ""Properties"": {
        ""Handler"": ""DCATInventory::DCATInventory.Function::FunctionHandler"",
        ""Runtime"": ""dotnetcore2.0"",
        ""CodeUri"": """",
        ""Description"": ""Default function"",
        ""MemorySize"": 256,
        ""Timeout"": 30,
        ""Role"": null,
        ""Policies"": [ ""AWSLambdaFullAccess"", ""AmazonRekognitionReadOnlyAccess"" ],
        ""Events"": {
            ""NewImagesBucket"" : {
                ""Type"" : ""S3"",
                ""Properties"" : {
                    ""Bucket"" : { ""Ref"" : ""Bucket"" },
                    ""Events"" : [
                        ""s3:ObjectCreated:*""
                    ],
                    ""Filter"" : {
                        ""S3Key"" : {
                            ""Rules"" : [{
                                ""Name"" : ""prefix"", 
                                ""Value"": ""Source/""
                            }]
                        }
                    }
                }
            }
        }
      }
    }
  },
  ""Outputs"" : {
    ""Bucket"" : {
        ""Value"" : {""Ref"":""Bucket""},
        ""Description"" : ""Bucket that will invoke the lambda function when new objects are created.""
    }
  }
}
</code></pre>

<p>This is the default code generated by the template with only the Filter spec added to the event properties.  I am receiving an error stating ""Rules key is invalid for this object"" on line 48.  I have read the documentation and googled this and this seems to be the correct syntax.  Did I specify something wrong here?  Thanks in advance.</p>",50687909,1,0,,2018-06-04 16:57:25.600 UTC,1,2018-06-04 19:55:59.370 UTC,,,,,2112527,1,0,c#|aws-lambda|amazon-cloudformation,119
How to set the GOOGLE_APPLICATION_CREDENTIALS from Google Cloud Vision in C#?,54410031,How to set the GOOGLE_APPLICATION_CREDENTIALS from Google Cloud Vision in C#?,"<p>I am following <a href=""https://cloud.google.com/vision/docs/libraries"" rel=""nofollow noreferrer"">this tutorial</a> to use Google Vision API, but even configuring the authentication credentials I get the following error:</p>

<blockquote>
  <p><strong>System.InvalidOperationException:</strong> 'The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See <a href=""https://developers.google.com/accounts/docs/application-default-credentials"" rel=""nofollow noreferrer"">https://developers.google.com/accounts/docs/application-default-credentials</a> for more information.'</p>
</blockquote>

<p>My code in Visual Studio 2017:</p>

<pre><code>        // Instantiates a client
        var client = ImageAnnotatorClient.Create();
        // Load the image file into memory
        var image = Google.Cloud.Vision.V1.Image.FromFile(@""C:\Users\Maicon\OneDrive\Área de Trabalho\keyboardSantander\keyboard.png"");
        // Performs label detection on the image file
        var response = client.DetectLabels(image);
        foreach (var annotation in response)
        {
            if (annotation.Description != null)
                debugOutput(annotation.Description);
        }
</code></pre>

<p>What can I do to fix this? Do I have to create a trial account to use the Google Cloud API?</p>",54525234,1,2,,2019-01-28 20:45:00.620 UTC,1,2019-02-04 22:26:13.927 UTC,2019-01-28 20:54:59.700 UTC,,472495,,10229463,1,0,c#|google-cloud-vision,434
Watson Visual Recognition Create Classifier 413 Request Entity Too Large,37786967,Watson Visual Recognition Create Classifier 413 Request Entity Too Large,"<p>I am trying to create a Watson Visual Recognition Create Classifier using v3 of the rest API following the documentation <a href=""https://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/doc/visual-recognition/customizing.shtml#goodclassifying"" rel=""nofollow"">https://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/doc/visual-recognition/customizing.shtml#goodclassifying</a> which states:</p>

<blockquote>
  <p>There are size limitations for training calls and data:
  The service accepts a maximum of 10,000 images or 100 MB per .zip file
  The service requires a minimum of 10 images per .zip file.
  The service accepts a maximum of 256 MB per training call.</p>
</blockquote>

<p>However, using a ""positive"" zip file of 48MB containing 594 images (max size of an image is 144Kb) and a ""negative"" zip file of  16MB containing 218 images (max size of an image is 114Kb) but I keep getting the error:</p>

<pre><code>&lt;html&gt;
&lt;head&gt;&lt;title&gt;413 Request Entity Too Large&lt;/title&gt;&lt;/head&gt;
&lt;body bgcolor=""white""&gt;
&lt;center&gt;&lt;h1&gt;413 Request Entity Too Large&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>In response to:</p>

<pre><code>curl -X POST -F ""good_positive_examples=@positive.zip"" 
-F ""negative_examples=@negative.zip"" 
-F ""name=myclassifier"" 
-H ""X-Watson-Learning-Opt-Out=true"" 
""https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classifiers?api_key=&lt;mykey&gt;&amp;version=2016-05-20""
</code></pre>

<p>I've kept trying reducing the file size by deleting images within the zips and re-trying but I'm well below the stated limits.</p>

<p>Anyone got any idea?</p>

<p>Thanks</p>",,2,3,,2016-06-13 10:07:35.540 UTC,1,2018-07-04 21:55:33.987 UTC,2018-07-04 21:55:33.987 UTC,,2891664,,894199,1,2,rest|curl|nginx|ibm-watson,596
how to set CCL option in google vision api?,51317429,how to set CCL option in google vision api?,"<p>i want to set CCL option in google vision api..</p>

<p>but api document is not support this infomation</p>

<p>I found that Google Image Search provides the following URL.</p>

<p><a href=""https://www.google.co.kr/search?as_st=y&amp;tbm=isch&amp;hl=ko&amp;as_q=apple&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;cr=&amp;as_sitesearch=&amp;safe=images&amp;"" rel=""nofollow noreferrer"">https://www.google.co.kr/search?as_st=y&amp;tbm=isch&amp;hl=ko&amp;as_q=apple&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;cr=&amp;as_sitesearch=&amp;safe=images&amp;</a><strong>tbs=sur:fc</strong></p>

<p>I wonder if this is also available in the GOOGLE VISION API. If possible, I want to know the URL that contains the method.</p>",,0,0,,2018-07-13 03:56:45.600 UTC,,2018-07-13 03:56:45.600 UTC,,,,,10073835,1,0,google-vision|vision-api,10
"Want to create a MusicPlayer App, How to work with MediaStore, to fetch songs, to create a playlist",56099062,"Want to create a MusicPlayer App, How to work with MediaStore, to fetch songs, to create a playlist","<p>I want to create an <strong>Android Music player</strong>, can someone guide me if <strong>MediaStore</strong> is the right tool to create one? as i really do not know what it is. I also want to create <strong>playlists</strong> in my application. is there any proper userguide to use it? or any tutorial that would help me create a music player with basic functionality that would also let user to create a personalized playlist with the android application. I am really a beginner with android.</p>

<p>If mediaStore is not right, then guide me what is. I want a simple music player that will fetch music from your internal storage and let people create their playlist. this is for a project, A Emotion based Music player, that will play music according to your mood, will fetch your mood details via facial recognition using <em>GOOGLE VISION API</em> and will play song accordingly, at first i wanted to classify music by my own but now it seems to be difficult that is why i want the user to create his own playlist and i will try to play the sad playlist is he is in a sad mood.</p>",,0,2,,2019-05-12 12:04:19.830 UTC,,2019-05-12 12:04:19.830 UTC,,,,,8202588,1,-4,java|android|mediastore|playlist|android-music-player,27
How to confidently validate object detection results returned from Google Cloud Vision,47614963,How to confidently validate object detection results returned from Google Cloud Vision,"<p>I am trying to build a program that can correctly and confidently identify an object with Google Cloud Vision (denoted as GCV henceforth). The results returned are correct most of the times with a certain accuracy score for each label, as such.</p>

<pre><code>{
    ""banana"": ""0.92345"",
    ""yellow"": ""0.91002"",
    ""minion"": ""0.89921"",
}
</code></pre>

<p>The environment I am working with has diverse set of lightning condition and objects are expected to placed in random position. When an object with different position is placed, the results returned from GCV will be slightly different because a different image is queried. For example,</p>

<pre><code>{
    ""banana"": ""0.82345"",
    ""lemon"": ""0.82211"",
    ""yellow"": ""0.81102"",
    ""minion"": ""0.79921"",
}
</code></pre>

<p>My program is designed in a way that, when object <code>banana</code> is detected with accuracy greater than certain value, then next action will be dispatched.</p>

<p>There are 3 clusters of object types. For instance, <code>banana</code> goes to container <code>A</code>, <code>apple</code> goes to container <code>B</code> and <code>orange</code> goes to container <code>C</code>.</p>

<p><strong>When I present my work to my professor, he questioned that how can I confidently define and validate the threshold value for each item, as respected to its respective cluster.</strong></p>

<p>I tried to obtain a mean score of banana by training hundreds of banana images but eventually I found that this is probably not the correct way of defining a threshold. My professor suggested to use K Nearest Neighbour to find similarity of those images but isn't that already a part of GCV? Even if what he suggested is correct, what is the correct approach to train a post GCV classifier, with the limited data returned from GCV?</p>",,1,0,,2017-12-03 03:29:25.420 UTC,,2017-12-03 17:53:01.047 UTC,2017-12-03 04:12:32.357 UTC,,5809351,,5809351,1,0,python|machine-learning|computer-vision|knn|google-cloud-vision,175
Looking for Google Vision API TypeScript definition file,54700930,Looking for Google Vision API TypeScript definition file,"<p>I am looking for a Google Vision API TypeScript definition file, but did not find any.
Do they exist or do I have to create my own?</p>",,1,0,,2019-02-15 00:03:30.197 UTC,,2019-02-18 21:43:46.080 UTC,,,,,3444107,1,0,typescript|google-cloud-platform|computer-vision|typescript-typings,90
How to check the quota limit in Google Cloud Vision API?,49543773,How to check the quota limit in Google Cloud Vision API?,"<p>Is there an API to see how many calls you've made this billing cycle to the Google Cloud Vision API?
I would like to add this information to a UI so user's know when they're about to make queries that will be charged.</p>",49719348,1,0,,2018-03-28 20:38:43.997 UTC,,2018-04-09 08:30:29.783 UTC,,,,,2275685,1,0,google-cloud-vision,416
Indexing Faces with AWS Rekognition,48456300,Indexing Faces with AWS Rekognition,"<p>I am new to AWS and am trying to use Rekognition to identify certain people in a crowd. I am currently trying to index the images of the separate individuals but have hit a snag when trying to create a collection. There seems to a data type compatibility issue when I try using Amazon.Rekognition.Model.S3Object(). I have provided the code below. Does anyone have a solution or a better method? Thank you for your time!</p>

<pre><code>    private static void TryIndexFaces()
    {
        S3Client = new AmazonS3Client();
        RekognitionClient = new AmazonRekognitionClient();

        IndexFacesRequest indexRequest = new IndexFacesRequest();
        Amazon.Rekognition.Model.Image img = new Amazon.Rekognition.Model.Image();

        ListObjectsV2Request req = new ListObjectsV2Request();
        req.BucketName = ""wem0020"";
        ListObjectsV2Response listObjectsResponse = S3Client.ListObjectsV2(req);


        CreateCollectionRequest ccr = new CreateCollectionRequest();
        ccr.CollectionId = ""TestFaces"";
        //RekognitionClient.CreateCollection(ccr);

        ListVersionsResponse lvr = S3Client.ListVersions(req.BucketName);
        string version = lvr.Versions[0].VersionId;

        foreach(Amazon.S3.Model.S3Object s3o in listObjectsResponse.S3Objects)
        {
            Console.WriteLine(s3o.Key);
            try
            {
                if (s3o.Key.EndsWith("".jpg""))
                {
                    Amazon.Rekognition.Model.S3Object reks3o = new Amazon.Rekognition.Model.S3Object();
                    reks3o.Bucket = req.BucketName;
                    reks3o.Name = s3o.Key;
                    Console.WriteLine(version);
                    reks3o.Version = version;
                    img.S3Object = reks3o;

                    indexRequest.Image = img;
                    indexRequest.CollectionId = ccr.CollectionId;

                    RekognitionClient.IndexFaces(indexRequest);
                }
            }
            catch (Exception ex)
            {
                Console.WriteLine(ex.Message);
            }
        }
    }
</code></pre>",,1,0,,2018-01-26 05:42:06.447 UTC,,2018-09-12 07:47:53.510 UTC,,,,,9270853,1,1,amazon-web-services|indexing|collections|face-recognition,116
IBM watson visual recognition services _visualRecognition.Classify() is not returning proper result for Custom classes,47849128,IBM watson visual recognition services _visualRecognition.Classify() is not returning proper result for Custom classes,"<p>I am developing VR tool in .NET framework using IBM watson visual recognition service. _visualRecognition. Classify () method was working fine for the custom classifiers before a week. Now I am running the same code but it's not working properly, it's not classifying any images with respect to created custom classes. It's working as default classify method even after passing classifierID's and Owner Id's. It's work as default classify method</p>

<p>Code:</p>

<pre><code>var result = _visualRecognition.Classify(imageByte, imagePath, ""image/jpg"", Urls, Classifiers, Owners, 0.8f, ""en"");
</code></pre>

<p>Before same code returning below result. Please refer below image:</p>

<p><a href=""https://i.stack.imgur.com/LnTT4.png"" rel=""nofollow noreferrer"">Result running same code before</a></p>

<p>Result ""One"" class in Custom classifiers.</p>

<p>But now same code is returning different result:</p>

<p><a href=""https://i.stack.imgur.com/P1h78.png"" rel=""nofollow noreferrer"">Same image, but returning different result</a></p>",,1,4,,2017-12-16 19:16:52.683 UTC,,2017-12-17 02:57:44.520 UTC,2017-12-16 20:04:02.807 UTC,,4579742,,4579742,1,0,ibm-watson|visual-recognition,121
My AWS Rekognition sample code doesn't run,55927749,My AWS Rekognition sample code doesn't run,"<p>I'm new to AWS API, and am trying to run a sample AWS Rekognition code (Celebrity Recognition) described <a href=""https://docs.aws.amazon.com/rekognition/latest/dg/celebrities-video-sqs.html"" rel=""nofollow noreferrer"">here</a>. All configurations and credentials are set and the app is running. But it's just stuck in the loop printing:</p>

<pre><code>Waiting for job: c0059a6ee383daf6e6be12c39ee609cfadcbexxxxxxxxxx
....................
....................
....................
</code></pre>

<p>And never get's out. Not sure if anything is wrong with the code or configurations or whatnot. </p>

<p>What are the problems? Why I don't see any results back? Here is the code also in the link.</p>

<pre><code>import java.util.List;

import com.amazonaws.services.rekognition.AmazonRekognition;
import com.amazonaws.services.rekognition.AmazonRekognitionClientBuilder;
import com.amazonaws.services.rekognition.model.CelebrityDetail;
import com.amazonaws.services.rekognition.model.CelebrityRecognition;
import com.amazonaws.services.rekognition.model.CelebrityRecognitionSortBy;
import com.amazonaws.services.rekognition.model.GetCelebrityRecognitionRequest;
import com.amazonaws.services.rekognition.model.GetCelebrityRecognitionResult;
import com.amazonaws.services.rekognition.model.NotificationChannel;
import com.amazonaws.services.rekognition.model.S3Object;
import com.amazonaws.services.rekognition.model.StartCelebrityRecognitionRequest;
import com.amazonaws.services.rekognition.model.StartCelebrityRecognitionResult;
import com.amazonaws.services.rekognition.model.Video;
import com.amazonaws.services.rekognition.model.VideoMetadata;
import com.amazonaws.services.sqs.AmazonSQS;
import com.amazonaws.services.sqs.AmazonSQSClientBuilder;
import com.amazonaws.services.sqs.model.Message;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;

public class VideoDetectCelebrity {


    private static String bucket = ""test-bucket"";
    private static String video = ""test.mp4""; 
    private static String queueUrl =  ""https://sqs.us-east-2.amazonaws.com/xxxx/aws-queue"";
    private static String topicArn=""arn:aws:sns:us-east-2:xxxx:aws-test"";
    private static String roleArn=""arn:aws:iam::xxxxxx:role/aws-test-role"";
    private static AmazonSQS sqs = null;
    private static AmazonRekognition rek = null;

    private static NotificationChannel channel= new NotificationChannel()
            .withSNSTopicArn(topicArn)
            .withRoleArn(roleArn);


    private static String startJobId = null;


    public static void main(String[] args)  throws Exception{

        sqs = AmazonSQSClientBuilder.defaultClient();
        rek = AmazonRekognitionClientBuilder.defaultClient();

        //=================================================
        StartCelebrities(bucket, video);
        //=================================================
        System.out.println(""Waiting for job: "" + startJobId);
        //Poll queue for messages
        List&lt;Message&gt; messages=null;
        int dotLine=0;
        boolean jobFound=false;

        //loop until the job status is published. Ignore other messages in queue.
        do{
            messages = sqs.receiveMessage(queueUrl).getMessages();
            if (dotLine++&lt;20){
                System.out.print(""."");
            }else{
                System.out.println();
                dotLine=0;
            }

            if (!messages.isEmpty()) {
                //Loop through messages received.
                for (Message message: messages) {
                    String notification = message.getBody();

                    // Get status and job id from notification.
                    ObjectMapper mapper = new ObjectMapper();
                    JsonNode jsonMessageTree = mapper.readTree(notification);
                    JsonNode messageBodyText = jsonMessageTree.get(""Message"");
                    ObjectMapper operationResultMapper = new ObjectMapper();
                    JsonNode jsonResultTree = operationResultMapper.readTree(messageBodyText.textValue());
                    JsonNode operationJobId = jsonResultTree.get(""JobId"");
                    JsonNode operationStatus = jsonResultTree.get(""Status"");
                    System.out.println(""Job found was "" + operationJobId);
                    // Found job. Get the results and display.
                    if(operationJobId.asText().equals(startJobId)){
                        jobFound=true;
                        System.out.println(""Job id: "" + operationJobId );
                        System.out.println(""Status : "" + operationStatus.toString());
                        if (operationStatus.asText().equals(""SUCCEEDED"")){
                            //============================================
                            GetResultsCelebrities();
                            //============================================
                        }
                        else{
                            System.out.println(""Video analysis failed"");
                        }

                        sqs.deleteMessage(queueUrl,message.getReceiptHandle());
                    }

                    else{
                        System.out.println(""Job received was not job "" +  startJobId);
                        //Delete unknown message. Consider moving message to dead letter queue
                        sqs.deleteMessage(queueUrl,message.getReceiptHandle());
                    }
                }
            }
        } while (!jobFound);


        System.out.println(""Done!"");
    }


    //Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
    //PDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-rekognition-developer-guide/blob/master/LICENSE-SAMPLECODE.)

    // Celebrities=====================================================================
    private static void StartCelebrities(String bucket, String video) throws Exception{

        StartCelebrityRecognitionRequest req = new StartCelebrityRecognitionRequest()
                .withVideo(new Video()
                        .withS3Object(new S3Object()
                                .withBucket(bucket)
                                .withName(video)))
                .withNotificationChannel(channel);

        StartCelebrityRecognitionResult startCelebrityRecognitionResult = rek.startCelebrityRecognition(req);
        startJobId=startCelebrityRecognitionResult.getJobId();

    } 

    private static void GetResultsCelebrities() throws Exception{

        int maxResults=10;
        String paginationToken=null;
        GetCelebrityRecognitionResult celebrityRecognitionResult=null;

        do{
            if (celebrityRecognitionResult !=null){
                paginationToken = celebrityRecognitionResult.getNextToken();
            }
            celebrityRecognitionResult = rek.getCelebrityRecognition(new GetCelebrityRecognitionRequest()
                    .withJobId(startJobId)
                    .withNextToken(paginationToken)
                    .withSortBy(CelebrityRecognitionSortBy.TIMESTAMP)
                    .withMaxResults(maxResults));


            System.out.println(""File info for page"");
            VideoMetadata videoMetaData=celebrityRecognitionResult.getVideoMetadata();

            System.out.println(""Format: "" + videoMetaData.getFormat());
            System.out.println(""Codec: "" + videoMetaData.getCodec());
            System.out.println(""Duration: "" + videoMetaData.getDurationMillis());
            System.out.println(""FrameRate: "" + videoMetaData.getFrameRate());

            System.out.println(""Job"");

            System.out.println(""Job status: "" + celebrityRecognitionResult.getJobStatus());


            //Show celebrities
            List&lt;CelebrityRecognition&gt; celebs= celebrityRecognitionResult.getCelebrities();

            for (CelebrityRecognition celeb: celebs) { 
                long seconds=celeb.getTimestamp()/1000;
                System.out.print(""Sec: "" + Long.toString(seconds) + "" "");
                CelebrityDetail details=celeb.getCelebrity();
                System.out.println(""Name: "" + details.getName());
                System.out.println(""Id: "" + details.getId());
                System.out.println(); 
            }
        } while (celebrityRecognitionResult !=null &amp;&amp; celebrityRecognitionResult.getNextToken() != null);

    }

}
</code></pre>

<p>Looking at my SQS dashboard, the <code>aws-queue</code> has no available messages when running the code:</p>

<p><a href=""https://i.stack.imgur.com/YH97n.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YH97n.jpg"" alt=""enter image description here""></a></p>",,0,7,,2019-04-30 19:43:37.273 UTC,1,2019-04-30 20:58:16.907 UTC,2019-04-30 20:58:16.907 UTC,,3204706,,3204706,1,2,java|amazon-web-services|amazon-s3|aws-sdk,44
How to read image from s3 bucket for Amazon Rekognition using python?,53815181,How to read image from s3 bucket for Amazon Rekognition using python?,"<p>Hi I am working on a project where I have a source image in s3 bucket and I want to compare it with images in my local computer. I have already set up aws cli. Here is the code. My image is in some bucket 'bx' with name 's.jpg'. Now I want to read it so I called get_object method and used open() to read but it didn't worked.</p>

<pre><code>import boto3
import os
if __name__ == ""__main__"":
    path2 = '/home/vivek/Desktop/tar/'
    t=[j for j in os.listdir(path2)]
    buck=boto3.client('s3')
    obj=buck.get_object(Bucket='bx',Key='s.jpg')
    imageSource=open(obj['Body'],'rb')
    for targetFile in t:
        client=boto3.client('rekognition')
        imageTarget=open(targetFile,'rb')
        response=client.compare_faces(SimilarityThreshold=70,
        SourceImage={'Bytes': imageSource.read()},
        TargetImage={'Bytes': imageTarget.read()})
</code></pre>

<p>I get an error :</p>

<pre><code>vivek@skywalker:~/Desktop/code$ python3 y.py
Traceback (most recent call last):
  File ""y.py"", line 9, in &lt;module&gt;
    imageSource=open(obj['Body'],'rb')
TypeError: expected str, bytes or os.PathLike object, not StreamingBody
</code></pre>",,0,3,,2018-12-17 12:23:37.290 UTC,,2018-12-17 12:37:18.407 UTC,2018-12-17 12:37:18.407 UTC,,10106930,,10106930,1,0,amazon-web-services|amazon-s3|amazon-rekognition,85
Need to compare a given face input image across an already stored labelled collection using AWS,55603140,Need to compare a given face input image across an already stored labelled collection using AWS,"<p>I have a collection of profile images from customers I need to be able to pass a selfie of the person and scan it across the collection of images and pull up the customer information.</p>

<p>Need to do the following using AWS Rekognition - </p>

<ul>
<li>Create a collection - Done</li>
<li>Add Images to the collection - Whats the REST API syntax for this</li>
<li>While adding the images to the collection also tag it with the customer name.</li>
<li>Take a selfie portrait and search across the collection and return the tag information which matches.</li>
</ul>

<p>Im using Flutter as a platform hence there is no support for AWS SDK so will need to make REST API calls.
However the AWS docs don't provide much information for REST support.</p>",,1,0,,2019-04-10 00:38:39.420 UTC,,2019-04-10 02:32:11.793 UTC,,,,,3493337,1,0,amazon-web-services|flutter|face-recognition|amazon-rekognition,17
How to send a local image instead of URL to Microsoft Computer Vision API using JAVA,49463736,How to send a local image instead of URL to Microsoft Computer Vision API using JAVA,"<p>The question is how to load image file and pass it as object to Microsoft Computer Vision API, all the sample code in Microsoft website is reading image from url.</p>

<pre><code>// This sample uses the Apache HTTP client library(org.apache.httpcomponents:httpclient:4.2.4)
// and the org.json library (org.json:json:20170516).
package com.mparnisari.test;

import java.awt.image.BufferedImage;
import java.io.File;
import java.net.URI;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;

import javax.imageio.ImageIO;

import org.apache.http.HttpEntity;
import org.apache.http.HttpResponse;
import org.apache.http.client.HttpClient;
import org.apache.http.client.methods.HttpPost;
import org.apache.http.entity.ContentType;
import org.apache.http.entity.FileEntity;
import org.apache.http.entity.StringEntity;
import org.apache.http.client.utils.URIBuilder;
import org.apache.http.impl.client.DefaultHttpClient;
import org.apache.http.util.EntityUtils;
import org.json.JSONObject;


public class Main
{    
    public static final String subscriptionKey = ""MY-KEY-HERE"";

    public static final String uriBase = 
        ""https://westcentralus.api.cognitive.microsoft.com/vision/v1.0/analyze"";

    public static void main(String[] args)
    {
        HttpClient httpclient = new DefaultHttpClient();

        try
        {
            URIBuilder builder = new URIBuilder(uriBase);

            builder.setParameter(""visualFeatures"", ""Categories,Description,Color"");
            builder.setParameter(""language"", ""en"");

            URI uri = builder.build();
            HttpPost request = new HttpPost(uri);

            request.setHeader(""Content-Type"", ""application/json"");
            request.setHeader(""Ocp-Apim-Subscription-Key"", subscriptionKey);

            // Request body.
            BufferedImage image = null;
            File f = null;
            f = new File(""C:\\Coffee.jpg""); //image file path
            image = ImageIO.read(f);

            File file = new File(""C:\\Coffee.jpg"");

            FileEntity reqEntityF = 
                new FileEntity(file, ContentType.APPLICATION_OCTET_STREAM);

            request.setEntity(reqEntityF);

            HttpResponse response = httpclient.execute(request);
            HttpEntity entity = response.getEntity();

            if (entity != null)
            {
                // Format and display the JSON response.
                String jsonString = EntityUtils.toString(entity);
                JSONObject json = new JSONObject(jsonString);
                System.out.println(""REST Response:\n"");
                System.out.println(json.toString(2));
            }
        }
        catch (Exception e)
        {
            System.out.println(e.getMessage());
        }
    }
}
</code></pre>

<p>The output is:</p>

<pre><code>REST Response:

{
  ""code"": ""BadArgument"",
  ""requestId"": ""7ecf2198-1b7f-44d0-9cc2-e05e28791281"",
  ""message"": ""JSON format error.""
}
</code></pre>

<p>As in other post in stackoverflow guide to use <code>FileEntity</code> to upload the image. But it is not working.</p>

<p>i think this part should somehow refactor to read image instead of a URL.</p>

<pre><code>// Execute the REST API call and get the response entity.
HttpResponse response = httpclient.execute(request);
HttpEntity entity = response.getEntity();
</code></pre>

<p>Let me know what is best solution to solve this problem, because if its possible to pass the image from local to the API, it would be great to have a for loop to analyze an image set.</p>",49481712,3,2,,2018-03-24 10:41:04.620 UTC,2,2019-04-25 12:51:56.773 UTC,2018-03-29 17:06:18.027 UTC,,872496,,7349924,1,1,java|api|image-processing|microsoft-cognitive,899
WARNING:oauth2client.util:build() takes at most 2 positional arguments (3 given),35755940,WARNING:oauth2client.util:build() takes at most 2 positional arguments (3 given),"<p>I am doing the ""Label Detection"" tutorial for the Google Cloud Vision API.<br>
When I pass an image to the command like so I expect to get back some json telling me what is in the image.</p>

<p>However, I am getting this error instead.</p>

<pre><code>    &gt;python label_request.py faulkner.jpg 
No handlers could be found for logger ""oauth2client.util""
WARNING:root:No module named locked_file
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/site-packages/googleapiclient/discovery_cache/__init__.py"", line 38, in autodetect
    from . import file_cache
  File ""/usr/local/lib/python2.7/site-packages/googleapiclient/discovery_cache/file_cache.py"", line 32, in &lt;module&gt;
    from oauth2client.locked_file import LockedFile
ImportError: No module named locked_file
Traceback (most recent call last):
  File ""label_request.py"", line 44, in &lt;module&gt;
    main(args.image_file)
  File ""label_request.py"", line 18, in main
    service = build('vision', 'v1', http, discoveryServiceUrl=API_DISCOVERY_FILE)
  File ""/usr/local/lib/python2.7/site-packages/oauth2client/util.py"", line 140, in positional_wrapper
    return wrapped(*args, **kwargs)
  File ""/usr/local/lib/python2.7/site-packages/googleapiclient/discovery.py"", line 202, in build
    raise e
googleapiclient.errors.HttpError: &lt;HttpError 403 when requesting https://vision.googleapis.com/$discovery/rest?version=v1 returned ""Project has not activated the vision.googleapis.com API. Please enable the API for project google.com:cloudsdktool (#32555940559).""&gt;
</code></pre>

<p>Lots going on here.<br>
But the Project API <em>is</em> enabled.<br>
So this is part of the error message is erroneous.</p>

<p>It seems that ""there was a change in the newest version of the oauth2client, v2.0.0, which broke compatibility with the google-api-python-client module"".<br>
<a href=""https://stackoverflow.com/a/35492604/2341218"">https://stackoverflow.com/a/35492604/2341218</a>  </p>

<p>I applied this fix ...</p>

<pre><code>pip install --upgrade git+https://github.com/google/google-api-python-client
</code></pre>

<p>After applying this fix, I get fewer errors ...</p>

<pre><code>    &gt;python label_request.py faulkner.jpg 
No handlers could be found for logger ""oauth2client.util""
Traceback (most recent call last):
  File ""label_request.py"", line 44, in &lt;module&gt;
    main(args.image_file)
  File ""label_request.py"", line 18, in main
    service = build('vision', 'v1', http, discoveryServiceUrl=API_DISCOVERY_FILE)
  File ""/usr/local/lib/python2.7/site-packages/oauth2client/util.py"", line 137, in positional_wrapper
    return wrapped(*args, **kwargs)
  File ""/usr/local/lib/python2.7/site-packages/googleapiclient/discovery.py"", line 209, in build
    raise e
googleapiclient.errors.HttpError: &lt;HttpError 403 when requesting https://vision.googleapis.com/$discovery/rest?version=v1 returned ""Project has not activated the vision.googleapis.com API. Please enable the API for project google.com:cloudsdktool (#32555940559).""&gt;
</code></pre>

<p>It appears that this error message:
""No handlers could be found for logger ""oauth2client.util""
is actually masking a more detailed warning/error message 
and that I can see the more detailed one by adding this code ...</p>

<pre><code>import logging 
logging.basicConfig()
</code></pre>

<p><a href=""https://stackoverflow.com/a/29966147/2341218"">https://stackoverflow.com/a/29966147/2341218</a></p>

<pre><code>    &gt;python label_request.py faulkner.jpg 
WARNING:oauth2client.util:build() takes at most 2 positional arguments (3 given)
Traceback (most recent call last):
  File ""label_request.py"", line 47, in &lt;module&gt;
    main(args.image_file)
  File ""label_request.py"", line 21, in main
    service = build('vision', 'v1', http, discoveryServiceUrl=API_DISCOVERY_FILE)
  File ""/usr/local/lib/python2.7/site-packages/oauth2client/util.py"", line 137, in positional_wrapper
    return wrapped(*args, **kwargs)
  File ""/usr/local/lib/python2.7/site-packages/googleapiclient/discovery.py"", line 209, in build
    raise e
googleapiclient.errors.HttpError: &lt;HttpError 403 when requesting https://vision.googleapis.com/$discovery/rest?version=v1 returned ""Project has not activated the vision.googleapis.com API. Please enable the API for project google.com:cloudsdktool (#32555940559).""&gt;
</code></pre>

<p>So no I am stuck on this error message:<br>
WARNING:oauth2client.util:build() takes at most 2 positional arguments (3 given)</p>

<p>It has been suggested that this error can be avoided by using named parameters instead of positional notation.<br>
<a href=""https://stackoverflow.com/a/16643215/2341218"">https://stackoverflow.com/a/16643215/2341218</a></p>

<p>However, I am uncertain exactly where I might make this change.<br>
I don't actually see the oauth2client.util:build() function in the code.<br>
Here is the google code (slightly modified):</p>

<pre><code>    &gt;cat label_request.py
import argparse
import base64
import httplib2

from apiclient.discovery import build
from oauth2client.client import GoogleCredentials

import logging
logging.basicConfig()

def main(photo_file):
  '''Run a label request on a single image'''

  API_DISCOVERY_FILE = 'https://vision.googleapis.com/$discovery/rest?version=v1'
  http = httplib2.Http()

  credentials = GoogleCredentials.get_application_default().create_scoped(
      ['https://www.googleapis.com/auth/cloud-platform'])
  credentials.authorize(http)

  service = build('vision', 'v1', http, discoveryServiceUrl=API_DISCOVERY_FILE)

  with open(photo_file, 'rb') as image:
    image_content = base64.b64encode(image.read())
    service_request = service.images().annotate(
      body={
        'requests': [{
          'image': {
            'content': image_content
           },
          'features': [{
            'type': 'LABEL_DETECTION',
            'maxResults': 1,
           }]
         }]
      })
    response = service_request.execute()
    label = response['responses'][0]['labelAnnotations'][0]['description']
    print('Found label: %s for %s' % (label, photo_file))
    return 0

if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument(
    'image_file', help='The image you\'d like to label.')
  args = parser.parse_args()
  main(args.image_file)
</code></pre>",35812864,2,2,,2016-03-02 19:03:42.697 UTC,,2016-04-12 19:25:01.080 UTC,2017-05-23 11:59:49.067 UTC,,-1,,2341218,1,2,python|google-api-client|google-api-python-client|google-cloud-vision,1102
"Trying to upload an image to Microsoft Azure api with NodeJS, rather than JQuery",50452142,"Trying to upload an image to Microsoft Azure api with NodeJS, rather than JQuery","<p>I am trying to upload an image for Microsoft Azure text recognition, but I only see support for a jquery submission.  </p>

<p>I have a Raspberry Pi taking a picture with a NodeJS app (pi-camera).  Then, I want to send this to the Azure api with that same app.  Is there any support for this?  I doesn't seem efficient to create a web page and open a browser to navigate to a picture, when I have a node app running.</p>

<p>The actual goal is to take a picture of my water meter with my Raspberry Pi, and then upload the image to have the number read and returned.</p>

<p>Thanks in advance.</p>",,0,2,,2018-05-21 15:34:32.893 UTC,,2018-05-21 15:51:25.950 UTC,,,,,9823837,1,0,javascript|node.js|azure,53
Demo response different from runtime response,44856326,Demo response different from runtime response,"<p>I'm trying to use the Google Cloud Vision API to OCR this image:</p>

<p><img src=""https://i.stack.imgur.com/jUrEa.jpg"" alt=""sample""></p>

<p>I'm using the following code the make the request:</p>

<pre><code>const resp = await fetch(
  `https://vision.googleapis.com/v1/images:annotate?key=${KEY}`, {
    method: 'POST',
    body: JSON.stringify({
      requests: [{
        image: {content: encoded},
        features: [{type: ""TEXT_DETECTION""}],
      }]
    }),
});
</code></pre>

<p>This works but there is some information missing from the result. If we look at the <code>text</code> field:</p>

<pre><code>Dog Search
D G O OD D ODG O O D D O
O D O O G G G D O D G OG G
OGD GOGD GO G GO G D
D D D G D DO DOO G D O O
O DGOGG D O O G G O O D
DOG
</code></pre>

<p>Here's that visualized:</p>

<p><img src=""https://i.stack.imgur.com/VUTOb.png"" alt=""1""></p>

<p>There are boxes around the characters which were recognized. But, if we put this image into the gcv <a href=""https://cloud.google.com/vision/docs/drag-and-drop"" rel=""nofollow noreferrer"">demo application</a>, we get this instead:</p>

<p><img src=""https://i.stack.imgur.com/rdpal.png"" alt=""2"">
And this is what <code>text</code> looks like:</p>

<pre><code>Dog Search
D GOOD D 0 D GOOD DO
0 D 0 0 G G G DOD GO GG
o G O G D 0 0 D G 0 0 D D D
D G D o o o G G o o G D Go
0 G D G O G D G O G G O G D
D D D G D DO DO O G D 0 0
O D GO G G D 0 0 G G 0 0 D
DOG
</code></pre>

<p>Here's a <a href=""https://gist.github.com/0xcaff/d06828a6d17653043c1a0531c317aef1"" rel=""nofollow noreferrer"">gist</a> with the requests + responses. I'm authenticating using a API token.</p>

<p>Why are the responses different? The requests are slightly different but not in a way which should affect the output. Right?</p>",,0,3,,2017-07-01 00:26:20.677 UTC,,2017-07-01 00:26:20.677 UTC,,,,,2456263,1,1,google-cloud-platform|google-cloud-vision,38
How to perform OCR in Android App,40020225,How to perform OCR in Android App,"<p>This question has been asked before (<a href=""https://stackoverflow.com/questions/1106202/is-there-any-free-ocr-library-for-android"">Is there any free OCR library for Android?</a>) but the answers are pretty old (6 years) and I hope that Optical Character Recognition possibilities have grown by now.</p>

<p>So I would need to extract some text from a photo to perform some analysis on it, for an Android App. I have heard about Google Cloud Vision API but I am not sure this is the best way to do it as it requires internet access for the app...</p>

<p>Do you know if there is any API that would allow this kind of feature for standalone apps ?</p>",,0,2,,2016-10-13 11:52:12.187 UTC,1,2016-10-17 13:35:32.433 UTC,2017-05-23 12:10:31.817 UTC,,-1,,4560470,1,3,java|android|ocr,319
Function to choose image file one by one automatically in R,54466934,Function to choose image file one by one automatically in R,"<p>I have a folder with 100+ images. I want to run an google vision analysis on each of them in R. Instead of running the analysis on one image at a time I want to create a function which will access each image one by one and run the analysis.</p>

<p>Using following code:</p>

<p><code>getGooglevisionResponse(file.choose(),feature = 'text_detection')</code></p>

<p>I am using <code>file.choose()</code> to choose one file at a time but I want to create a loop which will dynamically select each image and run the analysis on them ..
Used <code>list.files()</code> but getting below error
<code>the following condition has length &gt;1 and only the first element will be used</code></p>

<p>found one post but that is in python  unable to replicate it in R</p>

<p><a href=""https://github.com/andrikosrikos/Google-Cloud-Support/blob/master/Google%20Vision/multiple_features_request_single_API_call.py"" rel=""nofollow noreferrer"">https://github.com/andrikosrikos/Google-Cloud-Support/blob/master/Google%20Vision/multiple_features_request_single_API_call.py</a></p>",,1,2,,2019-01-31 18:18:58.587 UTC,,2019-02-04 18:13:28.507 UTC,2019-01-31 18:56:31.887 UTC,,8233109,,8233109,1,0,r|loops|functor|google-vision,80
Project oxford vision API ocr exception,40526090,Project oxford vision API ocr exception,"<p>Got a problem with project oxford vision API. The example from <a href=""https://github.com/Microsoft/ProjectOxford-ClientSDK"" rel=""nofollow noreferrer"">project oxford git</a> works fine and recognise text on images. But my code throws exception:</p>

<blockquote>
  <p>Exception of type 'Microsoft.ProjectOxford.Vision.ClientException' was thrown.
      at Microsoft.ProjectOxford.Vision.VisionServiceClient.HandleException(Exception exception)
      at Microsoft.ProjectOxford.Vision.VisionServiceClient.b__39_1[TRequest,TResponse](Exception e)
      at System.AggregateException.Handle(Func<code>2 predicate)
      at Microsoft.ProjectOxford.Vision.VisionServiceClient.&lt;SendAsync&gt;d__39</code>2.MoveNext()
  --- End of stack trace from previous location where exception was thrown ---
      at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task)
      at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
      at Microsoft.ProjectOxford.Vision.VisionServiceClient.d__32.MoveNext()
  --- End of stack trace from previous location where exception was thrown ---
      at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task)
      at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
      at System.Runtime.CompilerServices.TaskAwaiter<code>1.GetResult()
      at ..OcrWorker.&lt;UploadAndRecognizeImageAsync&gt;d__15.MoveNext() in ..\\OcrWorker.cs:line 165\r\n
  --- End of stack trace from previous location where exception was thrown ---
      at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task)
      at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)
      at System.Runtime.CompilerServices.TaskAwaiter</code>1.GetResult()
      at ..OcrWorker.d__14.MoveNext() in ..\OcrWorker.cs:line 127</p>
</blockquote>

<p>Class code:</p>

<pre><code>public string SubscriptionKey { get; set; }
    public string OcrResultText
    {
        get
        {
            if (FullOcrResult == null)
            {
                FullOcrResult = new StringBuilder();
            }
            string response = string.Empty;
            if (OcrDone)
            {
                response = FullOcrResult.ToString();
            }
            else
            {
                response = null;
            }
            return response;
        }
    }
    private bool OcrDone = true;
    public bool IsOcrDone { get { return OcrDone; } }
    private StringBuilder FullOcrResult;

    public OcrWorker(string appKey)
    {
        SubscriptionKey = appKey;
        FullOcrResult = new StringBuilder();
    }

    public string DoWorkSync(List&lt;Bitmap&gt; images)
    {
        if (OcrDone)
        {
            List&lt;IterationItem&gt; iteartionItems = new List&lt;IterationItem&gt;();
            int i = 0;
            OcrDone = false;
            foreach (var image in images)
            {
                IterationItem ocrIterationItem = new IterationItem();
                try
                {
                    Task&lt;IterationItem&gt; o = DoWorkForIterationAsync(image, i);
                    o.Wait();
                    ocrIterationItem = o.Result;
                }
                catch (Exception ex)
                {
                    var a = ex.GetBaseException();
                }
                iteartionItems.Add(ocrIterationItem);
                i++;
            }
            GetOcrResultFromIterations(iteartionItems);
            OcrDone = true;
        }
        return OcrResultText;
    }

    public void WriteResultToFile(string path)
    {
        if (OcrDone)
        {
            if (File.Exists(path))
            {
                File.Delete(path);
            }
            File.AppendAllText(path, OcrResultText);
        }
    }

    private void GetOcrResultFromIterations(List&lt;IterationItem&gt; iterationResults)
    {
        iterationResults = iterationResults.OrderBy(item =&gt; item.Number).ToList();
        foreach (var iterationItem in iterationResults)
        {
            var results = iterationItem.OcrResult;
            FullOcrResult.AppendLine();
            foreach (var item in results.Regions)
            {
                foreach (var line in item.Lines)
                {
                    foreach (var word in line.Words)
                    {
                        FullOcrResult.Append(word.Text);
                        FullOcrResult.Append("" "");
                    }
                    FullOcrResult.AppendLine();
                }
                FullOcrResult.AppendLine();
            }
        }
    }

    /// &lt;summary&gt;
    /// Perform the work for this scenario
    /// &lt;/summary&gt;
    /// &lt;param name=""imageUri""&gt;The URI of the image to run against the scenario&lt;/param&gt;
    /// &lt;param name=""upload""&gt;Upload the image to Project Oxford if [true]; submit the Uri as a remote url if [false];&lt;/param&gt;
    /// &lt;returns&gt;&lt;/returns&gt;
    private async Task&lt;IterationItem&gt; DoWorkForIterationAsync(Bitmap image, int iterationNumber)
    {
        var _status = ""Performing OCR..."";

        //
        // Upload an image
        //
        OcrResults ocrResult = await UploadAndRecognizeImageAsync(image, RecognizeLanguage.ShortCode);
        _status = ""OCR Done"";

        //
        // Log analysis result in the log window
        //
        return new IterationItem()
        {
            Number = iterationNumber,
            OcrResult = ocrResult
        };
    }

    /// &lt;summary&gt;
    /// Uploads the image to Project Oxford and performs OCR
    /// &lt;/summary&gt;
    /// &lt;param name=""imageFilePath""&gt;The image file path.&lt;/param&gt;
    /// &lt;param name=""language""&gt;The language code to recognize for&lt;/param&gt;
    /// &lt;returns&gt;&lt;/returns&gt;
    private async Task&lt;OcrResults&gt; UploadAndRecognizeImageAsync(Bitmap image, string language)
    {
        // -----------------------------------------------------------------------
        // KEY SAMPLE CODE STARTS HERE
        // -----------------------------------------------------------------------

        //
        // Create Project Oxford Vision API Service client
        //
        VisionServiceClient VisionServiceClient = new VisionServiceClient(SubscriptionKey);
        Log(""VisionServiceClient is created"");

        using (Stream imageMemoryStream = new MemoryStream())
        {
            image.Save(imageMemoryStream, ImageFormat.Bmp);
            //
            // Upload an image and perform OCR
            //
            Log(""Calling VisionServiceClient.RecognizeTextAsync()..."");
            OcrResults ocrResult = await VisionServiceClient.RecognizeTextAsync(imageMemoryStream, language);
            return ocrResult;
        }

        // -----------------------------------------------------------------------
        // KEY SAMPLE CODE ENDS HERE
        // -----------------------------------------------------------------------
    }

    //get ocred text
    class IterationItem
    {
        public int Number { get; set; }
        public OcrResults OcrResult { get; set; }
    }
    public static class RecognizeLanguage
    {
        public static string ShortCode { get { return ""en""; } }
        public static string LongName { get { return ""English""; } }
    }
</code></pre>

<p>Did anyone have same problem and how can i solve it?</p>",,1,2,,2016-11-10 11:07:41.603 UTC,,2016-11-11 08:51:11.950 UTC,,,,,5888942,1,1,c#|.net|ocr|microsoft-cognitive,1708
Google's Vision Api protobuf response object to Python dictionary,50160997,Google's Vision Api protobuf response object to Python dictionary,"<p>I'm working on a project in which I need to analyze an image using Google's Vision API and post the response to a Dynamodb table.</p>

<p>I have successfully implemented the Vision API, but not able to convert its response into Python Dictionary.</p>

<p>Here's what I have tried:</p>

<pre><code>       if form.is_valid():
            obj = form
            obj.imageFile = form.cleaned_data['imageFile']
            obj.textFile = form.cleaned_data['textFile']
            obj.save()
            print(obj.imageFile)
            # Process the image using Google's vision API
            image_path = os.path.join(settings.MEDIA_ROOT, 'images/', obj.imageFile.name)
            print(image_path)
            image = vision_image_manager(image_path)
            text_path = os.path.join(settings.MEDIA_ROOT, 'texts/', obj.textFile.name)
            text = nlp_text_manager(text_path)
            # print(image)
            # print(text)
            results = {
                'imageResponse': image,
                'textResult': text
            }
            print(results.values())
            print(type(results))
            post_to_dynamo_db(image, text)
</code></pre>

<p>Here's the Vision api implementation:</p>

<pre><code>def vision_image_manager(image_file):
    # Instantiates a client
    client = vision.ImageAnnotatorClient()
    file_name = str(image_file)
    with open(file_name, 'rb') as img_file:
        content = img_file.read()
    image = types.Image(content=content)
    response = client.label_detection(image=image)
    labels = response.label_annotations
    print('Labels:')
    for label in labels:
        print(label.description)
    return labels
</code></pre>

<p>And Here's the <code>post_to_dynamo_db</code> Function:</p>

<pre><code>def post_to_dynamo_db(image, text):
session = boto3.Session(
    aws_access_key_id=settings.AWS_SERVER_PUBLIC_KEY,
    aws_secret_access_key=settings.AWS_SERVER_SECRET_KEY
)
client = session.resource('dynamodb')
table = client.Table('basetbl')
result_dict = {
    'image': image,
    'text': text
}
json_dict = dict_to_item(result_dict)
# item = dict_to_item(result_dict)
table.put_item(
    Item={
        'id': int(generate_pid()),
        'response_obj': json_dict
    }
)
</code></pre>

<p>Now, It doesn't return any error but the <code>response_obj</code> is not posted in Database table because it's not the correct form of the object, the problem here is the <code>&lt;class 'google.protobuf.pyext._message.RepeatedCompositeContainer'&gt;</code> type of response returns from Google's API.</p>",50186387,1,2,,2018-05-03 17:42:20.873 UTC,,2018-05-05 06:21:03.807 UTC,2018-05-05 06:18:15.313 UTC,,7644562,,7644562,1,3,python|protocol-buffers|google-cloud-vision|google-protocol-buffer,1129
What does Rekognition count as one metadata?,47831050,What does Rekognition count as one metadata?,"<p>I want to create a collection of faces from 1500 face images and then <a href=""http://docs.aws.amazon.com/rekognition/latest/dg/API_SearchFacesByImage.html"" rel=""nofollow noreferrer"">compare</a> this collection with one reference face image. The final goal is to find which face from the collection is the most similar one to the reference face image.</p>

<p>So I want to retrieve one number for similarity for each pair of images (reference image and one face from the collection) each time.</p>

<p>So does this amount to 1500faces x 1similarity_metadata = 1500metadata or the similarity attribute is counted as one metadata for any number of face images?</p>

<p>In other words, does my request amount to 1500 metadata or 1 metadata for the 1500 faces?</p>

<p>I am using the free version and AWS specifies that:</p>

<blockquote>
  <p>As part of the AWS Free Tier, you can get started with Amazon
  Rekognition Image for free. Upon sign-up, new Amazon Rekognition
  customers can analyze 5,000 images per month and store up to 1,000
  face metadata each month, for the first 12 month.</p>
</blockquote>

<p>So I am asking this because I do not want to exceed the limit of 1000 face metadata each month.</p>",,1,0,,2017-12-15 11:05:58.440 UTC,,2017-12-16 19:19:57.187 UTC,2017-12-15 15:49:32.657 UTC,,271415,,9024698,1,1,amazon-web-services|metadata|amazon-rekognition,99
Microsoft Emotion API using requests Error 400,42733482,Microsoft Emotion API using requests Error 400,"<p>I am using Microsoft Emotion API using python + requests</p>

<p>Using the following code I am always getting a 400 error - </p>

<blockquote>
  <p>""Indicates JSON parsing error, faceRectangles cannot be parsed correctly, or count exceeds 64, or content-type is not recognized.""</p>
</blockquote>

<pre><code>import requests
n = int(raw_input())
url = ""https://westus.api.cognitive.microsoft.com/emotion/v1.0/recognize""
headers = {""Content-Type"": ""application/json"", ""Ocp-Apim-Subscription-Key"": ""xxxxxxxxxxxxxxxxxxxxxxxx""}
while n&gt;0:
    str = raw_input()
    body = '''{ ""url"": ""''' + str + '''"" }'''
    r = requests.post(url, data = body,headers = headers)
    print(r.status_code)
    if r.status_code == 200:    
        print(r.content)
    n = n-1
</code></pre>

<p>and the input</p>

<pre><code>1
http://i.imgur.com/ytsbJBy.jpg
</code></pre>",,1,1,,2017-03-11 09:20:47.020 UTC,,2017-03-13 06:30:55.553 UTC,,,,,4550928,1,0,python|azure,143
Google vision Api hangs on batch request,51320588,Google vision Api hangs on batch request,"<p>I am trying to integrate Google Vision API on our platform and I am facing some difficulties. The thread blocks on the API call and doesn't return. 
I think there are some problems with the authentication, but I am not sure.  </p>

<p>Here I create the Google credentials  bean from a json which looks like this(Obviously I have erased all the field values.).</p>

<p><code>{
  ""type"": """",
  ""project_id"": ,
  ""private_key_id"": """",
  ""private_key"": """",
  ""client_email"": ,
  ""client_id"": ,
  ""auth_uri"": ,
  ""token_uri"": ,
  ""auth_provider_x509_cert_url"": """",
  ""client_x509_cert_url"": """"
}</code></p>

<pre><code>@Bean
public GoogleCredentials getCredentials() throws IOException {
    return GoogleCredentials.fromStream(new FileInputStream(gcpCredentials));
}
</code></pre>

<p>Then I get the bean from another class and make the API call.</p>

<pre><code>      ImageAnnotatorSettings imageAnnotatorSettings = ImageAnnotatorSettings
            .newBuilder()
            .setCredentialsProvider(FixedCredentialsProvider.create(googleCredentials))
            .build();
</code></pre>

<p>I omit some code which gets the image,packs it etc</p>

<pre><code>ImageAnnotatorClient visionClient = ImageAnnotatorClient.create(imageAnnotatorSettings);
BatchAnnotateImagesResponse batchResponse = visionClient.batchAnnotateImages(requests);
</code></pre>",,0,1,,2018-07-13 08:13:06.613 UTC,,2018-07-13 08:20:51.257 UTC,2018-07-13 08:20:51.257 UTC,,10070828,,10070828,1,0,java|spring|google-cloud-platform|google-authentication|google-vision,126
Google Cloud Vision TEXT_DETECTION only on digits,36125830,Google Cloud Vision TEXT_DETECTION only on digits,"<p>Is there any way to constrain google cloud vision, especially for type TEXT_DETECTION to only recognize digits? I think it will greatly improve my result.</p>

<p>I cannot find any result or hint on the internet at all. Any help is appreciated.</p>",,1,0,,2016-03-21 08:08:04.457 UTC,,2016-06-09 21:25:36.483 UTC,2016-03-31 11:16:04.653 UTC,,1575900,,5400749,1,2,machine-learning|computer-vision|google-cloud-vision,789
Sunglass detection Google vision Face API,32260893,Sunglass detection Google vision Face API,<p>Is there a way to detect sunglasses (glasses) using the new Face API from google vision?</p>,32277028,1,1,,2015-08-27 23:13:56.177 UTC,,2015-08-28 17:51:57.587 UTC,,,,,1940684,1,0,google-play-services|google-vision,750
IAM Role ARN Rekognition create stream Processor,55135638,IAM Role ARN Rekognition create stream Processor,"<p>I  am trying to use AWS's face recognition from streaming Kinesis, but got stuck on create stream processor step with error:</p>

<blockquote>
  <p>err AccessDeniedException: status code: 400, request id: d871329b-4553-11e9-9d9b-bf3e1c3a90d6</p>
</blockquote>

<p>I have configuration with:
<br/>
IAM Policy</p>

<pre><code>{
""Version"": ""2012-10-17"",
""Statement"": [
    {
        ""Sid"": ""VisualEditor0"",
        ""Effect"": ""Allow"",
        ""Action"": [
            ""kinesisanalytics:*"",
            ""kinesisvideo:*"",
            ""kinesis:*"",
            ""rekognition:*""
        ],
        ""Resource"": ""*""
    }
]
}
</code></pre>

<p><br/>
The Role has 2 attached policies, AmazonRekognitionServiceRole and above custom policy.
<br/></p>

<pre><code>Role's Trust relationships: 

    {
      ""Version"": ""2012-10-17"",
      ""Statement"": [
        {
          ""Effect"": ""Allow"",
          ""Principal"": {
            ""Service"": ""rekognition.amazonaws.com""
          },
          ""Action"": ""sts:AssumeRole"",
          ""Condition"": {}
        }
      ]
    }
</code></pre>

<p><br/>
Code for testing (by Golang):
<br/></p>

<pre><code>func createStreamProcessor() {
        sess, err := session.NewSession(&amp;aws.Config{
            Region: aws.String(""ap-southeast-2""),
            //Credentials: credentials.NewSharedCredentials("""", ""default""),
        })
        fmt.Println(err)

        reko := rekognition.New(sess)
        result, err := reko.ListCollections(&amp;rekognition.ListCollectionsInput{
            MaxResults: aws.Int64(100),
        })

        fmt.Println(result)
        fmt.Println(err)
        input := rekognition.CreateStreamProcessorInput{
            Name: aws.String(""RdsStreamProcessor""),
            Input: &amp;rekognition.StreamProcessorInput{
                KinesisVideoStream: &amp;rekognition.KinesisVideoStream{
                    Arn: aws.String(""arn:aws:kinesisvideo:ap-southeast-2:nnnnnnnnn:stream/redisys-stream/1552295399763""),
                },
            },
            Output: &amp;rekognition.StreamProcessorOutput{
                KinesisDataStream: &amp;rekognition.KinesisDataStream{
                    Arn: aws.String(""arn:aws:kinesis:ap-southeast-2:nnnnnnnnn:stream/rds-face-recognition-stream""),
                },
            },
            RoleArn: aws.String(""arn:aws:iam::nnnnnnnnn:role/rds_recognition_role""),
            Settings: &amp;rekognition.StreamProcessorSettings{
                FaceSearch: &amp;rekognition.FaceSearchSettings{
                    CollectionId:       aws.String(""rds_customers""),
                    FaceMatchThreshold: aws.Float64(90),
                },
            },
        }
        fmt.Println(input)
        outPut, err := reko.CreateStreamProcessor(&amp;input)   
        if err != nil {
            fmt.Println(err.Error())
        }
        fmt.Println(outPut)
    }
</code></pre>",55380199,1,0,,2019-03-13 06:24:27.197 UTC,,2019-03-27 14:59:37.023 UTC,2019-03-13 06:43:05.800 UTC,,1831452,,1831452,1,0,aws-sdk|amazon-iam|amazon-kinesis|amazon-rekognition,44
AWS Rekognition gives an InvalidS3Exeption error,44532633,AWS Rekognition gives an InvalidS3Exeption error,"<p>Every time I run the command </p>

<pre><code>aws rekognition detect-labels --image ""S3Object={Bucket=BucketName,Name=picture.jpg}"" --region us-east-1
</code></pre>

<p>I get this error.</p>

<pre><code>InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectLabels operation: Unable to get image metadata from S3.  Check object key, region and/or access permissions.
</code></pre>

<p>I am trying to retrieve labels for a project I am working on but I can't seem to get past this step. I configured aws with my access key, secret key, us-east-1 region, and json as my output format.</p>

<p>I have also tried the code below and I receive the exact same error (I correctly Replaced BucketName with the name of my bucket.)</p>

<pre><code>import boto3

BUCKET = ""BucketName""
KEY = ""picture.jpg""

def detect_labels(bucket, key, max_labels=10, min_confidence=90, region=""eu-west-1""):
    rekognition = boto3.client(""rekognition"", region)
    response = rekognition.detect_labels(
        Image={
            ""S3Object"": {
                ""Bucket"": bucket,
                ""Name"": key,
            }
        },
        MaxLabels=max_labels,
        MinConfidence=min_confidence,
    )
    return response['Labels']


for label in detect_labels(BUCKET, KEY):
    print ""{Name} - {Confidence}%"".format(**label)
</code></pre>

<p>I am able to see on my user account that it is calling Rekognition.
<a href=""https://i.stack.imgur.com/4p5j0.gif"" rel=""nofollow noreferrer"">Image showing it being called from IAM.</a></p>

<p>It seems like the issue is somewhere with my S3 bucket but I haven't found out what.</p>",,1,2,,2017-06-13 22:32:28.683 UTC,1,2018-06-04 09:50:25.583 UTC,,,,,7970051,1,4,amazon-web-services|amazon-s3|amazon-rekognition,2229
How to fix a requests.exceptions.HTTPError while uploading to aws s3,55005298,How to fix a requests.exceptions.HTTPError while uploading to aws s3,"<p>im trying to upload a image file to aws s3 using the following code and it gives an following error saying</p>

<p>requests.expectations.HTTPError:400 Client Error: BadRequest for url:<a href=""http://dacsup.s3.amazonaws.com/0001249950"" rel=""nofollow noreferrer"">http://dacsup.s3.amazonaws.com/0001249950</a></p>

<p>the complete error is attached as an image =<a href=""https://i.stack.imgur.com/kMzcW.jpg"" rel=""nofollow noreferrer"">entire error image </a></p>

<p>the problem is mainly in upload part is the im not able to figure out the file_name
will it be rfid tag which is 0001249950 or will it be the scan variable that holds the value of rfid or some thing else  </p>

<pre><code>from AWSIoTPythonSDK.MQTTLib import AWSIoTMQTTClient
import RPi.GPIO as GPIO # RPi.GPIO can be referred as GPIO from now
import sys
import logging
import time
import getopt
from datetime import datetime
import picamera
import os
import tinys3
import json

ledPing = 38    # gpio pin20
ledPinr =16 #gpio pin23

def setup():
        GPIO.setmode(GPIO.BOARD)       # GPIO Numbering of Pins
        GPIO.setup(ledPing, GPIO.OUT)   # Set ledPin as output
        GPIO.setup(ledPinr, GPIO.OUT)   # Set ledPin as output
        GPIO.output(ledPing, GPIO.LOW)  # Set ledPin to LOW to turn Off the LED
        GPIO.output(ledPinr, GPIO.LOW)  # Set ledPin to LOW to turn Off the LED


# Usage
usageInfo = """"""Usage:
Use certificate based mutual authentication:
python rpi_rfid_rekognition.py -e &lt;a1mqv6zcxdhpip-ats.iot.us-east-2.amazonaws.com&gt; -r &lt;/home/pi/root-CA.crt&gt; -c &lt;/home/pi/RPI4.cert.pem&gt; -k &lt;/home/pi/RPI4.private.key&gt;
Type ""python rpi_rfid_rekognition.py -h"" for available options.
""""""
# Help info
helpInfo = """"""-e, --endpoint
    Your AWS IoT custom endpoint
-r, --rootCA
    Root CA file path
-c, --cert
    Certificate file path
-k, --key
    Private key file path
-h, --help
    Help information
""""""

# Read in command-line parameters
host = ""endpoint""
rootCAPath = ""/home/pi/root-CA.crt""
certificatePath = ""/home/pi/RPI4.cert.pem""
privateKeyPath = ""/home/pi/RPI4.private.key""
try:
    opts, args = getopt.getopt(sys.argv[1:], ""hwe:k:c:r:"", [""help"", ""endpoint="", ""key="",""cert="",""rootCA=""])
    if len(opts) == 0:
        raise getopt.GetoptError(""No input parameters!"")
    for opt, arg in opts:
        if opt in (""-h"", ""--help""):
            print(helpInfo)
            exit(0)
        if opt in (""-e"", ""--endpoint""):
            host = arg
        if opt in (""-r"", ""--rootCA""):
            rootCAPath = arg
        if opt in (""-c"", ""--cert""):
            certificatePath = arg
        if opt in (""-k"", ""--key""):
            privateKeyPath = arg
except getopt.GetoptError:
    print(usageInfo)
    exit(1)

# Missing configuration notification
missingConfiguration = False
if not host:
    print(""Missing '-e' or '--endpoint'"")
    missingConfiguration = True
if not rootCAPath:
    print(""Missing '-r' or '--rootCA'"")
    missingConfiguration = True
if not certificatePath:
    print(""Missing '-c' or '--cert'"")
    missingConfiguration = True
if not privateKeyPath:
    print(""Missing '-k' or '--key'"")
    missingConfiguration = True
if missingConfiguration:
    exit(2)

# photo properties
image_width = 400
image_height = 400
file_extension = '.png'


# AWS S3 properties
access_key_id = 'mykeyid'
secret_access_key = 'my_secreat_accesske'
bucket_name = 'my_bucket_name'

# RFID character map for hid device
hid = { 4: 'a', 5: 'b', 6: 'c', 7: 'd', 8: 'e', 9: 'f', 10: 'g', 11: 'h', 12: 'i', 13: 'j', 14: 'k', 15: 'l', 16: 'm', 17: 'n', 18: 'o', 19: 'p', 20: 'q', 21: 'r', 22: 's', 23: 't', 24: 'u', 25: 'v', 26: 'w', 27: 'x', 28: 'y', 29: 'z', 30: '1', 31: '2', 32: '3', 33: '4', 34: '5', 35: '6', 36: '7', 37: '8', 38: '9', 39: '0', 44: ' ', 45: '-', 46: '=', 47: '[', 48: ']', 49: '\\', 51: ';' , 52: '\'', 53: '~', 54: ',', 55: '.', 56: '/'  }
hid2 = { 4: 'A', 5: 'B', 6: 'C', 7: 'D', 8: 'E', 9: 'F', 10: 'G', 11: 'H', 12: 'I', 13: 'J', 14: 'K', 15: 'L', 16: 'M', 17: 'N', 18: 'O', 19: 'P', 20: 'Q', 21: 'R', 22: 'S', 23: 'T', 24: 'U', 25: 'V', 26: 'W', 27: 'X', 28: 'Y', 29: 'Z', 30: '!', 31: '@', 32: '#', 33: '$', 34: '%', 35: '^', 36: '&amp;', 37: '*', 38: '(', 39: ')', 44: ' ', 45: '_', 46: '+', 47: '{', 48: '}', 49: '|', 51: ':' , 52: '""', 53: '~', 54: '&lt;', 55: '&gt;', 56: '?'  }

# Configure logging
logger = logging.getLogger(""AWSIoTPythonSDK.core"")
logger.setLevel(logging.DEBUG)
streamHandler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
streamHandler.setFormatter(formatter)
logger.addHandler(streamHandler)

# Init AWSIoTMQTTClient
myAWSIoTMQTTClient = None

myAWSIoTMQTTClient = AWSIoTMQTTClient(""basicPubSub"")
myAWSIoTMQTTClient.configureEndpoint(host, 8883)
myAWSIoTMQTTClient.configureCredentials(rootCAPath, privateKeyPath, certificatePath)

# AWSIoTMQTTClient connection configuration
myAWSIoTMQTTClient.configureAutoReconnectBackoffTime(1, 32, 20)
myAWSIoTMQTTClient.configureOfflinePublishQueueing(-1)  # Infinite offline Publish queueing
myAWSIoTMQTTClient.configureDrainingFrequency(2)  # Draining: 2 Hz
myAWSIoTMQTTClient.configureConnectDisconnectTimeout(10)  # 10 sec
myAWSIoTMQTTClient.configureMQTTOperationTimeout(5)  # 5 sec

# camera setup
camera = picamera.PiCamera()
camera.resolution = (image_width, image_height)
camera.awb_mode = 'auto'

#led function

def redled():
        while True:

                GPIO.output(ledPinr, GPIO.HIGH)   # LED On
                GPIO.output(ledPinr, GPIO.LOW)   # LED Off
def greenled():
        while True:

                GPIO.output(ledPing, GPIO.HIGH)   # LED On
                GPIO.output(ledPing, GPIO.LOW)   # LED Off

def endprogram():

        GPIO.output(ledPing, GPIO.LOW)     # LED Off
        GPIO.cleanup()                    # Release resources

        GPIO.output(ledPinr, GPIO.LOW)     # LED Off
        GPIO.cleanup()                    # Release resources
# Start listening on RFID events
fp = open('/dev/hidraw0', 'rb')

def waitForRFIDScan():
    ss = """"
    shift = False
    done = False
    while not done:
       buffer = fp.read(8)
       for c in buffer:
          if ord(c) &gt; 0:
             if int(ord(c)) == 40:
                done = True
                break;
             if shift:
                if int(ord(c)) == 2 :
                   shift = True
                else:
                   ss += hid2[ int(ord(c)) ]
                   shift = False
             else:
                if int(ord(c)) == 2 :
                   shift = True
                else:
                   ss += hid[ int(ord(c)) ]
    return ss

def uploadToS3(file_name):
    filepath = file_name + file_extension
    camera.capture(filepath)
    conn = tinys3.Connection(access_key_id, secret_access_key)
    f = open(filepath, 'rb')
    conn.upload(filepath, f, bucket_name,
               headers={
               'x-amz-meta-cache-control': 'max-age=60'
               })
    if os.path.exists(filepath):
        os.remove(filepath)

# Custom MQTT message callback
def photoVerificationCallback(client, userdata, message):
    print(""Received a new message: "")
    data = json.loads(message.payload)
    try:
        similarity = data[1][0]['Similarity']
        print(""Received similarity: "" + str(similarity))
        if(similarity &gt;= 90):
            print(""Access allowed, opening doors."")
            print(""Thank you!"")
    except:
        pass
    print(""Finished processing event."")

def checkRFIDNumber(rfidnumber):
    return rfidnumber == '0001249950'

# Connect and subscribe to AWS IoT
myAWSIoTMQTTClient.connect()
myAWSIoTMQTTClient.subscribe(""smartdoor"", 1, photoVerificationCallback)
time.sleep(2)


# Publish to the same topic in a loop forever
while True:
    print(""waiting.."")
    scan = waitForRFIDScan()
    print(scan)
    if(checkRFIDNumber(scan)):
        print(""RFID correct, taking photo..."")
        uploadToS3(scan)
        setup()
        greenled()
        endprogram()
    else:
        print(""Bad RFID - Access Denied"")
        setup()
        redled()
        endprogram()
</code></pre>",,0,1,,2019-03-05 14:39:02.387 UTC,,2019-03-05 16:03:03.587 UTC,2019-03-05 16:03:03.587 UTC,,10599727,,10599727,1,0,python|amazon-s3|aws-lambda|aws-sdk|aws-iot,35
google cloud vision rest api with python django,49123683,google cloud vision rest api with python django,"<p>how can i use google cloud vision with python django rest api? My task is that i have a picture,i have to find similer picture from an another picture.is there any other solution to do this task?</p>",,1,0,,2018-03-06 05:00:41.700 UTC,,2018-03-09 12:00:24.200 UTC,,,,,8829102,1,0,python-2.7|django-rest-framework|google-cloud-vision,297
Creating Dynamodb composite key,51350903,Creating Dynamodb composite key,"<p>In Aws lambda function I am storing image And My image is my primary key. But No Case is I can store same image in different function as well like. John can be part of function1 and function2 as well. So when I store in both 2nd one got remove. My table structure is which I got by doing </p>

<pre><code>aws dynamodb describe-table --table-name athlete_collection
</code></pre>

<p>Result: </p>

<pre><code>""Table"": {
        ""TableName"": ""athlete_collection"",
        ""TableSizeBytes"": 77,
        ""CreationDateTime"": 1528273139.189,
        ""ProvisionedThroughput"": {
            ""ReadCapacityUnits"": 1,
            ""NumberOfDecreasesToday"": 0,
            ""WriteCapacityUnits"": 1
        },
        ""StreamSpecification"": {
            ""StreamViewType"": ""NEW_AND_OLD_IMAGES"",
            ""StreamEnabled"": true
        },
        ""TableArn"": ""arn:aws:dynamodb:us-east-1:969213561829:table/athlete_collection"",
        ""ItemCount"": 1,
        ""TableId"": ""8cc40165-5ac2-4aa2-9db4-f06ffad8639c"",
        ""LatestStreamLabel"": ""2018-06-13T07:22:00.651"",
        ""AttributeDefinitions"": [
            {
                ""AttributeName"": ""RekognitionId"",
                ""AttributeType"": ""S""
            }
        ],
        ""KeySchema"": [
            {
                ""AttributeName"": ""RekognitionId"",
                ""KeyType"": ""HASH""
            }
        ],
        ""LatestStreamArn"": ""arn:aws:dynamodb:us-east-1:969213561829:table/athlete_collection/stream/2018-06-13T07:22:00.651"",
        ""TableStatus"": ""ACTIVE""
    }
}
</code></pre>

<p>And I made this by querying like this </p>

<pre><code>aws dynamodb create-table --table-name athlete_collection \
--attribute-definitions AttributeName=RekognitionId,AttributeType=S \
--key-schema AttributeName=RekognitionId,KeyType=HASH \
--provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1 \
--region eu-west-1
</code></pre>",,0,2,,2018-07-15 18:10:54.203 UTC,1,2018-08-02 19:50:22.767 UTC,2018-08-02 19:50:22.767 UTC,,13302,,7749560,1,0,amazon-web-services|amazon-s3|amazon-dynamodb,31
Compare images and save in dynamodb aws,50699149,Compare images and save in dynamodb aws,"<p>Hi I want to write a lambda function which will work like.  I have two folder in  s3 bucket . in  1st box there are ""owner""  and 2nd have random pictures. I want to compare all pictures with owner and then save in dynamodb with owner name against everypicture . Atm I am lost in API of face detection and doing some thing  like this </p>

<pre><code>    BUCKET = ""ais-django""
KEY = ""20180530105812.jpeg""
FEATURES_BLACKLIST = (""Landmarks"", ""Emotions"", ""Pose"", ""Quality"", ""BoundingBox"", ""Confidence"")


def detect_faces(bucket, key, attributes=['ALL'], region=""eu-west-1""):
    rekognition = boto3.client(""rekognition"", region)
    response = rekognition.detect_faces(
        Image={
            ""S3Object"": {
                ""Bucket"": bucket,
                ""Name"": key,
            }
        },
        Attributes=attributes,
    )
    return response['FaceDetails']


for face in detect_faces(BUCKET, KEY):
    print
    ""Face ({Confidence}%)"".format(**face)
    # emotions
    for emotion in face['Emotions']:
        print
        ""  {Type} : {Confidence}%"".format(**emotion)
    # quality
    for quality, value in face['Quality'].iteritems():
        print
        ""  {quality} : {value}"".format(quality=quality, value=value)
    # facial features
    for feature, data in face.iteritems():
        if feature not in FEATURES_BLACKLIST:
            print
            ""  {feature}({data[Value]}) : {data[Confidence]}%"".format(feature=feature, data=data)
</code></pre>",,1,6,,2018-06-05 11:37:45.490 UTC,,2018-06-06 14:57:52.940 UTC,,,,,9733755,1,-2,python|amazon-web-services|amazon-s3|aws-lambda,135
com.google.api.gax.grpc.ApiException: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED,43404118,com.google.api.gax.grpc.ApiException: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED,"<p>Recently,I use google cloud vision api for detecting image label ,follow <a href=""https://cloud.google.com/vision/docs/reference/libraries"" rel=""nofollow noreferrer"">this</a>
,I  set up the  credentials for my application to authenticate its identity to the service and obtain authorization to perform task,then follow the API Documentation ,write code like this:</p>

<pre><code>ImageAnnotatorClient vision= ImageAnnotatorClient.create();
        ByteString imgBytes = ByteString.readFrom(new FileInputStream(""my_image_path.jpg""));
        List&lt;AnnotateImageRequest&gt; requests = new ArrayList&lt;AnnotateImageRequest&gt;();
        Image img = Image.newBuilder().setContent(imgBytes).build();

         Feature feat = Feature.newBuilder().setType(Type.LABEL_DETECTION).build();
         AnnotateImageRequest request = AnnotateImageRequest.newBuilder()
                    .addFeatures(feat)
                    .setImage(img)
                    .build();
                requests.add(request);
                BatchAnnotateImagesResponse response = vision.batchAnnotateImages(requests);
                List&lt;AnnotateImageResponse&gt; responses = response.getResponsesList();

                for (AnnotateImageResponse res : responses) {
                    if (res.hasError()) {
                      System.out.printf(""Error: %s\n"", res.getError().getMessage());
                      return;
                    }
                    for (EntityAnnotation annotation : res.getLabelAnnotationsList()) {
                        annotation.getAllFields().forEach((k, v)-&gt;System.out.printf(""%s : %s\n"", k, v.toString()));
                      }
                }
</code></pre>

<p>I get error :</p>

<pre><code>Exception in thread ""main"" com.google.api.gax.grpc.ApiException: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED
    at com.google.api.gax.grpc.ExceptionTransformingCallable$ExceptionTransformingFuture.onFailure(ExceptionTransformingCallable.java:109)
    at com.google.api.gax.core.ApiFutures$1.onFailure(ApiFutures.java:52)
    at com.google.common.util.concurrent.Futures$6.run(Futures.java:1310)
    at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:457)
    at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156)
    at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145)
    at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:202)
    at io.grpc.stub.ClientCalls$GrpcFuture.setException(ClientCalls.java:466)
    at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:442)
    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:481)
    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:398)
    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:513)
    at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:52)
    at io.grpc.internal.SerializingExecutor$TaskRunner.run(SerializingExecutor.java:154)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED
    at io.grpc.Status.asRuntimeException(Status.java:545)
    ... 13 more
</code></pre>

<p>why?</p>",,0,0,,2017-04-14 01:44:55.613 UTC,,2017-07-05 05:13:43.607 UTC,,,,,7728544,1,4,google-cloud-vision,370
How to get textOperations and Get Result?,45680183,How to get textOperations and Get Result?,"<p>I need to do a <strong>""Post""</strong> to get the <strong>textOperations</strong> and use this received value to do a <strong>""Get""</strong> and return the results.</p>

<p>I'm doing the <strong>""Post""</strong> however I do not get anything in console.log (), how do I get this <strong>""id""</strong> received and use it in <strong>""Get"" to return the results?</strong></p>

<p><strong>The API name is:</strong></p>

<blockquote>
  <p>Microsoft Face API</p>
  
  <p>My Code:</p>
</blockquote>

<pre><code>function HandWriteenTextAPI(){

  // CHAVE DE INSCRIÇÃO DA API.
  var API_KEY = """";

  // Deve-se utilizar a mesma região em que a chave de escrição da API está
  // NOTA: As chaves de inscrições de testes são geradas na região ""Westcentralus"".
  var uriBase = ""https://westcentralus.api.cognitive.microsoft.com/vision/v1.0/recognizeText?"";

  // Solicitar Parâmetros de Retorno do JSON.
  var params = {
    ""handwriting"": ""true""
  };

  // MOSTRA A IMAGEM RECEBIDA DA URL
  var sourceImageUrl = document.getElementById(""inputURLImage"").value;
  document.querySelector(""#imageReceived"").src = sourceImageUrl;

  // Executa a chamada da API RESTFULL via AJAX.
  $.ajax({

    // Utiliza a uriBase para retornar os valores dos Parâmetros
    url: uriBase + $.param(params),

    // Solicita os Headers
    beforeSend: function(xhrObj){
      xhrObj.setRequestHeader(""Content-Type"",""application/json"");
      xhrObj.setRequestHeader(""Ocp-Apim-Subscription-Key"", API_KEY);
    },

    // Tipo do AJAX
    type: ""POST"",

    // Solicita o Body
    data: '{""url"": ' + '""' + sourceImageUrl + '""}',
  })

  .done(function(data) {
    // Recebe o JSON e transforma em um Objeto
    var objJSON = JSON.parse(JSON.stringify(data, null, 2));
    console.log(objJSON);



  })
  .fail(function(jqXHR, textStatus, errorThrown) {
    // Mostra as mensagens de Erro.
    var errorString = (errorThrown === """") ? ""Error. "" : errorThrown + "" ("" + jqXHR.status + ""): "";
    errorString += (jqXHR.responseText === """") ? """" : jQuery.parseJSON(jqXHR.responseText).message;
    alert(errorString);
  });

};
</code></pre>",,1,1,,2017-08-14 17:50:59.780 UTC,,2017-08-14 22:03:08.553 UTC,,,,user8250085,,1,0,javascript|json|face-recognition|microsoft-cognitive,69
SDKs similar to Google Cloud Vision,54439669,SDKs similar to Google Cloud Vision,"<p>I'm doing a system in C # that needs to parse an image of a keyboard returning the position of the characters in it.</p>

<p>I tried to use IBM Watson but it does not return the position of the classifications, after that I tried to use Google Cloud Vision because in the site demo it returns the positions of the characters in JSON format, however I had problems with GOOGLE_APPLICATION_CREDENTIALS (look <a href=""https://stackoverflow.com/questions/54410031/how-to-set-the-google-application-credentials-from-google-cloud-vision-in-c?noredirect=1#comment95631227_54410031"">here</a>).</p>

<p>I would like to know if there is any other alternative, preferably free or with a lot of free access, to do this kind of reading of the image and return the position of the characters?</p>

<p>I do not need OCR I want to return the position of the character in the image</p>",54440022,1,0,,2019-01-30 11:34:20.640 UTC,1,2019-01-30 11:52:20.817 UTC,,,,,10229463,1,0,google-cloud-vision|visual-recognition,35
CORS enabled in AWS Getway for Angular App,56032884,CORS enabled in AWS Getway for Angular App,"<p>I'm trying to hit this URL:</p>

<p><a href=""https://2pfwj7aw0a.execute-api.us-west-2.amazonaws.com/hackathon/rekognition?key=test.jpg"" rel=""nofollow noreferrer"">https://2pfwj7aw0a.execute-api.us-west-2.amazonaws.com/hackathon/rekognition?key=test.jpg</a></p>

<p>As you can see if you pasted it in the browser it gives you the response back. I'm creating an Angular app and when I try to hit that end point I get the famous nightmare of CORS</p>

<blockquote>
  <p>...as been blocked by CORS policy: No 'Access-Control-Allow-Origin'
  header is present on the requested resource.</p>
</blockquote>

<p>Now, I have my API in <strong>AWS API Getway</strong>. I have clicked on <strong>Enable CORS</strong> it works for my other API Resources but not this one. after clicking that I then click <strong>Deploy API.</strong></p>

<p><strong>Angular code:</strong></p>

<pre><code>getImageData() {
    return this.http.get(this.BASE_URL, {
        params: {
            key: 'test.jpg'
        }
    });
}
</code></pre>

<p><a href=""https://i.stack.imgur.com/ePp2m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ePp2m.png"" alt=""enter image description here""></a></p>",,0,0,,2019-05-08 02:53:13.407 UTC,0,2019-05-08 03:23:05.267 UTC,2019-05-08 03:23:05.267 UTC,,5262452,,5262452,1,0,angular|amazon-web-services|cors|aws-api-gateway|amazon-rekognition,27
Regular pattern error for Amazon Rekognition index-faces method calling from S3,56043593,Regular pattern error for Amazon Rekognition index-faces method calling from S3,"<p>I am following this tutorial: <a href=""https://aws.amazon.com/blogs/machine-learning/easily-perform-facial-analysis-on-live-feeds-by-creating-a-serverless-video-analytics-environment-with-amazon-rekognition-video-and-amazon-kinesis-video-streams/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/machine-learning/easily-perform-facial-analysis-on-live-feeds-by-creating-a-serverless-video-analytics-environment-with-amazon-rekognition-video-and-amazon-kinesis-video-streams/</a></p>

<p>However, when I run the command</p>

<pre><code>
        aws rekognition index-faces --image '{""S3Object"":{""Bucket"":"" 
       &lt;S3BUCKET&gt;"",""Name"":""&lt;MYFACE_KEY&gt;.jpeg""}}' --collection-id 
    ""rekVideoBlog"" -- 
       detection-attributes ""ALL"" --external-image-id ""&lt;YOURNAME&gt;"" --region 
    us- 
       west-2
</code></pre>

<p>I get the following error:</p>

<pre><code>
An error occurred (ValidationException) when calling the IndexFaces operation: 3 validation errors detected: Value '""Alex""' at 'externalImageId' failed to satisfy constraint: Member must satisfy regular expression pattern: [a-zA-Z0-9_.\-:]+; Value '""rekVideoBlog""' at 'collectionId' failed to satisfy constraint: Member must satisfy regular expression pattern: [a-zA-Z0-9_.\-]+; Value '[""ALL""]' at 'detectionAttributes' failed to satisfy constraint: Member must satisfy constraint: [Member must satisfy enum value set: [ALL, DEFAULT]]


</code></pre>

<p>I have no idea what's wrong. I replaced the single quote with double quotes and double quotes with escaped double quotes (\"") so it'd work but I got that error.</p>",,0,0,,2019-05-08 14:57:12.953 UTC,,2019-05-10 10:35:30.143 UTC,2019-05-10 10:35:30.143 UTC,,10885720,,11057247,1,0,amazon-web-services|amazon-s3|aws-sdk|amazon-rekognition,18
Lambda AWS Rekognition to DynamoDB - Error,54816799,Lambda AWS Rekognition to DynamoDB - Error,"<p>I am using <a href=""https://medium.com/@Kalefive/https-medium-com-kalefive-using-aws-rekognition-and-lambda-to-analyze-images-on-the-fly-61cd1adae0af"" rel=""nofollow noreferrer"">this tutorial</a> to link Rekognition results to a DynamoDB table. 
It is giving me this error:</p>

<pre><code>{
""errorMessage"": ""Unable to get object metadata from S3. Check object key, region and/or access permissions."",
""errorType"": ""InvalidS3ObjectException"",
""stackTrace"": [
    ""Request.extractError (/var/runtime/node_modules/aws-sdk/lib/protocol/json.js:48:27)"",
    ""Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:105:20)"",
    ""Request.emit (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:77:10)"",
    ""Request.emit (/var/runtime/node_modules/aws-sdk/lib/request.js:683:14)"",
    ""Request.transition (/var/runtime/node_modules/aws-sdk/lib/request.js:22:10)"",
    ""AcceptorStateMachine.runTo (/var/runtime/node_modules/aws-sdk/lib/state_machine.js:14:12)"",
    ""/var/runtime/node_modules/aws-sdk/lib/state_machine.js:26:10"",
    ""Request.&lt;anonymous&gt; (/var/runtime/node_modules/aws-sdk/lib/request.js:38:9)"",
    ""Request.&lt;anonymous&gt; (/var/runtime/node_modules/aws-sdk/lib/request.js:685:12)"",
    ""Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:115:18)""
]
}
</code></pre>

<p>The code used from GitHub is <a href=""https://github.com/KaleFive/Categorize"" rel=""nofollow noreferrer"">this</a>.</p>

<p>I made sure the region-name is the same for the lambda-bucket and the table.</p>

<p>I am a starter in this, so any help will be appreciated!</p>

<p>Thanks!</p>

<p>Edit:
I made some modifications and now it is giving me this:</p>

<pre><code>{
""errorMessage"": ""Requested resource not found"",
""errorType"": ""ResourceNotFoundException"",
""stackTrace"": [
    ""Request.extractError (/var/runtime/node_modules/aws-sdk/lib/protocol/json.js:48:27)"",
    ""Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:105:20)"",
    ""Request.emit (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:77:10)"",
    ""Request.emit (/var/runtime/node_modules/aws-sdk/lib/request.js:683:14)"",
    ""Request.transition (/var/runtime/node_modules/aws-sdk/lib/request.js:22:10)"",
    ""AcceptorStateMachine.runTo (/var/runtime/node_modules/aws-sdk/lib/state_machine.js:14:12)"",
    ""/var/runtime/node_modules/aws-sdk/lib/state_machine.js:26:10"",
    ""Request.&lt;anonymous&gt; (/var/runtime/node_modules/aws-sdk/lib/request.js:38:9)"",
    ""Request.&lt;anonymous&gt; (/var/runtime/node_modules/aws-sdk/lib/request.js:685:12)"",
    ""Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:115:18)""
]
</code></pre>

<p>}</p>",54830776,2,8,,2019-02-21 21:53:53.040 UTC,1,2019-02-22 15:55:11.740 UTC,2019-02-21 22:49:28.310 UTC,,1133490,,7000874,1,1,node.js|amazon-web-services|amazon-s3,72
Drawing a rectangle in an ImageView,48840806,Drawing a rectangle in an ImageView,"<p>I am trying to implement face detection using Google Mobile Vision in an Android app. In the app, I have an Imageview and a button with text, ""Process"".</p>

<p>When the button is clicked, the code connects with Google's Vision API and detects the face. After detecting the face, I am trying to draw a rectangle around the face. For that purpose I am using ""Canvas"" available in Android.</p>

<p>The error (not exactly) is I am unable to see the rectangle around the face. Here is the code in my MainActivity.java file:</p>

<pre><code>package com.startertutorials.googlefacedetect;

import android.app.Activity;
import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.graphics.Canvas;
import android.graphics.Color;
import android.graphics.Paint;
import android.graphics.PointF;
import android.graphics.drawable.BitmapDrawable;
import android.os.Bundle;
import android.util.Log;
import android.util.SparseArray;
import android.view.Menu;
import android.view.MenuItem;
import android.view.View;
import android.widget.Button;
import android.widget.ImageView;
import android.widget.Toast;

import com.google.android.gms.vision.Frame;
import com.google.android.gms.vision.face.Face;
import com.google.android.gms.vision.face.FaceDetector;
import com.google.android.gms.vision.face.Landmark;

public class MainActivity extends Activity {

    ImageView imageView;
    Button btnProcess;

    Bitmap myBitmap;
    Canvas canvas;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);

        imageView = (ImageView)findViewById(R.id.imageView);
        btnProcess = (Button)findViewById(R.id.btnProcess);

        myBitmap = BitmapFactory.decodeResource(getApplicationContext().getResources(), R.drawable.me);
        //imageView.setImageBitmap(myBitmap);

        Bitmap tempBitmap = Bitmap.createBitmap(myBitmap.getWidth(), myBitmap.getHeight(), myBitmap.getConfig());
        canvas = new Canvas(tempBitmap);

        btnProcess.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                FaceDetector faceDetector = new FaceDetector.Builder(getApplicationContext())
                        .setTrackingEnabled(false)
                        .setLandmarkType(FaceDetector.ALL_LANDMARKS)
                        .setMode(FaceDetector.FAST_MODE)
                        .build();

                if (!faceDetector.isOperational()) {
                    Toast.makeText(MainActivity.this, ""Face Detector Setup Failed!"", Toast.LENGTH_SHORT).show();
                    return;
                }

                Frame frame = new Frame.Builder().setBitmap(myBitmap).build();
                SparseArray&lt;Face&gt; sparseArray = faceDetector.detect(frame);
                for (int i = 0; i &lt; sparseArray.size(); i++) {
                    Face face = sparseArray.valueAt(i);
                    detectLandmarks(face);
                }
            }
        });
    }

    private void detectLandmarks(Face face) {
        float height = face.getHeight();
        float width = face.getWidth();
        PointF p = face.getPosition();
        float cx = p.x;
        float cy = p.y;
        drawOnImageView(cx, cy, height, width);
        /*
        for(Landmark landmark:face.getLandmarks())
        {
            int cx = (int)landmark.getPosition().x;
            int cy = (int)landmark.getPosition().y;

            drawOnImageView(landmark.getType(),cx,cy);
        }
        */
    }

    private void drawOnImageView(float cx, float cy, float height, float width) {
        Paint myPaint = new Paint();
        myPaint.setColor(Color.GREEN);
        myPaint.setStyle(Paint.Style.STROKE);
        myPaint.setStrokeWidth(2);
        canvas.drawBitmap(myBitmap, 0, 0, null);
        canvas.drawRect(0, 0, 200, 200, myPaint);
        imageView.setImageBitmap(myBitmap);
        Log.d(""Success"", ""Face detected successfully"");
    }

    @Override
    public boolean onCreateOptionsMenu(Menu menu) {
        // Inflate the menu; this adds items to the action bar if it is present.
        getMenuInflater().inflate(R.menu.menu_main, menu);
        return true;
    }

    @Override
    public boolean onOptionsItemSelected(MenuItem item) {
        // Handle action bar item clicks here. The action bar will
        // automatically handle clicks on the Home/Up button, so long
        // as you specify a parent activity in AndroidManifest.xml.
        int id = item.getItemId();

        //noinspection SimplifiableIfStatement
        if (id == R.id.action_settings) {
            return true;
        }

        return super.onOptionsItemSelected(item);
    }
}
</code></pre>

<p>The output after I click the ""Process"" button in the app is as follows:</p>

<p><a href=""https://i.stack.imgur.com/cJXWo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cJXWo.png"" alt=""enter image description here""></a></p>

<p>In the logcat of Android studio, I can see the message ""Face detected successfully"". So, I am assuming that all my code is running. But I am unable to see the rectangle.</p>

<p><strong>Please help me with displaying a rectangle first. Later I will adjust it to display around the face.</strong></p>",,1,0,,2018-02-17 11:32:28.080 UTC,,2018-02-17 13:12:04.330 UTC,2018-02-17 13:12:04.330 UTC,,2649012,,2818144,1,0,android|imageview,312
How to extract name from id card picture,44812417,How to extract name from id card picture,"<p>I'm studying how to get a person name from pictures of their id cards, considering their ids can have different layouts.</p>

<p>Using OCR services I'm able to read the text from the card, yet I'm not sure how to identify what is the person's name.</p>

<p>Using Microsoft Custom Vision I was able to train the service to identify what kind of ID card was posted, since I know all available cards I'll accept.</p>

<p>Is there a way to map each kind of card to an area, or transform to extract the area, in a way I can use OCR only on it? This way I can extract only the name.</p>

<p>OBS: I open to using any kind o service that facilitates this</p>",,0,4,,2017-06-28 20:56:42.427 UTC,,2017-06-28 20:56:42.427 UTC,,,,,7095424,1,1,computer-vision|ocr,426
Compare faces on device,48749859,Compare faces on device,"<p>My users need to be able to authenticate themselves using a picture.
So that when they create a account on the phone a picture is selected and saved.
When they log in a <code>UIImagePickerController()</code> with <code>.sourceType = .camera</code> should take a picture and compare it to the saved picture. <br>I've found a possible duplicate <a href=""https://stackoverflow.com/questions/27937637/compare-faces-in-ios"">Link</a>, but this is very old and not really relevant anymore since the introduction of ARKit and Vision..<br>
I has to be done locally so <a href=""https://stackoverflow.com/questions/46483447/how-to-use-aws-rekognition-to-compare-face-in-swift-3"">Amazon Rekognition</a> is unfortunately not option, the same goes for <a href=""https://github.com/Microsoft/Cognitive-Face-iOS"" rel=""nofollow noreferrer"">Microsoft Cognitive Services</a> &amp; <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">Google's Cloud Vision API</a>.
<br> 
The Vision Frameworks has a <code>VNFaceObservation</code> class, that can detect faces, but I don't know how one would compare that to a saved one.</p>",48775339,1,1,,2018-02-12 15:27:30.590 UTC,,2018-05-29 21:23:17.060 UTC,2018-05-29 21:23:17.060 UTC,,6599590,,2853734,1,0,ios|authentication|arkit|apple-vision|facial-identification,271
Concatenate php array elements with url and then pass to ajax API request,48865892,Concatenate php array elements with url and then pass to ajax API request,"<p>I am currently working on a project which get the images(scandir by php) from the server(<a href=""http://SERVER_IP_ADDRESS/snap/"" rel=""nofollow noreferrer"">http://SERVER_IP_ADDRESS/snap/</a>) and then pass it to Microsoft Emotion API for emotion analysis. </p>

<p>However, after I pass the concatenated url to the ajax request, I got below error:</p>

<pre><code>Error: {""readyState"":4,""responseText"":""{\""error\"":
{\""code\"":\""BadBody\"",\""message\"":\""JSON parsing error.\""}}"",""status"":400,""statusText"":""Bad Request""}
</code></pre>

<p>My code is like this: </p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;JSSample&lt;/title&gt;
    &lt;script src=""http://ajax.googleapis.com/ajax/libs/jquery/1.9.0/jquery.min.js""&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;h2&gt;Face Rectangle&lt;/h2&gt;
&lt;ul id=""faceRectangle""&gt;
&lt;!-- Will populate list with response content --&gt;
&lt;/ul&gt;

&lt;h2&gt;Emotions&lt;/h2&gt;
&lt;ul id=""scores""&gt;
&lt;!-- Will populate list with response content --&gt;
&lt;/ul&gt;

&lt;body&gt;

&lt;script type=""text/javascript""&gt;

&lt;?php
    $dir    = ""snap"";
    $files2 = scandir($dir, 1); 
?&gt;
    var images = &lt;?php echo json_encode(array_slice($files2, 2), JSON_FORCE_OBJECT); ?&gt;;
//Say I want to get image no.221
    var info = ""http://SERVER_IP_ADDRESS/snap/"" + images[221]; 

    $(function() {
        // No query string parameters for this API call.
        var params = { };

        $.ajax({
            url: ""https://westus.api.cognitive.microsoft.com/emotion/v1.0/recognize?"" + $.param(params),
            beforeSend: function(xhrObj){
                // Request headers, also supports ""application/octet-stream""
                xhrObj.setRequestHeader(""Content-Type"",""application/json"");

                // NOTE: Replace the ""Ocp-Apim-Subscription-Key"" value with a valid subscription key.
                xhrObj.setRequestHeader(""Ocp-Apim-Subscription-Key"",""YOUR_KEY"");
            },
            type: ""POST"",
            // Request body
            data: '{""url"": info}',
        }).done(function(data) {
            // Get face rectangle dimensions
            var faceRectangle = data[0].faceRectangle;
            var faceRectangleList = $('#faceRectangle');

            // Append to DOM
            for (var prop in faceRectangle) {
                faceRectangleList.append(""&lt;li&gt; "" + prop + "": "" + faceRectangle[prop] + ""&lt;/li&gt;"");
            }

            // Get emotion confidence scores
            var scores = data[0].scores;
            var scoresList = $('#scores');

            // Append to DOM
            for(var prop in scores) {
                scoresList.append(""&lt;li&gt; "" + prop + "": "" + scores[prop] + ""&lt;/li&gt;"")
            }
        }).fail(function(err) {
            alert(""Error: "" + JSON.stringify(err));
        });
    });
    //Show the variables
    document.write(images[221]);
    document.write(info);
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>I tried to remove ""JSON_FORCE_OBJECT"" in json_encode while the same error shown.</p>",48869045,1,7,,2018-02-19 12:07:03.560 UTC,,2018-02-19 15:13:55.710 UTC,,,,,9284327,1,0,php|json|ajax|api|microsoft-cognitive,52
gcloud project id in node.js error is different from gcloud set project id?,55831279,gcloud project id in node.js error is different from gcloud set project id?,"<p>I'm trying to get Google Cloud Vision to work with node.js by following their documentation <a href=""https://cloud.google.com/vision/docs/quickstart-client-libraries#client-libraries-usage-nodejs"" rel=""nofollow noreferrer"">here</a>. Although I keep getting:</p>

<blockquote>
  <p>PERMISSION_DENIED: Cloud Vision API has not been used in project 5678.. before or it is disabled. Enable it by visiting <a href=""https://console.developers.google.com/apis/api/vision.googleapis.com/overview?project=5678"" rel=""nofollow noreferrer"">https://console.developers.google.com/apis/api/vision.googleapis.com/overview?project=5678</a>.. then retry</p>
</blockquote>

<p>To note though <strong>the project number is very different from what I see in gcloud's output</strong> when I gather information from the following commands: </p>

<pre><code>gcloud info |tr -d '[]' | awk '/project:/ {print $2}'
</code></pre>

<blockquote>
  <p>'my-set-project' &lt;=== set project id in use</p>
</blockquote>

<pre><code>gcloud projects list
</code></pre>

<p>which outputs:</p>

<blockquote>
  <p>PROJECT_ID='my-set-project' // &lt;=== Same id as ""gcloud info"" command<br>
  NAME='my-project-name'<br>
  PROJECT_NUMBER=1234.. // &lt;===== Different number from Node.js Error</p>
</blockquote>

<p>I have already enabled the api, downloaded a service key and setup the export GOOGLE_APPLICATION_CREDENTIALS=[path/to/my/service/key]. But right now I believe that the service key linkup is not the issue yet <strong>as I have not yet really have had gcloud pointing to 'my-set-project'</strong>.</p>

<p>I have also found a default.config at </p>

<pre><code>cat /Users/My_Username/.config/gcloud/application_default_credentials.json
</code></pre>

<p>which has:</p>

<blockquote>
  <p>{
    ""client_id"": ""5678..-fgrh // &lt;=== same number id as node.js error</p>
</blockquote>

<p>So how can I get gcloud-cli to switch to project ""1234"" which has the API enabled there? I thought doing the command:</p>

<pre><code>gcloud config set project 'my-set-project'
</code></pre>

<p>would get running node apps using gcp to use the project of '1234' instead of the default '5678'. Any help will be appreciated as I'm still getting used to the gcloud-cli. Thanks</p>",,0,0,,2019-04-24 13:30:13.780 UTC,,2019-04-24 13:30:13.780 UTC,,,,,3859456,1,1,google-cloud-platform|gcloud|google-cloud-vision,27
Google Cloud Vision not recognizing Arabic Characters,53457755,Google Cloud Vision not recognizing Arabic Characters,"<p>Hi I am trying Google Cloud Vision , to detect character and words in Arabic language from image. But when i try it gives me result in matching them with english: </p>

<p>Request code is as below: </p>

<pre><code>{
  ""requests"": [
{
  ""features"": [
    {
      ""type"": ""TEXT_DETECTION""
    }
  ],
  ""image"": {
    ""source"": {
      ""imageUri"": ""gs://dummy/noon-1.png""
    }
  },
  ""imageContext"": {
    ""languageHints"": [
      ""ar""
    ]
  }
}
]
}
</code></pre>",,1,1,,2018-11-24 11:42:26.650 UTC,,2018-11-26 14:08:51.613 UTC,,,,,4722857,1,0,google-cloud-vision,101
How to Properly Authenticate Google Vision API Using Polymer,55263570,How to Properly Authenticate Google Vision API Using Polymer,"<p>I am trying to run a test on the <code>Google Cloud Vision API</code> to see how it fares to the client side <code>Shape Detection API</code>.</p>

<p>I am hoping to <code>POST</code> JSON with a base64 encoded image and get image text and barcodes returned.</p>

<p>I have created a <code>GCP</code> project and API key per the tutorial at (<a href=""https://cloud.google.com/vision/docs/before-you-begin"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/before-you-begin</a>), but am getting an 401 error when trying to make requests. </p>

<blockquote>
  <p>error: {code: 401,…}<br>
  code: 401<br>
  message: ""Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential. See <a href=""https://developers.google.com/identity/sign-in/web/devconsole-project"" rel=""nofollow noreferrer"">https://developers.google.com/identity/sign-in/web/devconsole-project</a>.""<br>
  status: ""UNAUTHENTICATED""</p>
</blockquote>

<p>The request is written in Polymer 2.x as follows:</p>

<pre><code>&lt;iron-ajax id=""googleApi"" 
  body=""[[request]]"" 
  content-type=""application/json"" 
  handle-as=""json""
  headers$='{""Authorization"": ""Bearer [[GOOGLE_API_KEY]]""}' 
  last-response=""{{response}}"" 
  loading=""{{loading}}""
  method=""post"" 
  url=""https://vision.googleapis.com/v1/images:annotate""&gt;
&lt;/iron-ajax&gt;
</code></pre>

<p>...</p>

<pre><code>GOOGLE_API_KEY: {
  type: String,
  value: 'AIza0101010110100101101010'
}
</code></pre>

<p>...</p>

<pre><code>getRequest(image) {
  let encoded = image.toString('base64');
  this.request = {
    ""requests"": [{
      ""image"": {
        ""content"": encoded
      },
      ""features"": [{
        ""type"": ""LABEL_DETECTION"",
        ""maxResults"": 1
      }]
    }]
  };
  let request = this.$.googleApi.generateRequest();
  request.completes.then(req =&gt; {
    console.log('submission complete');
    console.log(this.response);
  })
  .catch(error =&gt; {
    console.log(error);
  })
}
</code></pre>

<p>How do I resolve this authentication error?  </p>

<p>It is an account admin issue? Improperly formatted code?</p>",55264083,1,0,,2019-03-20 14:45:51.550 UTC,,2019-03-20 15:08:12.210 UTC,,,,,9169546,1,0,polymer-2.x|google-apis-explorer,39
What color space is being used in Image Properties detection-Dominant Colors?,55500377,What color space is being used in Image Properties detection-Dominant Colors?,"<p>I'm using Google Cloud Vision API to detect dominant colors in images for a personal project. As shown below, Vision API returned RGBA values, pixel fractions, and scores for each image I tested. I was wondering why Alpha values are always missing, and in what color space (sRGB, AdobeRGB, Apple RGB, etc.) should the RGBA values make most sense?</p>

<p>{""colors"": [{""color"": {""red"": 196, ""green"": 193, ""blue"": 193}, ""score"": 0.37683305, ""pixelFraction"": 0.013152561}, {""color"": {""red"": 237, ""green"": 235, ""blue"": 234}, ""score"": 0.3126285, ""pixelFraction"": 0.97964054},</p>",,0,0,,2019-04-03 16:52:16.150 UTC,1,2019-04-03 16:52:16.150 UTC,,,,,11306849,1,1,google-cloud-vision,50
Resource not found Face API 1.0,43410910,Resource not found Face API 1.0,"<p>for the love of my life I just can not figure out why my jason format are all ways wrong , I am using Microsoft Face API 1.0 to create a person within the group</p>

<p>here is my code </p>

<pre><code>protected async void btnAddFaces_Click(object sender, EventArgs e)
{
        var client = new HttpClient();
        var queryString = HttpUtility.ParseQueryString(string.Empty);
        string personGroupId = txtFriendList.Text.ToLower();
        string persons = txtfriendName.Text.ToLower();
        // Request headers
        client.DefaultRequestHeaders.Add
         (""Ocp-Apim-Subscription-Key"", ""{YourKey}"");

        // Request parameters

        var uri = ""https://westus.api.cognitive.microsoft.com/face/v1.0/
         persongroups/wowlist/persons?"" + queryString;
        // HttpResponseMessage response;
        // not sure but I think here is my problem 
        string body = ""{\""name\"":\"""" + ""waheed"" + "","" + ""\""}"";
        // Request body
        using (var content = new StringContent
        (body, Encoding.UTF8, ""application/json""))
        {

            await client.PutAsync(uri, content)
                .ContinueWith(async responseTask =&gt;
                {
                    var responseBody = await responseTask.Result
                    .Content.ReadAsStringAsync();
                    txtFaceList.Text = responseBody.ToString();

                });
        }// end of using statement 
}
</code></pre>

<p>what should happen is a HTTP verb status OK 200 should be return back , all I get is </p>

<pre><code>{ ""error"": { ""code"": ""ResourceNotFound"", ""message"": ""The requested resource was not found."" } } 
</code></pre>

<p>I look at my previous post apply the same approach and it just does not work. can someone point me to the correct direction other than jumping off the roof.</p>

<p>thanks </p>",43421694,1,0,,2017-04-14 11:47:40.173 UTC,,2017-04-15 02:54:36.253 UTC,2017-04-14 12:03:33.717 UTC,,383710,,5609613,1,1,c#|microsoft-cognitive,530
Android ExifInterface Horizontal Flip Undefined,44167057,Android ExifInterface Horizontal Flip Undefined,"<p>After taking a picture using the Google Vision Library (<a href=""https://developers.google.com/android/reference/com/google/android/gms/vision/CameraSource"" rel=""nofollow noreferrer"">CameraSource</a>) I grab the orientation from Exif data using</p>

<pre><code>int orientationValue = exifInterface.getAttributeInt(ExifInterface.TAG_ORIENTATION, ExifInterface.ORIENTATION_NORMAL);
</code></pre>

<p>This is giving me the rotation of the camera at the time the image was taken however I can't get information on which camera was being used (front or back). There is a warning in the logs from the ExifInterface stating the following:</p>

<pre><code>W/ExifInterface: Skip the tag entry since tag number is not defined: 2
</code></pre>

<p>However ExifInterface defines 2 as <code>public static final int ORIENTATION_FLIP_HORIZONTAL = 2;</code>.</p>

<p>Why is this information not being parsed? Is there a different tag I need to use to get this information? The images are also not horizontally flipped when viewing in the gallery.</p>",,0,0,,2017-05-24 19:33:29.317 UTC,,2017-05-24 19:33:29.317 UTC,,,,,989757,1,2,android|android-camera|exif,398
"How to set multiple region for AWSServiceManager to use Lex,Rek,S3 Swift",51399511,"How to set multiple region for AWSServiceManager to use Lex,Rek,S3 Swift","<p>I can't able to set multiple region for AWS Service Manager. 
(Why multiple region? because S3,rekognition->APSoutheast2, Lex -> USWest1.)</p>

<p>When I have used Face Rekognition other Lex always worked on APSoutheast2 region. Check below image. Its seems like able set default only once. How to set for different purpose of using.</p>

<p>PS: Info plist configuration also not taking here.Thanks in Advance.</p>

<p><a href=""https://i.stack.imgur.com/QoCQz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QoCQz.png"" alt=""enter image description here""></a></p>",51486763,1,0,,2018-07-18 10:20:19.803 UTC,1,2018-07-23 20:27:21.323 UTC,2018-07-18 11:34:03.277 UTC,,8368982,,8368982,1,0,ios|swift|amazon-web-services|aws-sdk,95
"When settling multiple async promises in Guzzle, I'm only able to get result from the first promise",51958450,"When settling multiple async promises in Guzzle, I'm only able to get result from the first promise","<p>I've the following code, where I'm creating an array of Promises and then settling them:</p>

<pre><code>$processed = $this-&gt;process_pins( $pins-&gt;data, $board_id );
foreach ( $processed as $key =&gt; $result ) {
        var_dump($result);;
}

public function process_pins( array $pins, int $wp_id ) {

    $credentials = new Credentials( AMAZON_ACCESS_KEY, AMAZON_ACCESS_SECRET );

    $client = new LambdaClient( [
        'version'     =&gt; 'latest',
        'region'      =&gt; AMAZON_ACCESS_REGION,
        'credentials' =&gt; $credentials,
    ] );

    $promises = [];

    foreach ( $pins as $pin ) {
        $promise = $client-&gt;invokeAsync( [
            // The name your created Lambda function
            'FunctionName' =&gt; 'processRekognition',
            'Payload'      =&gt; json_encode( [ 'image_url' =&gt; $pin-&gt;image-&gt;original-&gt;url ] ),
        ] );
        $promise-&gt;then(
            function ( $result ) use ( $wp_id, $pin, &amp;$promise ) {
                $meta                   = get_post_meta( $wp_id, $pin-&gt;id, true );
                $meta['status']         = PIN_PROCESSING_STATUS_COMPLETE;
                $meta['processed_data'] = wp_json_encode( (string) $result-&gt;get( 'Payload' ) );
                update_post_meta( $wp_id, $pin-&gt;id, $meta );
                $promise-&gt;resolve( (string) $result-&gt;get( 'Payload' ) );
            },
            function ( $reason ) use ( $wp_id, $pin, &amp;$promise ) {
                $meta                   = get_post_meta( $wp_id, $pin-&gt;id, true );
                $meta['status']         = PIN_PROCESSING_STATUS_ERROR;
                $meta['processed_data'] = $reason;
                update_post_meta( $wp_id, $pin-&gt;id, $meta );
                $promise-&gt;reject( $reason );
            }
        );

        $promises[ $pin-&gt;id ] = $promise;
    }

    return Promise\settle( $promises )-&gt;wait();
}
</code></pre>

<p>What I would expect is that <code>$result</code> would always be an array with a 'stat' and a 'value', but I get that only the for the first result, all the other are Response objects.</p>

<p>This is what is shown from the var_dump:</p>

<pre><code>array(2) {
  [""state""]=&gt;
  string(9) ""fulfilled""
  [""value""]=&gt;
  string(94) ""[""tag1"",""tag2"",""https://i.pinimg.com/originals/3e/b2/f0/3eb2f0fbb5d7f357bf9c7f8363957cc6.png""]""
}
array(2) {
  [""state""]=&gt;
  string(9) ""fulfilled""
  [""value""]=&gt;
  object(Aws\Result)#1028 (1) {
    [""data"":""Aws\Result"":private]=&gt;
    array(6) {
      [""Payload""]=&gt;
      object(GuzzleHttp\Psr7\Stream)#1209 (7) {
        [""stream"":""GuzzleHttp\Psr7\Stream"":private]=&gt;
        resource(85) of type (stream)
        [""size"":""GuzzleHttp\Psr7\Stream"":private]=&gt;
        NULL
        [""seekable"":""GuzzleHttp\Psr7\Stream"":private]=&gt;
        bool(true)
        [""readable"":""GuzzleHttp\Psr7\Stream"":private]=&gt;
        bool(true)
        [""writable"":""GuzzleHttp\Psr7\Stream"":private]=&gt;
        bool(true)
        [""uri"":""GuzzleHttp\Psr7\Stream"":private]=&gt;
        string(10) ""php://temp""
        [""customMetadata"":""GuzzleHttp\Psr7\Stream"":private]=&gt;
        array(0) {
        }
      }
      [""StatusCode""]=&gt;
      int(200)
      [""FunctionError""]=&gt;
      string(0) """"
      [""LogResult""]=&gt;
      string(0) """"
      [""ExecutedVersion""]=&gt;
      string(7) ""$LATEST""
      [""@metadata""]=&gt;
      array(4) {
        [""statusCode""]=&gt;
        int(200)
        [""effectiveUri""]=&gt;
        string(90) ""https://lambda.us-west-2.amazonaws.com/2015-03-31/functions/processRekognition/invocations""
        [""headers""]=&gt;
        array(8) {
          [""date""]=&gt;
          string(29) ""Wed, 22 Aug 2018 00:35:24 GMT""
          [""content-type""]=&gt;
          string(16) ""application/json""
          [""content-length""]=&gt;
          string(2) ""94""
          [""connection""]=&gt;
          string(10) ""keep-alive""
          [""x-amzn-requestid""]=&gt;
          string(36) ""424350ab-a5a3-11e8-95f4-6d4c60dd39bb""
          [""x-amzn-remapped-content-length""]=&gt;
          string(1) ""0""
          [""x-amz-executed-version""]=&gt;
          string(7) ""$LATEST""
          [""x-amzn-trace-id""]=&gt;
          string(50) ""root=1-5b7cafcc-613f1f7fec4d9ca77ad86773;sampled=0""
        }
        [""transferStats""]=&gt;
        array(1) {
          [""http""]=&gt;
          array(1) {
            [0]=&gt;
            array(0) {
            }
          }
        }
      }
}
</code></pre>

<p>}
}</p>",,1,0,,2018-08-22 00:41:41.823 UTC,,2018-08-29 19:16:52.070 UTC,2018-08-22 00:47:29.020 UTC,,397861,,397861,1,1,php|asynchronous|guzzle,221
Amazon Rekognition for Video - getFaceSearch: index number,50360498,Amazon Rekognition for Video - getFaceSearch: index number,"<p>I’m new using <strong>Amazon Rekognition</strong> to analyze faces on a video. </p>

<p>I’m using <em>startFaceSearch</em> to start my analysis. After the job is completed successfully, I’m using the JobId generated to call <em>getFaceSearch</em>. </p>

<p>On my first video analyzed, the results were as expected. But when I analyze the second example some strange behavior occurs and I can’t understand why. </p>

<p>Viewing the JSON generated as results for my second video, completely different faces are identified with the same <em>index number</em>.</p>

<p>Please see the results below.   </p>

<pre><code>{
    ""Timestamp"": 35960,
    ""Person"": {
        ""Index"": 11,
        ""BoundingBox"": {
            ""Width"": 0.09375,
            ""Height"": 0.24583333730698,
            ""Left"": 0.1875,
            ""Top"": 0.375
        },
        ""Face"": {
            ""BoundingBox"": {
                ""Width"": 0.06993006914854,
                ""Height"": 0.10256410390139,
                ""Left"": 0.24475525319576,
                ""Top"": 0.375
            },
            ""Landmarks"": [
                {
                    ""Type"": ""eyeLeft"",
                    ""X"": 0.26899611949921,
                    ""Y"": 0.40649232268333
                },
                {
                    ""Type"": ""eyeRight"",
                    ""X"": 0.28330621123314,
                    ""Y"": 0.41610333323479
                },
                {
                    ""Type"": ""nose"",
                    ""X"": 0.27063181996346,
                    ""Y"": 0.43293061852455
                },
                {
                    ""Type"": ""mouthLeft"",
                    ""X"": 0.25983560085297,
                    ""Y"": 0.44362303614616
                },
                {
                    ""Type"": ""mouthRight"",
                    ""X"": 0.27296212315559,
                    ""Y"": 0.44758656620979
                }
            ],
            ""Pose"": {
                ""Roll"": 22.106262207031,
                ""Yaw"": 6.3516845703125,
                ""Pitch"": -6.2676968574524
            },
            ""Quality"": {
                ""Brightness"": 41.875026702881,
                ""Sharpness"": 65.948883056641
            },
            ""Confidence"": 90.114051818848
        }
    }
}

{
    ""Timestamp"": 46520,
    ""Person"": {
        ""Index"": 11,
        ""BoundingBox"": {
            ""Width"": 0.19034090638161,
            ""Height"": 0.42083331942558,
            ""Left"": 0.30681818723679,
            ""Top"": 0.17916665971279
        },
        ""Face"": {
            ""BoundingBox"": {
                ""Width"": 0.076486013829708,
                ""Height"": 0.11217948794365,
                ""Left"": 0.38680067658424,
                ""Top"": 0.26923078298569
            },
            ""Landmarks"": [
                {
                    ""Type"": ""eyeLeft"",
                    ""X"": 0.40642243623734,
                    ""Y"": 0.32347011566162
                },
                {
                    ""Type"": ""eyeRight"",
                    ""X"": 0.43237379193306,
                    ""Y"": 0.32369664311409
                },
                {
                    ""Type"": ""nose"",
                    ""X"": 0.42121160030365,
                    ""Y"": 0.34618207812309
                },
                {
                    ""Type"": ""mouthLeft"",
                    ""X"": 0.41044121980667,
                    ""Y"": 0.36520344018936
                },
                {
                    ""Type"": ""mouthRight"",
                    ""X"": 0.43202903866768,
                    ""Y"": 0.36483728885651
                }
            ],
            ""Pose"": {
                ""Roll"": 0.3165397644043,
                ""Yaw"": 2.038902759552,
                ""Pitch"": -1.9931464195251
            },
            ""Quality"": {
                ""Brightness"": 54.697460174561,
                ""Sharpness"": 53.806159973145
            },
            ""Confidence"": 95.216400146484
        }
    }
}
</code></pre>

<p>In fact, in this video, all faces have the same index number, regardless of they are different. Any suggestions?</p>",,1,5,,2018-05-15 23:30:09.330 UTC,,2018-05-18 11:46:53.523 UTC,2018-05-15 23:53:16.710 UTC,,174777,,6856150,1,0,amazon-web-services|amazon-rekognition,88
Problems with POST to Google Cloud Vision via Javascript,44862532,Problems with POST to Google Cloud Vision via Javascript,"<p>I'm attempting to make a very simple POST to Google Cloud Vision API via javascript with jquery. Testing in Chrome, I get a 400 error via the console and no further info to help in debugging. I'm hoping somebody out there has worked with Cloud Vision before or can at least see that I'm doing something obviously wrong here, say with formatting the request body (data). The entire test html / javascript below:</p>

<pre><code>&lt;html&gt;&lt;head&gt;
&lt;script type=""text/javascript"" src=""https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js""&gt;&lt;/script&gt;
&lt;script type=""text/javascript"" src=""https://cdnjs.cloudflare.com/ajax/libs/json2/20160511/json2.js""&gt;&lt;/script&gt;
&lt;script type=""text/javascript""&gt;

var p = {""requests"":[{  ""image"":{    ""source"":{""imageUri"":""https://cloud.google.com/vision/docs/images/car.png""}}  ,  ""features"": [{""type"":""LABEL_DETECTION"",""maxResults"":3}]    } ]};

$.ajax({
type: ""POST"",
url: ""https://vision.googleapis.com/v1/images:annotate?key=APIKEY"",
data: JSON.stringify(p),

headers: {
  ""Content-Type"": ""application/json"",
},

dataType: ""json"",   
success: function(data, textStatus, jqXHR) {
  alert(data);
}

});

&lt;/script&gt;
&lt;/head&gt;&lt;/html&gt;
</code></pre>

<p>I've been using the following docs for help:
<a href=""https://cloud.google.com/vision/docs/detecting-labels"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/detecting-labels</a>, to no avail.</p>

<p>FYI, I've tried the shorthand too, but no worky, same error:</p>

<pre><code>var p = {""requests"":[{  ""image"":{    ""source"":{""imageUri"":""https://cloud.google.com/vision/docs/images/car.png""}}  ,  ""features"": [{""type"":""LABEL_DETECTION"",""maxResults"":3}]    } ]};
$.post( ""https://vision.googleapis.com/v1/images:annotate?key=APIKEY"", JSON.stringify(p) , function(data) { alert(data); }       );
</code></pre>",,1,1,,2017-07-01 15:21:38.207 UTC,1,2017-07-02 00:20:07.573 UTC,2017-07-02 00:20:07.573 UTC,,4891910,,4891910,1,0,javascript|json|post|google-cloud-vision,983
Can't print google's vision API results to app screen,48709133,Can't print google's vision API results to app screen,"<p>I'm using Google's Vision API to identify certain features in an image. I have the Logo Detection working as the logo comes up in my terminal, but I can't get it to appear on my app screen. It continually prints ""No logos found"" - here's my code :</p>

<pre><code>    //Get logo annotations
            let logoAnnotations: JSON = logoResponses[""logoAnnotations""]
            let numLogos: Int = logoAnnotations.count
            var logos: Array&lt;String&gt; = []
            if numLogos &gt; 0 {
                var allResultsText:String = ""Logos: ""
                for index in 0..&lt;numLogos {
                    let logo = logoAnnotations[index][""logo""].stringValue
                    logos.append(logo)
                }
                for logo in logos {
                    if logos[logos.count - 1] != logo {
                        allResultsText += ""\(logo), ""
                    } else {
                        allResultsText += ""\(logo).""
                    }
                }
                self.allResults.text = allResultsText
            } else {
                self.allResults.text = ""No logos found""
            }
        }
</code></pre>

<p>This is the JSON response I'm getting:</p>

<pre><code>[
  {
    ""boundingPoly"": {
      ""vertices"": [
        {
          ""x"": 210,
          ""y"": 139
        },
        {
          ""x"": 229,
          ""y"": 139
        },
        {
          ""x"": 229,
          ""y"": 179
        },
        {
          ""x"": 210,
          ""y"": 179
        }
      ]
    },
    ""mid"": ""/m/04lg33"",
    ""score"": 0.18314756,
    ""description"": ""Ralph Lauren Corporation""
  }
]
</code></pre>

<p>How am I to access the value returned for the logo description, this case Ralph Lauren Corporation?</p>",49134343,1,0,,2018-02-09 15:31:45.660 UTC,,2018-03-06 15:24:58.993 UTC,2018-02-12 10:37:03.377 UTC,,4029561,,9079867,1,1,swift|swifty-json|google-vision,57
google cloud vision full text detection outputs words with no space in between,51219570,google cloud vision full text detection outputs words with no space in between,"<p>Google cloud vision full document detection outputs words with no space in between.</p>

<p>Appreciate the help.</p>",,0,0,,2018-07-07 03:06:02.787 UTC,,2018-07-14 17:36:55.727 UTC,2018-07-14 17:36:55.727 UTC,,9887326,,9887326,1,3,text|detection|google-vision,143
How can I use scan QR Code with a camera inside the layout (like Whatsapp Web scanner),46046085,How can I use scan QR Code with a camera inside the layout (like Whatsapp Web scanner),"<p>I'm developer but new in Android and I need to know how can I use camera inside a Activity Layout.</p>

<p>I know I need to use Surface View to insert camera inside an activity and currently my app is reading QR Codes with Google Vision using default camera (a button opens camera, the user takes the photo and my app perceive the activity result).</p>

<p>But I really need to implementation that function inside app with real-time scanner.</p>

<p>Someone can direct me?</p>",46047514,2,0,,2017-09-05 02:12:55.543 UTC,1,2017-09-05 05:25:02.773 UTC,,,,,5937939,1,0,android|camera|qr-code,945
How to Improve OCR on image with text in different colors and fonts?,51803569,How to Improve OCR on image with text in different colors and fonts?,"<p>I'm using the <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">Google Vision API</a> to extract the text from some pictures, however, I have been trying to improve the accuracy (confidence) of the results with no luck.</p>

<p>every time I change the image from the original I lose accuracy in detecting some characters.</p>

<p>I have isolated the issue to have multiple colors for different words with can be seen that words in red for example have incorrect results more often than the other words.</p>

<p>Example:</p>

<p>some variations on the image from gray scale or b&amp;w</p>

<p><a href=""https://i.stack.imgur.com/QiSKN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QiSKN.png"" alt=""Original Image""></a></p>

<p><a href=""https://i.stack.imgur.com/1CdMH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1CdMH.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/yhXbS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yhXbS.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/2pi7p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2pi7p.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/ANjd0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ANjd0.png"" alt=""enter image description here""></a></p>

<p>What ideas can I try to make this work better, specifically changing the colors of text to a uniform color or just black on a white background since most algorithms expect that?</p>

<p>some ideas I already tried, also some thresholding. </p>

<pre><code>dimg = ImageOps.grayscale(im)
cimg = ImageOps.invert(dimg)

contrast = ImageEnhance.Contrast(dimg)
eimg = contrast.enhance(1)

sharp = ImageEnhance.Sharpness(dimg)
eimg = sharp.enhance(1)
</code></pre>",,5,3,,2018-08-11 20:38:15.330 UTC,,2018-08-20 22:17:07.120 UTC,,,,,2056905,1,4,python|python-imaging-library|ocr|google-vision,1790
Where condition in dynamodb rekognition.search_faces_by_image function,50996222,Where condition in dynamodb rekognition.search_faces_by_image function,"<p>Hi I am getting database data in this code like this </p>

<pre><code>response = rekognition.search_faces_by_image(
                        CollectionId='athlete_collection',
                        Image={'Bytes': image_crop_binary}
                    )
</code></pre>

<p>I have a column named <code>event</code> in <code>athlete_collection</code>  so I want to put a condition that just get data where  event = 'newevent'</p>",50999093,1,0,,2018-06-22 22:18:42.587 UTC,,2018-06-23 07:45:02.280 UTC,,,,,7958560,1,0,amazon-web-services|amazon-s3|amazon-dynamodb|amazon-rekognition,49
AWS S3 - Could not connect to the end point url,45792942,AWS S3 - Could not connect to the end point url,"<p>i'm trying to use the recognition service from aws. Image was successfully taken and uploaded to S3. However unable to do the recognition due to some end point url. I check <a href=""https://stackoverflow.com/questions/40409683/aws-s3-cli-could-not-connect-to-the-endpoint-url"">this</a>, and my region was correct. </p>

<p>Error Message:<code>botocore.exceptions.EndpointConnectionError: Could not connect to the endpoint URL: ""https://rekognition.us-east-2.amazonaws.com/""
</code></p>

<p>Look through the <a href=""http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region"" rel=""nofollow noreferrer"">doc</a> know that my end point should be something like <code>s3-us-east-2.amazonaws.com</code>. </p>

<p>Sorry I am new to this, please feel free to point out any mistake had made. Thanks in advance</p>

<p><strong>python</strong></p>

<pre><code>def detect_labels(bucket, key, max_labels=10, min_confidence=90, region=""us-
east-2""):
  rekognition = boto3.client(""rekognition"",region) #tried change rekognition to s3
  response = rekognition.detect_labels(
    Image={
      ""S3Object"": { 
        ""Bucket"": bucket,
        ""Name"": key,
      }
      },
    MaxLabels=max_labels,
    MinConfidence=min_confidence,
  ) 
  return response['Labels']

takePhoto(file_path, file_name)
uploadToS3(file_path,file_name, BUCKET,location)
for label in detect_labels(BUCKET,file_name):
  print(""{Name} - {Confidence}%"".format(**label))
</code></pre>",45803765,1,0,,2017-08-21 08:53:27.020 UTC,,2017-08-21 18:45:09.173 UTC,,,,,5860143,1,0,python|amazon-web-services|amazon-s3,1511
Min3d not working with device camera,44558999,Min3d not working with device camera,"<p>I am doing research on Face detection.
I did this using Google Vision api.
Now I want to load 3D modal(.obj file) using min3D.
I was successful in loading 3D modal.</p>

<p>Problem:I am not able to load 3d objects from min3D when camera is being used in app.</p>

<p>Thanks.</p>",,0,0,,2017-06-15 05:10:07.110 UTC,,2017-06-15 05:10:07.110 UTC,,,,,8164084,1,1,android|face-detection|min3d,27
Basic Coke Logos not recognized in the distance,51737440,Basic Coke Logos not recognized in the distance,"<p>I cannot get Google Vision to detect Coke logos where they are less than 10% of the screen. Logo is approximately 200x30 but it is still pretty clearly discernible to a human eye. Visa logo next to it is a bit bigger and cannot be detected as well. </p>

<p>Anyone knows what is the minimum size for logo detection? These ones are easily recognized by mxnet.</p>

<p>I am using the regular sample code to detect it:</p>

<pre><code>client = vision.ImageAnnotatorClient()

with io.open(""tmp/""+filename, 'rb') as image_file:
    content = image_file.read()

image = vision.types.Image(content=content)

response = client.logo_detection(image=image)
logos = response.logo_annotations
print('Logos:')

for logo in logos:
    print(logo.description)
</code></pre>

<p>here is a sample image: <a href=""https://imgur.com/a/giXjpVy"" rel=""nofollow noreferrer"">https://imgur.com/a/giXjpVy</a></p>",,1,0,,2018-08-08 01:31:39.333 UTC,,2018-08-08 12:57:35.873 UTC,,,,,10189040,1,0,google-cloud-vision,28
Special characters which are identified as individual word in google Vision OCR?,52829583,Special characters which are identified as individual word in google Vision OCR?,"<p>I was trying to make the google vision OCR regex searchable. I have completed it and works pretty well when the document contains only English characters. But it fails when there is the text of other languages.</p>

<p>It's happening because I have only English characters in google vision word component as follows.</p>

<pre><code>VISION_API_WORD_COUNTERS = ""([a-zA-Z0-9]+)|([^a-zA-Z0-9 ])"";
VISION_API_WORD_COMPONENTS = ""[a-zA-Z0-9]"";
VISION_API_NOT_WORD_COMPONENTS = ""[^a-zA-Z0-9]"";
</code></pre>

<p>As I can't include characters from all the languages, I am thinking to include the inverse of above. Something like</p>

<pre><code>VISION_API_WORD_COMPONENTS = ""[^*ALL THE SPECIAL CHARACTERS WHICH ARE IDENTIFIED AS WORD BY GOOGLE VISION*]""
</code></pre>

<p>for example  <code>[^!@#$%^&amp;*()_+=]</code>.</p>

<p>So where can I find <em>ALL THE SPECIAL CHARACTERS WHICH ARE IDENTIFIED AS A SEPARATE WORD BY GOOGLE VISION</em>? </p>

<p>Trial and error, keep adding the special characters I find is one option.But that would be my last option.</p>",,0,0,,2018-10-16 06:58:37.780 UTC,,2018-10-19 18:30:58.360 UTC,2018-10-19 18:30:58.360 UTC,,8283737,,8283737,1,1,text|google-api|ocr|google-cloud-vision|google-vision,80
Web Camera with Raspberry Pi3 to detect objects using Google Cloud Vision,45296021,Web Camera with Raspberry Pi3 to detect objects using Google Cloud Vision,"<p>I want to integrate USB Web Camera with Raspberry Pi3 and send the images captured to Google Cloud Vision to detect objects. Any Python 3 library for doing the same?<br><br>
I have successfully integrated my web camera and able to stream video over URL using <a href=""http://www.instructables.com/id/How-to-Make-Raspberry-Pi-Webcam-Server-and-Stream-/"" rel=""nofollow noreferrer"">""Motion""</a>
<br><br>
Any library similar to Pi Camera or that can make me move forward from the above mentioned Motion library. would be of great help.</p>",45318391,1,0,,2017-07-25 06:57:33.020 UTC,,2017-07-26 05:49:10.830 UTC,,,,,734298,1,0,python|camera|raspberry-pi|google-cloud-platform|google-cloud-vision,496
"TypeError: write() argument must be str, not EntityAnnotation",54094156,"TypeError: write() argument must be str, not EntityAnnotation","<p>I am using the Google vision api to extract the text from an image and I also want to store this text in a .txt file.</p>

<p>Whenever I use <code>f.write(text.description)</code> I get:</p>

<blockquote>
  <p>UnicodeEncodeError</p>
</blockquote>

<p>With <code>f.write(text)</code> it gives me: </p>

<blockquote>
  <p>TypeError: write() argument must be str, not EntityAnnotation</p>
</blockquote>

<p><code>f.write(text.description.encode(""utf-8""))</code> gives me:</p>

<blockquote>
  <p>TypeError: write() argument must be str, not bytes.</p>
</blockquote>",,2,0,,2019-01-08 14:42:20.777 UTC,,2019-01-09 08:25:26.503 UTC,2019-01-08 15:10:17.533 UTC,,641914,,10823647,1,0,python,43
Google Cloud Vision API returning bad result?,48412094,Google Cloud Vision API returning bad result?,"<p>So I'm using the Google Cloud Vision API to get back data on an image. As I was testing, I came across a large image that apparently exceeded the max byte size length. So I decided it would be good to programmatically detect if you try to do that, and stop the process before sending to Google. </p>

<p>When I looked at the response from google it says: <code>""Request payload size exceeds the limit: 10485760 bytes.""</code></p>

<p>Okay, so I decided to figure out how to get the byte size of a base64 image and check to see if it exceeds <code>10485760</code> before sending to google. However, I tried 2 different solutions, both resulting in a byte size of <code>7907945</code> which is not greater than the max size on Google's end. </p>

<p>I would post the base64, but it is pretty large and any time I try to copy and paste, it freezes my browser, so I will refrain for now. I do apologize, I like to provide all information but I can't even put it in a gist to post here. Although I will keep trying.</p>

<p>Instead here is my conversion code (I also tried another solution I found on SO that gave me the same result): </p>

<pre><code>var b64str = atob(dataUri.replace('data:image/png;base64,',""""));
var byteLength = b64str.length;
</code></pre>

<p>Am I doing something wrong?</p>",48412160,1,0,,2018-01-23 22:46:17.717 UTC,,2018-01-23 22:51:37.027 UTC,,,,,6891201,1,0,javascript,264
How to scan the Barcode twice using Google vision API in the same activity?,51991109,How to scan the Barcode twice using Google vision API in the same activity?,"<p><a href=""https://i.stack.imgur.com/4dQLe.jpg"" rel=""nofollow noreferrer"">Activity screenshot</a></p>

<p>In the above activity, I have to scan two different barcode and assigned their values to the each <code>EditText</code>. But I am not able to do it. I have tried many logical approaches but none of them worked.</p>

<p>activity_scan_qr.xml</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;android.support.constraint.ConstraintLayout xmlns:android=""http://schemas.android.com/apk/res/android""
    xmlns:app=""http://schemas.android.com/apk/res-auto""
    xmlns:tools=""http://schemas.android.com/tools""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""
    tools:context="".ScanQR""&gt;
    &lt;android.support.design.widget.AppBarLayout
        android:layout_width=""match_parent""
        android:id=""@+id/ref""
        android:layout_height=""wrap_content""
        android:theme=""@style/AppTheme.AppBarOverlay""&gt;

    &lt;android.support.v7.widget.Toolbar
        android:id=""@+id/toolbar2""
        app:titleTextColor=""#000000""
        app:title=""Scan Barcode""
        android:layout_width=""match_parent""
        android:layout_height=""?attr/actionBarSize""
        android:background=""?attr/colorPrimary""
        app:popupTheme=""@style/AppTheme.PopupOverlay"" /&gt;
    &lt;/android.support.design.widget.AppBarLayout&gt;

    &lt;android.support.v7.widget.CardView
        android:id=""@+id/cv_qrcode""
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:layout_margin=""5dp""
        android:layout_marginEnd=""16dp""
        android:layout_marginStart=""16dp""
        android:elevation=""10dp""
        app:cardCornerRadius=""50dp""
        app:layout_constraintBottom_toBottomOf=""parent""
        app:layout_constraintEnd_toEndOf=""parent""
        app:layout_constraintHorizontal_bias=""0.482""
        app:layout_constraintStart_toStartOf=""parent""
        app:layout_constraintTop_toBottomOf=""@+id/ref""
        app:layout_constraintVertical_bias=""0.071""&gt;

        &lt;android.support.constraint.ConstraintLayout
            android:layout_width=""match_parent""
            android:layout_height=""match_parent""&gt;

            &lt;SurfaceView

                android:id=""@+id/camera_pre""
                android:layout_width=""350dp""
                android:layout_height=""350dp""
                app:layout_constraintBottom_toBottomOf=""parent""
                app:layout_constraintEnd_toEndOf=""parent""
                app:layout_constraintStart_toStartOf=""parent""
                app:layout_constraintTop_toTopOf=""parent"" /&gt;

            &lt;ImageView
                android:id=""@+id/imageView9""
                android:layout_width=""wrap_content""
                android:layout_height=""wrap_content""
                android:layout_marginBottom=""8dp""
                android:layout_marginEnd=""8dp""
                android:layout_marginStart=""8dp""
                android:layout_marginTop=""8dp""
                app:layout_constraintBottom_toBottomOf=""@+id/camera_pre""
                app:layout_constraintEnd_toEndOf=""parent""
                app:layout_constraintStart_toStartOf=""@+id/camera_pre""
                app:layout_constraintTop_toTopOf=""@+id/camera_pre""
                app:srcCompat=""@drawable/qr_overlay"" /&gt;
        &lt;/android.support.constraint.ConstraintLayout&gt;

    &lt;/android.support.v7.widget.CardView&gt;

    &lt;EditText
        android:id=""@+id/beacon_edittext""
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:layout_marginEnd=""8dp""
        android:layout_marginStart=""8dp""
        android:layout_marginTop=""20dp""
        android:background=""@drawable/search_bar""
        android:ems=""10""
        android:hint=""Enter Beacon number""
        android:inputType=""textPersonName""
        android:paddingBottom=""5dp""
        android:paddingLeft=""50dp""
        app:layout_constraintEnd_toEndOf=""parent""
        app:layout_constraintHorizontal_bias=""0.491""
        app:layout_constraintStart_toStartOf=""parent""
        app:layout_constraintTop_toBottomOf=""@+id/cv_qrcode"" /&gt;

    &lt;EditText
        android:id=""@+id/vin_edittext""
        android:layout_width=""wrap_content""

        android:layout_height=""wrap_content""
        android:layout_marginBottom=""8dp""
        android:layout_marginEnd=""8dp""
        android:layout_marginStart=""8dp""
        android:layout_marginTop=""8dp""
        android:background=""@drawable/search_bar""
        android:ems=""10""
        android:hint=""Enter VIN number""
        android:inputType=""textPersonName""
        android:paddingBottom=""5dp""
        android:paddingLeft=""50dp""

        app:layout_constraintBottom_toBottomOf=""parent""
        app:layout_constraintEnd_toEndOf=""parent""
        app:layout_constraintHorizontal_bias=""0.491""
        app:layout_constraintStart_toStartOf=""parent""
        app:layout_constraintTop_toBottomOf=""@+id/beacon_edittext""
        app:layout_constraintVertical_bias=""0.0"" /&gt;

    &lt;Button
        android:id=""@+id/assign""
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:layout_marginBottom=""8dp""
        android:layout_marginEnd=""8dp""
        android:layout_marginStart=""8dp""
        android:layout_marginTop=""8dp""
        android:text=""assign""
        android:background=""@drawable/assign""
        android:textColor=""@color/colorPrimary""
        app:layout_constraintBottom_toBottomOf=""parent""
        app:layout_constraintEnd_toEndOf=""parent""
        app:layout_constraintStart_toStartOf=""parent""
        app:layout_constraintTop_toBottomOf=""@+id/vin_edittext"" /&gt;




&lt;/android.support.constraint.ConstraintLayout&gt; 
</code></pre>

<p>ScanQR.java</p>

<pre><code>package anaghesh.beacons_test;

import android.Manifest;
import android.annotation.SuppressLint;
import android.content.Context;
import android.content.DialogInterface;
import android.content.SharedPreferences;
import android.content.pm.PackageManager;
import android.os.Bundle;
import android.os.Vibrator;
import android.support.annotation.NonNull;
import android.support.v4.app.ActivityCompat;
import android.support.v7.app.AppCompatActivity;
import android.support.v7.widget.Toolbar;
import android.util.Log;
import android.util.SparseArray;
import android.view.SurfaceHolder;
import android.view.SurfaceView;
import android.view.View;
import android.widget.Button;
import android.widget.EditText;

import com.google.android.gms.vision.CameraSource;
import com.google.android.gms.vision.Detector;
import com.google.android.gms.vision.barcode.Barcode;
import com.google.android.gms.vision.barcode.BarcodeDetector;

import java.io.IOException;

// IMP: Google vision API used. Formats to be controlled

public class ScanQR extends AppCompatActivity {

    SurfaceView cameraPreview;
    EditText beacon,vin;
    BarcodeDetector barcodeDetector;
    CameraSource cameraSource;
    Button assign;
    public final static String BEACON_NUM=""Beacon_num"";
    public final static String VIN_NUM=""vin_num"";


    final int RequestCameraPermissionID = 1001;


    @Override
    public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) {
        cameraPreview =  findViewById(R.id.camera_pre);

        switch (requestCode) {
            case RequestCameraPermissionID: {
                if (grantResults[0] == PackageManager.PERMISSION_GRANTED) {
                    if (ActivityCompat.checkSelfPermission(this, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {
                        return;
                    }
                    try {
                        cameraSource.start(cameraPreview.getHolder());

                    } catch (IOException e) {
                        e.printStackTrace();
                    }
                }
            }
            break;
        }
    }

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_scan_qr);
        toolbarSetup();
        cameraPreview =  findViewById(R.id.camera_pre);
        Log.e(""error"",""""+cameraPreview);
        beacon = findViewById(R.id.beacon_edittext);
        beacon.setText("""");//test

        vin = findViewById(R.id.vin_edittext);
        vin.setText("""");//test
        assign = findViewById(R.id.assign);
        assign.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                SharedPreferences sharedPreferences = getSharedPreferences(""Database"", MODE_PRIVATE);
                SharedPreferences.Editor editor = sharedPreferences.edit();
                editor.putString(BEACON_NUM,beacon.getText().toString());
                editor.putString(VIN_NUM,vin.getText().toString());
                editor.apply();
                final android.app.AlertDialog.Builder builder = new android.app.AlertDialog.Builder(ScanQR.this);
                builder.setTitle(""Checkin Successful"");
                //  builder.setIcon(R.mipmap.ic_launcher);
                builder.setMessage(""Beacon No. ""+sharedPreferences.getString(BEACON_NUM,"""")+"" is assigned to VIN ""+sharedPreferences.getString(VIN_NUM,""""))
                        .setCancelable(false)
                        .setPositiveButton(""OK"", new DialogInterface.OnClickListener() {
                            public void onClick(DialogInterface dialog, int id) {
                               finish();
                            }
                        });
                android.app.AlertDialog alert = builder.create();
                alert.show();

            }
        });
        barcodeDetector = new BarcodeDetector.Builder(this)
              //  .setBarcodeFormats(Barcode.QR_CODE)
                .build();
        cameraSource = new CameraSource
                .Builder(this, barcodeDetector)
                .setRequestedPreviewSize(640, 480)
                .build();
        //Add Event
        alert();
        cameraPreview.getHolder().addCallback(new SurfaceHolder.Callback() {
            @Override
            public void surfaceCreated(SurfaceHolder surfaceHolder) {
                if (ActivityCompat.checkSelfPermission(getApplicationContext(), android.Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {
                    //Request permission
                    ActivityCompat.requestPermissions(ScanQR.this,
                            new String[]{Manifest.permission.CAMERA},RequestCameraPermissionID);
                    return;
                }
                try {
                    cameraSource.start(cameraPreview.getHolder());
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }

            @Override
            public void surfaceChanged(SurfaceHolder surfaceHolder, int i, int i1, int i2) {

            }

            @Override
            public void surfaceDestroyed(SurfaceHolder surfaceHolder) {
                cameraSource.stop();

            }
        });

        barcodeDetector.setProcessor(new Detector.Processor&lt;Barcode&gt;() {
            @Override
            public void release() {

            }

            @Override
            public void receiveDetections(Detector.Detections&lt;Barcode&gt; detections) {
                final SparseArray&lt;Barcode&gt; qrcodes = detections.getDetectedItems();
                if(qrcodes.size() != 0)
                {
                    beacon.post(new Runnable() {
                        @Override
                        public void run() {
                            cameraSource.stop( );
                            Log.e(""QR "",""""+qrcodes.valueAt(0).displayValue);

                            Log.e(""Beacon "",""""+beacon.getText().toString());

                            Log.e(""VIN "",""""+vin.getText().toString());

                            if(beacon.getText().toString().isEmpty()) {
                                beacon.setText(qrcodes.valueAt(0).displayValue);
                                vinAlert();
                            }
                            else {
                                vin.setText(qrcodes.valueAt(0).displayValue);
                            }
                            Vibrator vibrator = (Vibrator) getSystemService(Context.VIBRATOR_SERVICE);
                            vibrator.vibrate(100);


                        }
                    });

                }

            }

        });
    }
    void toolbarSetup(){
        Toolbar toolbar = findViewById(R.id.toolbar2);
        setSupportActionBar(toolbar);
        getSupportActionBar().setDisplayHomeAsUpEnabled(true);
        getSupportActionBar().setDisplayShowHomeEnabled(true);
    }
    void alert(){
        final android.app.AlertDialog.Builder builder = new android.app.AlertDialog.Builder(ScanQR.this);
        builder.setTitle(""Scan Beacon Barcode"");
        //  builder.setIcon(R.mipmap.ic_launcher);
        builder.setMessage(""Please scan the beacon barcode"")
                .setCancelable(false)
                .setPositiveButton(""OK"", new DialogInterface.OnClickListener() {
                    public void onClick(DialogInterface dialog, int id) {

                    }
                });
        android.app.AlertDialog alert = builder.create();
        alert.show();

    }
    void vinAlert(){
        final android.app.AlertDialog.Builder builder = new android.app.AlertDialog.Builder(ScanQR.this);
        builder.setTitle(""Scan Car Barcode"");
        //  builder.setIcon(R.mipmap.ic_launcher);
        builder.setMessage(""Please scan the car barcode"")
                .setCancelable(false)
                .setPositiveButton(""OK"", new DialogInterface.OnClickListener() {
                    public void onClick(DialogInterface dialog, int id) {
                        restartCamera();
                    }
                });
        android.app.AlertDialog alert = builder.create();
        alert.show();
    }


    @SuppressLint(""MissingPermission"")
    public void restartCamera()
    {
        Log.e(""method"",""Restart Camera"");
        try {
            cameraSource.start();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
</code></pre>",,0,0,,2018-08-23 17:16:01.853 UTC,,2018-08-23 19:05:43.763 UTC,2018-08-23 19:05:43.763 UTC,,2649012,,10098813,1,0,android|barcode-scanner|google-vision|vision-api,51
Unable to access Microsoft FACE API,50423259,Unable to access Microsoft FACE API,"<p>I'm facing this issue today. Unable to access Microsoft FACE Api endpoint:</p>

<p><a href=""https://westus.api.cognitive.microsoft.com/face/v1.0"" rel=""nofollow noreferrer"">https://westus.api.cognitive.microsoft.com/face/v1.0</a></p>

<p>Anyone facing the same issue today? <br><br>
Error:
<a href=""https://i.stack.imgur.com/Cl5pv.png"" rel=""nofollow noreferrer"">enter image description here</a></p>",,1,0,,2018-05-19 08:19:39.740 UTC,,2018-06-05 05:54:13.100 UTC,,,,,9815055,1,0,api|face,30
Getting vertices where google vision API found words,51223852,Getting vertices where google vision API found words,"<p>I'm working with the Google Vision API.</p>

<p>I would like to get the vertices ((x,y) locations) of the rectangles where google vision found a block of words. So far I'm getting the text from the google client.</p>

<pre><code>credentials = service_account.Credentials.from_service_account_file(""/api-key.json"")
client = vision.ImageAnnotatorClient(credentials=credentials)

#open file
with io.open(path, 'rb') as image_file:
    content = image_file.read()

#call api
image = types.Image(content=content)
response = client.document_text_detection(image=image)
document = response.full_text_annotation
</code></pre>

<p>What I would like is to get the vertices for each block of words in <code>document.text</code>.</p>",,1,0,,2018-06-27 16:31:26.063 UTC,,2018-10-08 14:14:54.093 UTC,,,,Juan Castaño,9994744,1,0,python|vision,216
Amazon Face Recognition error in POSTMAN,48521816,Amazon Face Recognition error in POSTMAN,"<p>I want to integrate <strong>Amazon Rekognition</strong> for the Face Recognition.</p>

<p>I have created bucket and IAM user. I am trying to hit <strong><em>""RekognitionService.ListCollections""</em></strong> for the testing in POSTMAN but getting error as follows;</p>

<blockquote>
<pre><code> &lt;InvalidSignatureException&gt;   &lt;Message&gt;Credential should be scoped to
 correct service: 'rekognition'. &lt;/Message&gt;
 &lt;/InvalidSignatureException&gt;
</code></pre>
</blockquote>

<p>My request header is as follows;</p>

<pre><code>https://rekognition.us-east-2.amazonaws.com/

x-amz-content-sha256:STREAMING-AWS4-HMAC-SHA256-PAYLOAD
X-Amz-Target:RekognitionService.ListCollections
X-Amz-Date:20180130T123032Z
Host:rekognition.us-east-2.amazonaws.com
Content-Length:142
Authorization:AWS4-HMAC-SHA256 Credential=XXXXXXXXXX/20180130/us-east-2/RekognitionService.ListCollections/aws4_request, SignedHeaders=accept-encoding;content-length;content-type;host;x-amz-content-sha256;x-amz-date;x-amz-target, Signature=xxxxxxxxxxx
Content-Type:application/x-www-form-urlencoded
Accept-Encoding:identity
</code></pre>

<p>Can anyone please guide me how to test AWS apis in POSTMAN ?</p>",48636458,1,0,,2018-01-30 12:44:04.110 UTC,,2018-02-06 06:26:10.567 UTC,2018-01-30 12:51:52.327 UTC,,1086960,,1086960,1,2,amazon-web-services|amazon-s3|amazon|postman|face-recognition,161
Returning value from multiple promises within Meteor.method,49209222,Returning value from multiple promises within Meteor.method,"<p>After a bunch of looking into Futures, Promises, wrapAsync, I still have no idea how to fix this issue</p>

<p>I have this method, which takes an array of images, sends it to Google Cloud Vision for logo detection, and then pushes all detected images with logos into an array, where I try to return in my method.</p>

<pre class=""lang-js prettyprint-override""><code>Meteor.methods({
  getLogos(images){
    var logosArray = [];
    images.forEach((image, index) =&gt; {
        client
        .logoDetection(image)
        .then(results =&gt; {
            const logos = results[0].logoAnnotations;
            if(logos != ''){
                logos.forEach(logo =&gt; logosArray.push(logo.description));
            }
        })
    });
    return logosArray;      
  },
});
</code></pre>

<p>However, when the method is called from the client:</p>

<pre class=""lang-js prettyprint-override""><code>Meteor.call('getLogos', images, function(error, response) {
  console.log(response);
});
</code></pre>

<p>the empty array is always returned, and understandably so as the method returned <code>logosArray</code> before Google finished processing all of them and returning the results. </p>

<p>How to handle such a case?</p>",49212025,1,2,,2018-03-10 12:47:12.933 UTC,0,2018-03-13 11:07:32.373 UTC,2018-03-10 17:48:35.303 UTC,,1041642,,5855648,1,3,javascript|asynchronous|meteor|google-cloud-vision,160
Extract specific data from JSON data from Google Cloud Vision,50199393,Extract specific data from JSON data from Google Cloud Vision,"<p>I am quite new to Raspberry Pi and Python coding but I was successful in configuring Google Cloud Vision. However the JSON dump looks like:</p>

<pre><code>{
    ""responses"": [
        {
            ""faceAnnotations"": [
                {
                    ""angerLikelihood"": ""UNLIKELY"",
                    ""blurredLikelihood"": ""VERY_UNLIKELY"",
                    ""boundingPoly"": {
                        ""vertices"": [
                            {
                                ""x"": 129
                            },
                            {
                                ""x"": 370
                            },
                            {
                                ""x"": 370,
                                ""y"": 240
                            },
                            {
                                ""x"": 129,
                                ""y"": 240
                            }
                        ]
                    },
                    ""detectionConfidence"": 0.99543685,
                    ""fdBoundingPoly"": {
                        ""vertices"": [
                            {
                                ""x"": 162,
                                ""y"": 24
                            },
                            {
                                ""x"": 337,
                                ""y"": 24
                            },
                            {
                                ""x"": 337,
                                ""y"": 199
                            },
                            {
                                ""x"": 162,
                                ""y"": 199
                            }
                        ]
                    },
                    ""headwearLikelihood"": ""VERY_UNLIKELY"",
                    ""joyLikelihood"": ""VERY_UNLIKELY"",
                    ""landmarkingConfidence"": 0.77542377,
                    ""landmarks"": [
                        {
                            ""position"": {
                                ""x"": 210.93373,
                                ""y"": 92.71409,
                                ""z"": -0.00025338508
                            },
                            ""type"": ""LEFT_EYE""
                        },
                        {
                            ""position"": {
                                ""x"": 280.00177,
                                ""y"": 82.57283,
                                ""z"": 0.49017733
                            },
                            ""type"": ""RIGHT_EYE""
                        },
                        {
                            ""position"": {
                                ""x"": 182.08047,
                                ""y"": 77.89372,
                                ""z"": 6.825161
                            },
                            ""type"": ""LEFT_OF_LEFT_EYEBROW""
                        },
                        {
                            ""position"": {
                                ""x"": 225.82335,
                                ""y"": 72.88091,
                                ""z"": -13.963233
                            },
                            ""type"": ""RIGHT_OF_LEFT_EYEBROW""
                        },
                        {
                            ""position"": {
                                ""x"": 260.4491,
                                ""y"": 66.19005,
                                ""z"": -13.798634
                            },
                            ""type"": ""LEFT_OF_RIGHT_EYEBROW""
                        },
                        {
                            ""position"": {
                                ""x"": 303.87503,
                                ""y"": 59.69522,
                                ""z"": 7.8336163
                            },
                            ""type"": ""RIGHT_OF_RIGHT_EYEBROW""
                        },
                        {
                            ""position"": {
                                ""x"": 244.57729,
                                ""y"": 83.701904,
                                ""z"": -15.022567
                            },
                            ""type"": ""MIDPOINT_BETWEEN_EYES""
                        },
                        {
                            ""position"": {
                                ""x"": 251.58353,
                                ""y"": 124.68004,
                                ""z"": -36.52176
                            },
                            ""type"": ""NOSE_TIP""
                        },
                        {
                            ""position"": {
                                ""x"": 255.39096,
                                ""y"": 151.87607,
                                ""z"": -19.560472
                            },
                            ""type"": ""UPPER_LIP""
                        },
                        {
                            ""position"": {
                                ""x"": 259.96045,
                                ""y"": 178.62886,
                                ""z"": -14.095398
                            },
                            ""type"": ""LOWER_LIP""
                        },
                        {
                            ""position"": {
                                ""x"": 232.35422,
                                ""y"": 167.2542,
                                ""z"": -1.0750997
                            },
                            ""type"": ""MOUTH_LEFT""
                        },
                        {
                            ""position"": {
                                ""x"": 284.49316,
                                ""y"": 159.06075,
                                ""z"": -0.078973025
                            },
                            ""type"": ""MOUTH_RIGHT""
                        },
                        {
                            ""position"": {
                                ""x"": 256.94714,
                                ""y"": 163.11235,
                                ""z"": -14.0897665
                            },
                            ""type"": ""MOUTH_CENTER""
                        },
                        {
                            ""position"": {
                                ""x"": 274.47885,
                                ""y"": 125.8553,
                                ""z"": -7.8479633
                            },
                            ""type"": ""NOSE_BOTTOM_RIGHT""
                        },
                        {
                            ""position"": {
                                ""x"": 231.2164,
                                ""y"": 132.60686,
                                ""z"": -8.418254
                            },
                            ""type"": ""NOSE_BOTTOM_LEFT""
                        },
                        {
                            ""position"": {
                                ""x"": 252.96692,
                                ""y"": 135.81783,
                                ""z"": -19.805998
                            },
                            ""type"": ""NOSE_BOTTOM_CENTER""
                        },
                        {
                            ""position"": {
                                ""x"": 208.6943,
                                ""y"": 86.72571,
                                ""z"": -4.8503814
                            },
                            ""type"": ""LEFT_EYE_TOP_BOUNDARY""
                        },
                        {
                            ""position"": {
                                ""x"": 223.4354,
                                ""y"": 90.71454,
                                ""z"": 0.42966545
                            },
                            ""type"": ""LEFT_EYE_RIGHT_CORNER""
                        },
                        {
                            ""position"": {
                                ""x"": 210.67189,
                                ""y"": 96.09362,
                                ""z"": -0.62435865
                            },
                            ""type"": ""LEFT_EYE_BOTTOM_BOUNDARY""
                        },
                        {
                            ""position"": {
                                ""x"": 195.00711,
                                ""y"": 93.783226,
                                ""z"": 6.6310787
                            },
                            ""type"": ""LEFT_EYE_LEFT_CORNER""
                        },
                        {
                            ""position"": {
                                ""x"": 208.30045,
                                ""y"": 91.73073,
                                ""z"": -1.7749802
                            },
                            ""type"": ""LEFT_EYE_PUPIL""
                        },
                        {
                            ""position"": {
                                ""x"": 280.8329,
                                ""y"": 75.722244,
                                ""z"": -4.3266015
                            },
                            ""type"": ""RIGHT_EYE_TOP_BOUNDARY""
                        },
                        {
                            ""position"": {
                                ""x"": 295.9134,
                                ""y"": 78.8241,
                                ""z"": 7.3644505
                            },
                            ""type"": ""RIGHT_EYE_RIGHT_CORNER""
                        },
                        {
                            ""position"": {
                                ""x"": 281.82813,
                                ""y"": 85.56999,
                                ""z"": -0.09711724
                            },
                            ""type"": ""RIGHT_EYE_BOTTOM_BOUNDARY""
                        },
                        {
                            ""position"": {
                                ""x"": 266.6147,
                                ""y"": 83.689865,
                                ""z"": 0.6850431
                            },
                            ""type"": ""RIGHT_EYE_LEFT_CORNER""
                        },
                        {
                            ""position"": {
                                ""x"": 282.31485,
                                ""y"": 80.471725,
                                ""z"": -1.3341979
                            },
                            ""type"": ""RIGHT_EYE_PUPIL""
                        },
                        {
                            ""position"": {
                                ""x"": 202.4563,
                                ""y"": 66.06882,
                                ""z"": -8.493092
                            },
                            ""type"": ""LEFT_EYEBROW_UPPER_MIDPOINT""
                        },
                        {
                            ""position"": {
                                ""x"": 280.76108,
                                ""y"": 54.08935,
                                ""z"": -7.895889
                            },
                            ""type"": ""RIGHT_EYEBROW_UPPER_MIDPOINT""
                        },
                        {
                            ""position"": {
                                ""x"": 168.31839,
                                ""y"": 134.46411,
                                ""z"": 89.73161
                            },
                            ""type"": ""LEFT_EAR_TRAGION""
                        },
                        {
                            ""position"": {
                                ""x"": 332.23724,
                                ""y"": 109.35637,
                                ""z"": 90.81501
                            },
                            ""type"": ""RIGHT_EAR_TRAGION""
                        },
                        {
                            ""position"": {
                                ""x"": 242.81676,
                                ""y"": 67.845825,
                                ""z"": -16.629877
                            },
                            ""type"": ""FOREHEAD_GLABELLA""
                        },
                        {
                            ""position"": {
                                ""x"": 264.32065,
                                ""y"": 208.95119,
                                ""z"": -4.0186276
                            },
                            ""type"": ""CHIN_GNATHION""
                        },
                        {
                            ""position"": {
                                ""x"": 183.4723,
                                ""y"": 179.30655,
                                ""z"": 59.87147
                            },
                            ""type"": ""CHIN_LEFT_GONION""
                        },
                        {
                            ""position"": {
                                ""x"": 331.6927,
                                ""y"": 156.69931,
                                ""z"": 60.93835
                            },
                            ""type"": ""CHIN_RIGHT_GONION""
                        }
                    ],
                    ""panAngle"": 0.41165036,
                    ""rollAngle"": -8.687789,
                    ""sorrowLikelihood"": ""VERY_UNLIKELY"",
                    ""surpriseLikelihood"": ""VERY_UNLIKELY"",
                    ""tiltAngle"": 0.2050134,
                    ""underExposedLikelihood"": ""POSSIBLE""
                }
            ]
        }
    ]
}
</code></pre>

<p>Yes, it's an eyesore to look at. I am only wanting to extract the likelihood. Preferably in this format:</p>

<pre><code>Anger likelihood is UNLIKEY
Joy likelihood is VERY_UNLIKELY
Sorrow likelihood is VERY_UNLIKELY
Suprise likelihood is VERY_UNLIKELY
</code></pre>

<p>Python code can be found here:
<a href=""https://github.com/DexterInd/GoogleVisionTutorials/blob/master/camera-vision-face.py"" rel=""nofollow noreferrer"">https://github.com/DexterInd/GoogleVisionTutorials/blob/master/camera-vision-face.py</a></p>",,2,0,,2018-05-06 11:55:03.563 UTC,,2018-05-06 13:45:10.300 UTC,,,,,9748436,1,0,python|json|raspberry-pi,52
face api with Nodejs,54574681,face api with Nodejs,"<p>I am using azure face api using node js, below is the code. However instead of the image hosted some where i want to use my local image and post it. i tried different options but it is not recognizing the image format or invalid image url </p>

<p>below are the things i have tried </p>

<pre><code>1) var stream = fs.createReadStream('local image url');
2) var imageAsBase64 = fs.readFileSync('image.jpg','base64');
</code></pre>

<p>below is the code </p>

<pre><code>'use strict';

const request = require('request');

// Replace &lt;Subscription Key&gt; with your valid subscription key.
const subscriptionKey = '&lt;Subscription Key&gt;';

// You must use the same location in your REST call as you used to get your
// subscription keys. For example, if you got your subscription keys from
// westus, replace ""westcentralus"" in the URL below with ""westus"".
const uriBase = 'https://westcentralus.api.cognitive.microsoft.com/face/v1.0/detect';

const imageUrl =
    'https://upload.wikimedia.org/wikipedia/commons/3/37/Dagestani_man_and_woman.jpg';

// Request parameters.
const params = {
    'returnFaceId': 'true',
    'returnFaceLandmarks': 'false',
    'returnFaceAttributes': 'age,gender,headPose,smile,facialHair,glasses,' +
        'emotion,hair,makeup,occlusion,accessories,blur,exposure,noise'
};

const options = {
    uri: uriBase,
    qs: params,
   // body: '{""url"": ' + '""' + imageUrl + '""}',
   //body: stream,
   body:imageAsBase64,
    headers: {
        'Content-Type': 'application/json',
        'Ocp-Apim-Subscription-Key' : subscriptionKey
    }
};

request.post(options, (error, response, body) =&gt; {
  if (error) {
    console.log('Error: ', error);
    return;
  }
  let jsonResponse = JSON.stringify(JSON.parse(body), null, '  ');
  console.log('JSON Response\n');
  console.log(jsonResponse);
});
</code></pre>",54576162,1,0,,2019-02-07 13:41:33.350 UTC,,2019-02-07 15:45:21.490 UTC,,,,,1992023,1,1,node.js|azure|face-api,99
An existing connection was forcibly closed by the remote host in Azure Face API,54345710,An existing connection was forcibly closed by the remote host in Azure Face API,"<p>I'm facing an issue in the Azure Face API. It was working fine earlier.
Facing the below error 
An existing connection was forcibly closed by the remote host.</p>

<p>Could you please let me know what could be the reason for the same.</p>

<pre><code>try
            {
                await faceServiceClient.DeletePersonGroupAsync(_groupId);
            }
            catch (Microsoft.ProjectOxford.Common.ClientException exC)
            {

            }
            catch (FaceAPIException ex)
            {

            }
</code></pre>",,0,5,,2019-01-24 11:35:35.233 UTC,,2019-01-24 11:58:05.173 UTC,,,,,10961360,1,1,azure|api|face,257
Passing Base64 Bytes to AWS Rekognition Compare faces,53865532,Passing Base64 Bytes to AWS Rekognition Compare faces,"<p>As mentioned I am trying to pass Base64 encoded Images to the AWS API for comparing faces. But its giving me error : </p>

<pre><code>An error occurred (InvalidImageFormatException) when calling the CompareFaces operation: Request has invalid image format: InvalidImageFormatException
</code></pre>

<p>I tried earlier using S3 bucket images and it worked properly. But right now I am trying to send the images without using S3 bucket.</p>

<p>I am using a Lambda function, and I referred to <a href=""https://docs.aws.amazon.com/rekognition/latest/dg/faces-comparefaces.html"" rel=""nofollow noreferrer"">this documentation</a></p>

<p>My code (edited version) : </p>

<pre><code>source_image_string = ""/9j/4......."" //Base64 stringified image
target_image_string = ""/9j/4A......"" //Base64 stringified image
source_byte = base64.b64encode( bytes(source_image_string, ""utf-8"") )
target_byte = base64.b64encode( bytes(target_image_string, ""utf-8"") )

response=rekognition.compare_faces(SimilarityThreshold=70,SourceImage={'Bytes': source_byte},TargetImage={'Bytes': target_byte})
</code></pre>

<p>And the error that I am getting in the Cloudwatch is : </p>

<pre><code>An error occurred (InvalidImageFormatException) when calling the CompareFaces operation: Request has invalid image format: InvalidImageFormatException
Traceback (most recent call last):
File ""/var/task/lambda_function.py"", line 113, in lambda_handler
raise e
File ""/var/task/lambda_function.py"", line 105, in lambda_handler
response = compare_faces(source_image_string,target_image_string)
File ""/var/task/lambda_function.py"", line 43, in compare_faces
response=rekognition.compare_faces(SimilarityThreshold=70,SourceImage={'Bytes': source_byte},TargetImage={'Bytes': target_byte})
File ""/var/runtime/botocore/client.py"", line 314, in _api_call
return self._make_api_call(operation_name, kwargs)
File ""/var/runtime/botocore/client.py"", line 612, in _make_api_call
raise error_class(parsed_response, operation_name)
</code></pre>

<p>Where am I going wrong?, as the <code>source_byte</code> that I am passing is of the <code>Byte</code> format.</p>",,0,0,,2018-12-20 09:13:31.287 UTC,1,2018-12-20 09:13:31.287 UTC,,,,,6366458,1,0,amazon-web-services|amazon-rekognition,78
imageUri : path to local file,44519968,imageUri : path to local file,"<p>I am accessing google vision api using requests.post method in python (jupyter notebook)</p>

<p>in imageUri i can only specify weburl or bucket uri. I cannot specify local file name like ""/Users/pi/test.jpg""</p>

<pre><code>file_name = '/Users/mbp/Pictures/full moon.jpg'
data = {
  ""requests"":[
    {
      ""image"":{
    ""source"":{
      ""imageUri"": file_name
    }
  },
  ""features"":[
    {
      ""type"":""FACE_DETECTION"",
      ""maxResults"":1
    }
   ]
  }
 ]
}

r = requests.post(url=url,json=data)
x= json.loads(r.text)
print(x['responses'])
</code></pre>

<p>response i get is:</p>

<pre><code>[{'error': {'code': 3, 'message': 'image-annotator::Malformed request.: Unsupported URI protocol specified: /Users/mbp/Pictures/full moon.jpg'}}]
</code></pre>

<p>please help</p>",44548731,1,2,,2017-06-13 11:14:57.403 UTC,,2017-06-14 15:19:26.063 UTC,,,,,2933686,1,0,python|google-cloud-vision,707
What is the highest Google Cloud Vision score a image:annotation can return?,37935601,What is the highest Google Cloud Vision score a image:annotation can return?,"<p>I am using Google Cloud Vision for a project and I'm planning on filtering results based on how reliable the score for the LOGO_DETECTION was.</p>

<p>I was running some tests found the result a LOGO_DETECTION on a image showing only Google´s plain logo (image below) returned a score of only <strong>0.28542563</strong>. </p>

<p><a href=""https://i.stack.imgur.com/nzZor.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nzZor.png"" alt=""enter image description here""></a></p>

<p>Scores range from 0-1 so I found this quite strange. I was wondering if the highest score is actually 0, and 1 the lowest. But I couldn´t find any reference to any of this on the documentation.</p>

<p>Does anyone here know about this?</p>",39304998,1,0,,2016-06-21 04:27:30.873 UTC,0,2016-09-03 08:44:05.670 UTC,,,,,4860393,1,1,google-cloud-vision,223
Google Vision API text_detection Numbering Issue,56217832,Google Vision API text_detection Numbering Issue,"<p>I am using Google Vision API for ""TEXT_DETECTION"". Input for it is png image.(scanned document).
When I am running this API , its causing numbering issue.
Numbers for first two paragraphs are not visible.</p>

<p><a href=""https://cloud.google.com/vision/docs/detecting-text#vision-text-detection-python"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/detecting-text#vision-text-detection-python</a></p>

<p>Tried with same code.</p>

<p>It should extract exact numbering from original scanned document.</p>",,0,0,,2019-05-20 09:20:09.943 UTC,,2019-05-20 09:20:09.943 UTC,,,,,11527208,1,0,google-vision,8
Google Vision API label detection not working error: 'str' object has no attribute before request,47250652,Google Vision API label detection not working error: 'str' object has no attribute before request,"<p>I am trying to use the <a href=""https://cloud.google.com/vision/docs/detecting-labels"" rel=""nofollow noreferrer"">Google Vision API</a> to read the labels for a image. </p>

<p>I am executing this on a Google Compute Engine instance with access to all Cloud APIs. And I am using a service account for authentication</p>

<p>I keep getting the following error
<a href=""https://i.stack.imgur.com/taHIh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/taHIh.png"" alt=""enter image description here""></a></p>

<p>This the code I am executing </p>

<pre><code>import io
#from google.cloud import storage
#from google.cloud.vision_v1 import ImageAnnotatorClient
from google.oauth2 import service_account
# using old version of API

from google.cloud import vision
from google.cloud.vision import types


image_client = vision.ImageAnnotatorClient(credentials='credentials.json')


with io.open('/home/username/instagram-ml/userbucket/images/test_image.jpg','rb') as image_file:
    content = image_file.read()

image = types.Image(content=content)
#
image_response = image_client.label_detection(image =image)

labels = image_response.label_annotations
</code></pre>

<p>Up until line </p>

<pre><code>image_response = image_client.label_detection(image =image)
</code></pre>

<p>Everything works fine and I get no authentication issues. But when I execute the above line I suddenly get this error. </p>

<p>Pretty much following the instructions on this <a href=""https://cloud.google.com/vision/docs/detecting-labels"" rel=""nofollow noreferrer"">page</a></p>

<p>Not very sure what is going wrong</p>",,1,0,,2017-11-12 15:41:46.523 UTC,,2017-11-12 16:15:36.067 UTC,,,,,4709746,1,0,python|google-cloud-platform|google-cloud-vision,458
parse cloud http rerquest,42245229,parse cloud http rerquest,"<p>i am working on parse server with AWS , i am using microsoft face API  '<a href=""https://westus.api.cognitive.microsoft.com/face/v1.0/"" rel=""nofollow noreferrer"">https://westus.api.cognitive.microsoft.com/face/v1.0/</a>' for detenct and identify faces , as you know the are no any option to debug the parse cloud code and the only option is to use the tool from github  ""parse-cloud-debugger""
when i test the code there for the Parse.Cloud.httpRequest() all works fine and i get good response but after that when i put he same code to the real parse-server on AWS when i make the httpRequest in some reason i get (response.text) not as JSON object but as char array , that means i get array of thousands chars.</p>

<pre><code>function detectFace(imgUrl){
    return new Promise(function(resolve,reject){
        var url ='https://westus.api.cognitive.microsoft.com/face/v1.0/?   returnFaceId=true&amp;returnFaceLandmarks=true';
        console.log(url);
        Parse.Cloud.httpRequest({
            method:CONSTANTS.METHOD_POST,
            url:url,
            headers:{
                'Content-Type': CONSTANTS.CONTENT_TYPE_APP_JSON,
                'Ocp-Apim-Subscription-Key': CONSTANTS.MICROSOFT_FACE_API_KEY
            },
            json:{
                'processData':false,
                'url':imgUrl
            }
        }).then(function(httpResult){
            resolve(httpResult);
        }).catch(function(e){
            reject(e);
        });
    });
}
</code></pre>",,0,6,,2017-02-15 09:21:59.923 UTC,,2017-02-15 09:34:54.243 UTC,2017-02-15 09:34:54.243 UTC,,7137009,,4430490,1,0,javascript|amazon-web-services|parse-platform|parse-cloud,29
Windows ML - ONNX - Exception from HRESULT: 0x88900105,53029413,Windows ML - ONNX - Exception from HRESULT: 0x88900105,"<p>I'm reading this article : <a href=""https://blogs.msdn.microsoft.com/appconsult/2018/05/23/add-a-bit-of-machine-learning-to-your-windows-application-thanks-to-winml/"" rel=""nofollow noreferrer"">Add a bit of machine learning to your Windows application thanks to WinML</a> and reproduced the sample application. It currently works.</p>

<p>On Azure Custom Vision portal, i built my own vision model and exported it in ONNX 1.0 for Windows 10 build 1803, but when i'm trying running the sample with my own model, i have the Following exception :</p>

<p>Exception from HRESULT: 0x88900105</p>

<p>When the program go on this line :</p>

<pre><code>LearningModelEvaluationResultPreview evalResult = await learningModel.EvaluateAsync(binding, string.Empty);
</code></pre>

<p>It is a little tricky to know where it comes from because the exception is not very explicit.</p>

<p>I would like to know if you have encountered the same problem or have an idea where it might come from.</p>

<p>Edit: steps for reproduce the problem.</p>

<hr>

<p>Download my model here : <a href=""https://1drv.ms/u/s!AqIRdnJsFoE6iu4N0vI89qa-C76iZg"" rel=""nofollow noreferrer"">https://1drv.ms/u/s!AqIRdnJsFoE6iu4N0vI89qa-C76iZg</a></p>

<p>Clone the repository from GitHub : <a href=""https://github.com/Microsoft/Windows-AppConsult-Samples-UWP"" rel=""nofollow noreferrer"">https://github.com/Microsoft/Windows-AppConsult-Samples-UWP</a></p>

<p>Run the sample with a plane picture, the sample works.</p>

<p>Now In the solution, replace the existing (and working) PlanesModel.onnx by mine. </p>

<p>We get the exception.</p>

<p>Here all my project's configuration:</p>

<p><a href=""https://i.stack.imgur.com/31RU3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/31RU3.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/Cwwfn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cwwfn.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/FUyyX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FUyyX.png"" alt=""enter image description here""></a></p>",53054314,1,1,,2018-10-28 07:48:07.573 UTC,,2018-10-29 21:46:33.057 UTC,2018-10-29 06:57:52.573 UTC,,2052012,,2052012,1,1,uwp|azure-cognitive-services|onnx,85
(Android) How to overlay an image (ImageView) anchoring the to the eyes?,38292746,(Android) How to overlay an image (ImageView) anchoring the to the eyes?,"<p>Good afternoon!
Tell me how to overlay an image (ImageView), anchoring the to the eyes?
I want to track using Google Vision API.
Maybe there is an example of how to put a hat on head?</p>

<p><strong>Added. (12 Jule 2016)</strong></p>

<p>Use the following code:</p>

<pre><code>imageView.setX(mPosition.x);
imageView.setY(mPosition.y);
postInvalidate();
</code></pre>

<p>not anchoring (blue square) strictly to the eye, you can see it in the screenshots with different distance from the photos:</p>

<p><a href=""https://i.stack.imgur.com/kBNLi.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kBNLi.jpg"" alt=""enter image description here""></a></p>

<p>Please tell me why this happens?</p>",,1,0,,2016-07-10 14:05:40.377 UTC,,2016-09-29 07:18:07.670 UTC,2016-09-29 07:18:07.670 UTC,,1697459,,2692318,1,0,android|camera|tracking|face-recognition|android-vision,1067
Error: Could not find JobId Service: AmazonRekognition; Status Code: 400;,53799908,Error: Could not find JobId Service: AmazonRekognition; Status Code: 400;,"<p>I am using  this Java  Lambda  code provided by  AWS   to detect labels in a  video:</p>

<p><a href=""https://github.com/awsdocs/amazon-rekognition-developer-guide/blob/master/code_examples/java_examples/stored_video/java-lambda-handler-sns.java"" rel=""nofollow noreferrer"">https://github.com/awsdocs/amazon-rekognition-developer-guide/blob/master/code_examples/java_examples/stored_video/java-lambda-handler-sns.java</a></p>

<pre><code>//Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
//PDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-rekognition-developer-guide/blob/master/LICENSE-SAMPLECODE.)

package com.amazonaws.lambda.demo;

import com.amazonaws.services.lambda.runtime.Context;
import com.amazonaws.services.lambda.runtime.LambdaLogger;
import com.amazonaws.services.lambda.runtime.RequestHandler;
import com.amazonaws.services.lambda.runtime.events.SNSEvent;
import java.util.List;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.rekognition.AmazonRekognition;
import com.amazonaws.services.rekognition.AmazonRekognitionClientBuilder;
import com.amazonaws.services.rekognition.model.GetLabelDetectionRequest;
import com.amazonaws.services.rekognition.model.GetLabelDetectionResult;
import com.amazonaws.services.rekognition.model.LabelDetection;
import com.amazonaws.services.rekognition.model.LabelDetectionSortBy;
import com.amazonaws.services.rekognition.model.VideoMetadata;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;



public class JobCompletionHandler implements RequestHandler&lt;SNSEvent, String&gt; {

   @Override
   public String handleRequest(SNSEvent event, Context context) {

      String message = event.getRecords().get(0).getSNS().getMessage();
      LambdaLogger logger = context.getLogger(); 

      // Parse SNS event for analysis results. Log results
      try {
         ObjectMapper operationResultMapper = new ObjectMapper();
         JsonNode jsonResultTree = operationResultMapper.readTree(message);
         logger.log(""Rekognition Video Operation:========================="");
         logger.log(""Job id: "" + jsonResultTree.get(""JobId""));
         logger.log(""Status : "" + jsonResultTree.get(""Status""));
         logger.log(""Job tag : "" + jsonResultTree.get(""JobTag""));
         logger.log(""Operation : "" + jsonResultTree.get(""API""));

         if (jsonResultTree.get(""API"").asText().equals(""StartLabelDetection"")) {

            if (jsonResultTree.get(""Status"").asText().equals(""SUCCEEDED"")){
               GetResultsLabels(jsonResultTree.get(""JobId"").asText(), context);
            }
            else{
               String errorMessage = ""Video analysis failed for job "" 
                     + jsonResultTree.get(""JobId"") 
                     + ""State "" + jsonResultTree.get(""Status"");
               throw new Exception(errorMessage); 
            }

         } else
            logger.log(""Operation not StartLabelDetection"");

      } catch (Exception e) {
         logger.log(""Error: "" + e.getMessage());
         throw new RuntimeException (e);


      }

      return message;
   }

   void GetResultsLabels(String startJobId, Context context) throws Exception {

      LambdaLogger logger = context.getLogger();

      AmazonRekognition rek = AmazonRekognitionClientBuilder.standard().withRegion(Regions.US_EAST_1).build();

      int maxResults = 1000;
      String paginationToken = null;
      GetLabelDetectionResult labelDetectionResult = null;
      String labels = """";
      Integer labelsCount = 0;
      String label = """";
      String currentLabel = """";

      //Get label detection results and log them. 
      do {

         GetLabelDetectionRequest labelDetectionRequest = new GetLabelDetectionRequest().withJobId(startJobId)
               .withSortBy(LabelDetectionSortBy.NAME).withMaxResults(maxResults).withNextToken(paginationToken);

         labelDetectionResult = rek.getLabelDetection(labelDetectionRequest);

         paginationToken = labelDetectionResult.getNextToken();
         VideoMetadata videoMetaData = labelDetectionResult.getVideoMetadata();

         // Add labels to log
         List&lt;LabelDetection&gt; detectedLabels = labelDetectionResult.getLabels();

         for (LabelDetection detectedLabel : detectedLabels) {
            label = detectedLabel.getLabel().getName();
            if (label.equals(currentLabel)) {
               continue;
            }
            labels = labels + label + "" / "";
            currentLabel = label;
            labelsCount++;

         }
      } while (labelDetectionResult != null &amp;&amp; labelDetectionResult.getNextToken() != null);

      logger.log(""Total number of labels : "" + labelsCount);
      logger.log(""labels : "" + labels);

   }


}
</code></pre>

<p>When  checking the  results in cloudwatch  I receive an error: </p>

<pre><code>
05:23:48
Rekognition Video Operation:=========================

05:23:48
Job id: ""f647269c4f8bab504cfc9e50a2d89593e463c31755e3bdf41fac18b8be603d65""

05:23:48
Status : ""SUCCEEDED""

05:23:48
Job tag : null

05:23:48
Operation : ""StartLabelDetection""

05:23:49
*Error: Could not find JobId (Service: AmazonRekognition; Status Code: 400; Error Code: ResourceNotFoundException; Request ID: c4d9bbe0-00f2-11e9-8aa2-21ecd18719a6)*

05:23:49
com.amazonaws.services.rekognition.model.ResourceNotFoundException: Could not find JobId (Service: AmazonRekognition; Status Code: 400; Error Code: ResourceNotFoundException; Request ID: c4d9bbe0-00f2-11e9-8aa2-21ecd18719a6): java.lang.RuntimeException java.lang.RuntimeException: com.amazonaws.services.rekognition.model.ResourceNotFoundException: Could not find JobId (Service: AmazonRekognition; S

05:23:49
END RequestId: 4f531035-00f2-11e9-96d7-f55b78da9771

05:23:49
REPORT RequestId: 4f531035-00f2-11e9-96d7-f55b78da9771  Duration: 941.37 ms Billed Duration: 1000 ms Memory Size: 1024 MB   Max Memory Used: 98 MB
</code></pre>

<p>I  am using the root   account and have  set  up   AWS  as described in the  "" Create the AWS Toolkit for Eclipse Lambda Project""  Rekognition developer-guide.</p>

<p>Is the  problem  with the code  or something    else ?</p>",53909025,1,0,,2018-12-16 06:21:12.943 UTC,,2018-12-24 03:53:08.647 UTC,,,,,2841952,1,0,amazon-web-services|aws-lambda|aws-sdk,30
google vision textRecognizer.isOperational() method returns always false,53790848,google vision textRecognizer.isOperational() method returns always false,"<p>I am using google vision library for OCR application (according to <a href=""https://codelabs.developers.google.com/codelabs/mobile-vision-ocr/#1"" rel=""nofollow noreferrer"">https://codelabs.developers.google.com/codelabs/mobile-vision-ocr/#1</a>) and when I run the application textRecognizer.isOperational() method returns always false. I think it is because google play services cannot download the required OCR file but it works in some devices. Is it possible to manually download and add the file?
Thanks every one</p>",54177003,1,0,,2018-12-15 08:41:10.633 UTC,1,2019-01-14 07:05:10.730 UTC,,,,,2240957,1,0,ocr|google-vision|text-recognition,56
Interactive python daemon,49386559,Interactive python daemon,"<p>Is there a way to run a python that can <strong>process inputs interactively</strong>? Without user input. That way methods can be called <strong>without needed to import</strong> and initialize the script. </p>

<p>What I have:</p>

<pre><code>import very_heavy_package
very_heavy_package.initialize()

if very_heavy_package.check(input_file):
    do_something()
else:
    do_something_else()
</code></pre>

<p>I want something like:</p>

<pre><code>import very_heavy_package
very_heavy_package.initialize()

@entry_point()
def check_something(input_file):
    if very_heavy_package.check(input_file):
        do_something()
    else:
        do_something_else()
</code></pre>

<p><code>import</code> and <code>initialize()</code> lines take a very long time, but <code>check_something()</code> is pretty much instantaneous. I want to be able to <code>check_something()</code> multiple times on demand, without executing the full script all over.</p>

<hr>

<p>I know this could be achieved with a server built in flask, but it seems a little overkill. Is there a more ""local"" way of doing this?</p>

<p>This example in particular is about running some <em>Google Vision</em> processing in an image from a surveillance camera on a Raspberry Pi Zero. Initializing the script takes a while (~10s), but making the API request is very fast(&lt;100ms). I'm looking to achieve fast response time.</p>",49424745,3,0,,2018-03-20 14:11:53.763 UTC,,2018-03-22 09:20:19.143 UTC,,,,,4623227,1,1,python|python-3.x,77
"boto3 unknownservice ""mturk""",51450035,"boto3 unknownservice ""mturk""","<p>I am trying to use boto3 for Amazon Mechanical Turk. I was trying to get the client using the following code:</p>

<pre><code>import boto3
endpoint_url = 'https://mturk-requester.us-east-1.amazonaws.com'
aws_access_key_id = &lt;aws_access_key_id&gt;
aws_secret_access_key = &lt;aws_secret_access_key&gt;
region_name = 'us-east-1'

client = boto3.client('mturk',
    aws_access_key_id = aws_access_key_id,
    aws_secret_access_key = aws_secret_access_key,
    region_name=region_name,
    endpoint_url = endpoint_url
)
</code></pre>

<p>But I am getting the following error about UnknownService name:</p>

<pre><code>botocore.exceptions.UnknownServiceError: Unknown service: 'mturk'. Valid service 
names are: acm, apigateway, application-autoscaling, appstream, autoscaling, 
budgets, cloudformation, cloudfront, cloudhsm, cloudsearch, cloudsearchdomain, 
cloudtrail, cloudwatch, codebuild, codecommit, codedeploy, codepipeline, 
cognito-identity, cognito-idp, cognito-sync, config, datapipeline, devicefarm, 
directconnect, discovery, dms, ds, dynamodb, dynamodbstreams, ec2, ecr, ecs, efs, 
elasticache, elasticbeanstalk, elastictranscoder, elb, elbv2, emr, es, events, 
firehose, gamelift, glacier, health, iam, importexport, inspector, iot, iot-data,
kinesis, kinesisanalytics, kms, lambda, lightsail, logs, machinelearning,
marketplacecommerceanalytics, meteringmarketplace, opsworks, opsworkscm, pinpoint, 
polly, rds, redshift, rekognition, route53, route53domains, s3, sdb, servicecatalog,
ses, shield, sms, snowball, sns, sqs, ssm, stepfunctions, storagegateway, sts,
support, swf, waf, workspaces, xray
</code></pre>

<p>Why is <code>'mturk'</code> not in this list? The code I am using is taken from <a href=""https://requester.mturk.com/developer"" rel=""nofollow noreferrer"">mturk developer website</a>. </p>

<p>Any suggestion is welcome! Thanks in advance!</p>",,0,5,,2018-07-20 20:38:31.863 UTC,,2018-07-20 20:38:31.863 UTC,,,,,2504754,1,0,python|boto3|mechanicalturk,46
OpenCV drawing rectangle from Google cloud Vision API boundingPoly,56391486,OpenCV drawing rectangle from Google cloud Vision API boundingPoly,"<p>i have a google vision api object localization request that returns a response fine . I am wondering how do i use the response to draw rectangle using cv2.rectangle or cv2.polyLines . The response comes in the format below </p>

<pre><code>""boundingPoly"": {
            ""normalizedVertices"": [
              {
                ""x"": 0.77569866,
                ""y"": 0.37104446
              },
              {
                ""x"": 0.9412425,
                ""y"": 0.37104446
              },
              {
                ""x"": 0.9412425,
                ""y"": 0.81507325
              },
              {
                ""x"": 0.77569866,
                ""y"": 0.81507325
              }
            ]
          }
</code></pre>

<p>At the moment i can get the normalized vertices fine using </p>

<pre><code>object_.bounding_poly.normalized_vertices
</code></pre>

<p>I have tried to do this , but it doesnt work </p>

<pre><code>new_array = np.array(object_.bounding_poly.normalized_vertices)
cv2.polylines(frame, new_array, True, (0, 255, 0), 3)
</code></pre>

<p>Any Help would be appreciated :-)</p>",,0,0,,2019-05-31 08:35:27.933 UTC,,2019-05-31 08:35:27.933 UTC,,,,,2396065,1,0,opencv|opencv3.0|google-cloud-vision|google-vision|opencv4,10
Response 400 from Google Vision API OCR with a base64 string of specified image,49918950,Response 400 from Google Vision API OCR with a base64 string of specified image,"<p>I've read <a href=""https://stackoverflow.com/questions/43094048/how-to-use-the-google-vision-api-for-text-detection-from-base64-encoded-image"">How to use the Google Vision API for text detection from base64 encoded image?</a> but it doesn't help at all. <a href=""https://cloud.google.com/vision/docs/detecting-text"" rel=""nofollow noreferrer"">Cloud client library</a> is undesirable for me because I am doing many image processing (e.g. rotating, cropping, resizing, etc.) before and during OCR. Saving them as new files and re-read them as inputs of Google Vision API is rather inefficient.</p>

<p>Hence, I went check the documentation of posting requests directly:</p>

<ul>
<li><a href=""https://cloud.google.com/vision/docs/using-python"" rel=""nofollow noreferrer"">Using Python to send requests</a></li>
<li><a href=""https://cloud.google.com/vision/docs/base64"" rel=""nofollow noreferrer"">Base64 Encoding</a></li>
<li><a href=""https://cloud.google.com/vision/docs/ocr"" rel=""nofollow noreferrer"">Optical character recognition (OCR)</a>,</li>
</ul>

<p>and here are minimum codes to make the failure:</p>

<pre><code>import base64
import requests
import io

# Read the image file and transform it into a base64 string
with io.open(""photos/foo.jpg"", 'rb') as image_file:
    image = image_file.read()
content = base64.b64encode(image)

# Prepare the data for request
# Format copied from https://cloud.google.com/vision/docs/ocr
sending_request = {
  ""requests"": [
    {
      ""image"": {
        ""content"": content
      },
      ""features"": [
        {
          ""type"": ""TEXT_DETECTION""
        }
      ]
    }
  ]
}

# Send the request and get the response
# Format copied from https://cloud.google.com/vision/docs/using-python
response = requests.post(
    url='https://vision.googleapis.com/v1/images:annotate?key={}'.format(API_KEY),
    data=sending_request,
    headers={'Content-Type': 'application/json'}
)

# Then get 400 code
response
# &lt;Response [400]&gt;
print(response.text)
{
  ""error"": {
    ""code"": 400,
    ""message"": ""Invalid JSON payload received. Unexpected token.\nrequests=image&amp;reque\n^"",
    ""status"": ""INVALID_ARGUMENT""
  }
}
</code></pre>

<p>I went to my console and see there are indeed request errors for <code>google.cloud.vision.v1.ImageAnnotator.BatchAnnotateImages</code>, but I don't know what happened. Is it because the wrong format of sent <code>data</code> in <code>requests.post</code>?</p>",49922356,1,0,,2018-04-19 10:36:10.600 UTC,1,2018-04-19 14:30:52.430 UTC,,,,,6666231,1,2,python|ocr|google-vision,786
Google Cloud Vision Document OCR - keep layout in the resulted text,48077116,Google Cloud Vision Document OCR - keep layout in the resulted text,"<p>I use Google Cloud Vision Document OCR API. The resulted text that is returned by <code>com.google.cloud.vision.v1.AnnotateImageResponse.getFullTextAnnotation().getText()</code> is a little bit messy and lose the text formatting presented on the original image.</p>

<p>Is there with Google Cloud Vision Document OCR API a way to keep the layout(formatting) in the resulted text?</p>",,1,0,,2018-01-03 12:09:11.813 UTC,1,2018-01-24 14:09:11.750 UTC,2018-01-03 14:30:00.933 UTC,,1219755,,1219755,1,2,ocr|google-cloud-vision|google-vision,354
aws rekognition - issue with job id,49181313,aws rekognition - issue with job id,"<p>***EDIT: the issue is that there were items in the SQS queue that needed to be purged.*********</p>

<p>could you please help with an issue I am having?</p>

<p>I followed the steps in the URL below, but the first time I ran the Java code it failed and now I get this message every time the code runs:
""Job found was ""f4ead620611a136a66826461377976d4467eee36dd9e06070bb96bd94b182a35""
Job received was not job 9390b07d024dde7065189d8f99399418de75da42142d919c65be32f1f15c0885""</p>

<p><a href=""https://docs.aws.amazon.com/rekognition/latest/dg/procedure-person-search-videos.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/rekognition/latest/dg/procedure-person-search-videos.html</a></p>

<p>Does anyone know how to kill/delete the ""f4ead620611a136a66826461377976d4467eee36dd9e06070bb96bd94b182a35"" job?</p>",,1,1,,2018-03-08 19:47:36.713 UTC,,2018-03-08 20:11:56.867 UTC,2018-03-08 20:11:13.887 UTC,,9464157,,9464157,1,0,amazon-rekognition,65
Convert Google Vision API response to JSON,48623615,Convert Google Vision API response to JSON,"<p><strong>Task</strong>: </p>

<ul>
<li>Convert Google Vision API response to JSON</li>
</ul>

<p><strong>Problem</strong>:</p>

<ul>
<li>The return value from the API call is not in a JSON format</li>
</ul>

<p><strong>Python Function</strong></p>

<pre><code>def detect_logos(path):
""""""Detects logos in the file.""""""
client = vision.ImageAnnotatorClient()

# [START migration_logo_detection]
with io.open(path, 'rb') as image_file:
    content = image_file.read()

image = types.Image(content=content)

response = client.logo_detection(image=image)
logos = response.logo_annotations

print('Logos:')
print(logos)
print(type(logos))
</code></pre>

<p><strong>Google online JSON</strong></p>

<pre><code>""logoAnnotations"": [
{
  ""mid"": ""/m/02wwnh"",
  ""description"": ""Maxwell House"",
  ""score"": 0.41142157,
  ""boundingPoly"": {
    ""vertices"": [
      {
        ""x"": 74,
        ""y"": 129
      },
      {
        ""x"": 161,
        ""y"": 129
      },
      {
        ""x"": 161,
        ""y"": 180
      },
      {
        ""x"": 74,
        ""y"": 180
      }
    ]
  }
}
</code></pre>

<p><strong>Google Response (List)</strong>   </p>

<pre><code> [mid: ""/m/02wwnh""
description: ""Maxwell House""
score: 0.4114215672016144
bounding_poly {
  vertices {
    x: 74
    y: 129
  }
  vertices {
    x: 161
    y: 129
  }
  vertices {
    x: 161
    y: 180
  }
  vertices {
    x: 74
    y: 180
  }
}
]
</code></pre>

<p><strong>Type</strong>:</p>

<blockquote>
  <p>google.protobuf.internal.containers.RepeatedCompositeFieldContainer</p>
</blockquote>

<p><strong>Tried</strong>: </p>

<p><a href=""https://stackoverflow.com/questions/19734617/protobuf-to-json-in-python"">Protobuf to json in python</a></p>",48636894,2,4,,2018-02-05 13:24:50.813 UTC,1,2018-02-14 07:30:41.450 UTC,2018-02-06 05:50:31.087 UTC,,9234103,,9234103,1,3,python|json|google-api,1259
how to use google cloud vision to extract multiple text language in android studio,47537811,how to use google cloud vision to extract multiple text language in android studio,"<p>I am trying to build an android application (in android studio platform) which extracts different text languages from image using google cloud vision, but I have a problem in starting. </p>

<p>I don't know how to use google cloud files. Which files do I need to create or download and how to direct my API to extract multiple languages?</p>

<p>I got the API and this source code :</p>

<blockquote>
  <p>POST <a href=""https://vision.googleapis.com/v1/images:annotate?key=YOUR_API_KEY"" rel=""nofollow noreferrer"">https://vision.googleapis.com/v1/images:annotate?key=YOUR_API_KEY</a></p>
</blockquote>

<pre><code>{
  ""requests"": [
    {
      ""image"": {
        ""content"": ""/9j/7QBEUGhvdG9zaG9...base64-encoded-image-content...fXNWzvDEeYxxxzj/Coa6Bax//Z""
      },
      ""features"": [
        {
          ""type"": ""TEXT_DETECTION""
        }
      ]
    }
  ]
}



public static void detectText(String filePath, PrintStream out) throws Exception, IOException {
  List&lt;AnnotateImageRequest&gt; requests = new ArrayList&lt;&gt;();

  ByteString imgBytes = ByteString.readFrom(new FileInputStream(filePath));

  Image img = Image.newBuilder().setContent(imgBytes).build();
  Feature feat = Feature.newBuilder().setType(Type.TEXT_DETECTION).build();
  AnnotateImageRequest request =
      AnnotateImageRequest.newBuilder().addFeatures(feat).setImage(img).build();
  requests.add(request);

  try (ImageAnnotatorClient client = ImageAnnotatorClient.create()) {
    BatchAnnotateImagesResponse response = client.batchAnnotateImages(requests);
    List&lt;AnnotateImageResponse&gt; responses = response.getResponsesList();

    for (AnnotateImageResponse res : responses) {
      if (res.hasError()) {
        out.printf(""Error: %s\n"", res.getError().getMessage());
        return;
      }

      // For full list of available annotations, see http://g.co/cloud/vision/docs
      for (EntityAnnotation annotation : res.getTextAnnotationsList()) {
        out.printf(""Text: %s\n"", annotation.getDescription());
        out.printf(""Position : %s\n"", annotation.getBoundingPoly());
      }
    }
  }
}
</code></pre>",,1,0,,2017-11-28 17:43:37.480 UTC,,2017-11-30 19:16:12.923 UTC,2017-11-29 05:18:15.230 UTC,,5894241,,4748115,1,0,google-cloud-platform|ocr|google-vision|android-vision,974
How to use google cloud vision with Google App Engine Python?,43814654,How to use google cloud vision with Google App Engine Python?,"<p>I am trying to use <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/</a> in my Google App Engine Python application. Are there examples for using it?</p>

<p>I followed the tutorial: <a href=""https://www.programmableweb.com/news/how-to-build-monitoring-application-google-cloud-vision-api/how-to/2016/03/29?page=4"" rel=""nofollow noreferrer"">https://www.programmableweb.com/news/how-to-build-monitoring-application-google-cloud-vision-api/how-to/2016/03/29?page=4</a> </p>

<p>and</p>

<p>I am getting an error when I use the following after enabling the Cloud-Vision API in the API manager:</p>

<pre><code>from googleapiclient.discovery import build
from oauth2client.client import GoogleCredentials
</code></pre>

<p><strong>Error</strong></p>

<pre><code>   from googleapiclient.discovery import build
ImportError: No module named googleapiclient.discovery
</code></pre>

<p>Am I missing a dependency configuration?</p>",43818893,1,0,,2017-05-05 22:33:43.463 UTC,1,2017-05-09 15:56:02.197 UTC,2017-05-08 22:55:15.033 UTC,,376742,,376742,1,0,python|google-app-engine|google-api-python-client|google-cloud-vision,674
data usage restrictions on Google Vision [google-cloud-vision],42258263,data usage restrictions on Google Vision [google-cloud-vision],<p>Could not find any information on Google T &amp; C on whether Google Vision is allowed to use data it has to train it's API. Anyone knows?</p>,,0,0,,2017-02-15 19:20:02.283 UTC,,2017-10-07 15:28:06.587 UTC,2017-10-07 15:28:06.587 UTC,,712995,,7570933,1,1,vision,34
Specify language for response in Google Cloud Vision API,50764331,Specify language for response in Google Cloud Vision API,"<p>I'm using the Google Cloud Vision API to detect landmarks, webEntities and other things from a given image (check the docs <a href=""https://cloud.google.com/vision/docs/libraries"" rel=""nofollow noreferrer"">here</a>), I am specifically using the images:annotate endpoint, and I want to specify the language, I want the returned results to be in English.</p>

<p>Is there a way I can achieve that?</p>",,1,2,,2018-06-08 15:54:37.010 UTC,,2018-08-01 14:47:31.570 UTC,,,,,5294761,1,0,android|google-cloud-platform|google-cloud-vision,314
Using the Google Cloud Vision API with a simple API key,43748559,Using the Google Cloud Vision API with a simple API key,"<p>I am using the Google Cloud Vision Java API client documented here: <a href=""https://cloud.google.com/vision/docs/reference/libraries"" rel=""noreferrer"">https://cloud.google.com/vision/docs/reference/libraries</a>.</p>

<p>The following quickstart code works fine if I use the implicit default credentials by setting the GOOGLE_APPLICATION_CREDENTIALS environment variable to reference a json file for the right ""service account"".</p>

<pre><code>// Imports the Google Cloud client library
import com.google.cloud.vision.spi.v1.ImageAnnotatorClient;
import com.google.cloud.vision.v1.AnnotateImageRequest;
import com.google.cloud.vision.v1.AnnotateImageResponse;
import com.google.cloud.vision.v1.BatchAnnotateImagesResponse;

...


public class QuickstartSample {
  public static void main(String... args) throws Exception {
    // Instantiates a client
    ImageAnnotatorClient vision = ImageAnnotatorClient.create();

    ...

    BatchAnnotateImagesResponse response = vision.batchAnnotateImages(requests);
    List&lt;AnnotateImageResponse&gt; responses = response.getResponsesList();

    ...
  }
}
</code></pre>

<p>However, I want to authenticate to the API using a simple (single-string) API key rather than a service account, and I cannot find documentation explaining how to do that through this java library.  Is it possible?</p>",,1,4,,2017-05-02 23:03:10.067 UTC,3,2019-02-25 01:09:02.100 UTC,,,,,255999,1,7,java|google-api|api-key|google-cloud-vision,2055
How to get frame of detected Barcode using Google Vision api for Barcode detection,42185396,How to get frame of detected Barcode using Google Vision api for Barcode detection,<p>The google Vision's Barcode detection API works fine and gets the result of the scanned barcode using Android. But I didn't find any way to get the frame from which the barcode is detected. Is there any way to get that exact frame?</p>,,0,1,,2017-02-12 07:57:59.323 UTC,1,2017-02-12 07:57:59.323 UTC,,,,,4866249,1,5,java|android|frame|barcode-scanner|google-vision,219
How to send image byte to Lambda through Boto3?,51676317,How to send image byte to Lambda through Boto3?,"<p>Is it possible to send image byte through Lambda using Boto3? The byte will be sent to Lambda function which will then forward the image to Rekognition. I've tried this but it didn't work:  </p>

<pre><code>with open(image_name) as image_source:
    image_bytes = image_source.read()

context = base64.b64encode(b'{""custom"":{ \
    ""image_name"":""'+imagename+'"", \
    ""image_bytes"" : ""'+image_bytes+'""}}').decode('utf-8')

response = lambda_fn.invoke(
    ClientContext=context,
    FunctionName='recognize-face-in-image'
)
</code></pre>

<p>And this is the Lambda function code:  </p>

<pre><code>import boto3  
import base64  
import json  

def lambda_handler(event, context):
 print(""Lambda triggered..."")
 rek = boto3.client('rekognition')

 context_dict = context.client_context.custom
 image_bytes = context_dict[""image_bytes""]
 rekresp = rek.detect_faces(Image={'Bytes': image_bytes},Attributes=['ALL'])
 if not rekresp['FaceDetails']:
     print ""No face""
 else:
     print ""Got face""  
</code></pre>

<p>When I run it, this is the Lambda function error shown in Cloudwatch:  </p>

<blockquote>
  <p>An error occurred (ValidationException) when calling the DetectFaces
  operation: 1 validation error detected: Value
  'java.nio.HeapByteBuffer[pos=0 lim=0 cap=0]' at 'image.bytes' failed
  to satisfy constraint: Member must have length greater than or equal
  to 1: ClientError Traceback (most recent call last): File
  ""/var/task/lambda_function.py"", line 17, in lambda_handler rekresp =
  rek.detect_faces(Image={'Bytes': image_bytes},Attributes=['ALL']) File
  ""/var/runtime/botocore/client.py"", line 314, in _api_call return
  self._make_api_call(operation_name, kwargs) File
  ""/var/runtime/botocore/client.py"", line 612, in _make_api_call raise
  error_class(parsed_response, operation_name) ClientError: An error
  occurred (ValidationException) when calling the DetectFaces operation:
  1 validation error detected: Value 'java.nio.HeapByteBuffer[pos=0
  lim=0 cap=0]' at 'image.bytes' failed to satisfy constraint: Member
  must have length greater than or equal to 1</p>
</blockquote>",51718171,2,0,,2018-08-03 15:56:47.347 UTC,,2018-08-07 02:57:40.350 UTC,2018-08-04 03:32:45.113 UTC,,174777,,3359297,1,1,python|amazon-web-services|aws-lambda|boto3,477
Watson Visual-Recognition not using custom classifier,40512241,Watson Visual-Recognition not using custom classifier,"<p>I am working with Watson Visual Recognition and have successfully created a custom classifier. The classifier shows that it is ready with the following status: </p>

<pre><code>{
""classifier_id"": ""paintings_----"",
""name"": ""paintings"",
""owner"": ""--- owner id -----"",
""status"": ""ready"",
""created"": ""2016-11-09T14:55:45.835Z"",
""classes"": [
    {""class"": ""water""},
    {""class"": ""collage""},
    {""class"": ""forest""},
    {""class"": ""beach""},
    {""class"": ""still""},
    {""class"": ""abstract""},
    {""class"": ""building""},
    {""class"": ""garden""}
],
""retrained"": ""2016-11-09T15:11:50.740Z""
}
</code></pre>

<p>I am executing the following curl command to test this classifier: </p>

<pre><code>curl -X POST -F ""images_file=@IMG_5309.JPG"" -F ""parameters=@paintings.json"" ""https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classify?api_key={valid API key}&amp;version=2016-05-20&amp;threshold=0.0""
</code></pre>

<p>and the paintings.json file has the following content: </p>

<pre><code>{
""parameters"":{
  ""classifier_ids"": [
     ""water"",
     ""collage"",
     ""forest"",
     ""beach"",
     ""still"",
     ""abstract"",
     ""building"",
     ""garden""
    ] ,
  ""owner"":""me"",
  ""threshold"":"".5""
 }
}
</code></pre>

<p>Running this query returns the following result: </p>

<pre><code>{
""custom_classes"": 0,
""images"": [
    {
        ""classifiers"": [
            {
                ""classes"": [
                    {
                        ""class"": ""vegetation"",
                        ""score"": 1.0
                    },
                    {
                        ""class"": ""flower"",
                        ""score"": 0.668188,
                        ""type_hierarchy"": ""/products/gifts/flower""
                    },
                    {
                        ""class"": ""purple"",
                        ""score"": 0.268941,
                        ""type_hierarchy"": ""/colors/purple""
                    }
                ],
                ""classifier_id"": ""default"",
                ""name"": ""default""
            }
        ],
        ""image"": ""IMG_5309.JPG""
    }
],
""images_processed"": 1
}
</code></pre>

<p>Visual-Recognition is obviously not using my classifier file and I've probably missed something REALLY obvious. Any ideas on what I've missed? I am following the documentation here: <a href=""https://www.ibm.com/watson/developercloud/visual-recognition/api/v3/#classify_an_image"" rel=""nofollow noreferrer"">https://www.ibm.com/watson/developercloud/visual-recognition/api/v3/#classify_an_image</a> which states that the JSON parameters are: </p>

<p><strong>classifier_ids</strong> - An array of classifier IDs to classify the images against.</p>

<p><strong>owners</strong> - An array with the value(s) ""IBM"" and/or ""<strong>me</strong>"" to specify which classifiers to run.</p>

<p><strong>threshold</strong> - A floating point value that specifies the minimum score a class must have to be displayed in the response.</p>",40527628,3,0,,2016-11-09 16:58:26.057 UTC,,2018-03-12 06:01:57.930 UTC,,,,,2144140,1,0,curl|classification|visual-recognition,490
What are the correct params for Google Cloud Vision JSON Data,36416503,What are the correct params for Google Cloud Vision JSON Data,"<p>I'm getting this error:</p>

<p><code>error"": { ""code"": 400, ""message"": ""Invalid value at 'requests[0].image.content' (TYPE_BYTES), \""000002.jpg\"""", ""status"": ""INVALID_ARGUMENT"", ""details"": [ { ""@type"": ""type.googleapis.com/google.rpc.BadRequest""</code></p>

<p>And it may have something to do with the encoding of the images, not sure.  I am sending Google a bunch of <code>.jpg</code>s.  </p>

<p>Note that this is similar to <a href=""https://stackoverflow.com/questions/35925120/google-cloud-vision-getting-invalid-value-at-requests0-image-content"">this question</a>, but that one doesn't help a whole lot - there's no answer.</p>

<p>Here's my JSON:  </p>

<p><code>for f in $FILES
  do
    echo ""Original file name is $f""
    response=$(curl -v -H ""Accept: application/json"" -H ""Content-type:<br>
    application/json"" -X POST -d '{""requests"":[
       {
         ""image"":{
         ""content"":""'""$f""'""
       },
      ""features"":[
        {
          ""type"":""LABEL_DETECTION"",
          ""maxResults"":3
        }
      ]
    }]}' $baseURL)
    echo $response
done
</code></p>

<p>This is the first image I send it. <a href=""https://i.stack.imgur.com/ZN7wp.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZN7wp.jpg"" alt=""enter image description here""></a></p>

<p>Any thoughts on what's causing this?</p>",,1,0,,2016-04-05 03:15:37.117 UTC,,2016-04-11 11:00:52.253 UTC,2017-05-23 11:45:11.027 UTC,,-1,,464273,1,0,json|bash|google-cloud-vision,225
How to retain HTML formatting in GitHub readme file upon upload?,50786752,How to retain HTML formatting in GitHub readme file upon upload?,"<p>So i formatted the <code>README.md</code> file of a particular <code>GitHub</code> project using HTML as I found markdown to be quite limiting. Maybe I am not quite well versed with markdown or I prefer HTML, I am not sure. So the issue is, I have the README.md file on my local system and when I display it on browser using a Markdown plugin from Sublime Text, it shows up exactly as I want. But when I push the local README.md file to the server and try to view it in website, the formatting is lost completely. </p>

<p>Local formatting -</p>

<p><a href=""https://i.stack.imgur.com/19L3G.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/19L3G.jpg"" alt=""Local README.md""></a></p>

<p>GitHub website view -</p>

<p><a href=""https://i.stack.imgur.com/T7Get.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T7Get.jpg"" alt=""website README.md""></a></p>

<p>As you can see, the two files are totally different. How can I preserve the formatting once it is uploaded on the GitHub server ? </p>

<p>The following is the spaghetti HTML code. It is very ugly.  I was in a hurry and so ended up with such ugly code violating every aspect of DRY. Please excuse that for now. </p>

<pre><code>    &lt;div class=""header"" style=""width: 100%; display: flex;""&gt;
    &lt;div style=""font-size: 50px; font-family: arial; width: 50%;""&gt; Blind Reader&lt;/div&gt; 
    &lt;div style=""width: 50%; text-align: right; display: table; ""&gt;
        &lt;span style="" letter-spacing: 5px; padding-left: 150px; font-family: verdana; font-size: 11px;  display: table-cell;vertical-align: middle ;  width: 20px;""&gt; Developers &lt;/span&gt;
        &lt;a href=""https://github.com/boudhayan-dev"" style="" padding-right: 17px;""&gt;&lt;img src=""images/dev1.png"" style=""height: 60px; width: 60px;""&gt;&lt;/a&gt;
        &lt;a href=""https://github.com/chinmay4382"" style="" padding-right: 17px;""&gt;&lt;img src=""images/dev2.png"" style=""height: 60px; width: 60px;""&gt;&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

 &lt;div class=""badges-container""&gt;
    &lt;div class=""badges-body""&gt; 
        [![Ask Me Anything !](https://img.shields.io/badge/Ask%20me-anything-1abc9c.svg?longCache=true&amp;style=plastic)](https://GitHub.com/Naereen/ama) [![made-with-python](https://img.shields.io/badge/Made%20with-Python-blue.svg?longCache=true&amp;style=plastic)](https://www.python.org/) [![GitHub license](https://img.shields.io/github/license/Naereen/StrapDown.js.svg?longCache=true&amp;style=plastic)](https://github.com/Naereen/StrapDown.js/blob/master/LICENSE)  ![PyPI - Status](https://img.shields.io/pypi/status/Django.svg?style=plastic) ![Contributor](https://img.shields.io/badge/Contributors-2-orange.svg?longCache=true&amp;style=plastic) 
    &lt;/div&gt;
 &lt;/div&gt;


&lt;div class=""body-content""&gt; 
    &lt;span style=""font-size: 25px; font-family: verdana; color: #64686d;""&gt; Welcome to the &lt;span style=""color: #18529b;""&gt;Blind Reader&lt;/span&gt; project !&lt;/span&gt;
    &lt;br&gt;
    &lt;br&gt;
    &lt;div style=""font-size: 18px; font-family: verdana; text-align: justify;"" class=""introduction""&gt;Blind Reader is a portable, low-cost, reading device made for the blind people. The Braille machines are expensive and as a result are not accessible to many. &lt;strong&gt;Blind Reader &lt;/strong&gt;overcomes the limitation of conventional Braille machine by making it affordable for the common masses. The system uses OCR technology to convert images into text and reads out the text by using Text-to-Speech conversion.The system supports audio output via Speakers as well as headphone. The user also has the ability to pause the audio output whenever he desires. It also has the facility to store the images in their respective book folder, thereby creating digital backup simultaneously. With this system, the blind user does not require the complexity of Braille machine to read a book. All it takes is a button to control the entire system !
    &lt;/div&gt;
    &lt;div class=""dependency"" style=""font-family: verdana; font-size: 18px; padding-top: 30px;""&gt;
        &lt;span style=""font-size: 30px; font-family: verdana; font-weight: 500;""&gt;Dependency&lt;/span&gt;
        &lt;div style=""background:#757a79;height: 1.2px; width: 100%""&gt;&lt;/div&gt;&lt;br&gt;
        &lt;span style=""font-size: 18px; font-family: verdana; font-weight: 600;""&gt;Hardware Requirements:&lt;/span&gt;&lt;br&gt;
            &lt;ul&gt;
                &lt;li&gt;Raspberry Pi 3B.&lt;/li&gt;
                &lt;li&gt;Pi Camera.&lt;/li&gt;
                &lt;li&gt;Speakers / Headphones.&lt;/li&gt;
                &lt;li&gt;Push buttons - 2.&lt;/li&gt;
                &lt;li&gt;LDR - 1.&lt;/li&gt;
                &lt;li&gt;LED - 4.&lt;/li&gt;
                &lt;li&gt;Power supply - 5V,2A.&lt;/li&gt;
            &lt;/ul&gt;
        &lt;span style=""font-size: 18px; font-family: verdana; font-weight: 600;""&gt;Software Requirements:&lt;/span&gt;&lt;br&gt;
        &lt;ul&gt;
                &lt;li&gt;Python 3.&lt;/li&gt;
                &lt;li&gt;Python Dependencies:&lt;/li&gt;
                &lt;ul&gt;
                    &lt;li&gt;Rpi.GPIO&lt;/li&gt;
                    &lt;li&gt;Pygame library.&lt;/li&gt;
                    &lt;li&gt;picamera library.&lt;/li&gt;
                    &lt;li&gt;google-cloud.&lt;/li&gt;
                    &lt;li&gt;time.&lt;/li&gt;
                    &lt;li&gt;os.&lt;/li&gt;
                    &lt;li&gt;datetime.&lt;/li&gt;
                &lt;/ul&gt;
                &lt;li&gt;Google Cloud API - Vision , Text-to-Speech&lt;/li&gt;
            &lt;/ul&gt;
    &lt;/div&gt;
    &lt;div class=""code""  style=""font-family: verdana; font-size: 18px; padding-top: 30px;""&gt;
        &lt;span style=""font-size: 30px; font-family: verdana; font-weight: 500;""&gt;Usage&lt;/span&gt;
        &lt;div style=""background:#757a79;height: 1.2px; width: 100%""&gt;&lt;/div&gt;&lt;br&gt;
    &lt;/div&gt;
    &lt;div class=""usage-content"" style=""font-size: 18px; font-family: verdana; text-align: justify;""&gt;
        &lt;ul&gt;
            &lt;li&gt;
                Use the following code to install the Google cloud python dependency.&lt;br&gt;&lt;br&gt;&lt;code&gt;pip3 install --upgrade google-api-python-client&lt;br&gt;pip3 install --upgrade google-cloud-vision&lt;br&gt;pip3 install --upgrade google-cloud
                &lt;/code&gt;&lt;br&gt;&lt;br&gt;
                Use : &lt;a href=""https://developers.google.com/api-client-library/python/apis/vision/v1""&gt;Google CLoud Vision API &lt;/a&gt; for further Details.&lt;br&gt;&lt;br&gt;
            &lt;/li&gt;
            &lt;li&gt; Activate &lt;strong&gt;Cloud Vision API&lt;/strong&gt; and &lt;strong&gt;Google Cloud Text-to-Speech API&lt;/strong&gt; by visiting the dashboard and download the Service account credentials (Json file).&lt;/li&gt;
            &lt;br&gt;
            &lt;li&gt;
                Connect the hardware as follows:
                &lt;ul&gt;
                    &lt;li&gt;
                        Pi Camera --&gt; Camera Slot in Raspberry Pi 3.
                    &lt;/li&gt;
                    &lt;li&gt;
                        Pair Bluetooth Speaker / Insert headphone into Raspberry Pi 3 audio jack.
                    &lt;/li&gt;
                    &lt;li&gt;
                        LDR --&gt; GPIO 37.
                    &lt;/li&gt;
                    &lt;li&gt;
                        4 LEDs - GPIO 29 , 31 , 33 , 35 respectively.
                    &lt;/li&gt;
                    &lt;li&gt;
                        Push Button 1 ( Camera capture ) --&gt; GPIO 16.
                    &lt;/li&gt;
                    &lt;li&gt;
                        Push Button 2 ( Play/Pause audio ) --&gt; GPIO 18.
                    &lt;/li&gt;
                &lt;/ul&gt;
                &lt;br&gt;
            &lt;li&gt;
                Use the following code to start the system:
                &lt;br&gt;
                &lt;code&gt;
                    python3 //path/to/your/final.py/file
                &lt;/code&gt;
            &lt;/li&gt;
            &lt;br&gt;
            &lt;li&gt;
                Place the image to be read under the camera and press &lt;code&gt; Button 1 &lt;/code&gt; to read out a page.
            &lt;/li&gt;
        &lt;/ul&gt;
    &lt;/div&gt;
    &lt;div class=""system-images"" style=""font-family: verdana; font-size: 18px; padding-top: 30px;""&gt;
        &lt;span style=""font-size: 30px; font-family: verdana; font-weight: 500;""&gt;Demonstration&lt;/span&gt;
        &lt;div style=""background:#757a79;height: 1.2px; width: 100%""&gt;&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=""image-cotainer"" style=""display: flex;""&gt;
        &lt;div class=""image1"" style=""width: 50%""&gt; &lt;img src=""images/system1.jpg"" style=""width: 80%;""&gt;&lt;/div&gt;
        &lt;div class=""image2"" style=""width: 50%""&gt; &lt;img src=""images/system2.jpg"" style="" width: 80%; height: 80%; padding-top: 40px;""&gt;&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=""resources-section"" style=""font-family: verdana; font-size: 18px;""&gt;
        &lt;span style=""font-size: 30px; font-family: verdana; font-weight: 500;""&gt;Resources&lt;/span&gt;
        &lt;div style=""background:#757a79;height: 1.2px; width: 100%""&gt;&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=""resources-container"" style=""font-family: verdana; font-size: 18px;""&gt;
        &lt;ul&gt;&lt;br&gt;
            &lt;li&gt;
                &lt;a href=""https://cloud.google.com/python/docs/reference/""&gt;Google Cloud Platform.&lt;/a&gt;
            &lt;/li&gt;
            &lt;li&gt;
                &lt;a href=""https://www.pygame.org/news""&gt;Pygame python library.&lt;/a&gt;
            &lt;/li&gt;
            &lt;li&gt;
                &lt;a href=""https://www.raspberrypi.org/""&gt;Raspberry Pi.&lt;/a&gt;
            &lt;/li&gt;
            &lt;li&gt;
                &lt;a href=""https://www.python.org/""&gt;Python.&lt;/a&gt;
            &lt;/li&gt;
        &lt;/ul&gt;
    &lt;/div&gt;


&lt;/div&gt;
</code></pre>

<p>I have also takena  look at this <a href=""https://github.com/jch/html-pipeline/blob/master/lib/html/pipeline/sanitization_filter.rb#L41-L44"" rel=""nofollow noreferrer"">link</a>. It lists all the tags that are whitelisted by GitHub. And as I can see almost all the tags I have used are present here.</p>

<p>Please help.</p>",,1,6,,2018-06-10 18:19:15.090 UTC,,2018-06-11 14:48:48.350 UTC,2018-06-10 18:31:54.070 UTC,,9122915,,9122915,1,0,html|html5|github|markdown|github-flavored-markdown,258
Cropping a detected face,54725524,Cropping a detected face,"<p>i want to crop face detected from image that i capture from my phone. i'm using google vision API to detect face. i saw some questions similar to mine but they're using openCV.</p>

<p>i tried adding Bitmap.createBitmap();</p>

<p>but it can only accept int values. but my values has decimal so it's a float.</p>

<p>this is my code for face detection :</p>

<pre><code>private void scanFaces(){

     Bitmap bitmap = BitmapFactory.decodeFile(pathToFile);

    if (detector.isOperational() &amp;&amp; bitmap != null) {
        editedBitmap = Bitmap.createBitmap(bitmap.getWidth(), bitmap
                .getHeight(), bitmap.getConfig());
        float scale = getResources().getDisplayMetrics().density;
        Paint paint = new Paint(Paint.ANTI_ALIAS_FLAG);
        paint.setColor(Color.rgb(255, 61, 61));
        paint.setTextSize((int) (14 * scale));
        paint.setShadowLayer(1f, 0f, 1f, Color.WHITE);
        paint.setStyle(Paint.Style.STROKE);
        paint.setStrokeWidth(3f);
        Canvas canvas = new Canvas(editedBitmap);
        canvas.drawBitmap(bitmap, 0, 0, paint);
        Frame frame = new Frame.Builder().setBitmap(editedBitmap).build();
        SparseArray&lt;Face&gt; faces = detector.detect(frame);

        }

        if (faces.size() == 0) {

            deleteMediaFile();
            Intent intent = new Intent(FaceRegistration.this, NoFaceDetected.class);
            startActivity(intent);

        }
        else {
            Face face = faces.valueAt(index);
            canvas.drawRect(
                    face.getPosition().x,
                    face.getPosition().y,
                    face.getPosition().x + face.getWidth(),
                    face.getPosition().y + face.getHeight(), paint);
            imageView.setImageBitmap(editedBitmap);
        }
    } else {
        Toast.makeText(getApplicationContext(),""could not set up detector!"" , Toast.LENGTH_SHORT).show();
    }
}
</code></pre>",,0,0,,2019-02-16 17:03:21.213 UTC,,2019-02-16 18:56:42.127 UTC,2019-02-16 18:56:42.127 UTC,,2649012,,10918556,1,0,android|image|crop|face-detection,58
How to convert an image in imageView to text and present that text in a textView in android?,42731433,How to convert an image in imageView to text and present that text in a textView in android?,"<p>I am doing a module in which I need to [take an image -> crop it -> convert it to text] . I have done till taking a photo and cropping it and presenting it in imageView . But I am unable to convert that image in imageView to text . </p>

<p>This is the code :</p>

<pre><code>protected void onActivityResult(int requestCode, int resultCode, Intent data) {
        if (resultCode == RESULT_OK) {
            //user is returning from capturing an image using the camera
            if(requestCode == CAMERA_CAPTURE){
                //get the Uri for the captured image
                Uri uri = picUri;
                //carry out the crop operation
                performCrop();
                Log.d(""picUri"", uri.toString());

            }

            else if(requestCode == PICK_IMAGE_REQUEST){
                picUri = data.getData();
                Log.d(""uriGallery"", picUri.toString());
                performCrop();
            }

            //user is returning from cropping the image
            else if(requestCode == PIC_CROP){
                //get the returned data
                Bundle extras = data.getExtras();
                //get the cropped bitmap
                Bitmap thePic = (Bitmap) extras.get(""data"");
                //display the returned cropped image
                imageView.setImageBitmap(thePic);

            }

        }
    }
</code></pre>

<p>So what else can I do after [image.setImageBitmap(thePic)] to convert it to text and present in a TextView. 
I tried using Google Vision API code after [image.setImageBitmap(thePic)] but unable to convert it to text.It was showing an error saying 
[jni_helper.cc:110 Bitmap is of the wrong format: 4]<br>
[Fatal signal 6 (SIGABRT), code -6]</p>

<p>So, how to solve this problem.</p>",,1,1,,2017-03-11 04:44:25.413 UTC,,2017-03-11 05:18:53.653 UTC,2017-03-11 05:18:53.653 UTC,,5322454,,5322454,1,0,android|bitmap|textview|imageview,89
Using Image Recognition,45126387,Using Image Recognition,"<p>Is there any way of using Watsons image classifying abilities to extract information from an image of a document? Rather than simply classifying an image as a, b or c? </p>",,1,0,,2017-07-16 08:07:24.183 UTC,,2017-07-16 09:49:15.537 UTC,,,,,4777487,1,0,ibm-watson|watson,67
How to use javascript adapter to post an image to a server and get xml result?,38322210,How to use javascript adapter to post an image to a server and get xml result?,"<p>The adapter worked before and now the adapter does not work.
Mobile Foundation 8.0 beta on my local machine and did a migration to MobileFirst Foundation 8.0 GA release.</p>

<p>The <em>.java server</em> send the image to <em>alchemy</em> and just returns the <em>alchemy</em> xml result to MobileFirst adapter. </p>

<p>The Debug information from Android and iOS are different, so I list both.</p>

<p>a) <strong><em>iOS MobileClient:</em></strong></p>

<p>The information on the iOS is different from android.
It seems there could be a <strong>bug</strong> in the cordova mfp-pluging implementation for <strong>XML</strong> adapter:</p>

<p>Because it seems the adapter expects a <strong>""responseJSON""</strong> in the response json structure, this is what I think, based on the debug information..
The response, with is show by xCode debugger automatically, does NOT contain ""responseJSON"", but it contains all valid values from the alchemy result.</p>

<p>Just take a look here in the <strong>xCode Debug</strong> output:</p>

<p>Reason the it sames </p>

<pre><code>016-07-14 11:37:54.021 Check[5996:5470887] &gt;&gt;&gt; Image to get Tags formParams :  [object Object]
2016-07-14 11:37:54.021 Check[5996:5470887] &gt;&gt;&gt; Image to get Tags formParams :  [object Object]
2016-07-14 11:37:54.810 Check[5996:5470887] &gt;&gt;&gt; Failure, getTags is :  [object Object]
2016-07-14 11:37:54.810 Check[5996:5470887] &gt;&gt;&gt; Failure, getTags is :  {""responseHeaders"":{""Connection"":""Keep-Alive"",""Content-Type"":""application/json"",""X-Powered-By"":""Servlet/3.1"",""X-Backside-Transport"":""FAIL FAIL"",""X-Global-Transaction-ID"":""610291743"",""Date"":""Thu, 14 Jul 2016 09:37:54 GMT"",""Transfer-Encoding"":""Identity""},""status"":500,""responseText"":""{\""statusReason\"":\""Internal Server Error\"",\""responseHeaders\"":{\""Transfer-Encoding\"":\""chunked\"",\""X-Backside-Transport\"":\""FAIL FAIL\"",\""Connection\"":\""Keep-Alive\"",\""X-Global-Transaction-ID\"":\""78953555\"",\""Content-Language\"":\""en-US\"",\""Date\"":\""Thu, 14 Jul 2016 09:37:54 GMT\"",\""Content-Type\"":\""text/html;charset=UTF-8\"",\""X-Powered-By\"":\""Servlet/3.1\""},\""isSuccessful\"":false,\""responseTime\"":164,\""totalTime\"":174,\""warnings\"":[],\""errors\"":[\""org.xml.sax.SAXParseException: The element type \\\""meta\\\"" must be terminated by the matching end-tag \\\""&lt;/meta&gt;\\\"".\"",\""Failed to parse the payload from backend (procedure: getTagsForPicture)\""],\""info\"":[],\""statusCode\"":500}"",""responseJSON"":{""statusReason"":""Internal Server Error"",""responseHeaders"":{""Transfer-Encoding"":""chunked"",""X-Backside-Transport"":""FAIL FAIL"",""Connection"":""Keep-Alive"",""X-Global-Transaction-ID"":""78953555"",""Content-Language"":""en-US"",""Date"":""Thu, 14 Jul 2016 09:37:54 GMT"",""Content-Type"":""text/html;charset=UTF-8"",""X-Powered-By"":""Servlet/3.1""},""isSuccessful"":false,""responseTime"":164,""totalTime"":174,""warnings"":[],""errors"":[""org.xml.sax.SAXParseException: The element type \""meta\"" must be terminated by the matching end-tag \""&lt;/meta&gt;\""."",""Failed to parse the payload from backend (procedure: getTagsForPicture)""],""info"":[],""statusCode"":500},""errorCode"":500,""errorMsg"":""Request failed: internal server error (500)"",""invocationContext"":null}
2016-07-14 11:37:54.814 Check[5996:5470887] ERROR GET TAGS***** [object Object]
2016-07-14 11:38:09.341 Check[5996:5470887] Response Content : {""statusReason"":""OK"",""responseHeaders"":{""Transfer-Encoding"":""chunked"",""X-Backside-Transport"":""OK OK"",""Connection"":""Keep-Alive"",""X-Global-Transaction-ID"":""336117367"",""Content-Language"":""en-US"",""Date"":""Thu, 14 Jul 2016 09:38:09 GMT"",""Content-Type"":""text/xml;charset=ISO-8859-1"",""X-Powered-By"":""Servlet/3.1""},""isSuccessful"":true,""responseTime"":2055,""totalTime"":2057,""warnings"":[],""results"":{""totalTransactions"":""4"",""NOTICE"":""THIS API FUNCTIONALITY IS DEPRECATED AND HAS BEEN MIGRATED TO WATSON VISUAL RECOGNITION. THIS API WILL BE DISABLED ON MAY 19, 2017."",""usage"":""By accessing AlchemyAPI or using information generated by AlchemyAPI, you are agreeing to be bound by the AlchemyAPI Terms of Use: http://www.alchemyapi.com/company/terms.html"",""imageKeywords"":{""keyword"":[{""score"":""0.574443"",""text"":""large""},{""score"":""0.5"",""text"":""building""}]},""url"":"""",""status"":""OK""},""errors"":[],""info"":[],""statusCode"":200}
</code></pre>

<p>b) <strong>Android MobileClient:</strong></p>

<p>I get following <em>error</em> when I execute the javascript adapter  XML from a cordova mobile <strong>android</strong> client.</p>

<pre><code>1.  Array[2]
1.  0:""org.xml.sax.SAXParseException: The element type ""meta"" must be terminated by the matching end-tag ""&lt;/meta&gt;"".""
2.  1:""Failed to parse the payload from backend (procedure: getTagsForPicture)""
</code></pre>

<p>The full error information in the chrome console for the <strong>android app</strong>:</p>

<p><a href=""https://i.stack.imgur.com/qNZki.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qNZki.jpg"" alt=""enter image description here""></a></p>

<p>When I inspect the XML on the server log or in a poster, I don’t see any  information. </p>

<p>a) poster xml result:</p>

<p><a href=""https://i.stack.imgur.com/DvrKE.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DvrKE.jpg"" alt=""enter image description here""></a></p>

<p>b) .java server logs in bluemix:</p>

<p><a href=""https://i.stack.imgur.com/buhTT.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/buhTT.jpg"" alt=""enter image description here""></a></p>

<p><strong>The implementation of the Apdater and the MobileClient:</strong></p>

<p>a) <strong>The adapter code:</strong></p>

<pre><code>function getTagsForPicture(urlimage) {
var value = ""image="" + urlimage;

var requestStructure = {
     method : 'post',
     returnedContentType : 'xml',
     path : 'GetImageKeywords',
     parameters: {'image': urlimage },
     headers: {""Accept"":""application\/plain""}
};
MFP.Logger.warn(""Preparing request structure "" + JSON.stringify(requestStructure));
return MFP.Server.invokeHttp(requestStructure);
}
</code></pre>

<p>b) <strong>The cordova client code:</strong></p>

<pre><code> var formParams = {""params"":""['""+ image + ""']""}; 
 var getTagsRequest = new WLResourceRequest(
          ""/adapters/UploadPic/getTagsForPicture"",
          WLResourceRequest.POST);
          console.log('&gt;&gt;&gt; Image to get Tags formParams : ', formParams); 

          getTagsRequest.sendFormParameters(formParams);
          getTagsRequest.send().then(
              getTagsSuccess,
              getTagsFailure
          );
</code></pre>",40403872,1,6,,2016-07-12 07:11:48.333 UTC,,2016-11-03 14:18:49.203 UTC,2016-07-14 13:10:10.780 UTC,,4342353,,4342353,1,1,javascript|xml|post|ibm-mobilefirst,193
Android Amazon Rekognition Text Detection doesn't work with bytes but does with S3 Objects,48893088,Android Amazon Rekognition Text Detection doesn't work with bytes but does with S3 Objects,"<p>I'm trying to detect text in a photo taken with the Camera, but with no luck.</p>

<p>The code I'm using is:</p>

<pre><code>AWSCredentials credentials = new AWSCredentials() {
            @Override
            public String getAWSAccessKeyId() {
                return ""some access key id"";
            }

            @Override
            public String getAWSSecretKey() {
                return ""some secret key"";
            }
        };


        File file = new File(photoFilePath);
        int size = (int) file.length();
        byte[] bytes = new byte[size];
        try {
            BufferedInputStream buf = new BufferedInputStream(new FileInputStream(file));
            buf.read(bytes, 0, bytes.length);
            buf.close();
        } catch (FileNotFoundException e) {
            Timber.e(e);
        } catch (IOException e) {
            Timber.e(e);
        }

        AmazonRekognition rekognitionClient = new AmazonRekognitionClient(credentials);

        byte [] base64 = android.util.Base64.encode(bytes, Base64.DEFAULT);

        Image image = new Image().withBytes(ByteBuffer.wrap(base64));

        DetectTextRequest detectTextRequest = new DetectTextRequest().withImage(image);

        Observable.create((Observable.OnSubscribe&lt;String&gt;) observer -&gt; {
            try {
                DetectTextResult result = rekognitionClient.detectText(detectTextRequest);
                List&lt;TextDetection&gt; labels = result.getTextDetections();

                String alllabels = """";

                for (TextDetection detection : labels) {
                    alllabels += detection.getDetectedText();
                }

                observer.onNext(alllabels);
                observer.onCompleted();
            } catch (AmazonServiceException e) {
                Timber.e(e);
                observer.onError(e);
            } catch (AmazonClientException e) {
                Timber.e(e);
                observer.onError(e);
            }
        })
                .observeOn(AndroidSchedulers.mainThread())
                .subscribeOn(Schedulers.io())
                .subscribe(new Subscriber&lt;String&gt;() {
            @Override
            public void onNext(String item) {
                System.out.println(""Next: "" + item);
            }

            @Override
            public void onError(Throwable error) {
                System.err.println(""Error: "" + error.getMessage());
            }

            @Override
            public void onCompleted() {
                System.out.println(""Sequence complete."");
            }
        });
    }
</code></pre>

<p>This produces an exception with the message</p>

<pre><code>Failed to upload image; the format is not supported
</code></pre>

<p>When not encoding the bytes in base64 - it yields weird outputs, where each text detected is separated by a comma, like </p>

<blockquote>
  <p>S, !!:, 8, anons SAr, !!:, S, 8, anons, SAr, </p>
</blockquote>

<p>or</p>

<blockquote>
  <p>8B, 8B</p>
</blockquote>

<p>What might be wrong in with my example?</p>

<p>When using references to S3 Objects even with the same photo - everything works fine.</p>",49841423,2,0,,2018-02-20 19:46:30.853 UTC,0,2018-04-15 11:31:58.717 UTC,2018-02-20 20:09:33.463 UTC,,414365,,414365,1,1,android|amazon-web-services|aws-sdk|amazon-rekognition,307
Google Cloud Vision - Which region does Google upload the images to?,53886444,Google Cloud Vision - Which region does Google upload the images to?,"<p>I am building an OCR based solution to extract information from certain financial documents. 
As per the regulation in my country (India), this data cannot leave India.</p>

<p>Is it possible to find the region where Google Cloud Vision servers are located?
Alternately, is it possible to restrict the serving region from the GCP console?</p>

<p>This is what I have tried:</p>

<ol>
<li><p>I went through GCP Data Usage FAQ: <a href=""https://cloud.google.com/vision/docs/data-usage"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/data-usage</a></p></li>
<li><p>GCP Terms of Service:
<a href=""https://cloud.google.com/terms/"" rel=""nofollow noreferrer"">https://cloud.google.com/terms/</a>
(Look at point 1.4 Data Location on this page)</p></li>
<li><p>Talking to the GCP Sales rep. He did not know the answer.</p></li>
</ol>

<p>I know that I can talk to Google support but that requires $100 to activate, which is a lot for for me.</p>

<p>Any help would be appreciated. I went through the documentation for Rekognition as well but it seems to send some data outside for training so not considering it at the moment.</p>

<p>PS - Edited to make things I have tried clearer.</p>",,1,8,,2018-12-21 14:30:09.123 UTC,,2018-12-24 08:03:39.437 UTC,2018-12-22 12:01:31.350 UTC,,3524894,,3524894,1,4,google-cloud-platform|google-cloud-vision|amazon-rekognition,84
Str to AnnotateImageResponse in python,54644237,Str to AnnotateImageResponse in python,"<p>I have dumped response from google vision api for several images in text file. Now I need to do some manipulations, but response dumped is in string format.</p>

<p>So, how can I convert str to AnnotateImageResponse or JSON in python?</p>",,0,0,,2019-02-12 06:44:03.430 UTC,,2019-02-12 06:44:03.430 UTC,,,,,10563233,1,0,python-3.x|google-vision,27
Does Google Cloud Vision API have a feature to extract the age from an image of a face?,54425585,Does Google Cloud Vision API have a feature to extract the age from an image of a face?,"<p>I have some images of faces which I need to determine the rough age of the faces. Does Google Cloud Vision API have this feature to determine the age? From the documentation, I don't see any such feature. Google Cloud Vision Face Detection seem to be more about detecting expressions and the vertices of the objects in the image which I am not interested in knowing. </p>",54437491,1,0,,2019-01-29 16:31:16.333 UTC,1,2019-01-30 10:58:02.530 UTC,2019-01-30 10:58:02.530 UTC,,4037220,,10780075,1,0,google-cloud-vision,63
tutorial for textdetection using camera with google cloud api in android studio,54881611,tutorial for textdetection using camera with google cloud api in android studio,"<p>Is there any good tutorial video on how to create a simple textrecognition app using camera (surfaceview/imageview) in android studio? 
I have the key for google cloud api but I can't find any good tutorial, step-by-step, on how to create an app that can use this.</p>

<p>I may be asking too much but self-study is one of my weakness and using Android Studio is not taught at our school (yes, I'm still a student). </p>

<p>If you can provide any links, videos, or tutorial I will be grateful.</p>

<p>Note: I have used google vision api but that is limited only to latin based languages and I need to detect at least japanese and arabic characters. I have also used tess-two but when I took a picture of the letter ""A"" it outputs random characters.</p>",,0,0,,2019-02-26 08:54:29.420 UTC,,2019-02-26 08:54:29.420 UTC,,,,,9786592,1,0,android,9
Google Vision API performance,47991101,Google Vision API performance,"<p>Am trying to OCR around 50,000 documents(~20K size jpegs). Currently using a homegrown tesseract based solution but are looking to migrate to google vision for better accuracy</p>

<p>Based on quotas published at <a href=""https://cloud.google.com/vision/quotas"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/quotas</a>
my calculation is</p>

<p><strong>600 requests/min with upto 16 images per request will give us a throughput of 9600 OCRs per minute.</strong> </p>

<p>Has anyone done any bulk operations using google vision? We did some POCS where some of these bulk requests are taking ~5mins.</p>",,0,2,,2017-12-27 11:08:31.317 UTC,,2017-12-27 11:08:31.317 UTC,,,,,674298,1,0,google-vision,373
Error while sending Image for face matching to AWS Rekognition on Android,56346851,Error while sending Image for face matching to AWS Rekognition on Android,"<p>I'm getting an error while sending Image for recognition to AWS Rekognition. 
This is the code which I use:</p>

<pre><code>        val byteBuffer = ByteBuffer.allocate(facePicture.byteCount)
        facePicture.copyPixelsToBuffer(byteBuffer)
        val image = Image().withBytes(byteBuffer)

        val searchFacesByImageResult = rekognitionClient.searchFacesByImage(
            SearchFacesByImageRequest()
                .withCollectionId(collectionId)
                .withImage(image)
                .withMaxFaces(1)
                .withFaceMatchThreshold(88F)
        )
</code></pre>

<p>And this is an error:</p>

<pre><code>com.amazonaws.AmazonServiceException: 1 validation error detected: Value 'java.nio.HeapByteBuffer[pos=0 lim=0 cap=0]' at 'image.bytes' failed to satisfy constraint: Member must have length greater than or equal to 1 (Service: AmazonRekognition; Status Code: 400; Error Code: ValidationException; Request ID: 70a3f05c-8166-11e9-a1cb-fbae8cf4359b)
        at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:730)
        at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:405)
        at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:212)
        at com.amazonaws.services.rekognition.AmazonRekognitionClient.invoke(AmazonRekognitionClient.java:3006)
        at com.amazonaws.services.rekognition.AmazonRekognitionClient.searchFacesByImage(AmazonRekognitionClient.java:238
</code></pre>

<p>Exception looks like <code>ByteBuffer</code> is empty, I have debugged and checked that ByteBuffer is valid and is not empty</p>",,1,0,,2019-05-28 16:50:30.717 UTC,,2019-05-28 19:53:16.300 UTC,2019-05-28 19:15:20.917 UTC,,1307690,,1307690,1,1,android|amazon-web-services|kotlin|amazon-rekognition,20
Limit google vision api text detection to specific region,49522705,Limit google vision api text detection to specific region,"<p>I am trying to use google vision API to detect text from camera preview. However, I want the detected text to be within a specific region/rectangle in the camera review. </p>",49533829,1,1,,2018-03-27 21:29:37.953 UTC,,2019-01-13 19:56:09.890 UTC,2018-03-28 01:54:12.160 UTC,,9170457,,1217820,1,0,android|ocr|google-vision,603
How to process Google Vision API one by one and not in a block?,45981451,How to process Google Vision API one by one and not in a block?,"<p>I got a problem when using the Google Vision API.
I'm looping the process, to analyze several pictures, but when I print the results, all is coming in a block after 5 minutes of process, but I wanted to know if it's possible to start the program and make it print the results after each picture analyzis ? </p>

<p>Here's my code for the algorithm :</p>

<pre><code>bool space = false;
        // var image = Google.Cloud.Vision.V1.Image.FromFile(""C:\\temp\\sequence\\n"" + i + "".jpg"");
            var image = Google.Cloud.Vision.V1.Image.FromFile(path);
            var client = ImageAnnotatorClient.Create();
            var response = client.DetectLabels(image);
            CropHintsAnnotation confidence = client.DetectCropHints(image);

        bool car = false;
        bool vehicle = false;
        bool land = false;
        int score = 0;
        //System.IO.Directory

        foreach (var annotation in response)
        {
            textBox1.Text += annotation.Description + ""\r\n"";
            textBox1.Text += ""Score : "" + annotation.Score + ""\r\n"";
            vehicle = !annotation.Description.Equals(""vehicle"");
            car = !annotation.Description.Equals(""car"");
            land = !annotation.Description.Equals(""land vehicle"");

            if (car == false)
            {
                score += 1;
                //textBox1.Text += ""\r\nEmpty ?       "" + car + ""\r\n\r\n"";
            }
            else if (vehicle == false)
            {
                score += 1;
                //textBox1.Text += ""\r\nEmpty ?       "" + vehicle + ""\r\n\r\n"";
            }
            else if (land == false)
            {
                score += 1;
                //textBox1.Text += ""\r\nEmpty ?       "" + land + ""\r\n\r\n"";
            }
            else if (annotation.Description.Equals(""asphalt""))
            {
                score += -20;
                //textBox1.Text += ""\r\nEmpty ?  True \r\n\r\n"";
            }
            else
            {
                score += 0;
            }
        }
        if ( score &gt; 0)
        {
            //textBox1.Text += ""The parking space is taken \r\n\r\n"";
            space = true;
        }
        else
        {
            //textBox1.Text += ""The parking space is empty \r\n\r\n"";
            space = false;
        }
        return space;
</code></pre>

<p>I'm looping this with a foreach(Image file in Directory).</p>

<p>Any ideas to help me ? </p>

<p>Thanks a lot !</p>",45982072,1,3,,2017-08-31 12:26:38.067 UTC,,2017-08-31 13:00:07.750 UTC,,,,,8160001,1,0,c#|loops|google-vision,197
How can I right call methods in Spring boot tools class from Serviceclass?,51242545,How can I right call methods in Spring boot tools class from Serviceclass?,"<p>i have Spring-boot project, scaning the folders and indexing all finded photos with keywords from metadata.</p>

<p>i have the next structure :</p>

<pre><code>java
  de.stadt
    controllers
    entitys
    repository
    services
      ImageService
    tools
      ImageProcessing
      ScanDirs
</code></pre>

<p>when i'm trying to call Tools class ScanDirs with RecursiveTask from ImageService 
i have java.lang.NullPointerException: null
       how right to push data to constructur ScanDirs?</p>

<pre><code>at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:598)
        at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677)
        at java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:720)
        at java.util.concurrent.ForkJoinPool.invoke(ForkJoinPool.java:2616)
        at de.stadt.presse.service.ImageService.callScanDirsToolsClass(ImageService.java:67)
</code></pre>

<p>ImageService.class:</p>

<pre><code>@Service
public class ImageService {
  @Autowired
  private ImageRepository imageRepository;
  @Autowired
  private KeywordsService keywordsService;
  @Autowired
  private ScanDirs task;

  private static final int AVAILABLE_PROCESSORS = Runtime.getRuntime().availableProcessors();
  private final ForkJoinPool pool = new ForkJoinPool(AVAILABLE_PROCESSORS);

  public boolean callScanDirsToolsClass(String folderPath, String thumpPath, String googleVisionLocalPath,
                          int scaleHeight, int scaleHeightForGoogleVision, String strText) {


        task = new ScanDirs(folderPath, thumpPath, googleVisionLocalPath, scaleHeight,
 scaleHeightForGoogleVision, strText); //&lt;-----------------if pusching the data here false . how right to push the data to the constructur  ?
        final Boolean result = pool.invoke(task);
        System.out.println(result);

        return true;
          }
       }
</code></pre>

<p>the ScanDirs.class look like: </p>

<pre><code> @Service
 @NoArgsConstructor
public class ScanDirs extends RecursiveTask&lt;Boolean&gt; {

  @Autowired
  private ImageService imageService;
  @Autowired
  private KeywordsService keywordsService;

  ...

  public ScanDirs(String folder, String thum........) {
    ...
  }

  @Override
  protected Boolean compute() {


    Image image = imageService.findByImagePath(file.getPath());

  ...
</code></pre>

<p>}</p>",,0,2,,2018-07-09 09:45:45.813 UTC,,2018-07-09 10:25:35.690 UTC,2018-07-09 10:25:35.690 UTC,,2140635,,2140635,1,0,java|spring|spring-boot,26
Different text detection result from Google Vision API,47872907,Different text detection result from Google Vision API,"<p>Get different result for text detection from .Net code and demo app for the same image <a href=""https://i.stack.imgur.com/H21rL.png"" rel=""nofollow noreferrer"">google vision api result</a> and <a href=""https://i.stack.imgur.com/heYZt.png"" rel=""nofollow noreferrer"">.net result </a></p>

<p>this is my code:</p>

<pre><code>            var response = vision.Images.Annotate(
            new BatchAnnotateImagesRequest()
            {
                Requests = new[]
                {
                    new AnnotateImageRequest()
                    {
                        Features = new[]
                        {
                            new Feature()
                            {
                                Type =
                                    ""TEXT_DETECTION""
                            }
                        },
                        Image = image
                    }
                }
            }).Execute();
</code></pre>",,2,1,,2017-12-18 16:49:04.140 UTC,,2018-09-17 10:23:10.093 UTC,2018-01-05 16:01:59.687 UTC,,2781862,,7858642,1,2,.net|google-cloud-vision|google-vision,1328
How to develop a machine learning model for information extraction from a receipt,56002802,How to develop a machine learning model for information extraction from a receipt,"<p>I'm working on a project to extract information for receipt images. I'm using Google Vision API as OCR and I want to <strong>extract</strong> the <strong>Total</strong> and <strong>VAT</strong> from the receipt. I'm thinking of using Machine Learning approach because of the structure of the receipt is not the same.</p>

<p>Following are some commercial products of receipt scanning which use the ML approach,</p>

<ul>
<li><a href=""https://www.taggun.io/"" rel=""nofollow noreferrer"">https://www.taggun.io/</a></li>
<li><a href=""https://rossum.ai/"" rel=""nofollow noreferrer"">https://rossum.ai/</a></li>
</ul>

<p>The Google Vision API gives the raw texts and their bounding box. How do we extract the necessary information from the raw texts?</p>",,1,0,,2019-05-06 09:55:17.690 UTC,,2019-05-06 14:27:01.137 UTC,2019-05-06 13:14:37.413 UTC,,2653663,,11439773,1,0,machine-learning|ocr|information-extraction|receipt,27
Check if array of objects contains object with the same attribute,46090578,Check if array of objects contains object with the same attribute,"<p>Hey so I'm working on small project where I use google vision api, the point is to read barcodes and list them. 
I want to be able to read a barcode multiple times and just increase the count of the same 'barcodeItem' object that I have added in my array of barcodeItem objects. </p>

<pre><code>if(currentBarcode != null){
        boolean exists = false;
        BarcodeItem barcodeItem = new BarcodeItem(currentBarcode);
        for (BarcodeItem item : MainActivity.barcodesList){
            if(item.barcode == barcodeItem.barcode){
                item.itemCount++;
                exists = true;
            } else {
                //do nothing to item
            }
        }
        if(exists == false){
            MainActivity.barcodesList.add(barcodeItem);
        }
        currentBarcode = null;
    } else { //do nothing 
}
</code></pre>

<p>I've also tried using <code>contains</code>. Right now the code doesn't actually increase the count of the object, it always adds a new object to the list, is there a way I could check the list of objects for that same barcode and then increase the count accordingly?</p>

<p>EDIT: Okay, thanks for the answers, I actually managed to fix it. Forgot to mention that barcode attribute is type String, and also forgot about the fact that you don't compare Strings with <code>==</code> but with <code>equals</code> instead. Sorry and <strong>thank you</strong> all for taking the time to help me out.</p>",46090848,3,4,,2017-09-07 07:37:21.927 UTC,,2017-09-07 08:05:19.903 UTC,2017-09-07 07:59:12.150 UTC,,6523488,,6523488,1,1,java|android|arrays|list|object,768
"Image size is too small while uploading from local storage, Microsoft Face API",46790435,"Image size is too small while uploading from local storage, Microsoft Face API","<p>I m calling Microsoft Face API for detecting the face in an image. 
While loading an image from local , I always get an error as below </p>

<pre><code>{ code: 'InvalidImageSize',
  message: 'Image size is too small.' }
</code></pre>

<p>But using the same image via URL , its working fine.  </p>

<pre><code>const fs = require(""fs"");
const axios = require(""axios"");

axios({
    method : ""post"",
    url : "" https://westcentralus.api.cognitive.microsoft.com/face/v1.0/detect?returnFaceId=true"",
    headers : {
        'Content-Type': 'application/octet-stream',
        ""Ocp-Apim-Subscription-Key"" : ""2da8e41a647c4079b2f9a6XXXXXXXXX""
    },
    body : fs.readFileSync(""./photos/shiva3.jpeg"").toString(""base64"")
}).then((res)=&gt;{
    console.log(""Response"");
    console.log(res.data);
}).catch((err)=&gt;{
    console.log(err.response.data.error);
})
</code></pre>",,1,1,,2017-10-17 12:43:18.147 UTC,,2017-10-17 16:40:18.477 UTC,,,,,1746025,1,0,node.js|image|http|post|face-api,287
"how to implement ""similar product images"" in Amazon Rekognition",46236523,"how to implement ""similar product images"" in Amazon Rekognition","<p>i am using Amazon Rekognition when i upload an image to my s3 bucket , the api resonse i get is</p>

<pre><code>{
    ""Labels"": [
        {
            ""Confidence"": 99.30213165283203,
            ""Name"": ""Human""
        },
        {
            ""Confidence"": 99.30457305908203,
            ""Name"": ""People""
        },
        {
            ""Confidence"": 99.30457305908203,
            ""Name"": ""Person""
        },
        {
            ""Confidence"": 92.39805603027344,
            ""Name"": ""Clothing""
        },
        {
            ""Confidence"": 92.39805603027344,
            ""Name"": ""Denim""
        },
        {
            ""Confidence"": 92.39805603027344,
            ""Name"": ""Jeans""
        },
        {
            ""Confidence"": 92.39805603027344,
            ""Name"": ""Pants""
        },
        {
            ""Confidence"": 51.34967041015625,
            ""Name"": ""Accessories""
        },
        {
            ""Confidence"": 51.27912902832031,
            ""Name"": ""Footwear""
        },
        {
            ""Confidence"": 51.27912902832031,
            ""Name"": ""Shoe""
        }
    ],
    ""OrientationCorrection"": ""ROTATE_0""
}
</code></pre>

<p>is there any api to search for similar product (eg. search: 'striped blue t shirts' when i upload a blue striped tshirt) among images in my bucket.</p>",,2,0,,2017-09-15 09:41:48.513 UTC,,2018-06-01 11:23:26.837 UTC,2018-06-01 11:23:26.837 UTC,,174777,,4854309,1,1,amazon-web-services|amazon-s3|aws-lambda|amazon-rekognition,271
Error setting up AWS credentials in Kotlin,51527259,Error setting up AWS credentials in Kotlin,"<p>I am currently creating an android version of an IOS app that is built. I need to be able to access AWS services and I am having issues with my credentials.</p>

<p>In swift the way to set up credentials is to import the 'awsconfiguration.json' file, then write the following code:</p>

<pre><code>let credentialsProvider = AWSStaticCredentialsProvider(accessKey:""MY_ACCESS_KEY"", secretKey:""MY_KEY"")
       let configuration = AWSServiceConfiguration(
           region: AWSRegionType.USWest2,
           credentialsProvider: credentialsProvider)
       AWSServiceManager.default().defaultServiceConfiguration = configuration
</code></pre>

<p>I tried things from the following the documentation on aws and I cannot figure out what I am missing. I got the json file and pasted it in the /res/raw folder and am trying to call the following code from my MainActivity.</p>

<p><a href=""https://docs.aws.amazon.com/aws-mobile/latest/developerguide/how-to-android-sdk-setup.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/aws-mobile/latest/developerguide/how-to-android-sdk-setup.html</a></p>

<p><a href=""https://discuss.kotlinlang.org/t/kotlin-aws-credentials/8399"" rel=""nofollow noreferrer"">https://discuss.kotlinlang.org/t/kotlin-aws-credentials/8399</a></p>

<p><a href=""https://docs.aws.amazon.com/aws-mobile/latest/developerguide/getting-started.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/aws-mobile/latest/developerguide/getting-started.html</a></p>

<p>I get no errors in the IDE (Android Studio). But when I launch the app, it crashes immediately and I get the following error from logcat:</p>

<blockquote>
  <p>2018-07-25 14:20:10.476 22581-22581/? D/AWSMobileClient: Fetching the
  Cognito Identity. 2018-07-25 14:20:10.481 22581-22581/?
  D/AndroidRuntime: Shutting down VM 2018-07-25 14:20:10.484
  22581-22581/? E/AndroidRuntime: FATAL EXCEPTION: main
      Process: com.mammothvr.checkarafragments, PID: 22581
      java.lang.NoClassDefFoundError: Failed resolution of: Lorg/apache/commons/logging/LogFactory;
          at com.amazonaws.util.VersionInfoUtils.(VersionInfoUtils.java:41)
          at com.amazonaws.util.VersionInfoUtils.getUserAgent(VersionInfoUtils.java:77)
          at com.amazonaws.ClientConfiguration.(ClientConfiguration.java:43)
          at com.amazonaws.mobile.auth.core.IdentityManager.(IdentityManager.java:212)
          at com.amazonaws.mobile.client.AWSMobileClient.fetchCognitoIdentity(AWSMobileClient.java:280)
          at com.amazonaws.mobile.client.AWSMobileClient.initializeWithBuilder(AWSMobileClient.java:186)
          at com.amazonaws.mobile.client.AWSMobileClient.access$100(AWSMobileClient.java:74)
          at com.amazonaws.mobile.client.AWSMobileClient$InitializeBuilder.execute(AWSMobileClient.java:446)
          at com.mammothvr.checkarafragments.MainActivity.onCreate(MainActivity.kt:60)
          at android.app.Activity.performCreate(Activity.java:7136)
          at android.app.Activity.performCreate(Activity.java:7127)
          at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1271)
          at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2893)
          at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3048)
          at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:78)
          at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:108)
          at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:68)
          at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1808)
          at android.os.Handler.dispatchMessage(Handler.java:106)
          at android.os.Looper.loop(Looper.java:193)
          at android.app.ActivityThread.main(ActivityThread.java:6669)
          at java.lang.reflect.Method.invoke(Native Method)
          at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493)
          at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:858)
  2018-07-25 14:20:10.485 22581-22581/? E/AndroidRuntime: Caused by:
  java.lang.ClassNotFoundException: Didn't find class
  ""org.apache.commons.logging.LogFactory"" on path: DexPathList[[zip file
  ""/data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/base.apk"",
  zip file
  ""/data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_dependencies_apk.apk"",
  zip file
  ""/data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_resources_apk.apk"",
  zip file
  ""/data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_0_apk.apk"",
  zip file
  ""/data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_1_apk.apk"",
  zip file
  ""/data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_2_apk.apk"",
  zip file
  ""/data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_3_apk.apk"",
  zip file
  ""/data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_4_apk.apk"",
  zip file
  ""/data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_5_apk.apk"",
  zip file
  ""/data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_6_apk.apk"",
  zip file
  ""/data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_7_apk.apk"",
  zip file
  ""/data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_8_apk.apk"",
  zip file
  ""/data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_9_apk.apk""],nativeLibraryDirectories=[/data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/lib/arm64,
  /data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/base.apk!/lib/arm64-v8a,
  /data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_dependencies_apk.apk!/lib/arm64-v8a,
  /data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_resources_apk.apk!/lib/arm64-v8a,
  /data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_0_apk.apk!/lib/arm64-v8a,
  /data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_1_apk.apk!/lib/arm64-v8a,
  /data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_2_apk.apk!/lib/arm64-v8a,
  /data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_3_apk.apk!/lib/arm64-v8a,
  /data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_4_apk.apk!/lib/arm64-v8a,
  /data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_5_apk.apk!/lib/arm64-v8a,
  /data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_6_apk.apk!/lib/arm64-v8a,
  /data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_7_apk.apk!/lib/arm64-v8a,
  /data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_8_apk.apk!/lib/arm64-v8a,
  /data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_slice_9_apk.apk!/lib/arm64-v8a,
  /system/lib64]]
          at dalvik.system.BaseDexClassLoader.findClass(BaseDexClassLoader.java:134)
          at java.lang.ClassLoader.loadClass(ClassLoader.java:379)
          at java.lang.ClassLoader.loadClass(ClassLoader.java:312)
            ... 24 more
        Suppressed: java.io.IOException: No original dex files found for dex location
  /data/app/com.mammothvr.checkarafragments-T8bCup9H6KQ50PJscwADAQ==/split_lib_resources_apk.apk
          at dalvik.system.DexFile.openDexFileNative(Native Method)
          at dalvik.system.DexFile.openDexFile(DexFile.java:354)
          at dalvik.system.DexFile.(DexFile.java:101)
          at dalvik.system.DexFile.(DexFile.java:75)
          at dalvik.system.DexPathList.loadDexFile(DexPathList.java:394)
          at dalvik.system.DexPathList.makeDexElements(DexPathList.java:354)
          at dalvik.system.DexPathList.(DexPathList.java:164)
          at dalvik.system.BaseDexClassLoader.(BaseDexClassLoader.java:74)
          at dalvik.system.BaseDexClassLoader.(BaseDexClassLoader.java:65)
          at dalvik.system.PathClassLoader.(PathClassLoader.java:64)
  2018-07-25 14:20:10.485 22581-22581/? E/AndroidRuntime:     at
  com.android.internal.os.ClassLoaderFactory.createClassLoader(ClassLoaderFactory.java:73)
          at com.android.internal.os.ClassLoaderFactory.createClassLoader(ClassLoaderFactory.java:88)
          at android.app.ApplicationLoaders.getClassLoader(ApplicationLoaders.java:74)
          at android.app.ApplicationLoaders.getClassLoader(ApplicationLoaders.java:40)
          at android.app.LoadedApk.createOrUpdateClassLoaderLocked(LoadedApk.java:727)
          at android.app.LoadedApk.getClassLoader(LoadedApk.java:810)
          at android.app.LoadedApk.getResources(LoadedApk.java:1032)
          at android.app.ContextImpl.createAppContext(ContextImpl.java:2345)
          at android.app.ActivityThread.handleBindApplication(ActivityThread.java:5749)
          at android.app.ActivityThread.access$1100(ActivityThread.java:199)
          at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1650)
                ... 6 more</p>
</blockquote>

<p>Here is my MainActivity Kotlin code:</p>

<pre><code>package com.mammothvr.checkarafragments

import androidx.appcompat.app.AppCompatActivity
import android.os.Bundle
import android.util.Log
import android.view.View
import android.widget.Toast
import androidx.fragment.app.Fragment
import androidx.fragment.app.FragmentTransaction
import com.amazonaws.auth.AWSCredentials
import com.amazonaws.auth.AWSCredentialsProvider
import com.amazonaws.auth.BasicAWSCredentials
import com.amazonaws.mobile.auth.core.IdentityHandler
import com.amazonaws.mobile.auth.core.IdentityManager
import kotlinx.android.synthetic.main.activity_main.*
import com.amazonaws.mobile.client.AWSMobileClient;
import com.amazonaws.mobile.client.AWSMobileClient.InitializeBuilder
import com.amazonaws.mobile.config.AWSConfiguration
import androidx.core.app.CoreComponentFactory


class MainActivity : AppCompatActivity() {

    companion object {

        //AWS singleton link to mobile client
        private val TAG: String = this::class.java.simpleName
        // Used to load the 'native-lib' library on application startup.
        init {
            System.loadLibrary(""native-lib"")
        }
    }

    private var credentialsProvider: AWSCredentialsProvider? = null
    private var awsConfiguration: AWSConfiguration? = null


    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        setContentView(R.layout.activity_main)

        AWSMobileClient.getInstance().initialize(this) {

           // val credentials: AWSCredentials = BasicAWSCredentials(""MY_ACCESS_KEY"", ""MY_KEY"")

            credentialsProvider = AWSMobileClient.getInstance().credentialsProvider
            awsConfiguration = AWSMobileClient.getInstance().configuration

            IdentityManager.getDefaultIdentityManager().getUserID(object : IdentityHandler {
                override fun handleError(exception: Exception?) {
                    Log.e(TAG, ""Retrieving identity: ${exception?.message}"")
                }

                override fun onIdentityId(identityId: String?) {
                    Log.d(TAG, ""Identity = $identityId"")
                    val cachedIdentityId = IdentityManager.getDefaultIdentityManager().cachedUserID
                    // Do something with the identity here
                }
            })
        }.execute()

        hideSystemUI()

        val fragment = LoginFragment.newInstance()
        replaceFragment(fragment)

    }

    private fun replaceFragment(fragment: Fragment){
        val fragmentTransaction = supportFragmentManager.beginTransaction()
        fragmentTransaction.setTransition(FragmentTransaction.TRANSIT_FRAGMENT_FADE).replace(R.id.fragmentContainer, fragment).commit()
    }



    private fun hideSystemUI() {

        val decorView = window.decorView
        decorView?.systemUiVisibility = (View.SYSTEM_UI_FLAG_IMMERSIVE
                // Set the content to appear under the system bars so that the
                // content doesn't resize when the system bars hide and show.
                or View.SYSTEM_UI_FLAG_LAYOUT_STABLE
                or View.SYSTEM_UI_FLAG_LAYOUT_HIDE_NAVIGATION
                or View.SYSTEM_UI_FLAG_LAYOUT_FULLSCREEN
                // Hide the nav bar and status bar
                or View.SYSTEM_UI_FLAG_HIDE_NAVIGATION
                or View.SYSTEM_UI_FLAG_FULLSCREEN)
    }

    external fun stringFromJNI(): String
}
</code></pre>

<p>Here are my grade dependencies:</p>

<pre><code>dependencies {
    implementation fileTree(dir: 'libs', include: ['*.jar'])
    implementation""org.jetbrains.kotlin:kotlin-stdlib-jdk7:$kotlin_version""
    implementation 'androidx.appcompat:appcompat:1.0.0-alpha1'
    implementation 'androidx.constraintlayout:constraintlayout:1.1.2'
    testImplementation 'junit:junit:4.12'
    androidTestImplementation 'androidx.test:runner:1.1.0-alpha3'
    androidTestImplementation 'androidx.test.espresso:espresso-core:3.1.0-alpha3'
    implementation ""org.jetbrains.kotlin:kotlin-reflect:$kotlin_version""
    implementation 'pub.devrel:easypermissions:1.1.3'
    implementation 'com.android.support:design:27.1.1'

    implementation ('com.amazonaws:aws-android-sdk-s3:2.6.+') { transitive = true }
    implementation ('com.amazonaws:aws-android-sdk-rekognition:2.6.+') { transitive = true }
    implementation ('com.amazonaws:aws-android-sdk-cognitoauth:2.6.+@aar') { transitive = true }
    implementation ('com.amazonaws:aws-android-sdk-cognitoidentityprovider:2.6.+') { transitive = true }
    implementation ('com.amazonaws:aws-android-sdk-mobile-client:2.6.+@aar') { transitive = true }
    implementation ('com.amazonaws:aws-android-sdk-cognitoauth:2.6.+@aar') { transitive = true }


    // Cognito UserPools for SignIn
    implementation 'com.android.support:support-v4:26.+'
    implementation ('com.amazonaws:aws-android-sdk-auth-userpools:2.6.+@aar') { transitive = true }

    // Sign in UI Library
    implementation 'com.android.support:appcompat-v7:26.+'
    implementation ('com.amazonaws:aws-android-sdk-auth-ui:2.6.+@aar') { transitive = true }

}
</code></pre>

<p>This is my first Android project coming from a primarily c#/swift background. Any help is greatly appreciated.</p>",51529443,1,3,,2018-07-25 20:34:37.553 UTC,,2018-07-26 00:37:49.063 UTC,2018-07-25 20:41:51.610 UTC,,6575837,,6575837,1,0,android|amazon-web-services|kotlin|aws-sdk|aws-mobilehub,222
Send an image from AWS S3 to IBM Watson Visual Recognition,41903995,Send an image from AWS S3 to IBM Watson Visual Recognition,"<p>I am trying to send images stored on Amazon S3 storage to IBM Watson Visual Recognition Service. </p>

<p>The error i am getting is <code>Error: Invalid JSON content received. Unable to parse.</code></p>

<p>The following code is running on an Express server.</p>

<pre><code>function (imgResult) {
          var imgName = imgResult[0][0].imghash;
          var params = {
            images_file: s3.getObject(
              {
                Bucket: ""Bucket Address"",
                Key: `upload/${imgName}`
              }
            ).createReadStream()
          };
          visual_recognition.classify(params, function (err, res) {
            if (err) {
              console.log(err);
            } else {
              res.images[0].classifiers[0].classes.forEach(function (tagClass) {

                db.raw(`INSERT INTO smartfolio.tags VALUES (null, ${imgid.idimages}, '${tagClass.class}')`)
                  .then(function (results) {
                  })
                  .catch(function (err) {
                    console.log(err)
                  })
              });
            }
          });
        }
</code></pre>

<p>In the code above, imgResult is a response from a database query, containing the images name from the database. I know the problem lies in my params variable, but I am kind of lost on how to send the image from S3 to Watson. </p>

<p>The error: </p>

<pre><code>{ Error: Invalid JSON content received. Unable to parse.
    at Request._callback (C:\Users\pheon\Desktop\Smartfolio-1\node_modules\watson-developer-cloud\lib\requestwrapper.js:74:15)
    at Request.self.callback (C:\Users\pheon\Desktop\Smartfolio-1\node_modules\request\request.js:186:22)
    at emitTwo (events.js:106:13)
    at Request.emit (events.js:191:7)
    at Request.&lt;anonymous&gt; (C:\Users\pheon\Desktop\Smartfolio-1\node_modules\request\request.js:1081:10)
    at emitOne (events.js:96:13)
    at Request.emit (events.js:188:7)
    at IncomingMessage.&lt;anonymous&gt; (C:\Users\pheon\Desktop\Smartfolio-1\node_modules\request\request.js:1001:12)
    at IncomingMessage.g (events.js:292:16)
</code></pre>

<p>Any help will be greatly appreciated.
Thanks</p>",,1,5,,2017-01-27 22:23:16.500 UTC,0,2017-07-31 09:25:17.263 UTC,2017-01-28 18:58:29.347 UTC,,7351249,,7351249,1,0,amazon-web-services|express|amazon-s3|watson,169
How to recognize text from the PDF file using Firebase ML Kit?,55047291,How to recognize text from the PDF file using Firebase ML Kit?,"<p>I am developing an Android app to detect text from the PDF file.</p>

<p>First, I tried to use Google Cloud Vision API.
But it required to OAuth 2.0.
So I changed from it to Firebase ML Kit.</p>

<p>But when I run 'fromFilePath' method, NPE occurred.</p>

<pre><code>val file = getPdfFile()
Log.d(TAG, ""file.length: ${file.length()}"") // File size is printed correctly!

// NPE occurred while below code running
val image = FirebaseVisionImage.fromFilePath(context, Uri.fromFile(file))

// Because already NPE occurred, I cannot reach out to below code.
val detector = FirebaseVision.getInstance()
    .cloudDocumentTextRecognizer
</code></pre>

<blockquote>
<pre><code>Process: com.youknow.redact, PID: 13122
java.lang.NullPointerException: Attempt to invoke virtual method 'int android.graphics.Bitmap.getWidth()' on a null object reference
</code></pre>
</blockquote>

<p>It looks like the Firebase ML kit doesn't support PDF file, right?</p>

<p>Is there any good solution?</p>

<p>Is it impossible to recognize text from the PDF file using Firebase ML kit?</p>

<hr>

<p>I tried to test more file formats: JPG, TIFF</p>

<p>All is same, just input file is changed.
JPG works fine, but TIFF has the same problem.</p>

<pre><code> Caused by: java.lang.NullPointerException: Attempt to invoke virtual method 'int android.graphics.Bitmap.getWidth()' on a null object reference
    at com.google.android.gms.internal.firebase_ml.zzox.zza(Unknown Source)
    at com.google.firebase.ml.vision.common.FirebaseVisionImage.fromFilePath(Unknown Source)
</code></pre>",,1,0,,2019-03-07 15:24:18.120 UTC,,2019-03-08 06:30:51.290 UTC,2019-03-08 02:07:30.897 UTC,,1936853,,1936853,1,0,android|firebase|firebase-mlkit,99
How to get full text of a block in Google Cloud Vision API,55388663,How to get full text of a block in Google Cloud Vision API,"<p>I try to use Google Cloud Vision API to detect text of an image. After detecting, I get 1 page and 17 Blocks. I am trying to get text in each blocks and save it in a list, but it does not work. Here is my code:</p>

<pre><code>Image image = Image.FromFile(""./wwwroot/images/page1.PNG"");
ImageAnnotatorClient client = ImageAnnotatorClient.Create(channel);
var text = client.DetectDocumentText(image);
InvoiceFileModel invoice = new InvoiceFileModel();
List&lt;object&gt; itemList = new List&lt;object&gt;();
foreach (var page in text.Pages)
{
   foreach (var block in page.Blocks)
   {
       string item = string.Join(""\n"", paragraph.Words);
       itemList.Add(item);
   }
}
</code></pre>

<p>I would like to know is there any other way to get text. Thanks a lot.</p>",,0,0,,2019-03-28 00:59:09.953 UTC,,2019-03-28 00:59:09.953 UTC,,,,,9443594,1,1,c#|asp.net-core-mvc|google-cloud-vision,75
Google Vision API text detection strange behaviour - Javascript,44740363,Google Vision API text detection strange behaviour - Javascript,"<p>Recently something about the Google Vision API changed. I am using it to recognize text on receipts. All good until now. Suddenly, the API started to respond differently to my requests.</p>

<p>I sent the same picture to the API today, and I got a different response (from the past). I ensured nothing was changed in my code, so this is not the culprit.</p>

<p>Another strange thing is that, when I upload the image to <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/</a> in the response, under textAnnotations, I get an array of 183 entries. However when I post from my app I get an array of 113 entries. Below you can see my code.</p>

<pre><code>function googleScan(imageData) {
    var deferred = $q.defer();
    var url = ""https://vision.googleapis.com/v1/images:annotate?key=&lt;myAPIKey&gt;"";
    var payload = {
        requests: {
            image: {
                content: imageData.split(',')[1]
            },
            features: [{
                type: 'TEXT_DETECTION',
                maxResults:50
            }]
        }
    };
    $http.post(url, payload, { headers: { ""NoAuthToken"": true } }).then(function (response) {
        deferred.resolve(parseAnalyzedResult(response.data.responses[0].textAnnotations));
        console.log(response);
    }, function (error) {
        console.log(error);
    });
    return deferred.promise;
</code></pre>

<p>I am wondering if somehow my free subscription got altered and that's why I receive a different response. Is that even possible? Has anyone stumbled upon this kind of issue before?</p>",,2,0,,2017-06-24 19:47:06.750 UTC,1,2017-06-29 16:45:58.657 UTC,,,,,8210103,1,3,javascript|ocr|google-vision,1188
Google API PHP error 500,48551659,Google API PHP error 500,"<p>I'm using google vision API in one of my PHP script.</p>

<p>Script works well when I'm executing it through the terminal:</p>

<pre><code>php /var/www/html/my_script.php
</code></pre>

<p>But when I want to execute it from my browser I'm getting an error 500:</p>

<blockquote>
  <p>PHP Fatal error:  Uncaught
  Google\Cloud\Core\Exception\ServiceException: {\n  ""error"": {\n<br>
  ""code"": 401,\n    ""message"": ""Request had invalid authentication
  credentials. Expected OAuth 2 access token, login cookie or other
  valid authentication credential. See
  <a href=""https://developers.google.com/identity/sign-in/web/devconsole-project"" rel=""nofollow noreferrer"">https://developers.google.com/identity/sign-in/web/devconsole-project</a>."",\n
  ""status"": ""UNAUTHENTICATED""\n  }\n}\n</p>
</blockquote>

<p>I don't get why the error message suggests me to use OAuth 2, I don't need my user to log to his google account.</p>

<p>My code is the following:</p>

<pre><code>    namespace Google\Cloud\Vision\VisionClient;
    require('vendor/autoload.php');
    use Google\Cloud\Vision\VisionClient;

    $projectId = 'my_project_id';
$path = 'https://tedconfblog.files.wordpress.com/2012/08/back-to-school.jpg';

    $vision = new VisionClient([
        'projectId' =&gt; $projectId,
    ]);

    $image = $vision-&gt;image(file_get_contents($path), ['WEB_DETECTION']);
    $annotation = $vision-&gt;annotate($image);
    $web = $annotation-&gt;web();
</code></pre>",48552334,1,4,,2018-01-31 21:22:29.230 UTC,,2018-01-31 22:13:51.877 UTC,2018-01-31 21:59:35.803 UTC,,1754181,,1754181,1,-1,php|google-api|google-cloud-platform|google-vision,205
Google Vision Api Text Detection Line Break,53417035,Google Vision Api Text Detection Line Break,"<p>When I read an image with text, Google Vision inserts line breaks in the middle of the sentence. How can I do to avoid this. Here's an example of the image text and Google Vision return:</p>

<p>Text in the image:</p>

<pre><code>01 600149 CHICKEN M PR 1 UN X 3.500 (0.11)
02 600019 POTATO M PR 1 UN X 7.50 (0.24)
03 31820 COCA ZERO M PR 1 UN X 10.90 (0.00)
</code></pre>

<p>Google Vision Return:</p>

<pre><code>01 600149 CHICKEN M PR
02 600019 POTATO M PR
03 31820 COCA ZERO M PR
1 UN X 3,500 0.11)
1 UN X 7.50 (0.24)
1 UN X 10.90 (0.00)
</code></pre>

<p>Thank you,</p>",,1,0,,2018-11-21 16:55:24.283 UTC,,2019-03-15 13:45:20.173 UTC,2018-11-21 19:44:17.823 UTC,,3605712,,10687070,1,0,google-vision,97
uninitialized constant Google::Cloud::Vision::ImageAnnotator,53921024,uninitialized constant Google::Cloud::Vision::ImageAnnotator,"<p>Running this example
 <a href=""https://cloud.google.com/vision/docs/face-tutorial?hl=zh-tw"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/face-tutorial?hl=zh-tw</a></p>

<p>by rails on 'google-cloud-vision', '~> 0.31.0'</p>

<p>got the error </p>

<blockquote>
  <p>uninitialized constant Google::Cloud::Vision::ImageAnnotator</p>
</blockquote>

<p>here's the code</p>

<pre><code>require ""google/cloud/vision""


project_id = 'xxxxxx'


vision = Google::Cloud::Vision.new project: project_id


image_annotator = Google::Cloud::Vision::ImageAnnotator.new
</code></pre>",53927178,2,1,,2018-12-25 09:25:53.117 UTC,,2018-12-26 03:35:48.620 UTC,2018-12-26 03:32:16.773 UTC,,9995312,,9995312,1,-2,ruby-on-rails|ruby|google-cloud-vision,79
Google Vision API filtering label results by category type,51310239,Google Vision API filtering label results by category type,"<p>Is there a way to filter the label results by category type when calling the Google Vision API?</p>

<p>I have an iOS app that will take photos of food and want to only receive or read labels in category of food from the API. </p>

<p>Thanks.</p>",,0,1,,2018-07-12 16:09:21.953 UTC,,2018-07-13 02:27:53.983 UTC,2018-07-13 02:27:53.983 UTC,,5859957,,10066621,1,0,image-processing|google-vision,21
Azure Cognitive Services: Where are the missing Custom Vision Performance Statistics?,56379854,Azure Cognitive Services: Where are the missing Custom Vision Performance Statistics?,"<p>I am querying the Azure Custom Vision V3.0 Training API (see <a href=""https://westeurope.dev.cognitive.microsoft.com/docs/services/Custom_Vision_Training_3.0/operations/5c771cdcbf6a2b18a0c3b809"" rel=""nofollow noreferrer"">https://westeurope.dev.cognitive.microsoft.com/docs/services/Custom_Vision_Training_3.0/operations/5c771cdcbf6a2b18a0c3b809</a>) so I can generate per-tag ROCs myself via the GetIterationPerformance operation, part of whose output is:</p>

<pre><code>{u'averagePrecision': 0.92868346,
 u'perTagPerformance': [{u'averagePrecision': 0.4887446,
                         u'id': u'uuid1',
                         u'name': u'tag_name_1',
                         u'precision': 0.0,
                         u'precisionStdDeviation': 0.0,
                         u'recall': 0.0,
                         u'recallStdDeviation': 0.0},
                        {u'averagePrecision': 1.0,
                         u'id': u'uuid2',
                         u'name': u'tag_name_2',
                         u'precision': 0.0,
                         u'precisionStdDeviation': 0.0,
                         u'recall': 0.0,
                         u'recallStdDeviation': 0.0},
                    {u'averagePrecision': 0.9828302,
                     u'id': u'uuid3',
                     u'name': u'tag_name_3',
                     u'precision': 1.0,
                     u'precisionStdDeviation': 0.0,
                     u'recall': 0.5555556,
                     u'recallStdDeviation': 0.0}],
</code></pre>

<p>u'precision': 0.9859485,
 u'precisionStdDeviation': 0.0,
 u'recall': 0.3752228,
 u'recallStdDeviation': 0.0}</p>

<p>The precision and recall uncertainties, <code>precisionStdDeviation</code> and <code>recallStdDeviation</code> respectively, always seem to be 0.0. Is this user error and if not are there any plans to activate these stats?</p>",56383691,1,0,,2019-05-30 14:05:09.273 UTC,,2019-05-30 18:24:34.763 UTC,,,,,1021819,1,0,azure|azure-cognitive-services|microsoft-custom-vision,32
How to find if the image is captured form another image,47176260,How to find if the image is captured form another image,"<p>I am building an face recognition php application with web camara Using Amazon Rekognition API.</p>

<p>i did basic face matches using the API from below documents.</p>

<p><a href=""http://docs.aws.amazon.com/aws-sdk-php/v3/api/api-rekognition-2016-06-27.html#comparefaces"" rel=""nofollow noreferrer"">http://docs.aws.amazon.com/aws-sdk-php/v3/api/api-rekognition-2016-06-27.html#comparefaces</a></p>

<p>Now,   when i capture my faces in front of web camera ,Amazon api validate the face and searches the faces from collection And the problem is , when i show a image/photos in my phone gallery in front of web camera,it also validated by AWS api and returns the matches. I found there is a api detectLables , but it is not correctly detect it is real or image  of another image.</p>

<p>Is there any way to overcome this issue? i want to detect whether the captured image was captured real or from captured from another image?</p>",,1,2,,2017-11-08 09:42:47.330 UTC,,2018-11-14 08:07:08.727 UTC,2017-11-09 06:29:14.317 UTC,,4550806,,4550806,1,0,php|amazon-web-services|amazon-s3|aws-sdk|amazon-rekognition,138
How to make an API-key based REST call with Google Cloud client library,47920981,How to make an API-key based REST call with Google Cloud client library,"<p>I'm trying to make an API call to Google Cloud Vision API using an API key from Golang. But I'm getting a <code>400: bad request, invalid_grant error</code>.  </p>

<p>The apiKey/apiKeyOption part of the code below is mine.</p>

<p>What is the right way to make this call?  Is it possible at all?</p>

<pre><code>import (
    // ...
    ""google.golang.org/api/option""
    vision ""cloud.google.com/go/vision/apiv1""
    ""golang.org/x/net/context""
)

func getImageLabels(filename string) []string {

    ctx := context.Background()

    apiKey := ""..."" // I have a valid api key generated in the console.
    apiKeyOption := option.WithAPIKey(apiKey)

    client, err := vision.NewImageAnnotatorClient(ctx, apiKeyOption)

    // ...
    labels, err := client.DetectLabels(ctx, image, nil, 10)

}
</code></pre>

<p>...</p>

<pre><code>Failed to detect labels: rpc error: code = Internal desc = transport: oauth2: cannot fetch token: 400 Bad Request
Response: {
  ""error"" : ""invalid_grant""
}
</code></pre>",47925550,2,1,,2017-12-21 08:34:48.240 UTC,,2018-02-14 21:51:18.450 UTC,2017-12-21 14:25:58.020 UTC,,4482491,,791842,1,0,go|google-cloud-platform|api-key|google-cloud-vision,970
How to configure X-Ray for lambda service,49997694,How to configure X-Ray for lambda service,"<p>I have number of Lambda functions written in JavaScript for node 8.10 deployed by Serverless framework.</p>

<p>When I go to X-Ray on web console I can't find any data, all tables are empty there. My functions use also SNS, Rekognition, S3.</p>

<p>I was under impression that Node.js functions are automatically handled by X-Ray.</p>

<p>Do I need to add something to my lambda functions to make them ""discoverable"" via X-Ray?</p>",,1,1,,2018-04-24 09:11:33.707 UTC,,2018-04-30 20:35:12.847 UTC,,,,,4105584,1,1,node.js|aws-lambda|aws-xray,290
AWS lambda Unable to import module 'lambda_function': No module named PIL,50734416,AWS lambda Unable to import module 'lambda_function': No module named PIL,"<p>I am using a lambda function of <code>SearchFacesbyimage</code>  And I am using this doc   <code>https://aws.amazon.com/blogs/machine-learning/build-your-own-face-recognition-service-using-amazon-rekognition/</code></p>

<p>where for comparison I am using this </p>

<pre><code>from PIL import Image
</code></pre>

<p>And I am getting this error
 <code>Unable to import module 'lambda_function': No module named PIL</code></p>",,2,8,,2018-06-07 06:35:09.127 UTC,1,2019-04-03 15:28:03.223 UTC,,,,,8068702,1,0,amazon-web-services|amazon-s3|aws-lambda|python-imaging-library|amazon-rekognition,3148
How to create your own bucket for Google Cloud Vision?,54989726,How to create your own bucket for Google Cloud Vision?,"<p>Is it possible to create your own bucket with images where you can add extra information for each image. Then use Google Cloud Vision to search like they do now but as an extra also search the image in your bucket?</p>

<p>Reason: I have some images that, when I search them with Google Cloud Vision, return almost no text. For this reason I would add these type of images to a bucket and manually add more information. The next time a user takes a photo of the same thing, it needs to search inside this bucket and if found return the extra information about this image.</p>",55058109,2,4,,2019-03-04 18:51:54.480 UTC,,2019-03-08 06:50:05.807 UTC,,,,,10493792,1,0,google-cloud-storage|google-cloud-vision,41
Extract JSON Value After OCR Relative To Other Values?,45577550,Extract JSON Value After OCR Relative To Other Values?,"<p>I'm using Azure Computer Vision API to extract text from an image. In this particular use case i'm attempting to extract a bit of text that in the image looks like this ""Person ID: ########"" where the # is a numeric person number. </p>

<p>Here is a sample of the JSON returned from the API:</p>

<pre><code>{""language"": ""en"",
""textAngle"": 0.0,
""orientation"": ""Up"",
""regions"": [
  {
    ""boundingBox"": ""212,169,1384,359"",
    ""lines"": [
    {
      ""boundingBox"": ""228,169,281,36"",
      ""words"": [
        {
          ""boundingBox"": ""228,169,141,28"",
          ""text"": ""Output""
        },
        {
          ""boundingBox"": ""386,169,123,36"",
          ""text"": ""Report""
        }
      ]
    },
    {
      ""boundingBox"": ""212,279,287,25"",
      ""words"": [
        {
          ""boundingBox"": ""212,280,116,24"",
          ""text"": ""Person""
        },
        {
          ""boundingBox"": ""341,279,42,25"",
          ""text"": ""ID:""
        },
        {
          ""boundingBox"": ""408,279,91,25"",
          ""text"": ""15060""
        }
      ]
    },
    {
      ""boundingBox"": ""279,326,104,25"",
      ""words"": [
        {
          ""boundingBox"": ""279,326,104,25"",
          ""text"": ""Notes:""
        }
      ]
    }
  ]
},
  ""boundingBox"": ""2436,172,159,32"",
  ""lines"": [
    {
      ""boundingBox"": ""2436,172,159,32"",
      ""words"": [
        {
          ""boundingBox"": ""2436,172,159,32"",
          ""text"": ""Operator:""
        }
      ]
    }
  ]
},
{
  ""boundingBox"": ""2627,172,290,216"",
  ""lines"": [
    {
      ""boundingBox"": ""2627,172,103,32"",
      ""words"": [
        {
          ""boundingBox"": ""2627,172,103,32"",
          ""text"": ""Output""
        }
      ]
    },
      ""boundingBox"": ""2629,329,288,37"",
      ""words"": [
        {
          ""boundingBox"": ""2683,329,234,37"",
          ""text"": ""xm""
        }
      ]
    },
    {
      ""boundingBox"": ""2875,381,41,7"",
      ""words"": [
        {
          ""boundingBox"": ""2875,381,41,7"",
          ""text"": ""LEAR""
        }
      ]
    }
  ]
}
  ""boundingBox"": ""2304,2353,706,32"",
  ""lines"": [
    {
      ""boundingBox"": ""2304,2353,706,32"",
      ""words"": [
        {
          ""boundingBox"": ""2817,2353,193,32"",
          ""text"": ""Incorporated.""
        }
      ]}]}]}
</code></pre>

<p>I've trimmed it down quite a bit. You can see that the Person ID: 12345 gets split it up into a section with Person, ID:, 12345. </p>

<p>I need to extract the number from the person ID, but the way i do it currently, if the data output changes, it just won't work:</p>

<p>I'm currently doing something along these lines:</p>

<pre><code>Dim _tmp1 = o1(""regions"")(0)(""lines"")(1)(""words"")(1)(""text"")
Dim _tmp2 = o1(""regions"")(0)(""lines"")(1)(""words"")(2)(""text"")
</code></pre>

<p>Then i perform a trivial check to see if _tmp1 = ""ID:""</p>

<p>There has got to be a better way to grab the correction value. I thought about just extracting all the ""text"" keys and then attempt to do a match on Person ID: and grab the data after that until the next space, but then if the extract number contained an extra space it would fail in that method. </p>

<p>There is a method to process items that can not be auto extracted, i'm just trying to improve the chances that the auto extraction will not fail. </p>",45593128,1,0,,2017-08-08 20:33:07.820 UTC,,2017-08-09 14:10:44.130 UTC,,,,,1146000,1,1,json|vb.net,145
AWS Lamda function generates cloudwatch compilation error,53739849,AWS Lamda function generates cloudwatch compilation error,"<p>AWS provide  this Java code to perform video analysis in rekognition. However  when viewing this in Eclipse there  is a  error  message :</p>

<pre><code>The public type JobCompletionHandler must be defined in its own file
</code></pre>

<p>When  the function is called in AWS it also  complains in the cloudwatch logs with : </p>

<pre><code>Unresolved compilation problem: The public type JobCompletionHandler must be defined in its own file : java.lang.Error java.lang.Error: Unresolved compilation problem: The public type JobCompletionHandler must be defined in its own file at com.amazonaws.lambda.demo.JobCompletionHandler.&lt;init&gt;(LambdaFunctionHandler.java:19) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Met
Unresolved compilation problem:
The public type JobCompletionHandler must be defined in its own file
: java.lang.Error
java.lang.Error: Unresolved compilation problem:
The public type JobCompletionHandler must be defined in its own file
</code></pre>

<p>Here is  the full function provided by AWS :</p>

<pre><code>//Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
//PDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazonrekognition-developer-guide/blob/master/LICENSE-SAMPLECODE.)
package com.amazonaws.lambda.demo;
import com.amazonaws.services.lambda.runtime.Context;
import com.amazonaws.services.lambda.runtime.LambdaLogger;
import com.amazonaws.services.lambda.runtime.RequestHandler;
import com.amazonaws.services.lambda.runtime.events.SNSEvent;
import java.util.List;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.rekognition.AmazonRekognition;
import com.amazonaws.services.rekognition.AmazonRekognitionClientBuilder;
import com.amazonaws.services.rekognition.model.GetLabelDetectionRequest;
import com.amazonaws.services.rekognition.model.GetLabelDetectionResult;
import com.amazonaws.services.rekognition.model.LabelDetection;
import com.amazonaws.services.rekognition.model.LabelDetectionSortBy;
import com.amazonaws.services.rekognition.model.VideoMetadata;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
</code></pre>

<p>Can rename  this  public class and file name. Did not need to create a separate file.</p>

<pre><code>public class JobCompletionHandler implements RequestHandler&lt;SNSEvent, String&gt; {
    @Override
    public String handleRequest(SNSEvent event, Context context) {
        String message = event.getRecords().get(0).getSNS().getMessage();
        LambdaLogger logger = context.getLogger();
        // Parse SNS event for analysis results. Log results
        try {
            ObjectMapper operationResultMapper = new ObjectMapper();
            JsonNode jsonResultTree = operationResultMapper.readTree(message);
            logger.log(""Rekognition Video Operation:========================="");
            logger.log(""Job id: "" + jsonResultTree.get(""JobId""));
            logger.log(""Status : "" + jsonResultTree.get(""Status""));
            logger.log(""Job tag : "" + jsonResultTree.get(""JobTag""));
            logger.log(""Operation : "" + jsonResultTree.get(""API""));
            if (jsonResultTree.get(""API"").asText().equals(""StartLabelDetection"")) {
                if (jsonResultTree.get(""Status"").asText().equals(""SUCCEEDED"")){
                    GetResultsLabels(jsonResultTree.get(""JobId"").asText(), context);
                }
                else{
                    String errorMessage = ""Video analysis failed for job ""
                    + jsonResultTree.get(""JobId"")
                    + ""State "" + jsonResultTree.get(""Status"");
                    throw new Exception(errorMessage);
                }
            } else
            logger.log(""Operation not StartLabelDetection"");
        } catch (Exception e) {
            logger.log(""Error: "" + e.getMessage());
            throw new RuntimeException (e);
        }
        return message;
    }
    void GetResultsLabels(String startJobId, Context context) throws Exception {
        LambdaLogger logger = context.getLogger();
        AmazonRekognition rek =
        AmazonRekognitionClientBuilder.standard().withRegion(Regions.US_EAST_1).build();
        int maxResults = 1000;
        String paginationToken = null;
        GetLabelDetectionResult labelDetectionResult = null;
        String labels = """";
        Integer labelsCount = 0;
        String label = """";
        String currentLabel = """";
        //Get label detection results and log them.
        do {
            GetLabelDetectionRequest labelDetectionRequest = new
            GetLabelDetectionRequest().withJobId(startJobId)
            .withSortBy(LabelDetectionSortBy.NAME).withMaxResults(maxResults).withNextToken(paginationToken);
            labelDetectionResult = rek.getLabelDetection(labelDetectionRequest);
            paginationToken = labelDetectionResult.getNextToken();
            VideoMetadata videoMetaData = labelDetectionResult.getVideoMetadata();
            // Add labels to log
            List&lt;LabelDetection&gt; detectedLabels = labelDetectionResult.getLabels();
            for (LabelDetection detectedLabel : detectedLabels) {
                label = detectedLabel.getLabel().getName();
                if (label.equals(currentLabel)) {
                    continue;
                }
                labels = labels + label + "" / "";
                currentLabel = label;
                labelsCount++;
            }
        } while (labelDetectionResult != null &amp;&amp; labelDetectionResult.getNextToken() != null);
        logger.log(""Total number of labels : "" + labelsCount);
        logger.log(""labels : "" + labels);
    }
}
</code></pre>

<p>How can I correct this code so no errors are showing for the public class?</p>",,1,5,,2018-12-12 09:27:24.480 UTC,,2018-12-26 01:12:05.210 UTC,2018-12-14 12:24:48.153 UTC,,1362568,,2841952,1,0,eclipse|amazon-web-services|aws-java-sdk|aws-toolkit,35
Express-browserify and Watson Visual Recognition - TypeError: fs.existsSync is not a function,55292198,Express-browserify and Watson Visual Recognition - TypeError: fs.existsSync is not a function,"<p>I'm trying to get the Watson Visual Recognition to run client side by using express-browserify with reference to the <a href=""https://github.com/watson-developer-cloud/node-sdk/tree/master/examples/browserify"" rel=""nofollow noreferrer"">node-sdk</a> for watson-developer-cloud. The <code>VisualRecognitionV3</code> makes use of the <code>fs</code> package hence I get the <code>fs.existsSync</code> error when I'm trying to call it from the client-side as the browser doesn't know which filesystem to use. My question is how do I go about creating a so called 'abstraction layer' as I am restricted to using the <code>express-browserify</code> package for cross origin calls.</p>

<p>This <a href=""https://stackoverflow.com/questions/16640177/browserify-with-requirefs"">thread</a> is pretty helpful in shedding some light but I'm not sure where to start regarding the 'abstraction layer' or if there are any other solutions. Also, would something like socket.io work for this? I've linked a clone of the directory <a href=""https://github.com/kaishinaw/traffic-image-classifier"" rel=""nofollow noreferrer"">here</a> as it seems less clunky than pasting the multiple portions below.</p>

<p>The repository can be cloned and just requires a personal iam_apikey with relevant launch configuration. Appreciate any pointers. Thanks!</p>",,1,0,,2019-03-22 02:49:56.307 UTC,,2019-03-24 09:25:45.133 UTC,2019-03-22 08:07:13.870 UTC,,11206146,,11206146,1,0,express|browserify|ibm-watson|fs,42
How can I find a good distracter for a key using python,40957513,How can I find a good distracter for a key using python,"<p>What I am trying to do is to create a Multiple Choice Question (MCQ) generation to our fill in the gap style question generator. I need to generate distracters (Wrong answers) from the Key (correct answer). The MCQ is generated from educational texts that users input. We're trying to tackle this through combining Contextual similarity, similarity of the sentences in which the keys and the distractors occur in and Difference in term frequencies Any help? I was thinking of using big data datasets to generate related distractors such as the ones provided by google vision, I have no clue how to achieve this in python.</p>",40957655,1,0,,2016-12-04 10:10:47.677 UTC,,2016-12-04 10:41:51.830 UTC,,,,,5486053,1,1,python|machine-learning|nlp|artificial-intelligence,94
Delay in google vision callback,35740396,Delay in google vision callback,"<p>I am using <code>Google Vision Api</code> for face detection. I want to enable capture button when the face is detected in the camera otherwise disable. Its working fine, only the issue is when there is a face button is enabled, but on face not available, button disables after 1/1.5 seconds because <code>onDone</code> callback of <code>Tracker</code> is called after 1, or 1.5 seconds. </p>

<pre><code>private class GraphicFaceTracker extends Tracker&lt;Face&gt; {
        private GraphicOverlay mOverlay;
        private FaceGraphic mFaceGraphic;

        GraphicFaceTracker(GraphicOverlay overlay) {
            mOverlay = overlay;
            mFaceGraphic = new FaceGraphic(overlay);
        }

        /**
         * Start tracking the detected face instance within the face overlay.
         */
        @Override
        public void onNewItem(int faceId, Face item) {
            mFaceGraphic.setId(faceId);
        }

        /**
         * Update the position/characteristics of the face within the overlay.
         */
        @Override
        public void onUpdate(FaceDetector.Detections&lt;Face&gt; detectionResults, Face face) {
            mOverlay.add(mFaceGraphic);
            mFaceGraphic.updateFace(face);
            iv.post(new Runnable() {
                @Override
                public void run() {
                    iv.setEnabled(true);
                }
            });
        }

        /**
         * Hide the graphic when the corresponding face was not detected.  This can happen for
         * intermediate frames temporarily (e.g., if the face was momentarily blocked from
         * view).
         */
        @Override
        public void onMissing(FaceDetector.Detections&lt;Face&gt; detectionResults) {
            mOverlay.remove(mFaceGraphic);
        }

        /**
         * Called when the face is assumed to be gone for good. Remove the graphic annotation from
         * the overlay.
         */
        @Override
        public void onDone() {
            iv.post(new Runnable() {
                @Override
                public void run() {
                    iv.setEnabled(false);
                }
            });
            mOverlay.remove(mFaceGraphic);
        }
    }
</code></pre>

<blockquote>
  <p>How Can I quickly get a callback that face is not in the camera so
  disable the button. How to remove the delay?</p>
</blockquote>",,1,0,,2016-03-02 06:54:35.753 UTC,,2016-03-03 16:50:22.933 UTC,,,,,3027124,1,1,android|google-vision,750
Combine horizontally aligned polygons in a image,51493545,Combine horizontally aligned polygons in a image,"<p>I have an image on which I am performing OCR using Google Vision API, I get a result which contains the polygon vertices of each word. After drawing the polygons the image looks like this.. <a href=""https://i.stack.imgur.com/U4CfQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U4CfQ.jpg"" alt=""Ocr results""></a>
I now want to combine the boxes that are horizontally aligned. For eg: (SALES ITEMS), (S000828749 MB Shorts 12.00),...,(Subtotal 146.00)</p>

<p><strong>Things I tried</strong>:
I made a line from mid point of vertical edges and extended it to the image edge and counted how many polygons the line touches and color coded the polygon with same color as the line. I got an image something like this..<br>
<a href=""https://i.stack.imgur.com/JDkJD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JDkJD.png"" alt=""enter image description here""></a><br>
Not sure how to proceed and get the groups on single line.. </p>",51517728,1,5,,2018-07-24 08:05:54.627 UTC,,2018-07-25 11:25:56.217 UTC,,,,,4512724,1,2,python|numpy|opencv|image-processing|ocr,109
What options can improve OCR using Google CLoud Vision?,46516766,What options can improve OCR using Google CLoud Vision?,"<p>I'm testing the OCR with Google cloud vision and I find the results are particularly bad. 
My documents are in french but it misses many apostrophes and commas.
For example as input <a href=""https://i.stack.imgur.com/i1Ljv.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i1Ljv.jpg"" alt=""enter image description here""></a></p>

<p>With the code</p>

<pre><code>Request
        .post(`https://vision.googleapis.com/v1/images:annotate?key=AIzaSyAtArxxxxxxxxxxxxxxxxxpGrKrydU4`)
        .send({
          requests: [{
            image: { content: base64.replace('data:image/jpeg;base64,', '') },
            features: [{ type: 'DOCUMENT_TEXT_DETECTION' }],
            ""imageContext"": { ""languageHints"": [ ""fr"" ] }
          }]
        })
</code></pre>

<p>I get the result (with errors highlighted in yellow)
<a href=""https://i.stack.imgur.com/oKCyX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oKCyX.png"" alt=""enter image description here""></a></p>

<p>When I test the same image with <a href=""https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/"" rel=""nofollow noreferrer"">Microsoft Azure OCR</a>, the result is absolutely perfect, without having to indicate the language.</p>

<p>Has anyone come across a similar level of inaccuracy in Google Cloud Vision ?</p>",,1,1,,2017-10-01 20:50:09.503 UTC,1,2018-09-04 16:43:20.760 UTC,2017-10-02 03:33:26.927 UTC,,322020,,1223628,1,1,javascript|azure|ocr|google-cloud-vision,245
Detect Text in PDF/TIFF Files with Google Cloud Vision,52512050,Detect Text in PDF/TIFF Files with Google Cloud Vision,"<p>I want to detect text in PDF and TIFF files with Google Cloud Vision, but from the looks of it that can only be done if you first store the file to the Google Cloud Storage. Can this also be done without storing it in the cloud?</p>",52519990,2,0,,2018-09-26 07:16:46.280 UTC,,2018-09-26 14:22:54.597 UTC,,,,,3197825,1,1,c#|google-cloud-vision,152
Cross checking of labels of two different photos,45481935,Cross checking of labels of two different photos,"<p>I'm working on a project where my application needs to point out if 2 photos might be taken at the same place.</p>

<p>Google vision will analyze each image to a JSON file, containing the labels with the highest scores.
for example:</p>

<pre><code>    {
  ""labelAnnotations"": [
    {
      ""mid"": ""/m/0j_s4"",
      ""description"": ""metropolitan area"",
      ""score"": 0.91176826
    },
    {
      ""mid"": ""/m/039jbq"",
      ""description"": ""urban area"",
      ""score"": 0.8989959
    },
    {
      ""mid"": ""/m/01n32"",
      ""description"": ""city"",
      ""score"": 0.8808476
    },
    {
      ""mid"": ""/m/0vlys"",
      ""description"": ""tower block"",
      ""score"": 0.8789163
    }]}
</code></pre>

<p>Is there a know algorythm for cross checking the labels of two different photos, so it would give us the similarity between those photos.. using google vision?</p>",,0,0,,2017-08-03 10:43:02.857 UTC,1,2017-08-03 10:43:02.857 UTC,,,,,8411007,1,2,json|machine-learning|google-vision,22
"With Google Cloud Vision, any limitation of width and length in Text_Detection and any way to specify the texts within a picture?",35790590,"With Google Cloud Vision, any limitation of width and length in Text_Detection and any way to specify the texts within a picture?","<p>I am stuck with a couple of problems when using Google Cloud Vision.</p>

<p>After I have tested Text_Detection with the Google Platform, I have had the results that I wanted in many cases. However, sometimes I fail to have the intended result either when it's too wide or long. Is there any limit to length or width, or resolution to get a proper result?</p>

<p>Also is there any way to recognize the texts within a specific area of a picture?</p>

<p>If anyone knows about the issue that I am facing now, please enlighten me. :)
Thank you in advance. </p>",,1,0,,2016-03-04 07:42:00.497 UTC,,2016-04-28 15:49:11.537 UTC,,,,,6016986,1,0,google-cloud-vision,114
flash light is not working when camera is on,52536703,flash light is not working when camera is on,"<p>I'm creating barcode reading application using google vision and it consists a flash on/off function also. i was able to implement flash using ""CameraManager"" like in the following code due to the ""Camera"" is now deprecated.but when camera screen is on, flash light is not working.when camera is freeze(when barcode is detected i'm stoping the camerasource), flash light is working. but i want to flash light on/off with out regarding the camera view is on or stop.i need this get done without using the deprecated APIs.can some one tell me how i can get solved this problem. thanks in advance.</p>

<pre><code> private void setupBarcode(){
    cameraPreview = (SurfaceView) findViewById(R.id.cameraPreview);
    txtResult = findViewById(R.id.txtResult);

    barcodeDetector = new BarcodeDetector.Builder(this)
            .setBarcodeFormats(Barcode.ALL_FORMATS)
            .build();
    cameraSource = new CameraSource
            .Builder(this, barcodeDetector)
            .setAutoFocusEnabled(true)
            .build();
    //Add Event
    cameraPreview.getHolder().addCallback(new SurfaceHolder.Callback() {
        @Override
        public void surfaceCreated(SurfaceHolder surfaceHolder) {
            if (ActivityCompat.checkSelfPermission(getApplicationContext(), android.Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {
                //Request permission
                ActivityCompat.requestPermissions(ScanActivity.this,
                        new String[]{Manifest.permission.CAMERA}, RequestCameraPermissionID);
                return;
            }
            try {
                cameraSource.start(cameraPreview.getHolder());
                txtResult.setText("""");
            } catch (IOException e) {
                e.printStackTrace();
            }
        }

        @Override
        public void surfaceChanged(SurfaceHolder surfaceHolder, int i, int i1, int i2) {

        }

        @Override
        public void surfaceDestroyed(SurfaceHolder surfaceHolder) {
            cameraSource.stop();

        }
    });

    barcodeDetector.setProcessor(new Detector.Processor&lt;Barcode&gt;() {
        @Override
        public void release() {

        }

        @Override
        public void receiveDetections(Detector.Detections&lt;Barcode&gt; detections) {
            final SparseArray&lt;Barcode&gt; qrcodes = detections.getDetectedItems();
            if (qrcodes.size() != 0) {

                txtResult.post(new Runnable() {
                    @Override
                    public void run() {
                        okButton.setEnabled(true);
                        txtResult.setText(qrcodes.valueAt(0).displayValue);
                        cameraSource.stop();

                    }
                });
            }
        }
    });
}

private void flashLightOn() {
    CameraManager cameraManager = (CameraManager) getSystemService(Context.CAMERA_SERVICE);

    try {
        String cameraId = cameraManager.getCameraIdList()[0];
        cameraManager.setTorchMode(cameraId, true);
    } catch (CameraAccessException e) {
    }
}
</code></pre>",,0,0,,2018-09-27 12:09:32.850 UTC,,2018-09-27 12:09:32.850 UTC,,,,,10422209,1,1,android|android-camera2|flashlight,31
"Microsoft Face API complaining about bad Key, but Key working in console?",36974179,"Microsoft Face API complaining about bad Key, but Key working in console?","<p>I been trying to solve this error but I can't find what seems to be wrong.</p>

<p>I am using <code>Microsoft Cognitive Services Face API</code> with <code>python</code>. Here is my code:</p>

<pre><code>import requests
import json
import http.client, urllib, base64, json

body = {""URL"": ""http://www.scientificamerican.com/sciam/cache/file/35391452-5457-431A-A75B859471FAB0B3.jsdfpg"" }

headers = {
    ""Content-Type"": ""application/json"",
    ""Ocp-Apim-Subscription-Key"": ""xxx""
}

try:

    r=requests.post('https://api.projectoxford.ai/face/v1.0/detect?returnFaceId=true&amp;returnFaceLandmarks=false&amp;returnFaceAttributes=age,gender',json.dumps(body) , headers)
    print(r.content)
except Exception as e:
    print(format(e))
</code></pre>

<p>When I run the script I get:</p>

<pre><code>""code"":""Unspecified"",""message"":""Access denied due to invalid subscription key. Make sure you are subscribed to an API you are trying to call and provide the right key.""
</code></pre>

<p>The thing is that when I put the exact same Key on the <a href=""https://dev.projectoxford.ai/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236/console"" rel=""nofollow"">console</a> everything works fine. So I am pretty sure it is not the key. </p>

<p>The error must be on my code, but I can't find it. </p>

<p>Any tip in the right direction will be appreciated,
Thanks</p>",36974739,1,0,,2016-05-02 02:21:08.310 UTC,,2016-05-02 03:56:38.270 UTC,,,,,402933,1,0,python|face-recognition|microsoft-cognitive,506
How to properly translate detected face coordinates from front camera,42162320,How to properly translate detected face coordinates from front camera,"<p>I'm using Google Vision for face detection on Android. Currently my code:</p>

<pre><code>public void onPreviewFrame(byte[] data, Camera camera) {

        // creating Google Vision frame from a camera frame for face recognition
        com.google.android.gms.vision.Frame frame = new com.google.android.gms.vision.Frame.Builder()
                .setImageData(ByteBuffer.wrap(data), previewWidth,
                        previewHeight, ImageFormat.NV21)
                .setId(frameId++)
                .setRotation(com.google.android.gms.vision.Frame.ROTATION_270)
                .setTimestampMillis(lastTimestamp).build();

        // recognize the face in the frame
        SparseArray&lt;Face&gt; faces = detector.detect(frame);

        // wrong coordinates
        float x = faces.valueAt(0).getPosition().x; 
        float y = faces.valueAt(0).getPosition().y; 
}
</code></pre>

<p>The problem is that <code>x</code> and <code>y</code> are not correct and even negative sometimes. I know that to get correct coordinates it should be rotated somehow, but how exactly?</p>",42208145,1,1,,2017-02-10 14:54:21.830 UTC,,2017-02-13 15:52:18.567 UTC,,,,,685948,1,0,android|google-vision,619
ImportError: No module named cloud at App Engine App,47000735,ImportError: No module named cloud at App Engine App,"<p>I try to deploy my python script at app engine but face with an error: ""ImportError: No module named cloud"".</p>

<p>I did everything like in <a href=""https://cloud.google.com/appengine/docs/standard/python/tools/using-libraries-python-27#installing_a_third-party_library"" rel=""nofollow noreferrer"">Installing a third-party library manual</a>. But it doe not work :(</p>

<p>So, what I have:</p>

<p>1) app.yaml:</p>

<pre><code>runtime: python27
api_version: 1
threadsafe: false

handlers:
- url: /.*
  script: main.py
</code></pre>

<p>2) appengine_config.py</p>

<pre><code># appengine_config.py
from google.appengine.ext import vendor
# Add any libraries install in the ""lib"" folder.
vendor.add('lib')
</code></pre>

<p>3) requirements.txt</p>

<pre><code>googleapis-common-protos==1.5.3
google-cloud-vision==0.27.0
google-gax==0.15.15
google-resumable-media==0.3.1
google-api-python-client==1.6.4
google-auth==1.1.1
</code></pre>

<p>4) lib folders with all libs from</p>

<pre><code>Used command:
pip install -t lib -r requirements.txt
</code></pre>

<p>5) main.py that use google cloud vision lib.</p>

<pre><code>from google.cloud import vision # Imports the Google Cloud client library
from google.cloud.vision import types # Imports the Google Cloud client library
import os

def detect_labels_uri(uri):

# Setup Credentials
os.environ[""GOOGLE_APPLICATION_CREDENTIALS""] = 'my_servive_acc_key.json'

def detect_labels_uri(uri):
    client = vision.ImageAnnotatorClient()
    image = types.Image()
    image.source.image_uri = uri

    response = client.label_detection(image=image)
    labels = response.label_annotations
    print('Labels:')

    for label in labels:
        print 'Content-Type: text/plain'
        print ''
        print(label.description)

try:
    detect_labels_uri('https://i.ytimg.com/vi/gqYlS_r8I-Y/maxresdefault.jpg')
except:
    print 'Content-Type: text/plain'
    print ''
    print 'Oops! That was no valid URL. Try again...'
</code></pre>

<p>Error:</p>

<pre><code>Traceback (most recent call last): (/base/data/home/runtimes/python27_experiment/python27_lib/versions/1/google/appengine/runtime/cgi.py:122)
  File ""/base/data/home/apps/h~test-184307/20171029t190420.405148744830706371/main.py"", line 7, in &lt;module&gt;
    from google.cloud import vision # Imports the Google Cloud client library
ImportError: No module named cloud
</code></pre>

<p>Could you help me to understand where I was wrong?</p>",,0,8,,2017-10-29 13:29:16.373 UTC,1,2017-10-30 19:21:08.650 UTC,2017-10-30 19:21:08.650 UTC,,8851700,,8851700,1,2,python|google-app-engine|google-cloud-vision,535
how I can set the result number for getProductSearchResults()?,54752242,how I can set the result number for getProductSearchResults()?,"<p>this code always return 10 records, anyone knows how to set 100?</p>

<p>Is for use the API of Google Cloud Vision Product Search.</p>

<pre><code> # get the name of the product set

 $productSetPath = $productSearchClient-&gt;productSetName($projectId, $location, $productSetId);

        # product search specific parameters
        $productSearchParams = (new ProductSearchParams())
            -&gt;setProductSet($productSetPath)
            -&gt;setProductCategories([$productCategory]);

        # search products similar to the image
        $response = $imageAnnotatorClient-&gt;productSearch($image, $productSearchParams);

        if ($productSearchResults = $response-&gt;getProductSearchResults()) {

            $results = $productSearchResults-&gt;getResults(); //this always return 10 results. 

            foreach ($results as $result) {

                # display the product information.
                $product = $result-&gt;getProduct();
                $productName = $product-&gt;getName();

            }
        }
</code></pre>",,1,1,,2019-02-18 17:12:36.800 UTC,,2019-03-14 06:56:00.047 UTC,,,,,2722901,1,0,php|google-cloud-vision,31
Tesseract - Detecting small font size of image and convert to text,50698560,Tesseract - Detecting small font size of image and convert to text,"<p>I have a screenshot of the bank cheque, I need all the text from this screenshot but tesseract is unable to read it properly. I also tried to pre-process the image but the output fails miserably.</p>

<p>I am using ImageMagick for pre-processing and Tesseract for recognizing text.
The link to the image: <a href=""https://imgur.com/a/pcgizic"" rel=""nofollow noreferrer"">https://imgur.com/a/pcgizic</a></p>

<p>I am able to retrieve the account number, but not IFSC code and person name ""SRINIVAS""</p>

<p>The steps I am following is as follows: </p>

<pre><code>magick -density 300 check1.jpg -depth 8 -strip -background white -alpha off check1.png

magick convert check1.png -resize 250% res_check1.png

convert -brightness-contrast 10x30 res_check1.png b_res_check1.png

convert b_res_check1.png -threshold 45% bin_res_check1.png

tesseract bin_res_check1.png o_res_check1
</code></pre>

<p>Note: I tried to resize the image upto 400% but it did not work.
Google Vision API is able to read and convert every single text properly.</p>",50704299,1,0,,2018-06-05 11:06:17.230 UTC,,2018-06-05 16:12:01.210 UTC,2018-06-05 11:19:19.043 UTC,,1533709,,1533709,1,1,php|image-processing|imagemagick|ocr|tesseract,268
What encoding does blob.download_as_string() return?,55537358,What encoding does blob.download_as_string() return?,"<p>I am downloading a file from Google Storage as a byte string, b64 encoding it, and using that as input into the Google Vision API.</p>

<pre><code>storage_client = storage.Client(project=[PROJECT])
bucket = storage_client.get_bucket([BUCKET])
blob = bucket.blob([KEY])

content = blob.download_as_string()
b64content = base64.b64encode(content)

client = vision.ImageAnnotatorClient()
image = vision.types.Image(content=b64content)
</code></pre>

<p>I am getting a bad image error using the b64content.  However, if I use the non base64 content, my call to the Vision API succeeds:</p>

<pre><code>image = vision.types.Image(content=content)
</code></pre>

<p>Does blob.download_as_string() return a byte string that is already base64 encoded?</p>",55570936,1,0,,2019-04-05 14:14:13.453 UTC,,2019-04-08 17:29:41.753 UTC,2019-04-08 17:29:41.753 UTC,,6039122,,10335909,1,0,python-2.7|encoding|google-cloud-platform|google-cloud-storage|google-cloud-vision,72
When app crashes for live capture detect text in get an image from the AVCaptureDataSampleBufferDelegate call back did output,54898476,When app crashes for live capture detect text in get an image from the AVCaptureDataSampleBufferDelegate call back did output,"<p>When an app crashes for live capture detect text in getting an image from the <code>AVCaptureDataSampleBufferDelegate</code> call back did output.
Get image call back from the send Google vision API I have used that code 
Did anyone work?
My code </p>

<pre><code>import UIKit
import FirebaseMLVision
import FirebaseMLModelInterpreter
import AVKit

protocol ImageDelegate: class {
  func getImage(imageOutFrame: String)
}
var photoOutput: AVCaptureVideoDataOutput!
var videoDataOutputQueue: DispatchQueue!
var previewLayer:AVCaptureVideoPreviewLayer!
var captureDevice : AVCaptureDevice!
var captureSession = AVCaptureSession()
var context = CIContext()
weak var delegate: ImageDelegate?
var vision = Vision.vision()
var textRecognizer = vision.onDeviceTextRecognizer()

extension UIViewController: AVCaptureVideoDataOutputSampleBufferDelegate {      
  // To set the camera and its position to capture
  func setUpAVCapture(capturePreview: UIView) {
    captureSession.sessionPreset = AVCaptureSession.Preset.vga640x480
    guard let device = AVCaptureDevice
      .default(AVCaptureDevice.DeviceType.builtInWideAngleCamera,
               for: .video,
               position: AVCaptureDevice.Position.back) else {
                return
    }
    captureDevice = device
    captureDevice.isFocusModeSupported(.continuousAutoFocus)
    try! captureDevice.lockForConfiguration()
    captureDevice.focusMode = .continuousAutoFocus
    captureDevice.unlockForConfiguration()
    beginSession(capturePreview: capturePreview)
  }
// Function to setup the beginning of the capture session
  func beginSession(capturePreview : UIView){

    var deviceInput: AVCaptureDeviceInput!

    do {
      deviceInput = try AVCaptureDeviceInput(device: captureDevice)
      guard deviceInput != nil else {
        print(""error: cant get deviceInput"")
        return
      }
      if captureSession.canAddInput(deviceInput){
        captureSession.addInput(deviceInput)
      }         
      photoOutput = AVCaptureVideoDataOutput()
      photoOutput.alwaysDiscardsLateVideoFrames=true
      videoDataOutputQueue = DispatchQueue(label: ""VideoDataOutputQueue"")
      photoOutput.setSampleBufferDelegate(self, queue: videoDataOutputQueue)            
      if captureSession.canAddOutput(photoOutput){
        captureSession.addOutput(photoOutput)
      }          
      photoOutput.connection(with: .video)?.isEnabled = true          
      previewLayer = AVCaptureVideoPreviewLayer(session: captureSession)
      previewLayer.videoGravity = AVLayerVideoGravity.resizeAspectFill          
      let rootLayer :CALayer = capturePreview.layer
      rootLayer.masksToBounds=true          
      rootLayer.addSublayer(previewLayer)          
       captureDevice.configureDesiredFrameRate(0)          
      captureSession.startRunning()
    } catch let error as NSError {
      deviceInput = nil
       exit(0)
      print(""error: \(error.localizedDescription)"")
    }        
  }     
  public func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
    connection.videoOrientation = AVCaptureVideoOrientation.portrait            
    DispatchQueue.global(qos: .userInteractive).async {
      guard let imag = self.imageFromSampleBuffer(sampleBuffer : sampleBuffer) else { return }
    let image = VisionImage(image: imag)
    textRecognizer.process(image) { result, error in
      guard error == nil, let result = result else {
        return
      }
      let resultText = result.text
      let matched = self.matches(for: RegEx.wholeRegex , in: resultText)          
      let m = matched.joined(separator: """")
      print(m)          
      delegate?.getImage(imageOutFrame: m)
      }
  }
  }      
  func imageFromSampleBuffer(sampleBuffer : CMSampleBuffer) -&gt; UIImage? {
    guard let imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return nil }
    let ciImage = CIImage(cvPixelBuffer: imageBuffer)
    guard let cgImage = context.createCGImage(ciImage, from: ciImage.extent) else { return nil }        
    let img = UIImage(cgImage: cgImage)
    let n = img.fixedOrientation()
    return n
  }
  func stopCamera(){
    captureSession.stopRunning()
    captureDevice = nil
  }      
}    
extension AVCaptureDevice {      
  /// http://stackoverflow.com/questions/21612191/set-a-custom-avframeraterange-for-an-avcapturesession#27566730
  func configureDesiredFrameRate(_ desiredFrameRate: Int) {        
    var isFPSSupported = false        
    do {          
      if let videoSupportedFrameRateRanges = activeFormat.videoSupportedFrameRateRanges as? [AVFrameRateRange] {
        for range in videoSupportedFrameRateRanges {
          if (range.maxFrameRate &gt;= Double(desiredFrameRate) &amp;&amp; range.minFrameRate &lt;= Double(desiredFrameRate)) {
            isFPSSupported = true
            break
          }
        }
      }          
      if isFPSSupported {
        try lockForConfiguration()
        activeVideoMaxFrameDuration = CMTimeMake(value: 1, timescale: Int32(desiredFrameRate))
        activeVideoMinFrameDuration = CMTimeMake(value: 1, timescale: Int32(desiredFrameRate))
        unlockForConfiguration()
      }          
    } catch {
      print(""lockForConfiguration error: \(error.localizedDescription)"")
    }
  }
}
</code></pre>

<p>get let r<code>esultText = result.text</code> get only doller value 
for example live capture  </p>

<blockquote>
  <p>xcsrf$16.54245 ssxvcxv</p>
</blockquote>

<p>I got an Output for like</p>

<p>$16.54245</p>

<ul>
<li><blockquote>
  <p>My problem is app getting slow recognition of text. </p>
</blockquote></li>
<li>1) I have set frames rate but not working</li>
<li>2) I have used Dispatch but not working.</li>
<li><blockquote>
  <p>get frames -> converted to image  converted to image -> send google</p>
</blockquote></li>
<li><blockquote>
  <p>vision API detecting text - > converted to get the only exact output</p>
</blockquote></li>
</ul>

<p><strong>> Please share your code to be appreciated Thanks.</strong></p>",,0,1,,2019-02-27 05:17:41.690 UTC,,2019-02-27 05:24:38.873 UTC,2019-02-27 05:24:38.873 UTC,,11123020,,11123020,1,1,ios|swift|avcapturedevice|avcapture,27
How to read text from image using google vision in android,48443021,How to read text from image using google vision in android,<p>I want to read texts from image using google vision text API. I have created my own UI which contains surfaceView. How to integrate google API</p>,,0,3,,2018-01-25 12:35:01.717 UTC,,2018-01-25 12:35:01.717 UTC,,,,,8818953,1,0,android|surfaceview|google-vision|image-text,146
Image preprocessing for egg marking recognition with Tesseract,45979638,Image preprocessing for egg marking recognition with Tesseract,"<p>The goal is to make an app which can recognize egg markings, for example <code>0-DE-134461</code>. I tried both <a href=""https://github.com/tesseract-ocr/tesseract/wiki/4.0-with-LSTM#400-alpha-for-windows"" rel=""nofollow noreferrer"">Tesseract</a> and the <a href=""https://cloud.google.com/vision/docs/drag-and-drop"" rel=""nofollow noreferrer"">Google Vision API</a> on the following images. The results from both OCR engines are disastrous.</p>

<p><a href=""https://i.stack.imgur.com/Gb58Am.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Gb58Am.jpg"" alt=""German Egg""></a>
<a href=""https://i.stack.imgur.com/AJ686m.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AJ686m.jpg"" alt=""Spanish Egg""></a></p>

<p><em>0-DE-46042</em></p>

<pre><code>Tesseract → """"
Google Vision API → "" 2 ""
</code></pre>

<p><em>3-ES08234-25591</em></p>

<pre><code>Tesseract → """"
Google Vision API → "" Es1234-2SS ) R SHAH That is part ""
</code></pre>

<hr>

<h1>Cropped</h1>

<p>I manually cropped the images with Photoshop.</p>

<p><a href=""https://i.stack.imgur.com/nTFwhm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nTFwhm.jpg"" alt=""German Egg - Cropped""></a>
<a href=""https://i.stack.imgur.com/fxf7a.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fxf7a.jpg"" alt=""Spanish Egg - Cropped""></a></p>

<p><em>0-DE-46042</em></p>

<pre><code>Tesseract → """"
Google Vision API → """"
</code></pre>

<p><em>3-ES08234-25591</em></p>

<pre><code>Tesseract → ""3ΓÇöE503ΓÇÿ234-gg""
Google Vision API → "" -ESOT23-2559 ) ""
</code></pre>

<hr>

<h1>Thresholded</h1>

<p>I color-selected the text on both eggs manually with Photoshop and removed the background.
<a href=""https://i.stack.imgur.com/9VuOO.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9VuOO.jpg"" alt=""German Egg - Thresholded""></a>
<a href=""https://i.stack.imgur.com/Pkpoj.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Pkpoj.jpg"" alt=""Spanish Egg - Thresholded""></a></p>

<p><em>0-DE-46042</em></p>

<pre><code>Tesseract → ""OΓÇöDEΓÇö46042""
Google Vision API → "" O-DE-46042 ""
</code></pre>

<p><em>3-ES08234-25591</em></p>

<pre><code>Tesseract → """"
Google Vision API → "" 3-ESO8234-9 ""
</code></pre>

<hr>

<h1>Removing the circular warp?</h1>

<p>I would assume that the last preprocessing step should be removing the circular warp, but I wouldn't know how to do that manually using Photoshop, let alone automating that.</p>

<hr>

<h1>My questions</h1>

<ul>
<li>Am I heading in the right direction?</li>
<li>Are my preprocessing steps correct?</li>
<li>What would be the approach to automate these steps in, say, OpenCV?</li>
</ul>

<hr>

<h3>Extra info</h3>

<p>The command I used to get the tesseract OCR results:</p>

<pre><code>λ tesseract {egg_picture}.jpg --psm 7 stdout
</code></pre>

<p>The tesseract version:</p>

<pre><code>λ tesseract --version
tesseract 4.0.0-alpha.20170804
 leptonica-1.74.4
  libgif 4.1.6(?) : libjpeg 8d (libjpeg-turbo 1.5.0) : libpng 1.6.20 : libtiff 4.0.6 : zlib 1.2.8 : libwebp 0.4.3 : libopenjp2 2.1.
</code></pre>

<p>Platform: <code>Windows 10</code></p>

<hr>

<h3>Edit 1</h3>

<p>I applied adaptive thresholding on some egg marking images with OpenCV. These are the results so far:</p>

<p><img src=""https://i.stack.imgur.com/9ZZWYm.jpg"" alt=""IMG A (edit 1)"">
<img src=""https://i.stack.imgur.com/1i4dDm.jpg"" alt=""IMG C (edit 1)"">
<img src=""https://i.stack.imgur.com/maWpDm.jpg"" alt=""IMG B (edit 1)"">
<img src=""https://i.stack.imgur.com/mIXHZm.jpg"" alt=""IMG H (edit 1)"">
<img src=""https://i.stack.imgur.com/n1Z0ym.jpg"" alt=""IMG D (edit 1)"">
<img src=""https://i.stack.imgur.com/BOTqUm.jpg"" alt=""IMG E (edit 1)"">
<img src=""https://i.stack.imgur.com/EqErEm.jpg"" alt=""IMG G (edit 1)"">
<img src=""https://i.stack.imgur.com/RODPgm.jpg"" alt=""IMG I (edit 1)""></p>

<p>However, there's still lots of noise. I'm struggling to adjust the parameters so that it works well across different images.</p>

<hr>",,1,3,,2017-08-31 10:48:54.870 UTC,1,2017-09-07 14:30:18.410 UTC,2017-09-07 14:30:18.410 UTC,,5508855,,5508855,1,8,opencv|ocr|tesseract|google-vision,392
How to use Google Cloud Vision API client for nodejs to detect multiple types,49932603,How to use Google Cloud Vision API client for nodejs to detect multiple types,"<p>I am trying to use the @google-cloud/vision package to send requests to the Google Vision API. Is there a way to do multiple detections without doing something like: </p>

<pre><code>client
  .labelDetection(link)
  .then(...)

client
  .safeSearchDetection(link)
  .then(...)
</code></pre>

<p>Thanks!</p>",,2,0,,2018-04-20 01:37:30.700 UTC,1,2018-07-25 06:55:33.803 UTC,,,,,9125698,1,3,node.js|google-cloud-platform|google-vision,795
Data is not returned for every URL input,47157038,Data is not returned for every URL input,"<p>I am not getting the labels and other properties on all the URL's when using the Google Vision API. It randomly gives this error on some URL.</p>

<blockquote>
  <p>{'responses': [{'error': {'code': 7, 'message': ""We're not allowed to access the URL on your behalf. Please download the content and pass it in.""}}]}</p>
</blockquote>

<p>As in, on some runs I don't see this error on the URL and get data, and on the other runs I see the error. </p>",,1,2,,2017-11-07 11:40:55.843 UTC,,2017-11-08 22:45:07.873 UTC,2017-11-08 13:17:27.783 UTC,,322020,,8043993,1,1,google-cloud-vision,256
How to get video codec using AWS Rekognition without label detection?,51036159,How to get video codec using AWS Rekognition without label detection?,"<p>I need only video codec for videos from the s3 bucket. I don't want to use label detection operation to get the video metadata for video. Is there any way to get video codec for an video from s3 bucket?</p>

<p>How to get video codec when I upload a video into s3 bucket using <a href=""https://aws.amazon.com/rekognition/"" rel=""nofollow noreferrer"">AWS Rekognition</a> without label detection?</p>

<p>Kindly provide your thoughts.</p>

<p>Any inputs here really appreciated. </p>",,0,0,,2018-06-26 06:26:23.123 UTC,1,2018-06-27 10:25:02.640 UTC,2018-06-27 10:25:02.640 UTC,,2247949,,2247949,1,0,amazon-web-services|amazon-rekognition,30
Why does Google Vision recognize 'und' locale for images containing just numbers in text?,52705012,Why does Google Vision recognize 'und' locale for images containing just numbers in text?,"<p>I've provided an image to the Google Cloud Vision OCR API to be annotated. The image just contained a phone number.</p>

<p>Google Cloud Vision said the locale of the text was 'und'. Does this mean undefined? I'm not finding any information in the documentation.</p>",52707053,1,0,,2018-10-08 14:57:53.757 UTC,,2018-10-08 17:06:24.660 UTC,,,,,1128632,1,0,google-cloud-platform|google-vision,26
Google Cloud Vision API text detection google.gax.errors.RetryError: GaxError,44093669,Google Cloud Vision API text detection google.gax.errors.RetryError: GaxError,"<p>I'm new in Google API.
Recently, I use the Google Vision API but I met the following problem:</p>

<pre><code>google.gax.errors.RetryError: GaxError(Exception occurred in retry method that was not classified as transient, caused by &lt;_Rendezvous of RPC that terminated with (StatusCode.RESOURCE_EXHAUSTED, Insufficient tokens for quota 'DefaultGroup' and limit 'USER-100s' of service 'vision.googleapis.com' for consumer 'project_number:XXX'.)&gt;)
</code></pre>

<p>And I tried the solution about ""Create the service accout"" to generate the service json key and invoke it in py.script,
it will work first in almost 3~4 url, but it will error in next url.
This is my detect code:</p>

<pre><code>import argparse
import io
from google.cloud import vision
vision_client = vision.Client.from_service_account_json('/Users/bruce0621/Downloads/esun-bank-adc1897dba67.json')
...
def detect_text_uri(uri):
    """"""Detects text in the file located in Google Cloud Storage or on the Web.
    """"""
    vision_client = vision.Client()
    image = vision_client.image(source_uri=uri)

    texts = image.detect_text()
    print('Texts:')

    for text in texts:
        print('\n""{}""'.format(text.description))

        vertices = (['({},{})'.format(bound.x_coordinate, bound.y_coordinate)
                    for bound in text.bounds.vertices])

        print('bounds: {}'.format(','.join(vertices)))
</code></pre>

<p>And I invoke the ""detect.py"" in another py.script:</p>

<pre><code>detect.detect_text_uri('...')
</code></pre>",,1,0,,2017-05-21 05:59:41.797 UTC,,2017-06-05 18:42:23.933 UTC,2017-05-21 23:59:58.740 UTC,,7802200,,8042860,1,0,python|google-cloud-vision,476
"Google Cloud Vision API ""PERMISSION_DENIED""",35532645,"Google Cloud Vision API ""PERMISSION_DENIED""","<p>I am trying Google Cloud Vision API (beta) and it is returning ""Permission Denied"" message. But the ""Cloud Vision API"" is enabled for the project. Any help is appreciated.</p>

<p><strong>Error Details from Google APIs Explorer</strong></p>

<pre><code>403 OK

- Show headers -

{
 ""error"": {
  ""code"": 403,
  ""message"": ""Project has not activated the vision.googleapis.com API. Please enable the API for project google.com:apisexplorerconsole (#292824132082)."",
  ""status"": ""PERMISSION_DENIED"",
  ""details"": [
   {
    ""@type"": ""type.googleapis.com/google.rpc.Help"",
    ""links"": [
     {
      ""description"": ""Google developers console API activation"",
      ""url"": ""https://console.developers.google.com/project/292824132082/apiui/api""
     }
    ]
   }
  ]
 }
}
</code></pre>",35534861,5,0,,2016-02-21 05:25:24.940 UTC,0,2017-02-22 07:26:43.930 UTC,,,,,5957684,1,5,google-cloud-platform|google-cloud-vision,7318
"Amazon Rekognition photo match, how open photo compared?",56184573,"Amazon Rekognition photo match, how open photo compared?","<p>hi i am following this project on github, </p>

<p><a href=""https://github.com/dwarcher/amazon-rekognition-example"" rel=""nofollow noreferrer"">https://github.com/dwarcher/amazon-rekognition-example</a></p>

<p>i'm using the api amazon rekognition with success,
i would like to see how to open the image of the compared face, i think i have to add something in ""index.js"" </p>

<pre><code>var express = require('express');
var app = express();

var config = require('./config.js')

var multer  = require('multer')
var upload = multer({ dest: 'uploads/' });

var AWS = require('aws-sdk');
AWS.config.region = config.region;

var uuid = require('node-uuid');
var fs = require('fs-extra');
var path = require('path');


app.use(express.static('public'));

var rekognition = new AWS.Rekognition({region: config.region});

app.post('/api/recognize', upload.single(""image""), function (req, res, next) {
var bitmap = fs.readFileSync(req.file.path);


rekognition.searchFacesByImage({
    ""CollectionId"": config.collectionName,
    ""FaceMatchThreshold"": 70,
    ""Image"": { 
        ""Bytes"": bitmap,
    },
    ""MaxFaces"": 1
}, function(err, data) {
    if (err) {
        res.send(err);
    } else {
        if(data.FaceMatches &amp;&amp; data.FaceMatches.length &gt; 0 &amp;&amp; data.FaceMatches[0].Face)
        {
            res.send(data.FaceMatches[0].Face); 
        } else {
            res.send(""Not recognized"");
        }
      }
  });
});

app.listen(5555, function () {
console.log('Listening on port 5555!');
})
</code></pre>

<p>the sample photos i put in the faces folder. 
thank you !</p>",,0,0,,2019-05-17 10:30:02.933 UTC,,2019-05-17 10:30:02.933 UTC,,,,,7110233,1,0,amazon|amazon-rekognition,10
Google Vision API not working Grpc.Core.RpcException,55950028,Google Vision API not working Grpc.Core.RpcException,"<p>I'm trying to get Google Vision API to work with my project but having trouble. I keep getting the following error:</p>

<p>Grpc.Core.RpcException: 'Status(StatusCode=PermissionDenied, Detail=""This API method requires billing to be enabled</p>

<p>I've created a service account, billing is enabled and I have the .json file. I've got the Environment variable for my account for GOOGLE_APPLICATION_CREDENTIALS pointing to the .json file. </p>

<p>I've yet to find a solution to my problem using Google documentation or checking StackOverFlow.</p>

<pre><code>using Google.Cloud.Vision.V1;
using System;
using System.Collections.Generic;

namespace Vision
{
    internal static class GoogleVision
    {
        public static EntityAnnotation[] GetAnnotations(EventManager em, string filePath, string EventNr)
        {
            {
                ImageAnnotatorClient Client = ImageAnnotatorClient.Create();
                Image Image = Google.Cloud.Vision.V1.Image.FromFile(filePath);
                IReadOnlyList&lt;EntityAnnotation&gt; Response = Client.DetectLabels(Image);

                EntityAnnotation[] annotations = new EntityAnnotation[Response.Count];
                for (int i = 0; i &lt; annotations.Length; i++)
                {
                    annotations[i] = Response[i];
                }

                return annotations;
            }
        }
    }
}
</code></pre>",,1,0,,2019-05-02 10:03:03.727 UTC,,2019-05-02 10:50:23.137 UTC,,,,,10266595,1,0,c#|windows|visual-studio|google-api|environment-variables,19
How to use multithreading for LCD output on the raspberry pi,46865429,How to use multithreading for LCD output on the raspberry pi,"<p>Writing to the 16x2 LCD display on the raspberryp pi can take some time to finish, especially with the module I wrote that automatically scrolls text that exceeds the length of the display.</p>

<p>I need to use multithreading, or something similar, to send the output to the display and continue with the rest of the program. I've tried a couple things with multithreading, but haven't quite got it.</p>

<p>This is the working code without any multithreading. The method I want to be multithreaded is ""TextToLCD.ProcessFrameBuffer"".</p>

<h1>piBell.py</h1>

<pre><code>#!/usr/bin/env python3

import time
import rekognition
import TextToLCD
import PiPhoto
import json
import logging
import re
import threading
from queue import Queue

logFormatter = logging.Formatter(""%(asctime)s [%(name)-8.8s]/[%(funcName)-12.12s] [%(levelname)-5.5s]  %(message)s"")
rootLogger = logging.getLogger('piBell')

fileHandler = logging.FileHandler(""{0}/{1}.log"".format(""./"", ""piBell""), 'a')
fileHandler.setFormatter(logFormatter)
rootLogger.addHandler(fileHandler)

consoleHandler = logging.StreamHandler()
consoleHandler.setFormatter(logFormatter)
rootLogger.addHandler(consoleHandler)

reFace = re.compile('face|head|selfie|portrait|person', re.IGNORECASE)

def main(debugMode='INFO'):
    TextToLCD.Clear()
    rootLogger.setLevel(debugMode)
    imgRotation = 270
    imgPath = './'
    imgName = 'image.jpg'

    TextToLCD.ProcessFrameBuffer([""Scanning:"", "".................""], debugMode)
    PiPhoto.GetPhoto(imgPath + imgName, imgRotation, ""INFO"")

    rootLogger.info(""Sending image to rekognition."")
    TextToLCD.ProcessFrameBuffer([""Processing"","".................""], debugMode)

    jsonLabels = rekognition.get_labels(imgPath + imgName)
    rootLogger.info(""Obtained JSON payload from rekognition."")
    rootLogger.debug(json.dumps(jsonLabels))

    if len(json.dumps(jsonLabels)) &gt; 0:
        if IsFace(jsonLabels):
            if TestFace(imgPath + imgName):
                TextToLCD.ProcessFrameBuffer(['Hello', '      :)'], debugMode)

                celeb = IsCelebrity(imgPath + imgName)
                if celeb:
                    TextToLCD.ProcessFrameBuffer([""You look like:"", celeb], debugMode)
            else:
                rootLogger.info(""No face detected."")
                TextToLCD.ProcessFrameBuffer(['No face detected', '       :('], debugMode)

        else:
            rootLogger.info(""No face detected."")
            TextToLCD.ProcessFrameBuffer(['No face detected', '       :('], debugMode)
    else:
        rootLogger.error(""JSON payload from rekognition was empty."")

def IsFace(jsonPayload):
    for value in jsonPayload:
        rootLogger.info(""Label: "" + value['Name'] + "", Confidence: "" +  str(round(value['Confidence'])))
        rootLogger.debug(json.dumps(jsonPayload))

        if reFace.match(value['Name']) and round(value['Confidence']) &gt; 75:
            rootLogger.info(""Possible face match."")
            return True
    return False

def TestFace(img):
    jsonFaces = rekognition.get_faces(img)
    rootLogger.debug(json.dumps(jsonFaces))

    if len(json.dumps(jsonFaces)) &gt; 2:
        for item in jsonFaces:
            if item['Confidence']:
                if item['Confidence'] &gt; 75:
                    rootLogger.info(""Face detected. Confidence: "" + str(round(item['Confidence'])))
                    return True
    else:
        rootLogger.info(""No facial data obtained."")

    return False

def IsCelebrity(img):
    celebMatchAccuracy = 25
    jsonCelbFaces = rekognition.get_celebrities(img)
    rootLogger.debug(json.dumps(jsonCelbFaces))

    if len(json.dumps(jsonCelbFaces)) &gt; 2:
        for item in jsonCelbFaces:
            if item['MatchConfidence']:
                if item['MatchConfidence'] &gt; celebMatchAccuracy and item['Name']:
                    rootLogger.info(""Celebirity match detected: "" + item['Name'] + "", Confidence: "" + str(round(item['MatchConfidence'])))

                    return item['Name']
    else:
        rootLogger.info(""No celebirity match found."")

    return False


if __name__ == ""__main__"":
    main('INFO')
</code></pre>",,1,4,,2017-10-21 16:29:16.690 UTC,,2017-10-21 17:51:58.763 UTC,,,,,1149618,1,-1,python|multithreading|python-3.x|raspberry-pi3|lcd,206
"Google Cloud Vision API request from React webcam error: ""No image present""",53798619,"Google Cloud Vision API request from React webcam error: ""No image present""","<p>I'm trying to make a Google Cloud Vision API request from a React camera component, with my image being sent directly to the API as a base64 encoded string. Specifically, I'm using the react-camera npm to get a user's photo through their webcam or front-facing (selfie) mobile camera.</p>

<p>The error I keep getting is ""Error: No image present."" After several hours trying various ways to encode, decode, stringify, and otherwise massage the blob data, I haven't been able to get the image into a format that works. Here are the relevant sections of my camera.js component:</p>

<pre><code>import React, { Component } from 'react';
import axios from 'axios';
import Camera from 'react-camera';
import 'react-html5-camera-photo/build/css/index.css'
...

export default class App extends Component {

  constructor(props) {
    super(props);
    this.takePicture = this.takePicture.bind(this);
    this.state = {blob:''};
    this.confirmphoto.bind(this)
  }

  takePicture() {
     this.camera.capture()
     .then(blob =&gt; {
        this.img.src = URL.createObjectURL(blob)
        this.img.onload = () =&gt; {
           URL.revokeObjectURL(this.src)
        }
        this.setState({
           blob:blob
        })
     })
  }

  confirmphoto() {
     console.log(""blob contents:"", this.state.blob);
     axios.post(process.env.REACT_APP_BACK_END_SERVER + 'submitChallenge', this.state.blob)
     .then((res)=&gt;{
        console.log(res);
     })
     .catch((err)=&gt;{
        console.log(err);
     });//end axios call
  }
</code></pre>

<p>And here are the relevant parts of my API request route:</p>

<pre><code>var express = require('express');
var router = express.Router();
...
const vision = require('@google-cloud/vision');
const client = new vision.ImageAnnotatorClient();

function pictureIsValid(pictureFile) {

   // Stringify raw blob data
   var newObj = JSON.stringify(pictureFile);
   console.log(newObj);

   // Convert blob to base64
   var newObj = Buffer.from(newObj).toString('base64');
   console.log(newObj);

   // JSON request for Google Cloud Vision API

   const request = {
      ""requests"":[
          {
            ""image"":{
               ""content"": newObj
            },
            ""features"":[
               {
                  ""type"":""LABEL_DETECTION"",
                  ""maxResults"":1
               }
            ]
         }
      ]
   };

   // Making the request
   client.annotateImage(request)
   .then(function(res){
      console.log(""result: "", res);
   })
   .catch(function(err){
      console.log(err);
   });
};

router.post('/', function (req, res) {
   var data = req.body;
   console.log(""data passed type:"", typeof data);
   var test = pictureIsValid(data);
   res.sendStatus(200).send();
});
</code></pre>

<p>Using strategically placed console logs, the data does appear to be passed but as an object, and it is several lines of encrypted data that looks like this:</p>

<pre><code>{ ' ����\u0000\u0010JFIF\u0000\u0001\u0001\u0001\u0000H\
u0006\b\u0007\u0007\b\n\u0011\u000b\n\t\t\n\u0014\u000f\
u0017\u001a\u001d% \u001a\u001c#\u001c\u0017\u0017!,!#\'
(***\u0019\u001f.1-)1%)*(\u0001\u0007\b\b\n\t\n\u0013\u0
00b\u000b\u0013(\u001b\u0017\u001b(((((((( ...u000f\u000
7���j���_\'i�F;��`�Ҿ���\u0017R�l�פ�)�q�y��f��\r��i�S�S\\
�4\u0010\nv)qJ)�W\u0013\u0014���.*�q���8�LRaq���8�*����':
'' }
</code></pre>

<p>This is what is showing in the terminal after the base64 conversion of the foregoing:</p>

<pre><code>eyLvv73vv73vv73vv71cdTAwMDBcdTAwMTBKRklGXHUwMDAwXHUwMDAx
XHUwMDAxXHUwMDAxXHUwMDAwSFx1MDAwMEhcdTAwMDBcdTAwMDDvv73v
v71cdTAwMDBcZkFwcGxlTWFya1xu77+977+9XHUwMDAw77+9XHUwMDAw
XHUwMDA3XHUwMDA1XHUwMDA1XHUwMDA2XHUwMDA1XHUwMDA1XHUwMDA3
XHUwMDA2XHUwMDA2XHUwMDA2XGJcdTAwMDdcdTAwMDdcYlxuXHUwMDEx
XHUwMDBiXG5cdFx0XG5cdTAwMTRcdTAwMGZcdTAwMGZcZlx1MDAxMVx1
MDAxOFx1MDAxNVx1MDAxOVx1MDAxOVx1MDAxN1x1MDAxNVx1MDAx ...
</code></pre>

<p>It looks like base64, which is what the Google Cloud Vision API is supposed to accept, but I keep getting back the error that there is ""no image present."" I've also tried submitting the blob itself, stringified and non-stringified versions of the blob and the base64 looking data. </p>

<p>I have a sneaking suspicion that the blob data is not even image data to begin with. Does anyone know where I'm going wrong? Do I need to switch in a different camera/webcam node package?</p>",,0,2,,2018-12-16 01:12:03.597 UTC,,2018-12-16 01:12:03.597 UTC,,,,,10796127,1,0,javascript|node.js|reactjs|google-cloud-vision,107
Cannot build Unity project for HoloLens with IBM Watson SDK,47308379,Cannot build Unity project for HoloLens with IBM Watson SDK,"<p>I'm having troubles building my Unity project. I tried both a custom scene, created by me, and the example scenes located into <code>Examples/ServiceExamples</code>. 
Basically, I want to take a picture with the HoloLens webcam and call IBM Watson visual recognition service. All Mixed Reality settings have been configured correctly. When I try to build the project I get lots of errors and they're all related to Watson SDK. For instance, the first error is the following:</p>

<blockquote>
  <p>Assets\Watson\ThirdParty\WebSocketSharp\Ext.cs(54,36): error CS0234: The type or namespace name 'X509Certificates' does not exist in the namespace 'System.Security.Cryptography' (are you missing an assembly reference?)</p>
</blockquote>

<p>I tried to install directly from the solution some of these packages that i'm missing, but the error remains there. Do you think it could be due to my Visual Studio project settings (e.g.: .NET targets, ecc)?
Here are my Unity settings:</p>

<p><a href=""https://i.stack.imgur.com/uHnkg.png"" rel=""nofollow noreferrer"">My settings imgur link</a></p>

<p>Am I missing something else? Any ideas? Thanks!</p>",47310516,1,0,,2017-11-15 13:15:11.977 UTC,,2017-11-15 14:53:44.507 UTC,,,,,8945240,1,1,visual-studio|unity3d|ibm-watson|visual-recognition,171
Where to set GOOGLE_APPLICATION_CREDENTIALS variable in android project?,40225148,Where to set GOOGLE_APPLICATION_CREDENTIALS variable in android project?,"<p>I am developing an android application which requires use of google vision service provided by Google Cloud Platform.
For authentication, this uses a class <strong>GoogleCredential</strong> class, the code for which can be found here:</p>

<p><a href=""https://cloud.google.com/vision/docs/label-tutorial#authenticating"" rel=""nofollow"">https://cloud.google.com/vision/docs/label-tutorial#authenticating</a></p>

<p>I need to set <strong>GOOGLE_APPLICATION_CREDENTIALS</strong> variable as an environment variable pointing to a json file downloaded from another link, which is not important.</p>

<p><em>The question is</em>:
When I set the environment variable using</p>

<pre><code>Uri otherPath = Uri.parse(""android.resource://com.example.himanshu.myapplication/DtnPhotoShare-ff552ae2c96c.json"");
    System.setProperty(""**GOOGLE_APPLICATION_CREDENTIALS**"",otherPath.toString());
</code></pre>

<p><em>It gives me exception</em> :</p>

<blockquote>
  <p>java.io.IOException: The Application Default Credentials are not
  available. They are available if running on Google App Engine, Google
  Compute Engine, or Google Cloud Shell. Otherwise, the environment
  variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing
  to a file defining the credentials.</p>
</blockquote>

<p><em>in line</em> </p>

<pre><code>GoogleCredential credential =
                GoogleCredential.getApplicationDefault().createScoped(VisionScopes.all());
</code></pre>

<p>All the code can be found in the link given above.</p>",,1,1,,2016-10-24 18:32:54.020 UTC,,2017-01-04 13:18:20.740 UTC,2016-10-25 17:29:40.013 UTC,,322020,,3176272,1,3,android|google-cloud-vision,1170
Get correct image orientation by Google Cloud Vision api (TEXT_DETECTION),41285556,Get correct image orientation by Google Cloud Vision api (TEXT_DETECTION),"<p>I tried Google Cloud Vision api (TEXT_DETECTION) on 90 degrees rotated image. It still can return recognized text correctly. (see image below)</p>

<p>That means the engine can recognize text even the image is 90, 180, 270 degrees rotated.</p>

<p>However the response result doesn't include information of correct image orientation. (document: <a href=""https://developers.google.com/resources/api-libraries/documentation/vision/v1/java/latest/com/google/api/services/vision/v1/model/EntityAnnotation.html"" rel=""noreferrer"">EntityAnnotation</a>)</p>

<p>Is there anyway to not only get recognized text but also get the <strong>orientation</strong>?<br>
Could Google support it similar to (<a href=""https://developers.google.com/resources/api-libraries/documentation/vision/v1/java/latest/com/google/api/services/vision/v1/model/FaceAnnotation.html"" rel=""noreferrer"">FaceAnnotation</a>: getRollAngle)</p>

<p><a href=""https://i.stack.imgur.com/fTQrT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fTQrT.png"" alt=""enter image description here""></a></p>",41556922,4,2,,2016-12-22 14:36:16.473 UTC,4,2018-06-14 01:47:53.263 UTC,,,,,2358123,1,9,ocr|google-cloud-platform|google-cloud-vision,5475
How to get a position of custom object on image using vision recognition api,38634409,How to get a position of custom object on image using vision recognition api,"<p>I know there is a lot of vision recognition APIs such as Clarifai, Watson, Google Cloud Vision, Microsoft Cognitive Services which provide recognition of image content. The response of these services is simple json that contains different tags, for example</p>

<pre><code>{ 
   man: 0.9969295263290405,
   portrait: 0.9949591159820557,
   face: 0.9261120557785034
}
</code></pre>

<p>The problem is that I need to know not only what is on the image but also the position of that object. Some of those APIs have such feature but only for face detection.</p>

<p>So does anyone know if there is such API or I need to train own haar cascades on OpenCV for every object.</p>

<p>I will be very greatful for sharing some info.</p>",38647258,1,1,,2016-07-28 10:59:14.027 UTC,,2017-07-07 09:50:04.593 UTC,2016-07-28 13:43:06.740 UTC,,6649145,,6649145,1,3,computer-vision|image-recognition|object-detection,1711
How to enable Google Vision API to access Google Cloud Storage Bucket within same project,42125608,How to enable Google Vision API to access Google Cloud Storage Bucket within same project,"<p>I have uploaded some test images to a Google Cloud Bucket, but don't want to make them public (which would be cheating).  When I try to run a rest call for Google Vision API I get:</p>

<pre><code>{
  ""responses"": [
    {
      ""error"": {
        ""code"": 7,
        ""message"": ""image-annotator::User lacks permission.: Can not open file: gs://images-translate-156512/P1011234.JPG""
      }
    }
  ]
}
</code></pre>

<p>What are the steps to enable the Google Vision API to access Google Cloud Storage objects within the same project? At the moment I am using only the API key while I experiment with Google Vision.  I am suspecting a service account may be required and an ACL on the GCS objects.</p>

<p>I could bypass GCS altogether and base64 encode the image and send it Google Vision API, but really want to try and solve this use case.  Not used ACLs yet, or service accounts.</p>

<p>Any help appreciated</p>",,1,0,,2017-02-08 23:34:21.910 UTC,,2017-02-09 16:27:40.770 UTC,2017-02-09 16:27:40.770 UTC,,1241334,,1754307,1,5,google-cloud-storage|google-cloud-vision|google-vision,1209
Why do I get invalid JSON payload when calling google cloud vision API from appcelerator?,35885801,Why do I get invalid JSON payload when calling google cloud vision API from appcelerator?,"<p>I was trying to use Google vision API v1 by Alloy Appcelerator</p>

<p>I create a request HTTPClient and call API <a href=""https://vision.googleapis.com/v1/images:annotate?key=MY_APP_KEY"" rel=""nofollow"">https://vision.googleapis.com/v1/images:annotate?key=MY_APP_KEY</a></p>

<p>But i have get response text from google :</p>

<pre><code>  {
 error = {
     code = 400;
     details = (
                  {
                     ""@type"" = ""type.googleapis.com/google.rpc.BadRequest"";
                      fieldViolations = ({
                                        description = ""Invalid JSON payload received. Unknown name \""request\"": Cannot bind query parameter. Field 'request' could not be found in request message."";
                                        });
                  }
                );
     message = ""Invalid JSON payload received. Unknown name \""request\"": Cannot bind query parameter. Field 'request' could not be found in request message."";
     status = ""INVALID_ARGUMENT"";
  };
</code></pre>

<p>}</p>

<p>And there is my code use HTTP request by Alloy</p>

<pre><code>var requests =  
{
  ""requests"":[
    {
      ""image"":{
        ""content"": ""image_have_encodebase64"",
      },
      ""features"":[
        {
          ""type"":""TEXT_DETECTION"",
          ""maxResults"":1
        }
      ]
    }
  ]
};
var xhr = Titanium.Network.createHTTPClient();
xhr.open(""POST"", 'https://vision.googleapis.com/v1/images:annotate?key=MY_APP_KEY');
xhr.send(JSON.stringify(requests));
</code></pre>

<p>Thanks for your help</p>",,1,6,,2016-03-09 07:47:27.847 UTC,,2017-05-16 06:15:55.433 UTC,2016-03-10 12:19:34.063 UTC,,4626813,,6038145,1,1,google-api|appcelerator|appcelerator-titanium|google-vision,8323
Google Cloud Vision reverse image search fails on Azure App Service because GOOGLE_APPLICATION_CREDENTIALS file cannot be found,52186137,Google Cloud Vision reverse image search fails on Azure App Service because GOOGLE_APPLICATION_CREDENTIALS file cannot be found,"<p>I am attempting to perform a Google reverse image search using <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">Google Cloud Vision</a> on an Azure app service web app. </p>

<p>I have generated a googleCred.json, which the Google client libraries use in order to construct API requests. Google expects it to be available from an environment variable named GOOGLE_APPLICATION_CREDENTIALS.</p>

<p>The Azure app service that runs the web app has settings that mimic environment variables for the Google client libraries. The documentation is <a href=""https://docs.microsoft.com/en-us/azure/app-service/web-sites-configure"" rel=""nofollow noreferrer"">here</a>, and I have successfully set the variable here:</p>

<p><a href=""https://i.stack.imgur.com/IfwyU.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IfwyU.jpg"" alt=""enter image description here""></a></p>

<p>Furthermore, the googleCred.json file has been uploaded to the app service. Here is the <a href=""https://docs.microsoft.com/en-us/azure/app-service/app-service-deploy-ftp"" rel=""nofollow noreferrer"">documentation</a> I followed to use FTP and FileZilla to upload the file:</p>

<p><a href=""https://i.stack.imgur.com/Vo37d.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Vo37d.jpg"" alt=""enter image description here""></a></p>

<p>Also, the file permissions are as open as they can be:</p>

<p><a href=""https://i.stack.imgur.com/0Kl71.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0Kl71.jpg"" alt=""enter image description here""></a></p>

<p>However, when I access the web app in the cloud, I get the following error message: </p>

<blockquote>
  <p>Error reading credential file from location D:\site\wwwroot\Statics\googleCred.json: Could not find a part of the path 'D:\site\wwwroot\Statics\googleCred.json'. Please check the value of the Environment Variable GOOGLE_APPLICATION_CREDENTIALS</p>
</blockquote>

<p><strong>What am I doing wrong? How can I successfully use the Google Cloud Vision API on an Azure web app?</strong></p>",,2,1,,2018-09-05 13:17:28.807 UTC,,2018-11-01 20:12:32.423 UTC,,,,,9268003,1,0,azure|google-api|azure-web-app-service|google-cloud-vision|azure-app-service-plans,135
how to get Google Lens like sticky overlay graphic with Google Mobile vision API,52448693,how to get Google Lens like sticky overlay graphic with Google Mobile vision API,<p>I am using Google vision API for barcode reads but the overlay is not consistent and flickering but when I saw google lens the dots are much sticky and they are not flickering. anyone help me to create a smooth overlay using mobile vision API. I know ML kit will replace it in future yet i think they both work the same.</p>,,0,0,,2018-09-21 17:42:05.097 UTC,1,2018-09-21 17:42:05.097 UTC,,,,,8578027,1,0,android|mobile|vision,45
Update Text File Using Lambda,51932084,Update Text File Using Lambda,"<p>I want to be able to update a text file whenever I upload an image to the s3 bucket. This text file will contain on each line the results of Amazon Rekognition. However, the code I've written isn't working properly</p>

<pre><code>bucket_name = ""update-my-text-file""
rekognition = boto3.client('rekognition')
s3 = boto3.resource('s3')
bucket = s3.Bucket(bucket_name)

def handle_image(key):
    response = rekognition.detect_labels(
        Image={
            'S3Object': {
                'Bucket': bucket_name,
                'Name': key
            }
        }
    )
    return response


def lambda_handler(event, context):

    file_name = 'results.txt'
    object = s3.Object(bucket_name, 'tmp/results.txt')

    cli = boto3.client('s3')
    response = cli.get_object(Bucket=bucket_name, Key='tmp/results.txt')
    data = response['Body'].read()
    print('the data is ' + data)

    key = urllib.unquote_plus(event['Records'][0]['s3']['object']['key'].encode('utf8'))
    response = handle_image(key)
    print('the response is: ' + response)

    object.put(Body=data + '/n' + response)
</code></pre>",,1,1,,2018-08-20 13:42:28.440 UTC,,2018-08-20 21:49:12.940 UTC,2018-08-20 21:49:12.940 UTC,,174777,,2624768,1,-2,python|amazon-web-services|amazon-s3|aws-lambda,94
IBM Visual Recognition: How do I back up a custom classifier?,44594617,IBM Visual Recognition: How do I back up a custom classifier?,"<p>The IBM Visual Recognition classifier is simple to use and works well. However, custom classifier creation is expensive ($0.10/image) and time-consuming. Accidental deleting of a custom classifier puts any workflow using that classifier at risk. There is no obvious way in the API or dashboard to download, duplicate, or lock a custom classifier. This is a concern for production use.</p>

<p>How can I back up a custom classifier created using IBM Watson Visual Recognition? This question <a href=""https://developer.ibm.com/answers/questions/360524/how-can-i-backup-my-custom-classifier-on-visual-re.html"" rel=""nofollow noreferrer"">went unanswered on IBM's developer forum</a> and I am hoping someone from IBM can provide guidance here.</p>

<p>Thank you!</p>",,2,0,,2017-06-16 17:09:25.383 UTC,,2017-06-17 12:40:51.613 UTC,,,,,198374,1,1,ibm-watson|visual-recognition,89
Is there any api for reverse image search for android as powerful as Google reverse image search?,42631231,Is there any api for reverse image search for android as powerful as Google reverse image search?,"<p>I want to do reverse image search in my android app. I need some api as powerful as google reverse image search.
Is there any google reverse image search api for android? weather free or non-free</p>

<p>I also found <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">google cloud vision api</a> but the results - at least in it's <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">demo</a> - is not as specific as google reverse image search.</p>

<p>so is there any other api for reverse image search in android as powerful as google?
or is there any other method to use google reverse image search results inside the android app? 
I found <a href=""https://stackoverflow.com/a/29195961/5163584"">this</a> but seems google has detected and closed that server.</p>",,2,1,,2017-03-06 16:54:33.877 UTC,1,2017-05-05 14:49:45.413 UTC,2017-05-23 12:02:22.183 UTC,,-1,,5163584,1,0,android|google-app-engine|google-api|google-image-search|google-cloud-vision,1764
Android: Google Vision API detect faces and landmarks every frame of the camera preview?,42109691,Android: Google Vision API detect faces and landmarks every frame of the camera preview?,"<p>Is Google Vision API capable of detecting faces and landmarks every frame of the camera preview when the preview is showing?</p>

<p>If yes, could you please point me to a code snippet on how to do this? I already have the camera2 interface set up. Thanks!</p>",,1,1,,2017-02-08 09:49:34.600 UTC,,2017-02-09 15:54:18.897 UTC,,,,,2677752,1,0,android|face-detection|android-camera2|google-vision,857
Android Image to Bitmap for use in Google Vision Barcode api,41755822,Android Image to Bitmap for use in Google Vision Barcode api,"<p>I need to convert <code>android.media.Image</code> to a <code>BitMap</code> so that I can use it in the <a href=""https://developers.google.com/vision/barcodes-overview"" rel=""nofollow noreferrer"">Google Vision Barcode API</a>. I've tried the following...</p>

<pre><code>    byte[] imageData = new byte[image.getPlanes()[0].getBuffer().remaining()];
    Bitmap bmp = BitmapFactory.decodeByteArray(imageData, 0, imageData.length);
    Frame frame = new Frame.Builder().setBitmap(bmp).build();
    SparseArray&lt;Barcode&gt; barcodes = detector.detect(frame);
    System.out.println(barcodes.valueAt(0));
</code></pre>

<p>... but I get the following error:</p>

<pre><code>java.lang.NullPointerException: Attempt to invoke virtual method 'int android.graphics.Bitmap.getWidth()' on a null object reference.
</code></pre>

<p>Google's example code references so the deprecated <code>Camera</code> API but I am using <code>camera2</code> which is why I cannot use them for help.</p>",,1,0,,2017-01-20 03:53:18.847 UTC,,2017-01-20 06:01:37.417 UTC,,,,,5710376,1,0,java|android|bitmap|camera2|google-vision,557
Can I use the Google Cloud Vision API without activating the free trial?,40949801,Can I use the Google Cloud Vision API without activating the free trial?,"<p>Is there a way to test the Google Vision API in an application without activating my free trial? </p>

<p>I am trying to use the API in a sample test application, but I can't enable the Vision API without having a valid billing method added. </p>

<p>Error Message:  "" The API requires a valid billing method."" </p>

<p>When I try to enable billing from the Dashboard - Billing - It redirect to a page where I have to input my information in order to ""Try Cloud Platform for free"" and I have to click on a button with the message - ""Start my free trial"". Is there a way to enable billing without starting my free trial? </p>

<p>I just want to use the free tier (doesn't matter if I would have to put in my credit card) without 'wasting' my free trial -- I think so much money for trial could be spent better elsewhere...</p>",,2,0,,2016-12-03 16:14:08.010 UTC,,2016-12-17 18:19:33.033 UTC,2016-12-15 02:38:26.950 UTC,,3895774,,3452075,1,0,google-app-engine|google-api|google-cloud-platform|google-cloud-vision|google-vision,4138
Google Vision crashes when 2 codes are presented,48400312,Google Vision crashes when 2 codes are presented,"<p>I am using google vision in my application to read barcodes and qr-codes. This is working great, but it crashes when the scanned surface has 2 codes next to each other, like in the picture below. </p>

<p><a href=""https://i.stack.imgur.com/NQzrU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NQzrU.png"" alt=""enter image description here""></a></p>

<p>This happens, even though the codes are scanned properly if they are scanned seperately. Does anyone know how to stop this from happening?</p>

<p>Here is the code that manages the code scanning activity:</p>

<pre><code>public class ScanningActivity extends AppCompatActivity {

    SurfaceView cameraPreview;
    BarcodeDetector barcodeDetector;
    CameraSource cameraSource;
    final Integer requestCameraPermissionID = 1001;

    @Override
    protected void onCreate(@Nullable Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.home_activity_add_scanner);

        final int height = getSurfaceViewHeight();
        final int width = getSurfaceViewWidth();

        cameraPreview = findViewById(R.id.surfaceview_scanner);

        barcodeDetector = new BarcodeDetector.Builder(AddScanningActivity.this)
                .setBarcodeFormats(Barcode.ALL_FORMATS)
                .build();
        cameraSource = new CameraSource.Builder(AddScanningActivity.this, barcodeDetector)
                .setAutoFocusEnabled(true)
                .setRequestedPreviewSize(width, height)
                .build();
        barcodeDetector.setProcessor(new Detector.Processor&lt;Barcode&gt;() {
            @Override
            public void release() {

            }

            @Override
            public void receiveDetections(Detector.Detections&lt;Barcode&gt; detections) {
                final SparseArray&lt;Barcode&gt; barcodes = detections.getDetectedItems();
                if (barcodes.size() != 0) {
                    Vibrator vibrator = (Vibrator) getApplicationContext().getSystemService(VIBRATOR_SERVICE);
                    vibrator.vibrate(100);

                    Handler handler = new Handler(Looper.getMainLooper());
                    handler.post(new Runnable() {
                        @Override
                        public void run() {
                            cameraSource.release();
                        }
                    });
                }
            }
        });
        cameraPreview.getHolder().addCallback(new SurfaceHolder.Callback() {
            @Override
            public void surfaceCreated(SurfaceHolder surfaceHolder) {
                final Handler handler = new Handler();
                handler.postDelayed(new Runnable() {
                    @Override
                    public void run() {
                        if (ActivityCompat.checkSelfPermission(getApplicationContext(), Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {
                            ActivityCompat.requestPermissions(AddScanningActivity.this,
                                    new String[]{Manifest.permission.CAMERA}, requestCameraPermissionID);
                            return;
                        }
                        try {
                            cameraSource.start(cameraPreview.getHolder());
                        } catch (IOException e) {
                            e.printStackTrace();
                        }
                    }
                }, 100);
            }

            @Override
            public void surfaceChanged(SurfaceHolder surfaceHolder, int i, int i1, int i2) {

            }

            @Override
            public void surfaceDestroyed(SurfaceHolder surfaceHolder) {
                cameraSource.stop();
            }
        });
    }

    private Integer getSurfaceViewHeight() {
        Display display = getWindowManager().getDefaultDisplay();
        Point size = new Point();
        display.getSize(size);
        Integer screenHeight = size.y;

        Rect rectangle = new Rect();
        Window window = getWindow();
        window.getDecorView().getWindowVisibleDisplayFrame(rectangle);
        final int statusBarHeight = rectangle.top;

        int[] attrs = new int[]{R.attr.actionBarSize};
        TypedArray ta = this.obtainStyledAttributes(attrs);
        final int actionBarHeight = ta.getDimensionPixelSize(0, -1);
        ta.recycle();
        return screenHeight - statusBarHeight - actionBarHeight;
    }

    private Integer getSurfaceViewWidth() {
        Display display = getWindowManager().getDefaultDisplay();
        Point size = new Point();
        display.getSize(size);
        return size.x;
    }
}
</code></pre>",48419115,1,0,,2018-01-23 11:12:46.293 UTC,,2018-01-25 11:36:12.183 UTC,2018-01-24 11:46:59.787 UTC,,8395273,,8395273,1,0,android|crash|qr-code|barcode-scanner|google-vision,91
Microsoft Custom Vision only work for Visual Studio 2015?,44419153,Microsoft Custom Vision only work for Visual Studio 2015?,"<p><strong>The Problem:</strong>
When I try to install the packages for Microsoft Custom Vision in VS 2013, it fails. Does Custom Visions just not compatible with VS 2013, or is there another problem here?</p>

<p>When I try to install the Custom Vision package <strong>after the first failed attempt</strong> (without uninstalling Microsoft.Rest.ClientRuntime 2.3.2), I get a different response, as seen below:</p>

<pre><code>Attempting to resolve dependency 'Microsoft.Rest.ClientRuntime (≥ 2.3.2)'.
Attempting to resolve dependency 'Newtonsoft.Json (≥ 6.0.8)'.
Attempting to resolve dependency 'System.IO.FileSystem (≥ 4.3.0)'.
Attempting to resolve dependency 'System.Net.Http (≥ 4.3.1)'.
Installing 'System.IO.FileSystem 4.3.0'.
Successfully installed 'System.IO.FileSystem 4.3.0'.
Installing 'System.Net.Http 4.3.1'.
Successfully installed 'System.Net.Http 4.3.1'.
Installing 'Microsoft.Cognitive.CustomVision.Training 1.0.0'.
Successfully installed 'Microsoft.Cognitive.CustomVision.Training 1.0.0'.
Adding 'System.IO.FileSystem 4.3.0' to WpfApplication2.
Uninstalling 'System.IO.FileSystem 4.3.0'.
Successfully uninstalled 'System.IO.FileSystem 4.3.0'.
Install failed. Rolling back...
Install-Package : Could not install package 'System.IO.FileSystem 4.3.0'. You are trying to install this package into a project that targets 
'.NETFramework,Version=v4.5', but the package does not contain any assembly references or content files that are compatible with that framework. For 
more information, contact the package author.
At line:1 char:1
+ Install-Package Microsoft.Cognitive.CustomVision.Training
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : NotSpecified: (:) [Install-Package], InvalidOperationException
    + FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommand
</code></pre>

<p>Can anyone tell me what this means, or offer a potential fix?</p>

<p><strong>Qualifier:</strong>
My development team all use Visual Studio 2013, so I'd rather not change to 2015 or 2017.</p>

<p><strong><em>UPDATE:</em></strong> I have succumb to stress and installed VS 2017- still getting the same error:</p>

<pre><code>Could not install package 'System.IO.FileSystem 4.3.0'. You are trying to install this package into a project that targets '.NETFramework,Version=v4.5.2', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.
</code></pre>",44422834,1,0,,2017-06-07 17:25:40.417 UTC,,2017-06-07 21:11:15.777 UTC,2017-06-07 18:12:26.093 UTC,,8021300,,8021300,1,0,visual-studio-2013|nuget|microsoft-cognitive,103
Google APIs to use?,42992782,Google APIs to use?,"<p>i am currently working on a project which will use the device's camera function to take a photo of a ingredient list and then it will display text in the image.I have used google cloud vision API to display the text.However i want to be able to highlight the text that was displayed on the image itself.Something like the google translate application where the user takes a picture and the screen display a box to let the user to draw out which of the words they wanted to translate.May i know which API do i need to use in order to create this function?</p>

<p>As requested: This is what i had done so far, this is to display the text in images to a console.log()</p>

<pre><code>    takePicture() {
    Camera.getPicture({

      destinationType: Camera.DestinationType.DATA_URL,

      targetWidth: 1000,
      targetHeight: 1000
    }).then((imageData) =&gt; {
      // imageData is a base64 encoded string
      this.base64Image = ""data:image/jpeg;base64,"" + imageData;
      //this.detectText(imageData);
      this.getVision(imageData);
      //console.log(imageData);

    }, (err) =&gt; {
      console.log(err);
    });
  }



    getVision(base64image: string) {
    this.testService.getVisionLabels(base64image)
      .subscribe((sub) =&gt; {

        this.labels = sub.responses[0].textAnnotations;

        this.getText();


      });
  }

getText() {

this.labels.forEach((label) =&gt; { 
  let translation = {search: label.description, result: ''};
  //console.log(label.description);
  console.log(label.description);
});
</code></pre>",,0,4,,2017-03-24 06:25:43.063 UTC,,2017-03-24 07:11:21.663 UTC,2017-03-24 07:11:21.663 UTC,,1915893,,5738494,1,2,javascript|cordova|typescript|ionic2|google-translate,74
Detecting face landmarks points in android,38435653,Detecting face landmarks points in android,"<p>I am developing app in which I need to get face landmarks points on a cam like mirror cam or makeup cam. I want it to be available for iOS too. Please guide me for a robust solution.
I have used Dlib and Luxand.</p>

<p>DLIB: <a href=""https://github.com/tzutalin/dlib-android-app"" rel=""noreferrer"">https://github.com/tzutalin/dlib-android-app</a></p>

<p>Luxand: <a href=""http://www.luxand.com/facesdk/download/"" rel=""noreferrer"">http://www.luxand.com/facesdk/download/</a></p>

<p>Dlib is slow and having a lag of 2 sec approximately (Please look at the demo video on the git page) and luxand is ok but it's paid. My priority is to use an open source solution.
I have also use the Google vision but they are not offering much face landmarks points.
So please give me a solution to make the the dlib to work fast or any other option keeping cross-platform in priority.
Thanks in advance.</p>",,4,1,,2016-07-18 11:32:13.013 UTC,9,2016-07-21 05:48:46.947 UTC,,,,,2240213,1,5,android|dlib,8803
ImportError phonenumbers with google cloud dataflow python,50918217,ImportError phonenumbers with google cloud dataflow python,"<p>I am trying to do a relatively simple import of the module phonenumbers in Python.</p>

<p>I have tested the module on a seperate python file without any other imports and it works completely fine.</p>

<p>These are the packages I have installed:</p>

<pre><code>from __future__ import absolute_import
from __future__ import print_function

import argparse
import csv
import logging
import os
import phonenumbers

import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.options.pipeline_options import SetupOptions
</code></pre>

<p>And this is is my error message:</p>

<pre><code>Traceback (most recent call last):
  File ""clean.py"", line 114, in &lt;module&gt;
    run()
  File ""clean.py"", line 109, in run
    | 'WriteOutputText' &gt;&gt; beam.io.WriteToText(known_args.output))
  File ""C:\Python27\lib\site-packages\apache_beam\pipeline.py"", line 389, in __exit__
    self.run().wait_until_finish()
  File ""C:\Python27\lib\site-packages\apache_beam\runners\dataflow\dataflow_runner.py"", line 996, in wait_until_finish
    (self.state, getattr(self._runner, 'last_error_msg', None)), self)
apache_beam.runners.dataflow.dataflow_runner.DataflowRuntimeException: Dataflow pipeline failed. State: FAILED, Error:
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/dataflow_worker/batchworker.py"", line 733, in run
    self._load_main_session(self.local_staging_directory)
  File ""/usr/local/lib/python2.7/dist-packages/dataflow_worker/batchworker.py"", line 472, in _load_main_session
    pickler.load_session(session_file)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/internal/pickler.py"", line 247, in load_session
    return dill.load_session(file_path)
  File ""/usr/local/lib/python2.7/dist-packages/dill/dill.py"", line 363, in load_session
    module = unpickler.load()
  File ""/usr/lib/python2.7/pickle.py"", line 864, in load
    dispatch[key](self)
  File ""/usr/lib/python2.7/pickle.py"", line 1139, in load_reduce
    value = func(*args)
  File ""/usr/local/lib/python2.7/dist-packages/dill/dill.py"", line 766, in _import_module
    return __import__(import_name)
ImportError: No module named phonenumbers
</code></pre>

<p>Any help would be greatly appreciated, thanks!</p>

<p>EDIT : I have already installed phonenumbers with pip</p>

<pre><code>$ pip install phonenumbers
Requirement already satisfied: phonenumbers in c:\python27\lib\site-packages (8.9.7)
gax-google-logging-v2 0.8.3 has requirement google-gax&lt;0.13.0,&gt;=0.12.5, but you'll have google-gax 0.15.16 which is incompatible.
gcloud 0.18.3 has requirement google-gax&lt;0.13dev,&gt;=0.12.3, but you'll have google-gax 0.15.16 which is incompatible.
google-cloud-vision 0.29.0 has requirement requests&lt;3.0dev,&gt;=2.18.4, but you'll have requests 2.18.2 which is incompatible.
gax-google-pubsub-v1 0.8.3 has requirement google-gax&lt;0.13.0,&gt;=0.12.5, but you'll have google-gax 0.15.16 which is incompatible.
google-cloud-spanner 0.29.0 has requirement requests&lt;3.0dev,&gt;=2.18.4, but you'll have requests 2.18.2 which is incompatible.
</code></pre>

<p>This is the pip freeze output</p>

<pre><code>$ pip freeze
adal==1.0.1
apache-beam==2.4.0
asn1crypto==0.22.0
avro==1.8.2
azure==3.0.0
azure-batch==4.1.3
azure-common==1.1.12
azure-cosmosdb-nspkg==2.0.2
azure-cosmosdb-table==1.0.3
azure-datalake-store==0.0.22
azure-eventgrid==0.1.0
azure-graphrbac==0.40.0
azure-keyvault==0.3.7
azure-mgmt==2.0.0
azure-mgmt-advisor==1.0.1
azure-mgmt-applicationinsights==0.1.1
azure-mgmt-authorization==0.30.0
azure-mgmt-batch==5.0.1
azure-mgmt-batchai==0.2.0
azure-mgmt-billing==0.1.0
azure-mgmt-cdn==2.0.0
azure-mgmt-cognitiveservices==2.0.0
azure-mgmt-commerce==1.0.1
azure-mgmt-compute==3.0.1
azure-mgmt-consumption==2.0.0
azure-mgmt-containerinstance==0.3.1
azure-mgmt-containerregistry==1.0.1
azure-mgmt-containerservice==3.0.1
azure-mgmt-cosmosdb==0.3.1
azure-mgmt-datafactory==0.4.0
azure-mgmt-datalake-analytics==0.3.0
azure-mgmt-datalake-nspkg==2.0.0
azure-mgmt-datalake-store==0.3.0
azure-mgmt-devtestlabs==2.2.0
azure-mgmt-dns==1.2.0
azure-mgmt-eventgrid==0.4.0
azure-mgmt-eventhub==1.2.0
azure-mgmt-hanaonazure==0.1.1
azure-mgmt-iothub==0.4.0
azure-mgmt-iothubprovisioningservices==0.1.0
azure-mgmt-keyvault==0.40.0
azure-mgmt-loganalytics==0.1.0
azure-mgmt-logic==2.1.0
azure-mgmt-machinelearningcompute==0.4.1
azure-mgmt-managementpartner==0.1.0
azure-mgmt-marketplaceordering==0.1.0
azure-mgmt-media==0.2.0
azure-mgmt-monitor==0.4.0
azure-mgmt-msi==0.1.0
azure-mgmt-network==1.7.1
azure-mgmt-notificationhubs==1.0.0
azure-mgmt-nspkg==2.0.0
azure-mgmt-powerbiembedded==1.0.0
azure-mgmt-rdbms==0.1.0
azure-mgmt-recoveryservices==0.2.0
azure-mgmt-recoveryservicesbackup==0.1.1
azure-mgmt-redis==5.0.0
azure-mgmt-relay==0.1.0
azure-mgmt-reservations==0.1.0
azure-mgmt-resource==1.2.2
azure-mgmt-scheduler==1.1.3
azure-mgmt-search==1.0.0
azure-mgmt-servermanager==1.2.0
azure-mgmt-servicebus==0.4.0
azure-mgmt-servicefabric==0.1.0
azure-mgmt-sql==0.8.6
azure-mgmt-storage==1.5.0
azure-mgmt-subscription==0.1.0
azure-mgmt-trafficmanager==0.40.0
azure-mgmt-web==0.34.1
azure-nspkg==2.0.0
azure-servicebus==0.21.1
azure-servicefabric==6.1.2.9
azure-servicemanagement-legacy==0.20.6
azure-storage-blob==1.1.0
azure-storage-common==1.1.0
azure-storage-file==1.1.0
azure-storage-nspkg==3.0.0
azure-storage-queue==1.1.0
CacheControl==0.12.5
cachetools==2.1.0
certifi==2017.7.27.1
cffi==1.10.0
chardet==3.0.4
click==6.7
configparser==3.5.0
crcmod==1.7
cryptography==2.0.3
deprecation==2.0.3
dill==0.2.6
docopt==0.6.2
entrypoints==0.2.3
enum34==1.1.6
fasteners==0.14.1
firebase-admin==2.11.0
Flask==0.12.2
funcsigs==1.0.2
future==0.16.0
futures==3.2.0
gapic-google-cloud-datastore-v1==0.15.3
gapic-google-cloud-error-reporting-v1beta1==0.15.3
gapic-google-cloud-logging-v2==0.91.3
gapic-google-cloud-pubsub-v1==0.15.4
gax-google-logging-v2==0.8.3
gax-google-pubsub-v1==0.8.3
gcloud==0.18.3
google-api-core==0.1.4
google-apitools==0.5.20
google-auth==1.5.0
google-auth-httplib2==0.0.3
google-auth-oauthlib==0.2.0
google-cloud==0.33.1
google-cloud-bigquery==0.28.0
google-cloud-bigquery-datatransfer==0.1.1
google-cloud-bigtable==0.28.1
google-cloud-container==0.1.1
google-cloud-core==0.28.1
google-cloud-dataflow==2.4.0
google-cloud-datastore==1.4.0
google-cloud-dns==0.28.0
google-cloud-error-reporting==0.28.0
google-cloud-firestore==0.28.0
google-cloud-language==1.0.2
google-cloud-logging==1.4.0
google-cloud-monitoring==0.28.1
google-cloud-pubsub==0.30.1
google-cloud-resource-manager==0.28.1
google-cloud-runtimeconfig==0.28.1
google-cloud-spanner==0.29.0
google-cloud-speech==0.30.0
google-cloud-storage==1.6.0
google-cloud-trace==0.17.0
google-cloud-translate==1.3.1
google-cloud-videointelligence==1.0.1
google-cloud-vision==0.29.0
google-gax==0.15.16
google-resumable-media==0.3.1
googleapis-common-protos==1.5.3
googledatastore==7.0.1
grpc-google-iam-v1==0.11.4
grpc-google-logging-v2==0.8.1
grpc-google-pubsub-v1==0.8.1
grpcio==1.12.0
gunicorn==19.7.1
hdfs==2.1.0
httplib2==0.9.2
idna==2.5
ipaddress==1.0.18
iso8601==0.1.12
isodate==0.6.0
itsdangerous==0.24
Jinja2==2.9.6
jmespath==0.9.3
keyring==12.2.1
keystoneauth1==3.8.0
linecache2==1.0.0
MarkupSafe==1.0
mock==2.0.0
monotonic==1.5
msgpack==0.5.6
msrest==0.5.0
msrestazure==0.4.32
ndg-httpsclient==0.4.2
nelson==0.4.0
oauth2client==3.0.0
oauthlib==2.1.0
os-service-types==1.2.0
packaging==17.1
pathlib2==2.3.2
pbr==4.0.3
phonenumbers==8.9.7
ply==3.8
proto-google-cloud-datastore-v1==0.90.4
proto-google-cloud-error-reporting-v1beta1==0.15.3
proto-google-cloud-logging-v2==0.91.3
proto-google-cloud-pubsub-v1==0.15.4
protobuf==3.5.2.post1
psutil==5.4.6
psycopg2==2.7.3.2
pyasn1==0.4.3
pyasn1-modules==0.2.1
pycparser==2.18
pyjwt==1.5.0
pyOpenSSL==17.2.0
pyparsing==2.2.0
pyreadline==2.1
python-dateutil==2.7.3
pytz==2018.3
PyVCF==0.6.8
pywin32-ctypes==0.1.2
PyYAML==3.12
rackspaceauth==0.2.0
requests==2.18.2
requests-oauthlib==0.8.0
requests-toolbelt==0.8.0
rsa==3.4.2
scandir==1.7
six==1.10.0
SQLAlchemy==1.1.14
stevedore==1.28.0
traceback2==1.4.0
twilio==6.5.0
typing==3.6.4
unittest2==1.1.0
urllib3==1.22
virtualenv==16.0.0
Werkzeug==0.12.2
</code></pre>

<p>EDIT: Code ==</p>

<pre><code>from __future__ import absolute_import
from __future__ import print_function

import argparse
import csv
import logging
import os
from collections import OrderedDict
import phonenumbers

import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.options.pipeline_options import SetupOptions


class ParseCSVFn(beam.DoFn):
    """"""Parses the raw CSV data into a Python dictionary.""""""

    def process(self, elem):
        try:
            row = list(csv.reader([elem]))[0]
            month, day, year = row[2].split('/')
            birth_dict = {
                'day': day,
                'month': month,
                'year': year,
            }
            order_dict = OrderedDict(birth_dict)
            data_dict = {
                'phoneNumber': row[4],
                'firstName': row[0],
                'lastName': row[1],
                'birthDate': order_dict,
                'voterId': row[3],
            }

            order_data_dict = OrderedDict(data_dict)

            yield order_data_dict

        except:

            pass


def run(argv=None):
    """"""Pipeline entry point, runs the all the necessary processes""""""
    parser = argparse.ArgumentParser()
    parser.add_argument('--input',
                        type=str,
                        dest='input',
                        default='gs://wordcount_project/demo-contacts-small*.csv',
                        help='Input file to process.')
    parser.add_argument('--output',
                        dest='output',
                        # CHANGE 1/5: The Google Cloud Storage path is required
                        # for outputting the results.
                        default='gs://wordcount_project/cleaned.csv',
                        help='Output file to write results to.')

    known_args, pipeline_args = parser.parse_known_args(argv)
    pipeline_args.extend([
        # CHANGE 2/5: (OPTIONAL) Change this to DataflowRunner to
        # run your pipeline on the Google Cloud Dataflow Service.
        '--runner=DataflowRunner',
        # CHANGE 3/5: Your project ID is required in order to run your pipeline on
        # the Google Cloud Dataflow Service.
        '--project=--------',
        # CHANGE 4/5: Your Google Cloud Storage path is required for staging local
        # files.
        # '--dataset=game_dataset',
        '--staging_location=gs://wordcount_project/staging',
        # CHANGE 5/5: Your Google Cloud Storage path is required for temporary
        # files.
        '--temp_location=gs://wordcount_project/temp',
        '--job_name=cleaning-jobs',
    ])

    pipeline_options = PipelineOptions(pipeline_args)

    pipeline_options.view_as(SetupOptions).save_main_session = True

    with beam.Pipeline(options=pipeline_options) as p:
        (p
         | 'ReadInputText' &gt;&gt; beam.io.ReadFromText(known_args.input)
         | 'ParseDataFn' &gt;&gt; beam.ParDo(ParseCSVFn())
         # | 'JsonBirthDay' &gt;&gt; beam.ParDo(JsonBirthDay())
         # | 'MatchNumber' &gt;&gt; beam.ParDo(MatchNumber('phoneNumber'))
         # | 'MapData' &gt;&gt; beam.Map(lambda elem: (elem['phoneNumber'], elem['firstName'], elem['lastName'],
         #                                       elem['birthDate'], elem['voterId']))
         | 'WriteOutputText' &gt;&gt; beam.io.WriteToText(known_args.output))


if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    run()
</code></pre>

<p>I have also tried to install specific packages of google-gax and requests but it didn't seem to help</p>

<p>EDIT: new coding error:</p>

<pre><code>  File ""new_clean.py"", line 226, in &lt;module&gt;
    run()
  File ""new_clean.py"", line 219, in run
    | 'WriteToText' &gt;&gt; beam.io.WriteToText(known_args.output)
  File ""C:\Python27\lib\site-packages\apache_beam\pipeline.py"", line 389, in __exit__
    self.run().wait_until_finish()
  File ""C:\Python27\lib\site-packages\apache_beam\pipeline.py"", line 369, in run
    self.to_runner_api(), self.runner, self._options).run(False)
  File ""C:\Python27\lib\site-packages\apache_beam\pipeline.py"", line 382, in run
    return self.runner.run_pipeline(self)
  File ""C:\Python27\lib\site-packages\apache_beam\runners\dataflow\dataflow_runner.py"", line 324, in run_pipeline
    self.dataflow_client.create_job(self.job), self)
  File ""C:\Python27\lib\site-packages\apache_beam\utils\retry.py"", line 180, in wrapper
    return fun(*args, **kwargs)
  File ""C:\Python27\lib\site-packages\apache_beam\runners\dataflow\internal\apiclient.py"", line 461, in create_job
    self.create_job_description(job)
  File ""C:\Python27\lib\site-packages\apache_beam\runners\dataflow\internal\apiclient.py"", line 491, in create_job_description
    job.options, file_copy=self._gcs_file_copy)
  File ""C:\Python27\lib\site-packages\apache_beam\runners\dataflow\internal\dependency.py"", line 328, in stage_job_resources
    setup_options.requirements_file, requirements_cache_path)
  File ""C:\Python27\lib\site-packages\apache_beam\runners\dataflow\internal\dependency.py"", line 262, in _populate_requirements_cache
    processes.check_call(cmd_args)
  File ""C:\Python27\lib\site-packages\apache_beam\utils\processes.py"", line 44, in check_call
    return subprocess.check_call(*args, **kwargs)
  File ""C:\Python27\lib\subprocess.py"", line 186, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '['C:\\Python27\\python.exe', '-m', 'pip', 'download', '--dest', 'c:\\users\\james\\appdata\\local\\temp\\dataflow-requirements-cache', '-r', 'requirements.txt', '--no-binary', ':all:']' returned non-zero exit status 1
</code></pre>",50918556,1,0,,2018-06-18 22:21:22.367 UTC,,2018-06-20 19:56:47.653 UTC,2018-06-20 19:56:47.653 UTC,,9937191,,9937191,1,1,python|import|google-cloud-platform|google-cloud-dataflow|importerror,189
How to send a post request body as binary (equivalent to curl ... --data-binary) using the Node.js Request library?,35646392,How to send a post request body as binary (equivalent to curl ... --data-binary) using the Node.js Request library?,"<p>I'm attempting to work with Google Cloud Vision but cannot seem to send the post body as binary as required.</p>

<p>Edit:
The following is now working:</p>

<pre><code>request({
    url:""https://vision.googleapis.com/v1/images:annotate?key=&lt;api-key&gt;"",
    method:""post"",
    headers:{
        'content-type': 'application/json'
    },
    body: JSON.stringify(
          ""requests"":[{
              ""image"":{
                ""content"":&lt;base64 encoded image data&gt;
              },
              ""features"":[
                {
                  ""type"":""TEXT_DETECTION""
                }      
              ],
              ""imageContext"":{
                ""languageHints"": [
                  ""ar""
                ]
                }        
            }]  
        )
},function (error, response, body) {
    if (error) {
      res.send(error);
    }else{
      console.log(body);
      res.send(body);
    }
  }
);
</code></pre>",,0,4,,2016-02-26 07:58:40.593 UTC,,2016-02-26 08:32:23.907 UTC,2016-02-26 08:32:23.907 UTC,,5816588,,5816588,1,1,node.js|curl|binary|http-post,1020
Android cameraSource.stop() causing app to freeze,39641111,Android cameraSource.stop() causing app to freeze,"<p>I am building an app that has a qr scanner using the google vision api. I am having trouble stopping the camera after the qr code is read. the flow is <code>MainActivity -&gt; QrActivity</code>
once the qr-code received a detection the app should return to the main activity.</p>

<p>If i do not call <code>cameraSource.release()</code> it works fine but the device heats up a lot and has a significant impact on battery drain. however if i release the camera source the mainActivity becomes un-responsive and the app will crash. </p>

<p>Why is it becoming unresponsive? and where is the correct place to release the camera source?</p>

<p><strong>QrActivity</strong></p>

<pre><code>@Override
protected void onCreate(Bundle savedInstanceState) {
     super.onCreate(savedInstanceState);
    setContentView(R.layout.activity_qr);
    cancelBtn = (Button) findViewById(R.id.cancel_button);
    cancelBtn.setOnClickListener(new View.OnClickListener() {
        @Override
        public void onClick(View v) {
            onBackPressed();
        }
    });

    new QrReader(this);
}
</code></pre>

<p><strong>QrReader Class</strong></p>

<pre><code>public class QrReader {

    private static final String TAG = ""QrReader"";

    private SurfaceView cameraView;
    private TextView barcodeInfo;
    private BarcodeDetector barcodeDetector;
    private CameraSource cameraSource;
    private Activity mActivity;
    private AccessPointCredentials barCodeData;

    public QrReader(Activity activity) {
        this.mActivity = activity;

        cameraView = (SurfaceView) mActivity.findViewById(R.id.camera_view);
        barcodeInfo = (TextView) mActivity.findViewById(R.id.code_info);

        barcodeDetector =
                new BarcodeDetector.Builder(mActivity)
                        .setBarcodeFormats(Barcode.QR_CODE)
                        .build();

        cameraSource = new CameraSource
                .Builder(mActivity, barcodeDetector)
                .setAutoFocusEnabled(true)
                .build();

        cameraView.getHolder().addCallback(new SurfaceHolder.Callback() {

            @Override
            public void surfaceCreated(SurfaceHolder holder) {

                cameraSource = new CameraSource
                        .Builder(mActivity, barcodeDetector)
                        .setAutoFocusEnabled(true)
                        .setFacing(0)
                        .build();
                try {                  
                    cameraSource.start(cameraView.getHolder());

                } catch (Exception ioe) {
                    ioe.printStackTrace();
                }
            }

            @Override
            public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {
            }

            @Override
            public void surfaceDestroyed(SurfaceHolder holder) {
                  // Log.i(TAG, ""surfaceDestroyed: stopping camera Source"");

                  // cameraSource.release();
            }
        });

        barcodeDetector.setProcessor(new Detector.Processor&lt;Barcode&gt;() {
            @Override
            public void release() {
                Log.i(TAG, ""release: "");
            }

            @Override
            public void receiveDetections(Detector.Detections&lt;Barcode&gt; detections) {
                final SparseArray&lt;Barcode&gt; barCodes = detections.getDetectedItems();

                if (barCodes.size() != 0) {

                    Log.i(TAG, ""received a Barcode"");
                    barcodeInfo.post(new Runnable() {    // Use the post method of the TextView
                        public void run() {
                            barcodeInfo.setText(barCodes.valueAt(0).displayValue);

                        }
                    });
                    Gson g = new Gson();
                    try {
                        barCodeData = g.fromJson(barCodes.valueAt(0).rawValue, AccessPointCredentials.class);
                    } catch (Exception e) {
                        barCodeData = new AccessPointCredentials();
                        barCodeData.setSsid(barCodes.valueAt(0).rawValue);
                        barCodeData.setPass(null);
                        e.printStackTrace();
                    }

                    connectToWifi(barCodeData);

                    // CameraSource.release causes app to freeze

                    // cameraSource.release();
                }
            }
        });
    }

    private void connectToWifi(final AccessPointCredentials credentials) {

                //wificonnect code

    }

}
</code></pre>",,2,2,,2016-09-22 13:56:34.593 UTC,1,2017-03-02 10:41:49.743 UTC,2016-09-22 14:19:17.367 UTC,,1685748,,1685748,1,7,java|android|qr-code|google-vision,2509
Google Cloud Vision image matching in specific domain,45942150,Google Cloud Vision image matching in specific domain,"<p>Given a particular image, I'd like to be able to use Google Cloud Vision Web Detection to search for partial matches (<code>partialMatchingImages</code>) within a particular website, rather than the entire web, as is the default behavior. </p>

<p>I'm trying to get similar behavior as when you Search by Image in Google Images, upload an image, and type ""site:nytimes.com"" (for example) in the search bar. </p>

<p>Is this possible with the Google Cloud Vision API? </p>",48424521,1,0,,2017-08-29 14:46:49.873 UTC,,2018-03-18 15:08:31.413 UTC,,,,,2561339,1,2,image|google-cloud-platform|google-cloud-vision,220
Google Vision API. FaceDetector NoClassDefFoundError,44996852,Google Vision API. FaceDetector NoClassDefFoundError,"<p>I'm trying to integrate the Google Vision API for face detection.
But at creation of object FaceDetector, through FaceDetector.Builder the exception works and the application takes off.</p>

<p>The activity in which FaceDetector is created:</p>

<pre><code>public class LandmarkActivity extends AppCompatActivity{

@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.activity_landmark);
    InputStream inp = getResources().openRawResource(R.raw.face);
    Bitmap bmp = BitmapFactory.decodeStream(inp);
    Frame frame = new Frame.Builder().setBitmap(bmp).build();
    FaceDetector det = new FaceDetector.Builder(getApplicationContext()).build();
}}
</code></pre>

<p>AndroidManifest.xml:</p>

<pre><code>&lt;manifest xmlns:android=""http://schemas.android.com/apk/res/android""
package=""com.pobeda.ivan.opencvdetect""&gt;

&lt;uses-permission android:name=""android.permission.CAMERA"" /&gt;
&lt;uses-permission android:name=""android.permission.READ_EXTERNAL_STORAGE"" /&gt;
&lt;uses-permission android:name=""android.permission.WRITE_EXTERNAL_STORAGE"" /&gt;

&lt;uses-feature android:name=""android.hardware.camera"" /&gt;
&lt;uses-feature android:name=""android.hardware.camera.autofocus"" /&gt;

&lt;supports-screens
    android:anyDensity=""true""
    android:largeScreens=""true""
    android:normalScreens=""true""
    android:resizeable=""true""
    android:smallScreens=""true"" /&gt;

&lt;uses-permission android:name=""android.permission.INTERNET"" /&gt;

&lt;application
    android:allowBackup=""true""
    android:icon=""@mipmap/logo""
    android:label=""@string/app_name""
    android:supportsRtl=""true""
    android:theme=""@style/AppTheme""&gt;
    &lt;meta-data
        android:name=""com.google.android.gms.vision.DEPENDENCIES""
        android:value=""face"" /&gt;

    &lt;activity
        android:name="".Settings""
        android:label=""@string/title_activity_settings""
        android:parentActivityName="".MainActivity"" /&gt;
    &lt;activity
        android:name="".MainActivity""
        android:screenOrientation=""landscape""&gt;
        &lt;intent-filter&gt;
            &lt;action
                android:name=""android.intent.action.MAIN""
                android:theme=""@android:style/Theme.NoTitleBar.Fullscreen"" /&gt;

            &lt;category android:name=""android.intent.category.LAUNCHER"" /&gt;
        &lt;/intent-filter&gt;
    &lt;/activity&gt;
    &lt;activity android:name="".LandmarkActivity""&gt;&lt;/activity&gt;
&lt;/application&gt;
</code></pre>

<p></p>

<p>dependencies:</p>

<pre><code>dependencies {
compile fileTree(include: ['*.jar'], dir: 'libs')
androidTestCompile('com.android.support.test.espresso:espresso-core:2.2.2', {
    exclude group: 'com.android.support', module: 'support-annotations'
})
compile project(':openCVLibrary320')
compile 'com.google.android.gms:play-services-vision:9.4.0'
compile 'com.android.support:appcompat-v7:24.2.1'
compile 'com.google.android.gms:play-services-ads:11.0.1'
compile 'com.android.support:design:24.2.1'
compile 'com.android.support:support-v4:24.2.1'
compile 'com.android.support:support-vector-drawable:24.2.1'
compile 'com.android.support:recyclerview-v7:25.3.1'
compile 'com.android.support:cardview-v7:25.3.1'
compile 'junit:junit:4.12'
compile 'com.android.support.constraint:constraint-layout:1.0.2'
testCompile 'junit:junit:4.12'}
</code></pre>

<p>Full Error Log:</p>

<pre><code>07-09 15:42:12.073 5284-5284/com.pobeda.ivan.opencvdetect E/AndroidRuntime: FATAL EXCEPTION: main
                                                                        Process: com.pobeda.ivan.opencvdetect, PID: 5284
                                                                        java.lang.NoClassDefFoundError: com.google.android.gms.vision.face.internal.client.FaceSettingsParcel
                                                                            at com.google.android.gms.vision.face.FaceDetector$Builder.build(Unknown Source)
                                                                            at com.pobeda.ivan.opencvdetect.LandmarkActivity.onCreate(LandmarkActivity.java:25)
                                                                            at android.app.Activity.performCreate(Activity.java:6285)
                                                                            at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1108)
                                                                            at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2369)
                                                                            at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2476)
                                                                            at android.app.ActivityThread.access$900(ActivityThread.java:150)
                                                                            at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1344)
                                                                            at android.os.Handler.dispatchMessage(Handler.java:102)
                                                                            at android.os.Looper.loop(Looper.java:148)
                                                                            at android.app.ActivityThread.main(ActivityThread.java:5417)
                                                                            at java.lang.reflect.Method.invoke(Native Method)
                                                                            at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:764)
                                                                            at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:626)
</code></pre>

<p>p.s: When creating a FaceDetector object in an empty application, there were no errors or exceptions.</p>",,0,2,,2017-07-09 13:19:33.017 UTC,,2017-07-10 14:19:00.457 UTC,2017-07-10 14:19:00.457 UTC,,1000551,,8279191,1,1,android|computer-vision|google-vision,322
understanding crop hints by google vision ai,56052040,understanding crop hints by google vision ai,"<p>i am trying google vision api for crop hints and the output results are like below</p>

<p>i am having hard time understanding these crop hints to be able to use on my image. so first thing is the empty vertices. what are they? also i was hoping all the pairs will have a x and y value to draw on 2D space. but some has only x and some has only y. </p>

<p>finally how should i get my final image? i am using firebase cloud function which is a nodejs with typescript to operate on my original image to get final image?</p>

<p>ideally i want this to happen on device but it seems there are no cordova plugins yet to run autoML on device to capture and crop the image as soon as prominent object is detected.  </p>

<p>is there any other cordova plugin that can help to capture the promiment image auto capture as soon as it is visible in camera? </p>

<pre><code>{
  ""cropHintsAnnotation"": {
    ""cropHints"": [
      {
        ""boundingPoly"": {
          ""vertices"": [
            {},
            {
              ""x"": 2499
            },
            {
              ""x"": 2499,
              ""y"": 3132
            },
            {
              ""y"": 3132
            }
          ]
        },
        ""confidence"": 0.79999995,
        ""importanceFraction"": 0.98999995
      },
      {
        ""boundingPoly"": {
          ""vertices"": [
            {},
            {
              ""x"": 2499
            },
            {
              ""x"": 2499,
              ""y"": 2532
            },
            {
              ""y"": 2532
            }
          ]
        },
        ""confidence"": 0.79999995,
        ""importanceFraction"": 0.91999996
      },
      {
        ""boundingPoly"": {
          ""vertices"": [
            {},
            {
              ""x"": 2499
            },
            {
              ""x"": 2499,
              ""y"": 2098
            },
            {
              ""y"": 2098
            }
          ]
        },
        ""confidence"": 0.79999995,
        ""importanceFraction"": 0.81
      }
    ]
  }               
}
```


</code></pre>",,1,0,,2019-05-09 04:07:25.680 UTC,,2019-05-28 05:45:49.073 UTC,,,,,10758175,1,0,google-vision|automl|vision-api,14
how can i make aws rekogniton work with images programmatic uploaded from raspberry pi,55122015,how can i make aws rekogniton work with images programmatic uploaded from raspberry pi,"<p>I am trying to use aws rekognition to compare faces but it will give an error saying check if the object and bucket exist in same region </p>

<p>while uploading the image i have set the content type to image/jpeg format</p>

<p>but when i upload an image using aws console from computer the rekognition will work ! am i doing something wrong in this code</p>

<pre><code>import boto3

BUCKET = ""dacsup”
BUCKET_T = “targettt”
KEY_SOURCE = ""0001249950.jpg""
KEY_TARGET = ""0001249950.jpg""

Aws_accesss_key=”my key”
Aws_secreat_access_key”=”my secreat key”

def compare_faces(bucket, key, bucket_target, key_target, threshold=80, region=""us-east-2""):
rekognition = boto3.client(""rekognition"", region, Aws_accesss_key, Aws_secreat_access_key)
    response = rekognition.compare_faces(
        SourceImage={
            ""S3Object"": {
                ""Bucket"": bucket,
                ""Name"": key,
            }
        },
        TargetImage={
            ""S3Object"": {
                ""Bucket"": bucket_target,
                ""Name"": key_target,
            }
        },
        SimilarityThreshold=threshold,
    )
    return response['SourceImageFace'], response['FaceMatches']


source_face, matches = compare_faces(BUCKET, KEY_SOURCE, BUCKET_T, KEY_TARGET)

# the main source face
print ""Source Face ({Confidence}%)"".format(**source_face)

# one match for each target face
for match in matches:
    print ""Target Face ({Confidence}%)"".format(**match['Face'])
    print ""  Similarity : {}%"".format(match['Similarity'])
</code></pre>",,0,0,,2019-03-12 12:54:48.777 UTC,,2019-03-12 12:54:48.777 UTC,,,,,10599727,1,0,python|amazon-web-services|amazon-s3|boto3|amazon-rekognition,10
Tensorflow object detection api - Microsoft Custom Vision,56032143,Tensorflow object detection api - Microsoft Custom Vision,"<p>I created an object detection model on Azure Microsoft custom vision for satellite images( ship Dataset) ,then I exported a model by docker file and I get a zip file inside it 2 docker files (app and Azureml) inside docker file (app) there are 5 files  ( <code>app.py</code>, <code>labels.pbtxt</code>, <code>model.pb</code>, <code>object_detection.py</code>, <code>predict.py</code>). Then I used to program uses a TensorFlow-trained classifier to perform object detection, it loads the classifier uses it to perform object detection on a video and  It draws boxes and scores around the objects of interest in each frame of the video. So, I implement this program on collab and I gave it a path of my model(.pb) that I exported and labels(.txt) that I converted to (.pbtxt) with one item class <code>Ship</code>, and path of video that I want to do on it a detection and everything works well until I arrived to :</p>

<pre><code># Define input and output tensors (i.e. data) for the object detection classifier

# Input tensor is the image
image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')

# Output tensors are the detection boxes, scores, and classes
# Each box represents a part of the image where a particular object was detected
detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')

# Each score represents level of confidence for each of the objects.
# The score is shown on the result image, together with the class label.
detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')

# Number of objects detected
num_detections = detection_graph.get_tensor_by_name('num_detections:0')
</code></pre>

<p>I tried to open a (<code>predict.py</code>, <code>object_detection.py</code> and <code>app.py</code>) to see what is an equivalent name of each of these: <code>image_tensor:0,detection_boxes:0,.......</code>. I just find the equal of <code>image_tensor:0 --&gt; model_output:0</code>.
I didn't find the other names, please someone can help me to finish this and arrive to detect ships from the video.</p>

<p>This is all code:</p>

<pre><code># Import packages

import os
import cv2
import numpy as np
import tensorflow as tf
import sys

# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("".."")

# Import utilites
from utils import label_map_util
from utils import visualization_utils as vis_util

# Name of the directory containing the object detection module we're using
MODEL_NAME = 'inference_graph'
VIDEO_NAME = 'test.mov'

# Grab path to current working directory
CWD_PATH = os.getcwd()

# Path to frozen detection graph .pb file, which contains the model that is used
# for object detection.
PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')

# Path to label map file
PATH_TO_LABELS = os.path.join(CWD_PATH,'training','labelmap.pbtxt')

# Path to video
PATH_TO_VIDEO = os.path.join(CWD_PATH,VIDEO_NAME)

# Number of classes the object detector can identify
NUM_CLASSES = 6

# Load the label map.
# Label maps map indices to category names, so that when our convolution
# network predicts `5`, we know that this corresponds to `king`.
# Here we use internal utility functions, but anything that returns a
# dictionary mapping integers to appropriate string labels would be fine
label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)

# Load the Tensorflow model into memory.
detection_graph = tf.Graph()
with detection_graph.as_default():
    od_graph_def = tf.GraphDef()
    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
        serialized_graph = fid.read()
        od_graph_def.ParseFromString(serialized_graph)
        tf.import_graph_def(od_graph_def, name='')

    sess = tf.Session(graph=detection_graph)

# Define input and output tensors (i.e. data) for the object detection classifier

# Input tensor is the image
image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')

# Output tensors are the detection boxes, scores, and classes
# Each box represents a part of the image where a particular object was detected
detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')

# Each score represents level of confidence for each of the objects.
# The score is shown on the result image, together with the class label.
detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')

# Number of objects detected
num_detections = detection_graph.get_tensor_by_name('num_detections:0')

# Open video file
video = cv2.VideoCapture(PATH_TO_VIDEO)

while(video.isOpened()):

    # Acquire frame and expand frame dimensions to have shape: [1, None, None, 3]
    # i.e. a single-column array, where each item in the column has the pixel RGB value
    ret, frame = video.read()
    frame_expanded = np.expand_dims(frame, axis=0)

    # Perform the actual detection by running the model with the image as input
    (boxes, scores, classes, num) = sess.run(
        [detection_boxes, detection_scores, detection_classes, num_detections],
        feed_dict={image_tensor: frame_expanded})

    # Draw the results of the detection (aka 'visulaize the results')
    vis_util.visualize_boxes_and_labels_on_image_array(
        frame,
        np.squeeze(boxes),
        np.squeeze(classes).astype(np.int32),
        np.squeeze(scores),
        category_index,
        use_normalized_coordinates=True,
        line_thickness=8,
        min_score_thresh=0.80)

    # All the results have been drawn on the frame, so it's time to display it.
    cv2.imshow('Object detector', frame)

    # Press 'q' to quit
    if cv2.waitKey(1) == ord('q'):
        break

# Clean up
video.release()
cv2.destroyAllWindows()
</code></pre>",,0,4,,2019-05-08 00:51:17.243 UTC,,2019-05-08 05:04:39.970 UTC,2019-05-08 05:04:39.970 UTC,,6700019,,10405707,1,1,python|azure|tensorflow|object-detection-api|microsoft-custom-vision,47
Google Vision API: OCR of non-language text,43978170,Google Vision API: OCR of non-language text,"<p>I'd like to write android application for text recognition in images from camera. However, text to be recognized does not consist of regular words, it's just a sequence of letters, digits, slashes, etc. Is it still possible to use Google Vision API for this task? Or should I look for some other tools? I have read about tessaract library, but Google Vision seems to be easier for beginner developer. Would it give significant boost in accuracy of recognition?</p>",,1,1,,2017-05-15 11:21:24.297 UTC,,2018-04-03 02:06:24.827 UTC,,,,,5036656,1,0,android|ocr|google-vision|android-vision,488
oauth2client.client.HttpAccessTokenRefreshError: invalid_grant: Invalid JWT,49510330,oauth2client.client.HttpAccessTokenRefreshError: invalid_grant: Invalid JWT,"<p>I am using Google Cloud Vision API on my Raspberry PI. It works fine when I use it on my home (on which the cloud account was first accessed) network but if I access the API from a different network it raises a token refresh error. I have synchronized the time using NTP but is of no help.</p>

<p>Detailed error:</p>

<blockquote>
  <p>oauth2client.client.HttpAccessTokenRefreshError: invalid_grant: Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values and use a clock with skew to account for clock differences between systems.</p>
</blockquote>",49510656,1,0,,2018-03-27 10:21:08.463 UTC,,2018-03-27 10:36:21.520 UTC,2018-03-27 10:30:48.790 UTC,,1841839,,9557546,1,2,google-api|raspberry-pi|google-oauth|google-cloud-vision,443
primeng p-confirmDialog change layout on display,52768829,primeng p-confirmDialog change layout on display,"<p>I'm using primeng on angular6 but when the pdialog appears, the layout is changed and the layout is reduced, here is my p-dialog implementation</p>

<pre><code>&lt;p-confirmDialog header=""Confirmación Eliminar"" key=""confEliminarBici"" [baseZIndex]=""999"" [autoZIndex]=""false"" icon=""pi pi-exclamation-triangle""
    width=""425"" #cd &gt;
    &lt;p-footer&gt;
        &lt;button type=""button"" pButton icon=""pi pi-check"" label=""Aceptar"" (click)=""cd.accept()""&gt;&lt;/button&gt;
        &lt;button type=""button"" pButton icon=""pi pi-times"" label=""Cancelar"" (click)=""cd.reject()""&gt;&lt;/button&gt;
    &lt;/p-footer&gt;
&lt;/p-confirmDialog&gt;
</code></pre>

<p>here is my full implementation</p>

<pre><code>&lt;div class=""ui-g""&gt;
    &lt;!-- column --&gt;
    &lt;div class=""ui-g-12""&gt;
        &lt;p-panel header=""Bicicletas"" [style]=""{'margin-top':'20px'}""&gt;
            &lt;form #form=""ngForm"" (ngSubmit)=""form.valid &amp;&amp; guardar()""&gt;
                &lt;div class=""ui-g""&gt;
                    &lt;div class=""ui-g-12"" *ngIf=""!editar""&gt;
                        &lt;p-dataView #dv [value]=""bicicletas"" [paginator]=""true"" layout=""grid"" [rows]=""5""
                            paginatorPosition=""both"" filterBy=""codigoBien"" [sortField]=""sortField"" [sortOrder]=""sortOrder""&gt;
                            &lt;p-header&gt;
                                &lt;div class=""ui-helper-clearfix""&gt;
                                    &lt;div class=""ui-g""&gt;

                                        &lt;div class=""ui-g-6 ui-md-6 filter-container""&gt;
                                            &lt;div style=""position:relative""&gt;
                                                &lt;input type=""search"" pInputText placeholder=""Buscar por codigo de bien""
                                                    (keyup)=""dv.filter($event.target.value)""&gt;
                                            &lt;/div&gt;
                                        &lt;/div&gt;
                                        &lt;div class=""ui-g-6 ui-md-6"" style=""text-align:right""&gt;
                                            &lt;p-dataViewLayoutOptions&gt;&lt;/p-dataViewLayoutOptions&gt;
                                        &lt;/div&gt;
                                    &lt;/div&gt;
                                &lt;/div&gt;
                            &lt;/p-header&gt;

                            &lt;ng-template let-bicicleta pTemplate=""listItem""&gt;
                                &lt;div class=""ui-g"" style=""padding: 2em;border-bottom: 1px solid #d9d9d9""&gt;
                                    &lt;div class=""ui-g-12 ui-md-3"" style=""text-align:center""&gt;
                                        &lt;img src=""https://s3-us-west-2.amazonaws.com/krfacerekognition/{{bicicleta.foto}}""
                                            height=""200"" width=""200""&gt;
                                    &lt;/div&gt;
                                    &lt;div class=""ui-g-12 ui-md-8 car-details""&gt;
                                        &lt;div class=""ui-g""&gt;
                                            &lt;div class=""ui-g-2 ui-sm-6""&gt;Id: &lt;/div&gt;
                                            &lt;div class=""ui-g-10 ui-sm-6""&gt;&lt;b&gt;{{bicicleta.foto}}&lt;/b&gt;&lt;/div&gt;

                                            &lt;div class=""ui-g-2 ui-sm-6""&gt;Código: &lt;/div&gt;
                                            &lt;div class=""ui-g-10 ui-sm-6""&gt;&lt;b&gt;{{bicicleta.codigoBien}}&lt;/b&gt;&lt;/div&gt;

                                            &lt;div class=""ui-g-2 ui-sm-6""&gt;Descripción: &lt;/div&gt;
                                            &lt;div class=""ui-g-10 ui-sm-6""&gt;&lt;b&gt;{{bicicleta.descripcion}}&lt;/b&gt;&lt;/div&gt;

                                            &lt;div class=""ui-g-2 ui-sm-6""&gt;Estado: &lt;/div&gt;
                                            &lt;div class=""ui-g-10 ui-sm-6""&gt;&lt;b&gt;{{bicicleta.estado}}&lt;/b&gt;&lt;/div&gt;

                                            &lt;div class=""ui-g-2 ui-sm-6""&gt;Candado: &lt;/div&gt;
                                            &lt;div class=""ui-g-10 ui-sm-6""&gt;&lt;b&gt;{{bicicleta.candado?.descripcion}}&lt;/b&gt;&lt;/div&gt;
                                        &lt;/div&gt;
                                    &lt;/div&gt;
                                    &lt;div class=""ui-g-12 ui-md-1 search-icon""&gt;
                                        &lt;button pButton type=""button"" icon=""pi pi-search"" (click)=""ver(bicicleta)""
                                            class=""ui-button-success""&gt;&lt;/button&gt;
                                        &lt;button pButton type=""button"" icon=""fa fa-pencil-square-o"" (click)=""editarBicicleta(bicicleta)""&gt;&lt;/button&gt;
                                        &lt;button pButton type=""button"" icon=""fa fa-times"" class=""ui-button-danger""
                                            (click)=""confirmarEliminar(bicicleta)""&gt;&lt;/button&gt;

                                    &lt;/div&gt;
                                &lt;/div&gt;
                            &lt;/ng-template&gt;
                            &lt;ng-template let-bicicleta pTemplate=""gridItem""&gt;
                                &lt;div style=""padding:.5em"" class=""ui-g-12 ui-md-3""&gt;
                                    &lt;p-panel [header]=""bicicleta.codigoBien"" [style]=""{'text-align':'center'}""&gt;
                                        &lt;img src=""https://s3-us-west-2.amazonaws.com/krfacerekognition/{{bicicleta.foto}}""
                                            width=""60""&gt;
                                        &lt;div class=""car-detail""&gt;{{bicicleta.descripcion}} -
                                            {{bicicleta.disponibilidad}}&lt;/div&gt;
                                        &lt;hr class=""ui-widget-content"" style=""border-top:0""&gt;
                                        &lt;button pButton type=""button"" icon=""pi pi-search"" class=""ui-button-success""
                                            (click)=""ver(bicicleta)"" style=""margin-top:0""&gt;&lt;/button&gt;
                                        &lt;button pButton type=""button"" icon=""fa fa-pencil-square-o"" (click)=""editarBicicleta(bicicleta)""&gt;&lt;/button&gt;
                                        &lt;button pButton type=""button"" icon=""fa fa-times"" class=""ui-button-danger""
                                            (click)=""confirmarEliminar(bicicleta)""&gt;&lt;/button&gt;

                                    &lt;/p-panel&gt;
                                &lt;/div&gt;
                            &lt;/ng-template&gt;
                        &lt;/p-dataView&gt;
                    &lt;/div&gt;
                    &lt;div class=""ui-g-12"" *ngIf=""editar""&gt;


                        &lt;div class=""ui-grid ui-grid-responsive ui-grid-pad ui-fluid"" style=""margin: 10px 0px"" *ngIf=""bicicletaSelecionado!=null""&gt;
                            &lt;div class=""ui-grid-row""&gt;
                                &lt;div class=""ui-grid-col-2""&gt;
                                    {{cols[0].header}} *:
                                &lt;/div&gt;
                                &lt;div class=""ui-grid-col-6""&gt;
                                    &lt;input pInputText type=""text"" name=""codigoBien"" [(ngModel)]=""bicicletaSelecionado.codigoBien""
                                        required /&gt;
                                &lt;/div&gt;
                                &lt;div class=""ui-grid-col-4""&gt;
                                    &lt;p-message severity=""error"" text=""Codigo de Bien is requerido"" *ngIf=""form.submitted  &amp;&amp; !form.form.controls.codigoBien?.valid""&gt;&lt;/p-message&gt;
                                &lt;/div&gt;
                            &lt;/div&gt;
                            &lt;div class=""ui-grid-row""&gt;
                                &lt;div class=""ui-grid-col-2""&gt;
                                    {{cols[1].header}} *:
                                &lt;/div&gt;
                                &lt;div class=""ui-grid-col-6""&gt;
                                    &lt;p-dropdown [options]=""estados"" [(ngModel)]=""bicicletaSelecionado.estado"" name=""estado""&gt;&lt;/p-dropdown&gt;
                                &lt;/div&gt;
                                &lt;div class=""ui-grid-col-4""&gt;

                                &lt;/div&gt;
                            &lt;/div&gt;
                            &lt;div class=""ui-grid-row""&gt;
                                &lt;div class=""ui-grid-col-2""&gt;
                                    {{cols[2].header}} *:
                                &lt;/div&gt;
                                &lt;div class=""ui-grid-col-6""&gt;
                                    &lt;input pInputText type=""text"" name=""descripcion"" [(ngModel)]=""bicicletaSelecionado.descripcion""
                                        required /&gt;
                                &lt;/div&gt;
                                &lt;div class=""ui-grid-col-4""&gt;
                                    &lt;p-message severity=""error"" text=""Descripción de Bien is requerido"" *ngIf=""form.submitted  &amp;&amp; !form.form.controls.descripcion?.valid""&gt;&lt;/p-message&gt;
                                &lt;/div&gt;
                            &lt;/div&gt;

                            &lt;div class=""ui-grid-row""&gt;
                                &lt;div class=""ui-grid-col-2""&gt;
                                    Fotografia *:
                                &lt;/div&gt;
                                &lt;div class=""ui-grid-col-6""&gt;
                                    {{bicicletaSelecionado.foto}}
                                    &lt;input name=""file"" type=""file"" id=""adjuntar"" accept=""image/*"" class=""ui-g-12 ui-md-12 ui-lg-12 inputUpload""
                                        (change)=""fileChangeEvent($event)"" placeholder=""Subir Imagen..."" /&gt;

                                &lt;/div&gt;
                                &lt;div class=""ui-grid-col-4""&gt;
                                    &lt;p-message severity=""error"" text=""El campo Fotrográfia es requerido"" *ngIf=""form.submitted  &amp;&amp; bicicletaSelecionado.id == null &amp;&amp; documento == null""&gt;&lt;/p-message&gt;
                                &lt;/div&gt;
                            &lt;/div&gt;
                            &lt;div class=""ui-grid-row""&gt;
                                &lt;div class=""ui-grid-col-2""&gt;
                                    Candado:
                                &lt;/div&gt;
                                &lt;div class=""ui-grid-col-6""&gt;
                                    &lt;p-dropdown [options]=""candadosItem"" [(ngModel)]=""bicicletaSelecionado.candadoId""
                                        name=""candado""&gt;&lt;/p-dropdown&gt;

                                &lt;/div&gt;
                                &lt;div class=""ui-grid-col-4""&gt;

                                &lt;/div&gt;
                            &lt;/div&gt;




                        &lt;/div&gt;

                    &lt;/div&gt;
                    &lt;div class=""ui-g-12"" style=""text-align: right""&gt;

                        &lt;button pButton *ngIf=""!editar"" type=""button"" icon=""pi pi-plus"" label=""Nuevo"" (click)=""form.onReset();nuevo()""
                            class=""ui-button-info"" style=""margin-right: .25em""&gt;&lt;/button&gt;
                        &lt;button pButton *ngIf=""editar"" type=""submit"" icon=""pi pi-save"" label=""Guardar"" class=""ui-button-info""
                            style=""margin-right: .25em""&gt;&lt;/button&gt;
                        &lt;button pButton *ngIf=""editar"" type=""button"" icon=""pi pi-times"" label=""Cancelar"" (click)=""editar=false""
                            class=""ui-button-info"" style=""margin-right: .25em""&gt;&lt;/button&gt;

                    &lt;/div&gt;

                &lt;/div&gt;

            &lt;/form&gt;
        &lt;/p-panel&gt;

    &lt;/div&gt;

    &lt;!-- column --&gt;                                                                                 
&lt;/div&gt;


&lt;p-confirmDialog header=""Confirmación Eliminar"" key=""confEliminarBici"" [baseZIndex]=""999"" [autoZIndex]=""false"" icon=""pi pi-exclamation-triangle""
    width=""425"" #cd &gt;
    &lt;p-footer&gt;
        &lt;button type=""button"" pButton icon=""pi pi-check"" label=""Aceptar"" (click)=""cd.accept()""&gt;&lt;/button&gt;
        &lt;button type=""button"" pButton icon=""pi pi-times"" label=""Cancelar"" (click)=""cd.reject()""&gt;&lt;/button&gt;
    &lt;/p-footer&gt;
&lt;/p-confirmDialog&gt;



&lt;p-dialog header=""Detalle Bicicleta"" [(visible)]=""displayDialog"" [responsive]=""true"" showEffect=""fade"" width=""750"" &gt;
    &lt;div class=""ui-g"" *ngIf=""bicicletaSelecionado"" style=""padding: 2em;border-bottom: 1px solid #d9d9d9""&gt;
        &lt;!-- &lt;div class=""ui-g-12"" style=""text-align:center""&gt;
            &lt;img src=""assets/showcase/images/demo/car/{{selectedCar.brand}}.png""&gt;
        &lt;/div&gt; --&gt;

        &lt;div class=""ui-g-6 ui-md-6"" style=""text-align:center""&gt;
            &lt;img src=""https://s3-us-west-2.amazonaws.com/krfacerekognition/{{bicicletaSelecionado.foto}}"" height=""200""
                width=""200""&gt;
        &lt;/div&gt;
        &lt;div class=""ui-g-6 ui-md-6 car-details""&gt;
            &lt;div class=""ui-g""&gt;
                &lt;div class=""ui-g-6 ui-sm-6""&gt;Id: &lt;/div&gt;
                &lt;div class=""ui-g-6 ui-sm-6""&gt;&lt;b&gt;{{bicicletaSelecionado.foto}}&lt;/b&gt;&lt;/div&gt;

                &lt;div class=""ui-g-6 ui-sm-6""&gt;Código: &lt;/div&gt;
                &lt;div class=""ui-g-6 ui-sm-6""&gt;&lt;b&gt;{{bicicletaSelecionado.codigoBien}}&lt;/b&gt;&lt;/div&gt;

                &lt;div class=""ui-g-6 ui-sm-6""&gt;Descripción: &lt;/div&gt;
                &lt;div class=""ui-g-6 ui-sm-6""&gt;&lt;b&gt;{{bicicletaSelecionado.descripcion}}&lt;/b&gt;&lt;/div&gt;

                &lt;div class=""ui-g-6 ui-sm-6""&gt;Estado: &lt;/div&gt;
                &lt;div class=""ui-g-6 ui-sm-6""&gt;&lt;b&gt;{{bicicletaSelecionado.estado}}&lt;/b&gt;&lt;/div&gt;

                &lt;div class=""ui-g-2 ui-sm-6""&gt;Candado: &lt;/div&gt;
                &lt;div class=""ui-g-10 ui-sm-6""&gt;&lt;b&gt;{{bicicletaSelecionado.candado?.descripcion}}&lt;/b&gt;&lt;/div&gt;
            &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/p-dialog&gt;
</code></pre>

<p>displaying as the following:</p>

<p><a href=""https://i.stack.imgur.com/0D4cq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0D4cq.png"" alt=""Layout""></a></p>",,1,1,,2018-10-11 20:58:11.510 UTC,,2018-10-16 23:58:07.513 UTC,2018-10-16 23:58:07.513 UTC,,2366535,,2366535,1,0,angular6|primeng|primeng-dialog,312
Google Cloud Vision (Chrome) Results Limit to 5,45341208,Google Cloud Vision (Chrome) Results Limit to 5,"<p>I noticed that the Google Chrome Extension Cloud Vision returns labels or web detection with a limit of 5 labels/URLs. See the background.js file in
<a href=""https://github.com/GoogleCloudPlatform/machine-learning-browser-extension/tree/master/chrome"" rel=""nofollow noreferrer"">Google Cloud Vision Chrome Extension</a></p>

<p>How could I increase the limit?</p>

<p>I have tried to add <code>max_result: 100</code> or <code>limit:100</code> and it did not work.</p>

<pre><code>  var detect = function(type, b64data, cb) {
  var url = 'https://vision.googleapis.com/v1/images:annotate?key='+API_KEY;
  var data = {
    requests: [{
      image: {content: b64data},
      features: [{'type':type}],
      max_result: 100
    }]
  }
  http('POST', url, JSON.stringify(data), cb);
};
</code></pre>",45342096,1,0,,2017-07-27 04:05:03.007 UTC,,2017-07-27 05:31:03.720 UTC,,,,,7873127,1,1,javascript|google-chrome-extension|google-cloud-vision,293
Cloud Vision API - PDF OCR,36728347,Cloud Vision API - PDF OCR,"<p>I just tested the Google Cloud Vision API to read the text, if exist, in a image.</p>

<p>Until now I installed the Maven Server and the Redis Server. I just follow the instructions in this page.</p>

<p><a href=""https://github.com/GoogleCloudPlatform/cloud-vision/tree/master/java/text"" rel=""noreferrer"">https://github.com/GoogleCloudPlatform/cloud-vision/tree/master/java/text</a></p>

<p>Until now I was able to tested with .jpg files, is it possible to do it with tiff files or pdf?? </p>

<p>I am using the following command:</p>

<pre><code>java -cp target/text-1.0-SNAPSHOT-jar-with-dependencies.jar     com.google.cloud.vision.samples.text.TextApp ../../data/text/
</code></pre>

<p>Inside the text directory, I have the files in jpg format.</p>

<p>Then to read the converted file, I don't know how to do that, just I run the following command </p>

<pre><code>java -cp target/text-1.0-SNAPSHOT-jar-with-dependencies.jar com.google.cloud.vision.samples.text.TextApp
</code></pre>

<p>And I get the message to enter a word or phrase to search in the converted files. Is there a way to see the whole document transformed?</p>

<p>Thanks!</p>",44001550,3,3,,2016-04-19 20:03:30.070 UTC,4,2019-01-29 14:58:19.253 UTC,,,,,4633661,1,9,google-cloud-vision,8843
Using Fluture with AWS Service,56351778,Using Fluture with AWS Service,"<p>I'm using a fluture to handle the response from an AWS service request.</p>

<p>I get the expected response using a callback or a Promise wrapped around the callback. When I try to use a fluture, looks like I am getting back a regurgitation of the request. Gotta be something stoopid... (again)</p>

<pre><code>const Rekognition = require ('aws-sdk/clients/rekognition');
const rekognition = new Rekognition ({
    region: 'us-east-1'
});

const fs = require ('fs');
const Future = require ('fluture');


const imageBytes = fs.readFileSync ('../data/image.jpg');


const params = {
    Image: {
        Bytes: imageBytes
    }
};

const detectText = Future((rej, res) =&gt;
    rekognition.detectText(params, (err, data) =&gt; err ? rej(err) : res(data)));

detectText.fork(console.error, console.log);

</code></pre>

<p>Expected results:
{ TextDetections:
   [ { DetectedText: 'text1',
       Type: 'LINE',
       Id: 0,
       Confidence: 98.7948989868164,
       Geometry: [Object] },
     { DetectedText: 'text2',...</p>

<p>Actual results:
c5GeDWkmkn3ZpFJK/UszSxBOCN2AR7Gs0uqtHlSDuGHX+EnuakC43xxqN6ABWY/e+lRiOaNrg+UWKqGAHfii0bXZv...</p>",,0,0,,2019-05-29 00:58:17.273 UTC,,2019-05-29 00:58:17.273 UTC,,,,,5659116,1,0,javascript|node.js|amazon-web-services|fluture,35
AWS Rekognition throws exception Unable to get image metadata from S3,53102455,AWS Rekognition throws exception Unable to get image metadata from S3,"<p>I try to use AWS recognition with Java SDK.
I have the code below but it throws exception:</p>

<pre><code>    private static final EnvironmentVariableCredentialsProvider CREDENTIALS_PROVIDER = new EnvironmentVariableCredentialsProvider();

    public List&lt;TextDetection&gt; recognize(String bucket, String name) {
        var amazonRekognition = createAmazonRekognition();
        var detectTextRequest = new DetectTextRequest()
                .withImage(getImage(bucket, name));
        var detectTextResult = amazonRekognition.detectText(detectTextRequest);
        return detectTextResult.getTextDetections();
    }

    private AmazonRekognition createAmazonRekognition() {
        return AmazonRekognitionClientBuilder.standard()
                .withCredentials(CREDENTIALS_PROVIDER)
                .build();
    }

    private Image getImage(String bucket, String name) {
        return new Image()
                .withS3Object(new S3Object()
                        .withName(name)
                        .withBucket(bucket));
    }
</code></pre>

<p>I've create a new IAM with permissions <em>AmazonS3FullAccess</em> and <em>AmazonRekognitionFullAccess</em>.</p>

<p>When I run the execute method it throws:</p>

<pre><code>Unable to get image metadata from S3. Check object key, region and/or access permissions.
</code></pre>",53102456,2,0,,2018-11-01 13:38:43.783 UTC,,2019-04-02 10:52:55.620 UTC,,,,,5651359,1,1,java|amazon-web-services|amazon-rekognition,91
Passing data PHP(client-side) to C#(server-side),26010360,Passing data PHP(client-side) to C#(server-side),"<p>I want to develop a web based face recognition API. The system that will process face recognition is in c# application(opencv). My problem is how to pass data from php to c#? I already tested it by using fleck websocket, it could use the webcam of website(client) and send the image byte via websocket to c# opencv application(server) and return the processed output again to the website. see <a href=""http://www.smartjava.org/content/face-detection-using-html5-javascript-webrtc-websockets-jetty-and-javacvopencv"" rel=""nofollow"">http://www.smartjava.org/content/face-detection-using-html5-javascript-webrtc-websockets-jetty-and-javacvopencv</a> for similar result. However I am looking for an alternative aside from websocket, because I want to make my own API like rekognition and I don't know where to start.</p>

<p>Hoping for help :)</p>",,1,3,,2014-09-24 06:49:58.117 UTC,,2014-09-24 06:52:10.027 UTC,,,,,4073337,1,0,javascript|c#|php|html5|opencv,134
aws lambda function handler python,49868678,aws lambda function handler python,"<p>I am new to AWS, im trying to write a function whenever there is a new object created in s3 bucket, rekognition will start analysis. I looked at AWS documentation for lambda function handler(python), it gives a general syntax structure for handling, but what operators should I use to call the name of new object in s3 bucket? I hardly find any, can anyone please help? thank you so much</p>

<pre><code>import boto3

client = boto3.client('rekognition')
s3 = boto3.resource('s3')
bucket = s3.bucket('my_bucket')
for obj in bucket.object.all():
    print(obj.key)

def my_handler(event,context):
    income_Name = event.***** # not sure what operator here?


    response = client.search_faces_by_image(
        CollectionId='my_collection',
        Image={
           #'Bytes': b'bytes',
           'S3Object': {
              'Bucket': 'my_bucket',
              'Name': income_name,

        },
        MaxFaces=123,
        FaceMatchThreshold=70
    )
    return response[]
</code></pre>",49869889,1,2,,2018-04-17 01:53:52.770 UTC,,2018-04-17 04:43:39.003 UTC,,,,,7028560,1,0,python|amazon-web-services|handler,482
AWS Connection error to Endpoint URL for Rekognition,51133277,AWS Connection error to Endpoint URL for Rekognition,"<p>I have just started with AWS Rekognition and I have run into a problem that I can't seem to solve.</p>

<p>I am using the Python script supplied on <a href=""https://docs.aws.amazon.com/rekognition/latest/dg/text-detecting-text-procedure.html"" rel=""nofollow noreferrer"">Detecting Text in an Image - Amazon Rekognition</a> to test how the service works and how I could possibly integrate it into other apps.</p>

<p>I know that I have entered the correct data for the config and credential files, found here: </p>

<pre><code>~/.aws/credentials
</code></pre>

<p>as other services such as S3 work without a problem using command line code. In the supplied code (I will include it at the end) I have specified the correct bucket as well as the name of the picture I'm trying to use.
When running the script, on terminal everything works fine until after a few seconds the following error message is displayed: </p>

<pre><code>botocore.exceptions.EndpointConnectionError: Could not connect to the endpoint 
 URL: ""https://rekognition.eu-west-2.amazonaws.com/""
</code></pre>

<p>I have also tried several other availability zones such as: </p>

<pre><code>us-east-1, us-west-1, eu-central-1
</code></pre>

<p>yet they all result in the same error. </p>

<p>A similar issue has already been discussed in <a href=""https://stackoverflow.com/questions/38533897/botocore-exceptions-endpointconnectionerror-could-not-connect-to-the-endpoint"">python - botocore.exceptions.EndpointConnectionError: Could not connect to the endpoint - Stack Overflow</a>. However, the solution offered in that discussion did not solve the problem I am encountering. I would appreciate any help and tips that can solve this issue. </p>

<pre><code>import boto3

bucket='bucket'
photo='inputtext.jpg'

client=boto3.client('rekognition')

response=client.detect_text(Image={'S3Object':{'Bucket':bucket,'Name':photo}})

textDetections=response['TextDetections']
print response
print 'Matching faces'
for text in textDetections:
        print 'Detected text:' + text['DetectedText']
        print 'Confidence: ' + ""{:.2f}"".format(text['Confidence']) + ""%""
        print 'Id: {}'.format(text['Id'])
        if 'ParentId' in text:
            print 'Parent Id: {}'.format(text['ParentId'])
        print 'Type:' + text['Type']
        print
</code></pre>",51135450,1,0,,2018-07-02 09:45:51.053 UTC,,2018-07-02 11:45:01.723 UTC,2018-07-02 11:38:19.443 UTC,,174777,,10020490,1,1,python|macos|amazon-web-services|amazon-rekognition,424
To search images in S3 folder by keywords,50536717,To search images in S3 folder by keywords,"<p>What is the best and simple way to search and get images list from S3 by using English keywords. Or do I have to use the Rekognition to store all the image metadatas into database?</p>

<p>My development is using Php.</p>",,2,1,,2018-05-25 20:54:25.540 UTC,,2018-05-26 12:06:35.697 UTC,,,,,157620,1,0,php|amazon-s3|amazon-rekognition,53
Error on CompareFacesRequest as part of AWS's Rekognize on Android,49380672,Error on CompareFacesRequest as part of AWS's Rekognize on Android,"<p>I apologize if my question is not worded properly, I don't post here very often.</p>

<p>I am having difficulty with launching my request to match 2 faces in 2 different images using Amazon Web Services on my Android application. My code is provided below:</p>

<pre><code>    CognitoCachingCredentialsProvider credentialsProvider = new CognitoCachingCredentialsProvider(
                getApplicationContext(), // Context
                ""xxxxxxx"", // Identity Pool ID
                Regions.US_EAST_1 // Region
        );

    AmazonRekognitionClient client = new AmazonRekognitionClient(credentialsProvider);

    ByteBuffer buffer = ByteBuffer.allocate(croppedBitmap.getByteCount()); //Create a new buffer
        croppedBitmap.copyPixelsToBuffer(buffer); //Move the byte data to the buffer

try {
    //get first image from phone
    File dhruv = new File(""/sdcard/temp/dhruv.jpg"");
    InputStream inputStream = new 
    FileInputStream(dhruv.getAbsolutePath().toString());

    //convert to ByteBuffer
    ByteBuffer byteBuffer = 
    ByteBuffer.wrap(IOUtils.toByteArray(inputStream));
    Log.d(""lol"", Arrays.toString(byteBuffer.array()));

    Image image = new Image();
    Image image2 = new Image();
    image.withBytes(buffer);
    image2.withBytes(byteBuffer);

    CompareFacesRequest compare = new CompareFacesRequest();
    compare.withSourceImage(image);
    compare.withTargetImage(image2);

    CompareFacesResult result = client.compareFaces(compare);
    result.getFaceMatches();
} catch(...) {} // catched the error
</code></pre>

<p>The error I am getting is this:</p>

<pre><code>3247/com.busradeniz.detection D/lol: 1 validation error detected: Value 
'java.nio.HeapByteBuffer[pos=0 lim=0 cap=0]' at 'sourceImage.bytes' failed 
to satisfy constraint: Member must have length greater than or equal to 1 
(Service: AmazonRekognition; Status Code: 400; Error Code: 
ValidationException; Request ID: a0489079-2c17-11e8-b8b8-23c9eaea153d)
</code></pre>

<p>What is happening in my code is I am converting an image taken from my file path on the android (which is confirmed to be correct) and converting it to a ByteBuffer so that I can pass it Image object created by AWS using withBytes. I did the same for another ByteBuffer, except I converted a BitMap to a ByteBuffer instead (this is not shown in the code). Through debugging I have logged and found that both ByteBuffers are non-empty and fully functional. I have also already tried passing the images in the CompareFacesRequest constructor instead of using the withSource and withTarget image methods. I have also tried calling getBytes on both the Image objects to see if the ByteBuffers really did pass through. </p>

<p>The error hints at me that I am passing 2 empty Image objects with the request, hence it says that there has to be one or more bytes to pass in the Image objects. But I am not sure this is the case. I can't for the life of me figure out why this is happening, it seems to work everywhere else. I would really really appreciate if someone could help me and determine an answer??</p>

<p>Thanks so much,
Dhruv  </p>",,1,0,,2018-03-20 09:36:28.773 UTC,,2018-03-26 19:15:54.517 UTC,2018-03-26 19:15:54.517 UTC,,9282303,,4207989,1,0,java|android|amazon-web-services|amazon-s3|knox-amazon-s3-client,115
Determine if a handwritten signature is present in a Bitmap on Android,55514176,Determine if a handwritten signature is present in a Bitmap on Android,"<p>I have an app where I am trying to determine if there is a handwritten text in a bitmap (a signature). This must not be done using a cloud solution, only locally (I'm using Google Vision, locally, to also scan a QR and detect an ""end of document"" through OCR at the same time, and it works decently well. The resulting bitmap needs to be checked if the person hand-signed the paper in the bitmap.</p>

<p>My solution looks like this (after I have scanned the QR code and did the OCR, and obtained the resulting bitmap):</p>

<pre><code>public void createPaletteAsync(Bitmap bitmap) {
        Palette.from(bitmap).maximumColorCount(40).generate(p -&gt; {
            // Use generated instance
            List&lt;Palette.Swatch&gt; swatches = p.getSwatches();
            for (Palette.Swatch swatch : swatches) {
                float[] hslForCurrentSwatch = swatch.getHsl();
                Log.i(TAG, ""HSL value is: "" + Float.toString(hslForCurrentSwatch[0])
                        + ""with a pixel population of: "" + swatch.getPopulation());
                if (Math.round(hslForCurrentSwatch[0]) &gt;= 210 &amp;&amp; Math.round(hslForCurrentSwatch[0]) &lt;= 240) {
                    ((TextView) findViewById(R.id.semnat)).setText(""Signed document"");
                    return;
                }
            }
            ((TextView) findViewById(R.id.semnat)).setText(""Unsigned document"");
        });
    }
</code></pre>

<p>So basically, I check if there are any swatches in the Bitmap that contain blue hues from 210 to 240 and if there are, I consider the document as ""signed"". Obviously, this is problematic, as it only works for documents signed with a blue pen, and it requires the Bitmap only to contain the signed paper, with no ""blue objects"" in the photo.</p>

<p>Can you imagine any other way in which, locally (without any cloud-based solution), one could determine if the document is signed or not?</p>",,0,0,,2019-04-04 10:55:30.437 UTC,,2019-04-04 10:55:30.437 UTC,,,,,5627584,1,0,android|bitmap|ocr,20
Research publications related to google vision API face detection,39735249,Research publications related to google vision API face detection,<p>It is great to get face detection using Google vision API for Android but there is no where I can find how they did that and how it is compared to other state of the art face detection techniques like Deep-learning etc. Did they publish their research somewhere?</p>,,0,0,,2016-09-27 22:24:01.163 UTC,,2016-09-27 22:24:01.163 UTC,,,,,2752869,1,1,face-detection|vision,57
Scan QR code without using google play services,42871797,Scan QR code without using google play services,"<p>I have an android app which on local WiFi and doesn't require an internet connection so I am looking for a solution which can scan QR codes without requiring Google Play Services as no internet connection is there so I do not want to update Google Play Services. Currently, I am doing it by using Google Vision API but somehow(if possible) I want to remove this dependency.</p>",43013047,1,0,,2017-03-18 07:40:19.220 UTC,,2018-01-27 12:31:35.570 UTC,2018-01-27 12:31:35.570 UTC,,3210285,,4546990,1,1,android|qr-code|barcode,1668
Google vision predict return empty response,55892455,Google vision predict return empty response,"<p>I use Google vision API to predict image labels and i dont understand why my response is empty, but cannot be sure how the response should be.</p>

<pre><code>    $predictionServiceClient = new PredictionServiceClient();

    try {
        $formattedName = $predictionServiceClient-&gt;modelName($this-&gt;config['project_id'], $this-&gt;config['region_name'], $this-&gt;config['dataset']);
        $payload       = new ExamplePayload([
            'image' =&gt; new Image([
                'image_bytes' =&gt; file_get_contents(""https://www.example.com/image.jpg"")
            ])
        ]);

        $response = $predictionServiceClient-&gt;predict($formattedName, $payload);
    } 
    finally {
        $predictionServiceClient-&gt;close();
    }
</code></pre>

<p>The region_name is <code>us-central1</code>.</p>

<p>It seems like all fine, there is no error in the predict proccess, but the response i get is:</p>

<pre><code>RepeatedField {
  -container: []
  -type: 11
  -klass: ""Google\Cloud\AutoMl\V1beta1\AnnotationPayload""
  -legacy_klass: ""Google\Cloud\AutoMl\V1beta1\AnnotationPayload""
}
</code></pre>

<p>I not understand why the container is empty, what should be in the container, and how my results should be shown.</p>

<p>Anyone know how to get the real results of the predict proccess?</p>

<p>Thanks.</p>",,0,0,,2019-04-28 16:44:02.777 UTC,,2019-04-28 16:44:02.777 UTC,,,,,3460080,1,0,php|api|predict|google-vision,15
Recognizing faces in a streaming video AWS - How to connect live video into Kinesis video stream?,53756545,Recognizing faces in a streaming video AWS - How to connect live video into Kinesis video stream?,"<p>I am new to AWS.. Now I am ok with Recognizing faces in static images. I would like to Recognize faces in a streaming video.. I refer this link<a href=""https://docs.aws.amazon.com/rekognition/latest/dg/recognize-faces-in-a-video-stream.html"" rel=""nofollow noreferrer"">enter link description here</a> But I don't understand how to connect the live video into kinesis video stream. Anyone can help me to understand the processor .</p>",,1,0,,2018-12-13 06:59:56.070 UTC,,2019-03-07 20:15:19.460 UTC,,,,,9265440,1,0,amazon-web-services|aws-sdk|facial-identification,35
Does Google Cloud Vision OCR API have better accuracy and performance than Tesseract OCR API,45559285,Does Google Cloud Vision OCR API have better accuracy and performance than Tesseract OCR API,"<p>I have integrated Google Cloud Vision API in my java application for text recognition from complex formatted documents. One of my colleague suggested to use ""Tesseract API"".Can anyone please give difference between these two API's.And which is better in terms of accuracy or have any advantage over other.TIA</p>",,1,1,,2017-08-08 04:26:01.947 UTC,2,2017-08-09 18:25:43.803 UTC,,,,,5238800,1,5,tesseract|google-cloud-vision,9850
"Attempting to embed AWS Lex chatbot into React site, failing with Error: Missing credentials in config (see console for details)",54740969,"Attempting to embed AWS Lex chatbot into React site, failing with Error: Missing credentials in config (see console for details)","<p>I am comfortable with many aspects of AWS, but not with Cognito. I have built Lex style chatbots as Alexa skills, but this is my first time attemtping to integrate a Lex chatbot into an existing website. The website is coded in React.js using www.codesandbox.io as I'm just learning React. I'm comfortable with C#, but React is new for me. If some of the code below doesn't seem to do much, it's just because I""m testing different components, functions versus classes etc. Anyway, I've built a simple chatbot in Lex and have attempted to integrate it into this React site, but am getting the error - Error: Missing credentials in config (see console for details) whenever I attempt to chat with the bot. Any ideas? I'm not sure what I""m doing wrong. </p>

<pre><code>import React from ""react"";
import ReactDOM from ""react-dom"";
import bootstrap from ""bootstrap""; // eslint-disable-line no-unused-vars
import ""../node_modules/bootstrap/dist/css/bootstrap.min.css"";
import ""./styles.css"";
import LexChat from ""react-lex"";
import AWS from 'aws-sdk';

AWS.config.update({
  region: ""us-west-2"",
  credentials: new AWS.CognitoIdentityCredentials({
    IdentityPoolId: ""us-west-2:&lt;redacted&gt;""
  })
});

class CDataRow extends React.Component {
  render() {
    return (
      &lt;h1&gt;
        Name:{this.props.myname}
        &lt;br /&gt;
        Email:{this.props.email}
      &lt;/h1&gt;
    );
  }
}

const TitleCard = props =&gt; (
  &lt;div class=""container""&gt;
    &lt;div class=""media""&gt;
      &lt;span class=""media-left""&gt;
        &lt;img
          src=""https://content.mactores.com/2017/05/19105931/Amazon-Rekognition.png""
          alt=""...""
        /&gt;
      &lt;/span&gt;
      &lt;div class=""media-body""&gt;
        &lt;h3 class=""media-heading""&gt;
          {props.titleheading} &lt;h5&gt;{props.subtitle}&lt;/h5&gt;
        &lt;/h3&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
);

const Datarow = props =&gt; (
  &lt;div&gt;
    &lt;b&gt;Name:{props.myname}&lt;/b&gt;
    &lt;br /&gt;
    &lt;b&gt;Email:{props.email}&lt;/b&gt;
  &lt;/div&gt;
);
const Layout = props =&gt; (
  &lt;div&gt;
    &lt;header&gt;Super Header of Document&lt;/header&gt;
    &lt;main&gt;{props.children}&lt;/main&gt;
    &lt;footer&gt;Super Footer Copyright mystuff 2018 (C)&lt;/footer&gt;
  &lt;/div&gt;
);
const DomainOutside = props =&gt; (
  &lt;div&gt;
    &lt;div class=""panel panel-default""&gt;
      &lt;div class=""panel-body""&gt;{props.children}&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
);
const TabListRow = props =&gt; (
  &lt;div class=""container-fluid""&gt;
    &lt;div class=""row""&gt;
      &lt;div class=""col-md-2""&gt;
        &lt;div class=""card""&gt;
          &lt;h5 class=""card-header""&gt;{props.cardtitle}&lt;/h5&gt;
          &lt;div class=""card-body""&gt;
            &lt;p class=""card-text""&gt;{props.cardcontent}&lt;/p&gt;
          &lt;/div&gt;
          &lt;div class=""card-footer""&gt;{props.cardfooter}&lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=""col-md-10""&gt;
        &lt;h2&gt;{props.blockheader}&lt;/h2&gt;
        &lt;p&gt;{props.blockcontent}&lt;/p&gt;
        &lt;p&gt;
          &lt;a class=""btn"" href=""#""&gt;
            View details »
          &lt;/a&gt;
        &lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
);

function App(props) {
  console.log(&lt;CDataRow myname=""JoJo"" email=""jojo@ops.org"" /&gt;);
  return (
    &lt;div&gt;
      &lt;TitleCard
        titleheading=""{Lex Integration Test}""
        subtitle=""Embedding Lex into a website""
      /&gt;
      &lt;LexChat
        botName=""TestReactBot""
        IdentityPoolId=""us-west-2:&lt;redacted&gt;""
        placeholder=""Placeholder text""
        style={{ position: ""absolute"" }}
        backgroundColor=""#FFFFFF""
        height=""430px""
        headerText=""Store help""
      /&gt;
    &lt;/div&gt;
  );
}

const rootElement = document.getElementById(""root"");
ReactDOM.render(&lt;App version=""1"" /&gt;, rootElement);
</code></pre>",,0,1,,2019-02-18 05:32:11.873 UTC,,2019-02-18 05:32:11.873 UTC,,,,,8390889,1,1,reactjs|embed|credentials|aws-lex,40
"Google vision unable to convert ""image path"" to an image",49589030,"Google vision unable to convert ""image path"" to an image","<p>Google vision is throwing me the following error on ruby on rails which had me baffled.</p>

<p><strong>Unable to convert ""image_path"" to an Image</strong> </p>

<p>However, I am able to display each image form it's respective path if I use the image_tag method for rails. Please advise as I am new to this, thank you.</p>

<pre><code>&lt;% @uploads.each do |u| %&gt;
&lt;% require ""google/cloud/vision""
   vision = Google::Cloud::Vision.new(
   project: ""first-ocr-project"",
   keyfile: JSON.parse(ENV['GOOGLECLOUD_API_KEY'])
   )

   #Error Unable to convert ""image_path"" to an Image
   raw_data = vision.image(u.photos[0])
%&gt;

#Able to displays all photos from path
&lt;%= image_tag u.photos[0] %&gt;&lt;/br&gt;

&lt;% end %&gt;
</code></pre>",,1,2,,2018-03-31 14:52:30.787 UTC,,2018-06-27 08:03:13.373 UTC,,,,,9578992,1,0,ruby-on-rails|ruby|google-cloud-vision|google-vision,68
Microsoft Face API [find similar] api key error,49890108,Microsoft Face API [find similar] api key error,"<p>So I'm trying to follow The microsoft face api documentation <a href=""https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395237"" rel=""nofollow noreferrer"">here</a> for the ""FindSimilar"" feature. There is an example at the bottom of the page where I use this code:</p>

<pre><code>########### Python 3.2 #############
import http.client, urllib.request, urllib.parse, urllib.error, base64

headers = {
# Request headers
'Content-Type': 'application/json',
'Ocp-Apim-Subscription-Key': '{api key}',
}

params = urllib.parse.urlencode({
})

try:
    conn = http.client.HTTPSConnection('westus.api.cognitive.microsoft.com')
    conn.request(""POST"", ""/face/v1.0/findsimilars?%s"" % params, ""{body}"", 
headers)
    response = conn.getresponse()
    data = response.read()
    print(data)
    conn.close()
except Exception as e:
    print(""[Errno {0}] {1}"".format(e.errno, e.strerror))
</code></pre>

<p>I'm getting an error where it tells me my subscription key is invalid, but I checked my azure account status and I see no issues:</p>

<pre><code>b'\n\t\t\t\t\t{""error"":{""code"":""Unspecified"",""message"":""Access denied due to invalid subscription key. Make sure you are subscribed to an API you are trying to call and provide the right key.""}}\n                \t\t'
</code></pre>",,1,2,,2018-04-18 02:13:20.643 UTC,,2018-04-18 07:16:27.787 UTC,,,,,6318255,1,0,python-3.x|azure|azure-blob-storage|face-api,136
Firebase Storage Trigger Not Working,48901888,Firebase Storage Trigger Not Working,"<p>I created a custom bucket which I use to serve content at a specific url. In Firebase I created a bucket called ""images.mydomain.com"" and I wanted to use this so my users can simply go to <a href=""http://images.mydomain.com/imagename.jpg"" rel=""nofollow noreferrer"">http://images.mydomain.com/imagename.jpg</a> to access uploaded images.</p>

<p>I wanted to have a firebase trigger on my new bucket that does Google vision analysis on the image. Here is my function:</p>

<pre><code>export const imageUploaded = functions.storage
.bucket('images.mydomain.com').object().onChange( event =&gt; {
    //Cool stuff happens here
});
</code></pre>

<p>However, when I try to deploy the function I get this error in the console</p>

<blockquote>
  <p>⚠  functions[imageUploaded]: Deployment error.</p>
  
  <p>Failed to configure trigger GCS Bucket: imagebucket</p>
</blockquote>

<p>When I go to the function logs in Firebase I see the exact same error</p>

<blockquote>
  <p>Failed to configure trigger GCS Bucket: imagebucket</p>
</blockquote>

<p>I also went to the Google Cloud Console to look at the functions to hopefully get some more insight on the issue. When I looked at the function, it had the correct bucket configured for the function but it had that exact same error without any explanation.</p>

<p>I have tried to create a new bucket using that but I get the exact same error. It seems that no matter what I do I keep getting the same error. I've tried adding permission and every It doesn't make any sense.</p>

<p>Does anyone know what this is about and/or how I can get this working? I had this working on the main bucket, but I really need this to work for my new bucket.</p>

<p>Thanks</p>

<p>A quick update:</p>

<p>I tried to deploy to my production environment (my app isn't in market yet so no one is actually using it) and it worked perfectly, the trouble is, I really need my development environment working which isn't right now.</p>",,1,1,,2018-02-21 09:08:12.770 UTC,1,2018-02-21 20:04:06.233 UTC,2018-02-21 10:06:18.517 UTC,,3681937,,3681937,1,0,firebase|google-cloud-functions|firebase-storage,687
Watson: Error message - excessive number of https connections to this service,41397369,Watson: Error message - excessive number of https connections to this service,"<p>I'm working with the Watson Visual Recognition service using Cygwin and curl. I am submitting a zip of images to create a new class in an existing classifier, however I am getting this response: </p>

<blockquote>
  <p>""If you are seeing this message, you are likely making an excessive number of concurrent HTTP connections to this service. Please check the concurrency limits for your assigned service tier.""</p>
</blockquote>

<p>I have added a card to my account so I am now on the pay-as-you-go tier.  Other questions like this on Stack Overflow have suggested an internal service error.  I have checked Watson's status which shows no problems.</p>

<p>The other thing I should mention is that I am not an experienced command line user, - I made some code mistakes causing cygwin to do hang, so I closed the Cygwin windows on these occasions, without explicitly closing the https connection, - might this be contributing? How can I do this?</p>

<hr>

<p>This question is similar to the one I am asking, however the difference is that I have checked the service status which appears to be ok: <a href=""https://stackoverflow.com/questions/39900391/ibm-watson-visual-recognition-error-excessive-number-of-concurrent-http-conne?rq=1"">IBM Watson VIsual Recognition - ERROR: excessive number of concurrent HTTP connections</a></p>",,0,3,,2016-12-30 13:50:44.703 UTC,,2017-06-28 13:27:24.110 UTC,2017-05-23 11:46:43.070 UTC,,-1,,4005671,1,1,ibm-cloud|ibm-watson|visual-recognition,147
How do I set the timeout when using the DetectText method of the Google Vision API? I need a sample source,55466156,How do I set the timeout when using the DetectText method of the Google Vision API? I need a sample source,"<p>I want to run OCR using the DetectText method of the Google Vision API. I want to prepare for the situation that the OCR program that I develop is disconnected in the middle of running. So I want to generate an error if there is no response within 2 seconds after calling the DetectText method. (Default is 10 minutes, set to 600000 milisecond). Thank you for your help. In the sample source will be even more helpful.</p>

<p>Thank you.</p>

<pre><code>using Google.Cloud.Vision.V1;          

var image = Google.Cloud.Vision.V1.Image.FromFile(sFilename);
var client = ImageAnnotatorClient.Create();
var response = client.DetectText(image);
</code></pre>",,1,0,,2019-04-02 02:36:58.800 UTC,,2019-04-03 20:15:33.103 UTC,2019-04-03 17:15:27.520 UTC,,11296429,,11296429,1,0,c#|google-vision,26
Laravel 5 Error installing wapnen/google-cloud-vision-php,52126381,Laravel 5 Error installing wapnen/google-cloud-vision-php,"<p>I'm using Laravel 5.3, I want to install wapnen/google-cloud-vision-php, when i add composer require wapnen/google-cloud-vision-php, it come out with the error   Could not find package wapnen/google-cloud-vision-php at any version for your minimum-stability (stable). Check the package spelling or your minimum-stability. What do that mean?</p>",,1,2,,2018-09-01 08:29:23.910 UTC,,2018-09-01 08:47:33.937 UTC,,,,,7159831,1,-2,php|laravel-5,124
Google authentication using Google Clous Vision API in R,54904596,Google authentication using Google Clous Vision API in R,"<p>At the moment I am trying to use Image Recognition using Google Cloud Vision API in R. It works until the authorization, but this is the error I got:</p>

<pre><code>&gt; c &lt;- fromJSON(file.choose())
&gt; options(""googleAuthR.client_id"" = c$installed$client_id)
&gt; options(""googleAuthR.client_secret"" = c$installed$client_secret)
&gt; options(""googleAuthR.scopes.selected"" = 
+           c(""https://www.googleapis.com/auth/cloud-platform""))
&gt; googleAuthR::gar_auth()



  Waiting for authentication in browser...
    Press Esc/Ctrl + C to abort
    Authentication complete.
    Error in oauth2.0_access_token(endpoint, app, code = code, user_params = user_params,  : 
      Unauthorized (HTTP 401). Failed to get an access token.

&gt; 
&gt;Label Detection
&gt; 
&gt; 
&gt; p &lt;- getGoogleVisionResponse(file.choose(), 
+                              feature = ""LABEL_DETECTION"")

2019-02-27 12:24:43&gt; No authorization yet in this session!
2019-02-27 12:24:43&gt; NOTE: a  .httr-oauth  file exists in current working directory.
 Run authentication function to use the credentials cached for this session.
Error: Invalid token
</code></pre>

<p>Does someone knows what possibly goes wrong? In the cloud console I activated Google Vision API. </p>",,0,0,,2019-02-27 11:37:15.510 UTC,,2019-02-27 11:37:15.510 UTC,,,,,11124760,1,0,r|image-recognition|google-cloud-vision|google-vision,26
IBM Watson Visual Recognition Service in Bluemix always returning empty content,31741189,IBM Watson Visual Recognition Service in Bluemix always returning empty content,"<p>I'm trying to test the IBM Watson Visual Recognition Service in Bluemix using the API tester.</p>

<p>1st I want to get the list of valid labels:</p>

<ol>
<li>I open the API tester: <a href=""http://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/apis/#!/visual-recognition/getLabelService"" rel=""nofollow"">http://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/apis/#!/visual-recognition/getLabelService</a></li>
<li>I issue an empty string</li>
<li>Response Body: no content, Response Code: 0</li>
</ol>

<p>While reading the source code of the demo app I was inferring the labels, e.g. ""Animal""</p>

<ol>
<li>I open this link: <a href=""http://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/apis/#!/visual-recognition/recognizeLabelsService"" rel=""nofollow"">http://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/apis/#!/visual-recognition/recognizeLabelsService</a></li>
<li>I upload an images and set label to ""Animal""</li>
<li>Response Body: no content, Response Code: 0</li>
</ol>

<p>Any idea what I'm doing wrong?</p>

<p>The demo app seems to work quite well, at least it recognizes an image of Obama as ""person, president, obama"" :)</p>",,1,0,,2015-07-31 08:07:29.367 UTC,,2016-01-07 04:45:29.460 UTC,2016-01-07 04:45:29.460 UTC,,456564,,3656912,1,3,ibm-cloud|ibm-watson|visual-recognition,607
How to upload an image at url for Google Vision API - Ruby,41126381,How to upload an image at url for Google Vision API - Ruby,"<h2>How do I upload an S3 URL image properly for Google Vision?</h2>

<p>I am attempting to send an image (saved at an AWS S3 URL) to Google Vision with Base64 encoding per the 2nd option in the <a href=""https://cloud.google.com/vision/docs/best-practices#image_data"" rel=""nofollow noreferrer"">documentation</a> listed below: </p>

<blockquote>
  <p><strong>Images sent to the Google Cloud Vision API can be supplied in two ways:</strong></p>
  
  <ol>
  <li><p>Using Google Cloud Storage URIs of the form gs://bucketname/path/to/image_filename</p></li>
  <li><p>As image data sent within the JSON request. Because image data must be supplied as ASCII text, all image data should be escaped using base64 encoding.</p></li>
  </ol>
</blockquote>

<p>I am using the <a href=""https://github.com/GoogleCloudPlatform/google-cloud-ruby/blob/master/google-cloud-vision/README.md"" rel=""nofollow noreferrer"">Google-Cloud-Vision Gem</a>.</p>

<p>I have tried <a href=""https://stackoverflow.com/a/1547631/2628223"">this previous answer about Base64 encoding</a>, with a minor modification:</p>

<pre><code>require 'google/cloud/vision'
require 'base64'
require 'googleauth'
require 'open-uri'

encoded_image = Base64.strict_encode64(open(image_url, &amp;:read))

@vision = Google::Cloud::Vision.new
image = @vision.image(encoded_image)
annotation = @vision.annotate(image, labels: true, text: true)
</code></pre>

<p>I have tried images at AWS URLs and images at other urls.</p>

<p><strong>Every time I get this error from the Google-Cloud-Vision gem: 
<code>ArgumentError: Unable to convert (my_base_64_encoded_image) to an image</code></strong></p>

<h3>Update - successfully encoded and decoded image in ruby only</h3>

<p>I have confirmed that this code: <code>encoded_image = Base64.strict_encode64(open(image_url, &amp;:read))</code> works via the following:</p>

<pre><code># Using a random image from the interwebs
image_url = ""https://storage.googleapis.com/gweb-uniblog-publish-prod/static/blog/images/google-200x200.7714256da16f.png""
encoded_image = Base64.strict_encode64(open(image_url, &amp;:read))

### now try to decode the encoded_image
File.open(""my-new-image.jpg"", ""wb"") do |file|
  file.write(Base64.strict_decode64(encoded_image))
end
### great success
</code></pre>

<p><strong>So what's google's problem with this? I am properly encoded.</strong></p>",41127724,1,0,,2016-12-13 16:48:58.640 UTC,,2016-12-14 16:51:13.447 UTC,2017-05-23 11:45:41.337 UTC,,-1,,2628223,1,1,ruby-on-rails|ruby|google-app-engine|ruby-on-rails-4|google-cloud-vision,717
Google Vision Api text detection layout information from bounding box,40984635,Google Vision Api text detection layout information from bounding box,"<p>I am trying to use google vision api to perform OCR on my images. The Json Output to the API call returns recognized words with bounding box information. </p>

<p>Could someone please tell me how to use this bounding box information to do layout analysis of my image? </p>

<p>If there is a library which takes this as input and returns sentences instead of words?</p>

<pre><code>{
  ""description"": ""Ingredients:"",
  ""boundingPoly"": {
    ""vertices"": [
      {
        ""x"": 14,
        ""y"": 87
      },
      {
        ""x"": 53,
        ""y"": 87
      },
      {
        ""x"": 53,
        ""y"": 98
      },
      {
        ""x"": 14,
        ""y"": 98
      }
    ]
  }
},
{
  ""description"": ""Chicken"",
  ""boundingPoly"": {
    ""vertices"": [
      {
        ""x"": 55,
        ""y"": 87
      },
      {
        ""x"": 77,
        ""y"": 87
      },
      {
        ""x"": 77,
        ""y"": 98
      },
      {
        ""x"": 55,
        ""y"": 98
      }
    ]
  }
},
</code></pre>

<p>For instance in the above json, the words 'Ingredients:' 'Chicken' are on the same line. Is there a library which can give me this information out of the box? </p>

<p>Image used for OCR <a href=""https://i.stack.imgur.com/5ImPd.jpg"" rel=""nofollow noreferrer"">source image</a></p>",,2,0,,2016-12-05 22:37:20.217 UTC,1,2018-04-04 10:34:58.683 UTC,2016-12-06 13:34:58.080 UTC,,322020,,2745862,1,2,image-processing|ocr|google-vision,1194
Google cloud vision api request error,49944780,Google cloud vision api request error,"<p>I'm trying to make a request to Google Cloud Vision lib using AFNetworking but I get a 400 error. I am not sure where the issue is. I make the request as suggested in the documentation .</p>

<p><a href=""https://cloud.google.com/vision/docs/request#json_request_format"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/request#json_request_format</a> </p>

<pre><code>NSString *base64 = [self encodeToBase64String:[UIImage imageNamed:@""IMG_5081.JPG""]];
    NSDictionary *d3 = @{@""requests"":
                             @[
                                 @{@""image"":@{@""content"":base64}},
                             @{@""features"":@[@{@""type"":@""LABEL_DETECTION"",@""maxResults"":@(1)}]}
                              ]
                         };



AFHTTPSessionManager *manager = [AFHTTPSessionManager manager];

NSString *functionURL = @""https://vision.googleapis.com/v1/images:annotate?key=AIzaSyBO3NtyPlLnagHEauMGpBHC603RnDS6BFM"";

AFHTTPRequestSerializer * requestSerializer = [AFHTTPRequestSerializer serializer];
AFHTTPResponseSerializer * responseSerializer = [AFHTTPResponseSerializer serializer];
manager.responseSerializer = responseSerializer;
manager.requestSerializer = requestSerializer;
[manager.requestSerializer setTimeoutInterval:50.0];
[manager.requestSerializer setValue:@""application/json"" forHTTPHeaderField:@""Content-Type""];
[manager POST:functionURL parameters:parameters progress:nil success:^(NSURLSessionDataTask * _Nonnull task, id  _Nullable responseObject) {

        NSHTTPURLResponse *httpResponse = (NSHTTPURLResponse*)task.response;
        if (httpResponse.statusCode == 200 || httpResponse.statusCode == 201) {

            NSData * data = (NSData *)responseObject;
            success(YES,data,httpResponse);
        }else{
            failure(nil, httpResponse.statusCode);//success(NO,responseObject);
        }

    } failure:^(NSURLSessionDataTask * _Nullable task, NSError * _Nonnull error) {
        failure(error, error.code);
    }];
</code></pre>

<p>The error I get is:</p>

<pre><code>Error Domain=com.alamofire.error.serialization.response Code=-1011 ""Request failed: bad request (400)"" UserInfo={NSLocalizedDescription=Request failed: bad request (400), NSErrorFailingURLKey=https://vision.googleapis.com/v1/images:annotate?key=AIzaSyBO3NtyPlLnagHEauMGpBHC603RnDS6BFM, com.alamofire.serialization.response.error.data=&lt;7b0a2020 22657272 6f72223a 207b0a20 20202022 636f6465 223a2034 30302c0a 20202020 226d6573 73616765 223a2022 496e7661 6c696420 4a534f4e 20706179 6c6f6164 20726563 65697665 642e2055 6e6b6e6f 776e206e 616d6520 5c227265 71756573 74735b5d 5b666561 74757265 735d5b5d 5b6d6178 52657375 6c74735d 5c223a20 43616e6e 6f742062 696e6420 71756572 79207061 72616d65 7465722e 20466965 6c642027 72657175 65737473 5b5d5b66 65617475 7265735d 5b5d5b6d 61785265 73756c74 735d2720 636f756c 64206e6f 74206265 20666f75 6e642069 6e207265 71756573 74206d65 73736167 652e5c6e 496e7661 6c696420 4a534f4e 20706179 6c6f6164 20726563 65697665 642e2055 6e6b6e6f 776e206e 616d6520 5c227265 71756573 74735b5d 5b696d61 67655d5b 636f6e74 656e745d 5c223a20 43616e6e 6f742062 696e6420 71756572 79207061 72616d65 7465722e 20466965 6c642027 72657175 65737473 5b5d5b69 6d616765 5d5b636f 6e74656e 745d2720 636f756c 64206e6f 74206265 20666f75 6e642069 6e207265 71756573 74206d65 73736167 652e5c6e 496e7661 6c696420 4a534f4e 20706179 6c6f6164 20726563 65697665 642e2055 6e6b6e6f 776e206e 616d6520 5c227265 71756573 74735b5d 5b666561 74757265 735d5b5d 5b747970 655d5c22 3a204361 6e6e6f74 2062696e 64207175 65727920 70617261 6d657465 722e2046 69656c64 20277265 71756573 74735b5d 5b666561 74757265 735d5b5d 5b747970 655d2720 636f756c 64206e6f 74206265 20666f75 6e642069 6e207265 71756573 74206d65 73736167 652e222c 0a202020 20227374 61747573 223a2022 494e5641 4c49445f 41524755 4d454e54 222c0a20 20202022 64657461 696c7322 3a205b0a 20202020 20207b0a 20202020 20202020 22407479 7065223a 20227479 70652e67 6f6f676c 65617069 732e636f 6d2f676f 6f676c65 2e727063 2e426164 52657175 65737422 2c0a2020 20202020 20202266 69656c64 56696f6c 6174696f 6e73223a 205b0a20 20202020 20202020 207b0a20 20202020 20202020 20202022 64657363 72697074 696f6e22 3a202249 6e76616c 6964204a 534f4e20 7061796c 6f616420 72656365 69766564 2e20556e 6b6e6f77 6e206e61 6d65205c 22726571 75657374 735b5d5b 66656174 75726573 5d5b5d5b 6d617852 6573756c 74735d5c 223a2043 616e6e6f 74206269 6e642071 75657279 20706172 616d6574 65722e20 4669656c 64202772 65717565 7374735b 5d5b6665 61747572 65735d5b 5d5b6d61 78526573 756c7473 5d272063 6f756c64 206e6f74 20626520 666f756e 6420696e 20726571 75657374 206d6573 73616765 2e220a20 20202020 20202020 207d2c0a 20202020 20202020 20207b0a 20202020 20202020 20202020 22646573 63726970 74696f6e 223a2022 496e7661 6c696420 4a534f4e 20706179 6c6f6164 20726563 65697665 642e2055 6e6b6e6f 776e206e 616d6520 5c227265 71756573 74735b5d 5b696d61 67655d5b 636f6e74 656e745d 5c223a20 43616e6e 6f742062 696e6420 71756572 79207061 72616d65 7465722e 20466965 6c642027 72657175 65737473 5b5d5b69 6d616765 5d5b636f 6e74656e 745d2720 636f756c 64206e6f 74206265 20666f75 6e642069 6e207265 71756573 74206d65 73736167 652e220a 20202020 20202020 20207d2c 0a202020 20202020 2020207b 0a202020 20202020 20202020 20226465 73637269 7074696f 6e223a20 22496e76 616c6964 204a534f 4e207061 796c6f61 64207265 63656976 65642e20 556e6b6e 6f776e20 6e616d65 205c2272 65717565 7374735b 5d5b6665 61747572 65735d5b 5d5b7479 70655d5c 223a2043 616e6e6f 74206269 6e642071 75657279 20706172 616d6574 65722e20 4669656c 64202772 65717565 7374735b 5d5b6665 61747572 65735d5b 5d5b7479 70655d27 20636f75 6c64206e 6f742062 6520666f 756e6420 696e2072 65717565 7374206d 65737361 67652e22 0a202020 20202020 2020207d 0a202020 20202020 205d0a20 20202020 207d0a20 2020205d 0a20207d 0a7d0a&gt;, com.alamofire.serialization.response.error.response=&lt;NSHTTPURLResponse: 0x60000003c220&gt; { URL: https://vision.googleapis.com/v1/images:annotate?key=AIzaSyBO3NtyPlLnagHEauMGpBHC603RnDS6BFM } { Status Code: 400, Headers {
    ""Cache-Control"" =     (
        private
    );
    ""Content-Encoding"" =     (
        gzip
    );
    ""Content-Length"" =     (
        350
    );
    ""Content-Type"" =     (
        ""application/json; charset=UTF-8""
    );
    Date =     (
        ""Fri, 20 Apr 2018 15:03:07 GMT""
    );
    Server =     (
        ESF
    );
    Vary =     (
        Origin,
        ""X-Origin"",
        Referer
    );
    ""alt-svc"" =     (
        ""hq=\"":443\""; ma=2592000; quic=51303433; quic=51303432; quic=51303431; quic=51303339; quic=51303335,quic=\"":443\""; ma=2592000; v=\""43,42,41,39,35\""""
    );
    ""x-content-type-options"" =     (
        nosniff
    );
    ""x-frame-options"" =     (
        SAMEORIGIN
    );
    ""x-xss-protection"" =     (
        ""1; mode=block""
    );
} }}
</code></pre>",,1,0,,2018-04-20 15:12:39.837 UTC,,2018-04-20 15:29:59.010 UTC,2018-04-20 15:22:23.077 UTC,,3695849,,3695849,1,0,ios|objective-c|afnetworking|google-vision,304
Google Cloud Vision - Fails to provide good OCR on image with 2 columns of text,47281901,Google Cloud Vision - Fails to provide good OCR on image with 2 columns of text,"<p>The output of double column text is not coming right order when I pass the double column text image to google cloud vision API's TEXT_DETECTION/DOCUMENT_TEXT_DETECTION as it is taking one line from 1st column and then next line from another column and appending it.</p>

<p>You can see the results of the output is not aligned properly in the order they should be according to the double column. Is there a way to correct results from google vision API, or to correct it using the JSON file output?  </p>

<p><a href=""https://i.stack.imgur.com/kdnUv.jpg"" rel=""nofollow noreferrer""> To view input image in JPG[click here]</a></p>

<p><strong>Output-</strong></p>

<blockquote>
  <p>6.6 Healthcare access times. [56] impairment of their vision can use this to recognize peo- 6.6 Healthcare access ple within their vicinity
  without them saying a word.152|| Visual impairment has the ability to
  create consequences for health and well being. Visual impairment is
  increasing especially among older people. It is recognized that those
  6.5.3 Communication development individuals with visual impairment are likely to have lim- ited access to information and healthcare
  facilities, and Visual impairment can have profound effects on the de-
  may not receive the best care possible because not all velopment of
  infant and child communication. The lan- health care professionals are
  aware of specific needs re- guage and social development of a child or
  infant can lated to vision. be very delayed by the inability to see
  the world around them. . A prerequisite of effective health care could
  very well be having staff that are aware that people may have problems
  with vision. Social development Social development includes in-
  teractions with the people surrounding the infant in the Communication
  and different ways of being able beginning of its life. To a child
  with vision, a smile from to communicate with visually impaired
  clients must a parent is the first symbol of recognition and communi
  be tailored to individual needs and available at all cation, and is
  almost an instant factor of communication. For a visually impaired
  infant, recognition of a parent's voice will be noticed at
  approximately two months old, but a smile will only be evoked through
  touch between 7 Epidemiology parent and baby. This primary form of
  communication is greatly delayed for the child and will prevent other
  forms of communication from developing. Social interactions The WHO
  estimates that in 2012 there were 285 million are more complicated
  because subtle visual cues are miss- visually impaired people in the
  world, of which 246 mil- ing and facial expressions from others are
  lost. lion had low vision and 39 million were blind.[4] Due to delays
  in a child's communication development, of those who are blind 90%
  live in the developing they may appear to be disinterested in social
  activity with world. [56] Worldwide for each blind person, an average
  peers, non-communicative and un-education on how to of 3.4 people have
  low vision, with country and regional communicate with other people.
  This may cause the child variation ranging from 2.4 to 5.5.[57] to be
  avoided by peers and consequently over protected By age: Visual
  impairment is unequally distributed by family members. across age
  groups. More than 82% of all people who are blind are 50 years of age
  and older, although they repre- sent only 19% of the world's
  population. Due to the ex- anguage development with sight much of what
  is pected number of years lived in blindness (blind years), learned by
  a child is learned through imitation of others, childhood blindness
  remains a significant problem, with where as a visually impaired child
  needs very planned in an estimated 1.4 million blind children below
  age 15. struction directed at the development of postponed imi- By
  gender: Available studies consistently indicate that in tation. A
  visually impaired infant may jabber and imitate every region of the
  world, and at all ages, females have a words sooner than a sighted
  child, but may show delay significantly higher risk of being visually
  impaired than when combining words to say themselves, the child may
  males. tend to initiate few questions and their use of adjectives is
  infrequent. Normally the child's sensory experiences are By geography:
  Visual impairment is not distributed uni- not readily coded into
  language and this may cause them formly throughout the world. More
  than 90% of the to store phrases and sentences in their memory and re
  world's visually impaired live in developing countries. [57] peat them
  out of context. The language of the blind child Since the estimates of
  the 1990s, new data based on the does not seem to mirror their
  developing knowledge of 2002 global population show a reduction in the
  number the world, but rather their knowledge of the language of of
  people who are blind or visually impaired, and those others. who are
  blind from the effects of infectious diseases, but A visually impaired
  child may also be hesitant to explore an increase in the number of
  people who are blind from the world around them due to fear of the
  unknown and conditions related to longer life spans. 157|| also may be
  discouraged from exploration by overprotec- In 1987, it was estimated
  that 598,000 people in the tive family members. Without concrete
  experiences, the United States met the legal definition of blindness.
  Is Of child is not able to develop meaningful concepts or the this
  number, 58% were over the age of 65.581 In 1994- language to describe
  or think about them. 55] 1995, 1.3 million Americans reported legal
  blindness. 159|</p>
</blockquote>",,2,0,,2017-11-14 09:21:57.157 UTC,,2017-11-20 18:30:39.213 UTC,2017-11-16 17:29:25.940 UTC,,8204362,,8204362,1,2,android|json|google-cloud-platform|ocr|google-cloud-vision,444
Android Studio Google Vision Barcode scanning in fragment Buttomviewer,55831997,Android Studio Google Vision Barcode scanning in fragment Buttomviewer,"<p>I'm trapped in a problem. I tried to use Google Vision for barcode scanning. Now I wanted to integrate the whole thing into a ButtomView as an own fragment but everything remains dark. I don't get ERROR either. </p>

<p>I just want when I press on Home in this example the reader (camera) starts and I can scan barcode. 
If I open another tab, the scanning is stopped. I have add to activity_main.xml a framelayout and a buttomnavigationView</p>

<p>activity_main.xml</p>

<pre><code>    &lt;FrameLayout
        android:id=""@+id/fragment_container""
        android:layout_width=""match_parent""
        android:layout_height=""match_parent""
        android:layout_above=""@id/navigation""/&gt;

    &lt;android.support.design.widget.BottomNavigationView
        android:id=""@+id/navigation""
        android:layout_width=""0dp""
        android:layout_height=""wrap_content""
        android:layout_marginStart=""0dp""
        android:layout_marginEnd=""0dp""
        android:background=""?android:attr/windowBackground""
        app:layout_constraintBottom_toBottomOf=""parent""
        app:layout_constraintLeft_toLeftOf=""parent""
        app:layout_constraintRight_toRightOf=""parent""
        app:menu=""@menu/navigation"" /&gt;

&lt;/android.support.constraint.ConstraintLayout&gt;```



</code></pre>

<p>Creat a fragment Layout with all necessary outlets.
fragment_scan.xml</p>

<pre><code>

    &lt;LinearLayout
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:orientation=""vertical""&gt;

        &lt;SurfaceView
            android:layout_width=""640px""
            android:layout_height=""480px""
            android:layout_centerVertical=""true""
            android:layout_alignParentLeft=""true""
            android:id=""@+id/camera_view""/&gt;

        &lt;TextView
            android:text="" code reader""
            android:layout_width=""wrap_content""
            android:layout_height=""wrap_content""
            android:id=""@+id/txtContent""/&gt;
        &lt;Button
            android:layout_width=""wrap_content""
            android:layout_height=""wrap_content""
            android:text=""Process""
            android:id=""@+id/button""
            android:layout_alignParentTop=""true""
            android:layout_alignParentStart=""true"" /&gt;
        &lt;ImageView
            android:layout_width=""wrap_content""
            android:layout_height=""wrap_content""
            android:id=""@+id/imgview""/&gt;
    &lt;/LinearLayout&gt;
&lt;/RelativeLayout&gt;```

</code></pre>

<p>In the mainActivity.java I managed the different fragments to show and works fine. The surface view, imageViewand the txtConten. </p>

<p>Now the problem. The linking works good but everything which is linked to the layout is here but black.
scanfragment.java</p>

<pre><code>
import android.os.Bundle;
import android.support.annotation.NonNull;
import android.support.annotation.Nullable;
import android.support.v4.app.Fragment;
import android.support.v4.app.FragmentTransaction;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.support.v7.app.AppCompatActivity;
import android.os.Bundle;
import android.util.Log;
import android.util.SparseArray;
import android.view.SurfaceHolder;
import android.view.SurfaceView;
import android.view.View;
import android.widget.Button;
import android.widget.TextView;
import java.io.IOException;
import com.google.android.gms.vision.CameraSource;
import com.google.android.gms.vision.Detector;
import com.google.android.gms.vision.Frame;
import com.google.android.gms.vision.barcode.Barcode;
import com.google.android.gms.vision.barcode.BarcodeDetector;

public class scanfragment extends Fragment {

    TextView barcodeInfo;
    SurfaceView cameraView;
    CameraSource cameraSource;

    @Nullable
    @Override
    public View onCreateView(@NonNull LayoutInflater inflater, @Nullable ViewGroup container, @Nullable Bundle savedInstanceState) {
        //return super.onCreateView(inflater, container, savedInstanceState);
        View SCANVIEW = inflater.inflate(R.layout.fragment_scan, container, false);
        cameraView = (SurfaceView)  SCANVIEW.findViewById(R.id.camera_view);
        barcodeInfo = (TextView) SCANVIEW.findViewById(R.id.txtContent);

        BarcodeDetector barcodeDetector =
                new BarcodeDetector.Builder(container.getContext())
                        .setBarcodeFormats(Barcode.CODE_128)//QR_CODE)
                        .build();
        cameraSource = new CameraSource
                .Builder(container.getContext(), barcodeDetector)
                .setRequestedPreviewSize(640, 480)
                .build();
        cameraView.getHolder().addCallback(new SurfaceHolder.Callback() {
            @Override
            public void surfaceCreated(SurfaceHolder holder) {

                try {
                    cameraSource.start(cameraView.getHolder());
                } catch (IOException ie) {
                    Log.e(""CAMERA SOURCE"", ie.getMessage());
                }
            }

            @Override
            public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {
            }

            @Override
            public void surfaceDestroyed(SurfaceHolder holder) {
                cameraSource.stop();
            }
        });


        barcodeDetector.setProcessor(new Detector.Processor&lt;Barcode&gt;() {
            @Override
            public void release() {
            }

            @Override
            public void receiveDetections(Detector.Detections&lt;Barcode&gt; detections) {

                final SparseArray&lt;Barcode&gt; barcodes = detections.getDetectedItems();

                if (barcodes.size() != 0) {
                    barcodeInfo.post(new Runnable() {    // Use the post method of the TextView
                        public void run() {
                            barcodeInfo.setText(    // Update the TextView
                                    barcodes.valueAt(0).displayValue
                            );
                        }
                    });
                }
            }
        });

        return inflater.inflate(R.layout.fragment_scan,container, false);
    } 
}```
</code></pre>

<p>I think the problem is the linking between the fragment and the variables.
In the tutorial everything is in the MainActivity and here in the fragment.
In the mainActivity it was linked like this:</p>

<pre><code>BarcodeDetector barcodeDetector =
                new BarcodeDetector.Builder(this)
                        .setBarcodeFormats(Barcode.CODE_128)//QR_CODE)
                        .build();
        cameraSource = new CameraSource
                .Builder(this, barcodeDetector)
                .setRequestedPreviewSize(640, 480)
                .build();
</code></pre>

<p>Any help would be great.</p>",55837116,1,1,,2019-04-24 14:03:51.267 UTC,,2019-04-24 19:16:59.237 UTC,2019-04-24 18:26:17.810 UTC,,1045666,,1045666,1,-1,java|android|xml,22
Amazon Rekognition usable with images hosted on other server/site than S3?,48448138,Amazon Rekognition usable with images hosted on other server/site than S3?,"<p>do I understand the Documentation right and it's not possible to use the Amazon Rekognition service with ""external"" links without priorly save the images on Amazon S3 or another local server?</p>

<p>Why simple web hosted images are not supported?</p>

<p>Thanksf
fp</p>",48935690,1,0,,2018-01-25 16:52:56.367 UTC,,2018-02-22 19:27:46.953 UTC,,,,,3165675,1,0,amazon-rekognition,51
Error 429 - too much requests - trying to delete all classifiers of IBM Watson Visual Recognition service,37690111,Error 429 - too much requests - trying to delete all classifiers of IBM Watson Visual Recognition service,"<p>I'm trying to delete all classifiers of my instance of IBM Watson Visual Recognition service so that I can create only the new classifiers that are used for my app.</p>

<p>To do it, I wrote Node.js code that lists all the classifiers and sends a delete request.</p>

<p>When I executed it (hundreds of delete requests in parallel), I received a <code>429 error - too many requests</code>. After that, all of my delete requests (even individual ones) received an <code>404 error - Cannot delete classifier</code>.</p>

<p>My questions are:</p>

<ol>
<li>Is there a better way to delete all classifiers that is not doing it one by one?</li>
<li>Why am I unable to delete individual classifiers now? Is there some policy that blocks me after a 429 too many requests error?</li>
</ol>

<p>This is the 429 error that I received in the multiple delete requests</p>

<pre><code>code: 429,
  error: '&lt;HTML&gt;&lt;BODY&gt;&lt;span class=\'networkMessage\'&gt;&lt;h2&gt;Wow, is it HOT in here!&lt;/h2&gt;My CPU cores are practically burning up thanks to all the great questions from wonderful humans like you.&lt;p&gt;Popularity has its costs however. Right now I just can\'t quite keep up with everything. So I ask your patience while my human subsystems analyze this load spike to get me more Power.&lt;p&gt;I particularly want to &lt;b&gt;thank you&lt;/b&gt; for bringing your questions. PLEASE COME BACK - soon and frequently! Not only do I learn from your usage, but my humans learn from workload spikes (like this one) how to better adjust capacity with Power and Elastic Storage.&lt;p&gt;So again, thank you for helping to make me smarter and better. I\'m still young and growing, but with your patience and help, I hope to make you proud someday!&lt;/p&gt;Your buddy,&lt;br&gt;Watson&lt;br&gt;&lt;span class=\'watsonIcon\'&gt;&lt;/span&gt;p.s. Please share your experiences in the Watson C
</code></pre>

<p>Edit:</p>

<p>I noticed that the error apparently happens only when I try to delete a ""default"" classifier that is provided by the service (like ""Graphics"", ""Color"", ""Black_and_white"", etc.). The deletion works fine for when I try do delete a classifier that I created with  my own pictures.</p>

<p>Is it a characteristic of the service that I'm not allowed to delete the default classifiers ? If it is, any special reason for that ? The app that I'm building doesn't need all those built-in classifiers, so it is useless to have all that. </p>

<p>I understand that I can inform the list of classifiers that I want to use when I request a new classification, but in this situation I'll need to keep a separated list of my classifiers and will not be able to request a more generic classification without getting the default classifiers in the result.</p>

<p>I'm using node js module ""watson-developer-cloud"": ""^1.3.1"" - I'm not sure what API versions it uses internally. I just noticed there is a newer version available. I'll update it and report back here if there is any difference.</p>

<p>This is the JS function that I'm using to delete a single classifier</p>

<pre><code>function deleteClassifier(classifier_id,callback){
    var data = {
      ""classifier_id"": classifier_id,
    };
    visualRecognition.deleteClassifier(data,function(err, response) {
      if (err){
        callback(err);
      }
      else{
        callback(null, response);
      }
    });
  }
</code></pre>

<p>-Edit</p>

<p>The occurred when I was using V2 API - But I believe it is not related to API version. See the accepted answer</p>",38679677,2,0,,2016-06-07 21:58:21.150 UTC,,2016-07-31 00:17:24.843 UTC,2016-07-31 00:14:48.670 UTC,,1956187,,1956187,1,0,node.js|ibm-cloud|ibm-watson|visual-recognition,354
How can I retrieve the raw Json that is returned by AWS Rekognition?,50958350,How can I retrieve the raw Json that is returned by AWS Rekognition?,"<p>For a comparison task I want to save all data returned by AWS Rekognition in .NET, in this case by DetectFaces, as json for later extraction.
How can I get the raw json? The .NET SDK does not offer any methods. I tried to serialize the face details without success.</p>

<pre><code>[...]
AmazonRekognitionClient rekognitionClient = new AmazonRekognitionClient(destination::Amazon.RegionEndpoint.EUWest1);

DetectFacesRequest detectFacesRequest = new DetectFacesRequest()
   {
       Image = awsImage,
       Attributes = new List&lt;String&gt;() { ""ALL"" }
   };

   try
   {
       DetectFacesResponse detectFacesResponse = rekognitionClient.DetectFaces(detectFacesRequest);
       List&lt;FaceDetail&gt; details = detectFacesResponse.FaceDetails;
   }catch {...}
</code></pre>",50958501,1,0,,2018-06-20 23:21:28.123 UTC,,2018-06-21 09:11:37.603 UTC,2018-06-21 09:11:37.603 UTC,,6539634,,6539634,1,0,c#|.net|json|aws-sdk|amazon-rekognition,149
How to point Google Cloud Vision API credential key file to GOOGLE_APPLICATION_CREDENTIALS variable in yii2 framework,54605435,How to point Google Cloud Vision API credential key file to GOOGLE_APPLICATION_CREDENTIALS variable in yii2 framework,"<p>I want to use Google Cloud Vision API for image recognition, Everything installed fine in my yii2 framework. </p>

<p>I'm getting authentication error like :</p>

<pre><code>&gt;  Google\Cloud\Core\Exception\ServiceException
&gt;     {
&gt;     ""error"": {
&gt;     ""code"": 403,
&gt;     ""message"": ""The request is missing a valid API key."",
&gt;     ""status"": ""PERMISSION_DENIED""
&gt;     }
&gt;     }
&gt;     1. in C:\xampp\htdocs\vofms\vendor\google\cloud-core\src\RequestWrapper.php
&gt; at line 336
&gt;     32732832933033133233333433533633733
</code></pre>

<p>How to point my key.json file to GOOGLE_APPLICATION_CREDENTIALS environment variable In yii2 framework.</p>

<p>Thanks</p>",,1,3,,2019-02-09 10:44:15.527 UTC,,2019-02-09 18:37:06.227 UTC,2019-02-09 10:55:04.177 UTC,,3731614,,9013681,1,0,php|api|yii2|cloud|vision,141
What's the proper way to extract data from a google protobuff object?,50767594,What's the proper way to extract data from a google protobuff object?,"<p>I have never encountered this sort of collection or object before until now (its the response from a request to Google-Cloud-Vision API).</p>

<p>I wrote a class that uses the API and does what I want correctly. However the only way that I can extract/manipulate data in the response is by using this module:</p>

<pre><code>from google.protobuf.json_format import MessageToJson 
</code></pre>

<p>I basically serialized the protobuff into a string and then used regex to get the data that I want. </p>

<p>There MUST be a better way than this. Any suggestions? I was hoping to have the API response give me a json dict or json dict of dicts etc... All I could come up with was turning the response into a string though.</p>

<p>Here is the file from the github repository:
<a href=""https://github.com/rootVIII/image_analyzer/blob/master/image_analyzer.py"" rel=""nofollow noreferrer"">image_analyzer.py</a></p>

<p>Thank you all in advance.</p>",50767788,2,0,,2018-06-08 19:53:29.190 UTC,,2018-06-10 06:38:56.483 UTC,2018-06-08 20:00:32.210 UTC,user9915877,,user9915877,,1,0,json|python-3.x|python-requests|protocol-buffers|google-cloud-vision,84
Ionic1 with compare face aws service,45836819,Ionic1 with compare face aws service,"<p>I want to integrate the Amazon rekognition, compare face api in my ionic project. Can anyone suggest any sample or way to that.
I am trying on simple JS but getting an authorization error.</p>",,0,2,,2017-08-23 10:11:05.163 UTC,,2017-08-23 10:11:05.163 UTC,,,,,1291129,1,0,javascript|ionic-framework|aws-sdk|amazon-rekognition|aws-sdk-js,44
how I can upload batch images with name on s3 using boto?,50483439,how I can upload batch images with name on s3 using boto?,"<p>I am uploading images to a folder currently on local . like in site/uploads.
And After searching I got that for uploading images to s3 I have to do like this </p>

<pre><code>import boto3

s3 = boto3.resource('s3')

# Get list of objects for indexing
images=[('image01.jpeg','Albert Einstein'),
      ('image02.jpeg','Candy'),
      ('image03.jpeg','Armstrong'),
      ('image04.jpeg','Ram'),
      ('image05.jpeg','Peter'),
      ('image06.jpeg','Shashank')
      ]

# Iterate through list to upload objects to S3   
for image in images:
    file = open(image[0],'rb')
    object = s3.Object('rekognition-pictures','index/'+ image[0])
    ret = object.put(Body=file,
                    Metadata={'FullName':image[1]}
                    )
</code></pre>

<p><strong>Clarification</strong></p>

<p>Its my code to send images and name to S3 . But I dont know how to get image in this line of code   <code>images=[('image01.jpeg','Albert Einstein'),</code>       like how can I get this image in this code from  /upload/image01.jpeg   .  and 2ndly how can I get images from s3 and show in my website image page ?</p>",,3,8,,2018-05-23 08:29:06.047 UTC,1,2018-05-25 08:54:01.607 UTC,2018-05-23 10:40:33.873 UTC,,7749560,,7749560,1,2,python|amazon-web-services|boto3,680
Cannot send more than 300 requests in one go in azure,43277815,Cannot send more than 300 requests in one go in azure,"<p>We are sending requests to Microsoft emotion API to find the emotions every second. But in a single go, we are able to send only 300 requests that is for 5 minutes. After 5 minutes, it stops sending the responses. If we start the the application again, we are able to send the requests for another 5 minutes.</p>

<p>Account is ""Pay as you go Standard"". </p>

<p>Thank you.</p>",,0,5,,2017-04-07 12:11:31.780 UTC,1,2017-04-07 12:51:40.080 UTC,2017-04-07 12:51:40.080 UTC,,3405686,,7832580,1,0,azure|microsoft-cognitive,60
"Google Cloud Vision - PHP - ""Request had insufficient authentication scopes""",45849380,"Google Cloud Vision - PHP - ""Request had insufficient authentication scopes""","<p>Trying to use Google Cloud vision to analyze files already stored in Google Cloud Storage. My code:</p>

<pre><code>$vision = new VisionClient([
                            'projectId'   =&gt; $projectId,
                            'keyFilePath' =&gt; &lt;json key file&gt;,
                            'scopes'      =&gt; 'https://www.googleapis.com/auth/cloud-platform'
                           ]);
</code></pre>

<p>I grab a full http path to my file, which is valid, but when I:</p>

<pre><code>$image = $vision-&gt;image( fopen(&lt;file&gt;, 'r'),  [ 'LABEL_DETECTION' ]);
</code></pre>

<p>I get the error:</p>

<pre><code>Fatal error: Uncaught exception 
 'Google\Cloud\Core\Exception\ServiceException' with message '{
    ""error"": {
         ""code"": 403,
         ""message"": ""Request had insufficient authentication scopes."",
         ""status"": ""PERMISSION_DENIED""
    }
  }'
</code></pre>

<p>I can open the file fine ($x = fopen(filename) works), so I'm not sure what's happening here. Is there a way I can check what my service client has in the way of permissions?</p>",,1,1,,2017-08-23 21:14:26.457 UTC,,2017-08-23 22:20:25.890 UTC,,,,,4122302,1,0,php|google-cloud-vision,210
How Scalable is the Google Vision API?,44915072,How Scalable is the Google Vision API?,"<p>I have an IOS app where users can upload images.
I want to run all these images through Google's Vision API.
Could someone please let me know how realistic this idea is?</p>

<p>Let's say that I want to run 1000 images through their API.
How much would this cost me?</p>

<p>In the question title, I used the word scalable because I'm worried that using this service would be really expensive.</p>

<p>This is mainly a question about how much it costs to get Google to scan each an image.</p>

<p>Thanks!</p>",44915158,2,3,,2017-07-04 23:18:10.887 UTC,,2017-07-04 23:29:15.220 UTC,,,,,2251258,1,0,google-cloud-platform|google-vision,210
Ubuntu client awscli unsupported by AWS MSK Kafka?,56027677,Ubuntu client awscli unsupported by AWS MSK Kafka?,"<p>Are kafka commands through the awscli not supported in ubuntu?  Do we have to use 'Amazon Linux 2 AMI (HVM), SSD Volume Type' for any consumers/producers interacting with MSK?</p>

<p>When we stood up an Ubuntu 18.04 instance it saying that </p>

<blockquote>
  <p>awscli is already the newest version (1.14.44-1ubuntu1).</p>
</blockquote>

<p>and <code>aws kafka help</code> shows:</p>

<blockquote>
  <p>ubuntu@ip-xxxxxxxxx:~/kafka_2.12-2.1.0$ aws kafka help usage: aws
  [options]   [ ...] [parameters] To
  see help text, you can run:</p>
  
  <p>aws help   aws  help   aws   help aws:
  error: argument command: Invalid choice, valid choices are:</p>
  
  <p>acm                                      | alexaforbusiness apigateway
  | application-autoscaling appstream                                |
  appsync athena                                   | autoscaling
  autoscaling-plans                        | batch budgets<br>
  | ce cloud9                                   | clouddirectory
  cloudformation                           | cloudfront cloudhsm<br>
  | cloudhsmv2 cloudsearch                              |
  cloudsearchdomain cloudtrail                               |
  cloudwatch codebuild                                | codecommit
  codepipeline                             | codestar cognito-identity<br>
  | cognito-idp cognito-sync                             | comprehend
  cur                                      | datapipeline dax<br>
  | devicefarm directconnect                            | discovery dms 
  | ds dynamodb                                 | dynamodbstreams ec2<br>
  | ecr ecs                                      | efs elasticache<br>
  | elasticbeanstalk elastictranscoder                        | elb
  elbv2                                    | emr es<br>
  | events firehose                                 | gamelift glacier<br>
  | glue greengrass                               | guardduty health<br>
  | iam importexport                             | inspector iot<br>
  | iot-data iot-jobs-data                            | kinesis
  kinesis-video-archived-media             | kinesis-video-media
  kinesisanalytics                         | kinesisvideo kms<br>
  | lambda lex-models                               | lex-runtime
  lightsail                                | logs machinelearning<br>
  | marketplace-entitlement marketplacecommerceanalytics             |
  mediaconvert medialive                                | mediapackage
  mediastore                               | mediastore-data
  meteringmarketplace                      | mgh mobile<br>
  | mq mturk                                    | opsworks opsworkscm<br>
  | organizations pinpoint                                 | polly
  pricing                                  | rds redshift<br>
  | rekognition resource-groups                          |
  resourcegroupstaggingapi route53                                  |
  route53domains sagemaker                                |
  sagemaker-runtime sdb                                      |
  serverlessrepo servicecatalog                           |
  servicediscovery ses                                      | shield sms
  | snowball sns                                      | sqs ssm<br>
  | stepfunctions storagegateway                           | sts support
  | swf transcribe                               | translate waf<br>
  | waf-regional workdocs                                 | workmail
  workspaces                               | xray s3api<br>
  | s3 configure                                | deploy configservice<br>
  | opsworks-cm runtime.sagemaker                        | history help</p>
</blockquote>",56028845,1,0,,2019-05-07 17:24:20.227 UTC,,2019-05-07 18:52:28.870 UTC,2019-05-07 18:15:05.180 UTC,,1736218,,1736218,1,0,amazon-web-services|amazon-ec2|apache-kafka,26
Can Google Cloud Vision API be trained using your image data?,40136543,Can Google Cloud Vision API be trained using your image data?,"<p>IBM Watson has a capability where you can train the classifiers on Watson using your images but I am unable to find a similar capability on Google Cloud Vision API? What I want is that I upload 10-15 classes of images and on the bases of upload images classify any images loaded after that. IBM Bluemix (Watson) has this capability but their pricing is significantly higher than Google. I am open to other services as well, if prices ares below Google's </p>",,4,0,,2016-10-19 16:06:20.953 UTC,3,2019-04-11 02:38:51.727 UTC,,,,,1596433,1,3,machine-learning|computer-vision|google-cloud-vision|watson,4862
Turn On Flash Light During Bar code detection with Vision API - Xamarin Android,53103165,Turn On Flash Light During Bar code detection with Vision API - Xamarin Android,"<p>I am using Google Vision API to Scan Bar code in my mobile application. Along with that I am trying to turn on the flash light. But in the latest version of Vision API I can't find any method to enable flash. So I am using the below method for enabling and disabling Flash (which is working Fine). But the Camera API is deprecated and I am not sure how to implement
the same using Camera2 API.</p>

<p><strong>The code I am using - With deprecated Camera API - Working</strong>  </p>

<pre><code>public void TurnFlashOn(bool status = false)
        {
            var camera = GetCamera(_cameraSource);
            var paramts = camera.GetParameters();
            if (status)
                paramts.FlashMode = Camera.Parameters.FlashModeTorch;
            else
                paramts.FlashMode = Camera.Parameters.FlashModeOff;
            camera.SetParameters(paramts);
        }



        private Camera GetCamera(CameraSource cameraSource)
        {
            var sourceObject = cameraSource.JavaCast&lt;Java.Lang.Object&gt;();
            var fields = sourceObject.Class.GetDeclaredFields();
            foreach (var field in fields)
            {
                if (field.Type.CanonicalName.Equals(""android.hardware.camera"", StringComparison.OrdinalIgnoreCase))
                {
                    field.Accessible = true;
                    var camera = field.Get(sourceObject);
                    var cCamera = (Camera)camera;
                    return cCamera;
                }
            }
            return null;
        }
</code></pre>

<p>I tried the below code, but it is firing an exception <strong>""Android.Hardware.Camera2.CameraAccessException: CAMERA_IN_USE (4): setTorchMode:1654:""</strong></p>

<p>Camera 2 API try to switch on flash along with Vision API</p>

<pre><code> public void TurnOnFlash()
        {
            var manager = (CameraManager)Context.GetSystemService(Context.CameraService);
            manager.RegisterTorchCallback(new FlashLightCallback(), null);
            manager.SetTorchMode(manager.GetCameraIdList()[0], true);
        }

        public void TurnOffFlash()
        {
            var manager = (CameraManager)Context.GetSystemService(Context.CameraService);
            manager.SetTorchMode(manager.GetCameraIdList()[0], false);
        }
</code></pre>

<p><strong>Also,</strong></p>

<p>How long Google provide access for deprecated API. Is it ok to go with deprecated API. Any thoughts on this? </p>",,0,1,,2018-11-01 14:20:00.040 UTC,,2019-04-28 19:46:17.050 UTC,,,,,958941,1,0,xamarin.android|android-camera|android-camera2|google-vision,125
Can Microsoft Cognitive APIs be used in local cloud,43875720,Can Microsoft Cognitive APIs be used in local cloud,"<p>I am working on running few examples on Microsoft Face API, I would like to know if there is a possibility that i can send the request to my local cloudlet to fetch and retrieve information instead of sending the request to Microsoft?</p>

<p>Thanks!</p>",43906063,1,0,,2017-05-09 16:56:25.697 UTC,,2017-05-11 03:32:15.950 UTC,,,,,7189498,1,0,cloud|face-recognition|microsoft-cognitive,103
Mock context.Context to test lambdacontext.FromContext,50490782,Mock context.Context to test lambdacontext.FromContext,"<p>I'm building an aws lambda using <a href=""https://github.com/aws/aws-sdk-go"" rel=""nofollow noreferrer"">aws-sdk-go</a> and <a href=""https://github.com/aws/aws-lambda-go"" rel=""nofollow noreferrer"">aws-lambda-go</a> and I'm stuck with a little problem.</p>

<p>I want to test my lambda handler. To do so, I need to mock a valid <a href=""https://golang.org/pkg/context/#Context"" rel=""nofollow noreferrer"">context.Context</a> containing valid attributes for <a href=""https://godoc.org/github.com/aws/aws-lambda-go/lambdacontext#LambdaContext"" rel=""nofollow noreferrer"">lamdacontext.LambdaContext</a> and satisfy <code>lambdacontext.FromContext</code>.</p>

<p>I cannot seem to find a way to build such mock, since <code>lambdacontext.FromContext</code> always returns me <code>_, false</code>.</p>

<p>Here's my main, with a simple handler on a <a href=""https://godoc.org/github.com/aws/aws-lambda-go/events#SNSEvent"" rel=""nofollow noreferrer"">events.SNSEvent</a> event:</p>

<pre><code>package main

import (
    ""context""
    ""github.com/aws/aws-lambda-go/events""
    ""github.com/aws/aws-lambda-go/lambda""
    ""github.com/aws/aws-lambda-go/lambdacontext""
)

func main() {
    lambda.Start(handleRequest)
}

func handleRequest(ctx context.Context, snsEvent events.SNSEvent) error {
    lc, ok := lambdacontext.FromContext(ctx); if !ok {
        // Always false
        ...
        return someErr
    }
    . . .
    return nil
}
</code></pre>

<p>And here's my test for <code>handleRequest</code>:</p>

<pre><code>package main

import (
    ""context""
    ""github.com/aws/aws-lambda-go/events""
    ""github.com/aws/aws-lambda-go/lambdacontext""
    ""github.com/stretchr/testify/assert""
    ""gitlab.easy-network.it/meg/aml-rekognition/testdata""
    ""testing""
)

const imgMock = `
{
  \""some_parameter\"": \""some_value\""
}`

func TestHandleRequest(t *testing.T) {

    c := context.Background()

    ctxV := context.WithValue(c, """", map[string]interface{}{
        ""AwsRequestID"" : ""some_aws_id"",
        ""InvokedFunctionArn"" : ""some_arn"",
        ""Identity"" : lambdacontext.CognitoIdentity{},
        ""ClientContext"" : lambdacontext.ClientContext{},
    })

    snsEventMock := events.SNSEvent{
        Records: []events.SNSEventRecord{
            {
                SNS: events.SNSEntity{
                    Message: imgMock,
                },
            },
        },
    }

    err := handleRequest(ctxV, snsEventMock)
    assert.NoError(t, err)

}
</code></pre>

<p>I also tried other mocks like passing it a struct with these parameters etc, but I always get <code>false</code>. For instance, I tried also:</p>

<pre><code>type TestMock struct {
    AwsRequestID string
    InvokedFunctionArn string
    Identity lambdacontext.CognitoIdentity
    ClientContext lambdacontext.ClientContext
}

func TestHandleRequest(t *testing.T) {

    c := context.Background()

    testMock := TestMock{
        AwsRequestID : ""some_aws_id"",
        InvokedFunctionArn : ""some_arn"",
        Identity : lambdacontext.CognitoIdentity{},
        ClientContext : lambdacontext.ClientContext{},
    }

    ctxV := context.WithValue(c, """", testMock)

    . . .

}
</code></pre>

<p>I checked out the source of <code>FromContext</code> and I've been scratching my head for a while.</p>

<pre><code>// LambdaContext is the set of metadata that is passed for every Invoke.
type LambdaContext struct {
    AwsRequestID       string
    InvokedFunctionArn string
    Identity           CognitoIdentity
    ClientContext      ClientContext
}

// An unexported type to be used as the key for types in this package.
// This prevents collisions with keys defined in other packages.
type key struct{}

// The key for a LambdaContext in Contexts.
// Users of this package must use lambdacontext.NewContext and 
lambdacontext.FromContext

// instead of using this key directly.
var contextKey = &amp;key{}

// FromContext returns the LambdaContext value stored in ctx, if any.
func FromContext(ctx context.Context) (*LambdaContext, bool) {
    lc, ok := ctx.Value(contextKey).(*LambdaContext)
    return lc, ok
}
</code></pre>

<p>Of course, it returns <code>false</code> even if I just pass a <code>context.Background()</code> to it.</p>

<p>Any idea on how should I build a valid <code>context.Context</code> to let <code>lambdacontext.FromContext</code> return <code>true</code>? </p>",50491002,1,0,,2018-05-23 14:18:25.140 UTC,,2018-05-23 14:39:23.090 UTC,2018-05-23 14:24:45.560 UTC,,4684539,,4684539,1,3,unit-testing|go|aws-lambda|aws-sdk-go|aws-lambda-go,805
Using Lambda to call Amazon Rekognition on uploaded images,44394496,Using Lambda to call Amazon Rekognition on uploaded images,"<p>Does anyone know if it is possible to use Lambda to call Rekognition functions when an image is uploaded to an S3 Bucket?</p>

<p>I am looking at integrating a Raspberry Pi device with a Pi Cam to take photos and do some face recognition.</p>",44396605,2,0,,2017-06-06 15:46:23.847 UTC,,2017-07-14 14:34:57.783 UTC,2017-06-07 00:09:09.193 UTC,,174777,,4269705,1,1,amazon-s3|amazon-rekognition,1612
Comparing two similar pictures to get similarity value,55211515,Comparing two similar pictures to get similarity value,"<p>I am trying to create my own app and I need to compare two pictures.</p>

<p>A bit of clarification.</p>

<ul>
<li><p>Picture will contain a symbol written on a piece of paper.</p></li>
<li><p>I will have the ""Original"" picture of piece of paper with symbol on it.</p></li>
<li><p>I need to compare newly captured picture of symbol to the Original picture and determine that if they are both same picture of same symbol.</p></li>
<li><p>The newly captured picture of symbol could be taken from a different angle.</p></li>
</ul>

<p>I looked at OpenCV and Google Vision , but I am a bit lost, on how to do it.</p>

<p>My Question</p>

<p>I have the ""Original"" picture like this
<a href=""https://i.stack.imgur.com/JSACc.jpg"" rel=""nofollow noreferrer"">Original picture</a></p>

<p>I have newly captured picture with same symbol on it, but take from a different angle, like this <a href=""https://i.stack.imgur.com/p1fUC.jpg"" rel=""nofollow noreferrer"">comparing image</a></p>

<p>I need to determine if they are ""same""(similar) or they are different.</p>

<p>Thank in advance.</p>",,1,4,,2019-03-17 20:20:18.883 UTC,,2019-03-18 22:58:58.053 UTC,,,,,7338118,1,1,image|opencv|image-processing|similarity|google-vision,53
Improving Computer Vision descriptions and tags,38984901,Improving Computer Vision descriptions and tags,"<p>I have been testing the Microsoft Computer Vision API with some pictures I took and it is not able to properly identify what I am uploading. Is there a way I could teach it what I am uploading is?</p>

<p>My tests have been using <a href=""https://www.microsoft.com/cognitive-services/en-us/computer-vision-api"" rel=""nofollow noreferrer"">https://www.microsoft.com/cognitive-services/en-us/computer-vision-api</a>.</p>

<p>A sample image:</p>

<p><a href=""https://i.stack.imgur.com/w7OOD.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w7OOD.jpg"" alt=""enter image description here""></a></p>

<p>It should include ""bottle"" in the tags at the very least.</p>",39047218,1,0,,2016-08-16 21:39:28.650 UTC,,2016-08-19 20:31:21.453 UTC,2016-08-17 13:35:02.647 UTC,,1983809,,1983809,1,0,computer-vision,53
Rekognition add identifier for user face being added to a collection,48868820,Rekognition add identifier for user face being added to a collection,"<p>I'm using Rekognition for face authentication.</p>

<p>When I register a user I have</p>

<ol>
<li>their user id</li>
<li>multiple photos of that user</li>
</ol>

<p>How can I associate/label all those photos with that id when indexing/adding their faces to a collection?</p>

<p>When I search by face in a collection I want to be able to get back their id.</p>",,1,0,,2018-02-19 15:00:23.763 UTC,,2018-02-19 18:35:51.577 UTC,,,,,6034824,1,1,amazon-web-services|amazon-rekognition,193
"AS rekognition in Kotlin, problem with the client",54306129,"AS rekognition in Kotlin, problem with the client","<p>I'm trying the Rekognition API from aws in my new Android app in kotlin, but when I try to create the client, my app crash.
I put the json file in the raw folder.
This is my code: </p>

<pre><code> private var credentialsProvider: AWSCredentialsProvider? = null
private var awsConfiguration: AWSConfiguration? = null

//==============================================================================================
// Activity Methods
//==============================================================================================

/**
 * Initializes the UI and initiates the creation of a face detector.
 */
public override fun onCreate(icicle: Bundle?) {
    super.onCreate(icicle)
    setContentView(R.layout.main)

    AWSMobileClient.getInstance().initialize(this) {
        credentialsProvider = AWSMobileClient.getInstance().credentialsProvider
        awsConfiguration = AWSMobileClient.getInstance().configuration
        Log.d(TAG, ""AWSMobileClient is initialized"")
        setCollectionImage()
    }.execute()
}
private fun getBitmapFromAsset(strName: String): Bitmap? {
    val assetManager = this.assets
    val istr: InputStream
    var bitmap: Bitmap? = null
    try {
        istr = assetManager.open(strName)
        bitmap = BitmapFactory.decodeStream(istr)
    } catch (e: IOException) {
    }
    return bitmap
}

private fun getBytesFromBitmap(bitmap: Bitmap?): ByteArray {
    val stream = ByteArrayOutputStream()
    bitmap!!.compress(Bitmap.CompressFormat.JPEG, 70, stream)
    return stream.toByteArray()
}

private fun setCollectionImage() {
    val rekognitionClient = AmazonRekognitionClient(credentialsProvider!!.credentials)
    //val client = AmazonRekognitionClient(credentialsProvider!!.credentials)
    val pictureByteArray = getBytesFromBitmap(getBitmapFromAsset(""IMG_20180611_125536.jpg""))

    val collectionId = ""ConstanceID""
    val imageByteBuffer = ByteBuffer.wrap(pictureByteArray)
    val personName = ""Constance"" // nom de la personne qu'on souhaite sauvegarder
    val request = IndexFacesRequest()
            .withCollectionId(collectionId)
            .withExternalImageId(personName)
            .withImage(Image().withBytes(imageByteBuffer))
    val result = rekognitionClient.indexFaces(request)
    val recorded = result.faceRecords.isEmpty().not()
}
</code></pre>",,0,1,,2019-01-22 10:24:33.990 UTC,,2019-01-22 10:48:27.027 UTC,2019-01-22 10:48:27.027 UTC,,7409774,,6709759,1,0,android|amazon-web-services|kotlin|amazon-rekognition,34
"Azure OCR unable to detect roman character ""I"" and ""II""",55296808,"Azure OCR unable to detect roman character ""I"" and ""II""","<p>I have this image <a href=""https://i.stack.imgur.com/YUepY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YUepY.png"" alt=""enter image description here""></a>.</p>

<p>I am using Azure Computer Vision API - v2.0,
combination of Recognize Text API(POSt) and Get Recognize Text Operation Result(GET) as mentioned in <a href=""https://westcentralus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/587f2c6a154055056008f200"" rel=""nofollow noreferrer"">https://westcentralus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/587f2c6a154055056008f200</a>. to detect text characters in the image.</p>

<p>Currently it is able to detect all the characters except letter I and II.
Can someone help?</p>",,0,2,,2019-03-22 09:43:05.783 UTC,,2019-03-22 09:48:52.860 UTC,2019-03-22 09:48:52.860 UTC,,5086352,,5086352,1,0,azure|text|computer-vision|ocr,15
Google cloud vision web detection API returns only 10 responses,48548552,Google cloud vision web detection API returns only 10 responses,<p>I am using Google cloud vision web detection API for detecting where the images have been used. But I always get 10 responses maximum even for Google's logo. Is it limit of the API or I am missing something because there is nothing mentioned in documentation.</p>,48562794,1,0,,2018-01-31 17:55:43.073 UTC,,2018-02-01 12:27:52.410 UTC,,,,,2802393,1,2,google-cloud-platform|google-vision,368
AWS Rekognition Javascript SDK - UnknownError: Bad Request,51109673,AWS Rekognition Javascript SDK - UnknownError: Bad Request,"<p>I am attempting to use AWS Rekognition API through the AWS Javascript SDK and am receiving <code>UnknownError: Bad Request</code> when I attempt to start any of their detections services. I want to run label detection, but have the same error while attempting the others, such as celebrity face detection. I have made sure that my account has access to the Rekognition API and made sure that my credentials are correct (or at least the same credentials work for S3 attached to the same account). </p>

<p><strong>My Code</strong></p>

<pre><code>var aws = require('aws-sdk');
const region = 's3-us-west-2';
const bucket = process.env.S3_VIDEO_BUCKET;
var requesttoken = randString(10) //generates random string
var key = 'path/to/key.mp4';

// used to check for keys to available files
var service = new aws.S3({
  accessKeyId: process.env.ACCESS_KEY,
  secretAccessKey: process.env.ACCESS_SECRET,
  region: region,
  endpoint: 'https://'+region+'.amazonaws.com/',
})

var rekognition = new aws.Rekognition({
  accessKeyId: process.env.ACCESS_KEY,
  secretAccessKey: process.env.ACCESS_SECRET,
  region: region,
  endpoint: 'https://'+region+'.amazonaws.com/',
  apiVersion: '2016-06-27'
})

// check that key is reachable in S3
this.service.getObject({Bucket:bucket,Key:key}, function(err, data){
  if(err){
    console.error(err, err.stack)
  } else {
    console.log(data)
  }
})

var params = {
  Video: {
    S3Object: {
      Bucket: bucket,
      Name: key
    }
  },
  ClientRequestToken: requesttoken,
  NotificationChannel: {
    RoleArn: 'arn:aws:sns:us-west-2:000000000:example',
    SNSTopicArn: 'example'
  }
};

rekognition.startContentModeration(params, (err,data)=&gt;{
  if (err) {console.log(err, err.stack); return;};
  console.log(data)
})
</code></pre>

<p>When I run this code I get </p>

<pre><code>{ UnknownError: Bad Request
at Request.extractError 
(/Users/username/Desktop/project_directory/node_modules/aws-sdk/lib/protocol/json.js:48:27)
at Request.callListeners (/Users/username/Desktop/project_directory/node_modules/aws-sdk/lib/sequential_executor.js:105:20)
at Request.emit (/Users/username/Desktop/project_directory/node_modules/aws-sdk/lib/sequential_executor.js:77:10)
at Request.emit (/Users/username/Desktop/project_directory/node_modules/aws-sdk/lib/request.js:683:14)
at Request.transition (/Users/username/Desktop/project_directory/node_modules/aws-sdk/lib/request.js:22:10)
at AcceptorStateMachine.runTo (/Users/username/Desktop/project_directory/node_modules/aws-sdk/lib/state_machine.js:14:12)
at /Users/username/Desktop/project_directory/node_modules/aws-sdk/lib/state_machine.js:26:10
at Request.&lt;anonymous&gt; (/Users/username/Desktop/project_directory/node_modules/aws-sdk/lib/request.js:38:9)
at Request.&lt;anonymous&gt; (/Users/username/Desktop/project_directory/node_modules/aws-sdk/lib/request.js:685:12)
at Request.callListeners (/Users/username/Desktop/project_directory/node_modules/aws-sdk/lib/sequential_executor.js:115:18)
message: 'Bad Request',
code: 'UnknownError',
statusCode: 400,
time: 2018-06-29T21:08:27.184Z,
requestId: '2809C3770B525EF0',
retryable: false,
retryDelay: 57.00430269412582 }
</code></pre>

<p><strong>My Question(s)</strong></p>

<p>Is anyone familiar enough with the Rekognition API/AWS in conjunction with the JS SDK that they know what Bad Request might be indicating in this circumstance? Is there somewhere in the AWS Documentation that explains what this error might indicate?</p>",51110045,1,0,,2018-06-29 21:35:04.600 UTC,,2018-06-29 22:33:50.693 UTC,2018-06-29 22:33:50.693 UTC,,8341311,,8341311,1,1,javascript|node.js|amazon-web-services|aws-sdk|amazon-rekognition,174
Firebase MLkit not detecting barcodes from existing file/jpg,55957522,Firebase MLkit not detecting barcodes from existing file/jpg,"<p>I am trying to allow users to ""import"" an existing picture from their phones local filesystem that will be scanned for barcodes. I am able to get the uri's of the image but once I try to pass it to MLKit no barcodes are detected even though the original image is of a barcode. </p>

<p>I have worked on this pretty extensively and done quite a bit of googling but have found no solution. At first I thought the issue might be with rotation since firebase seems to be pretty finicky with things not rotated the correct way. So I tried manually rotating the image (in code and also by editing with google photos). I tried resizing the photos (both dimension and compression) and changing the jpg to png. I have tried using both firebase vision methods that seem applicable to this use case [FirebaseVisionImage.fromBitmap(bitmap); and FirebaseVisionImage.fromFilePath(context, uri);] I also tried using the regular google vision library which is supposed to be rotation agnostic. I have even tried using a cropping library in case I was somehow managing to resize and compress wrong.</p>

<p>I am testing this on a google pixel. None of the above produce any results. There are no errors. The array that returns detected barcodes is just empty. The documentation seems to be very vague in terms of if there are specific sizes (dimensions or file size) that work better than others or file types that are better. An additional piece of the puzzle that is really confusing me is that a screenshot of a barcode will work but any photo from the actual camera will not.</p>",,0,0,,2019-05-02 17:31:30.857 UTC,,2019-05-02 17:31:30.857 UTC,,,,,3326587,1,0,java|android|firebase|barcode-scanner|firebase-mlkit,26
japanese encoder in response from google cloud vision OCR,56285629,japanese encoder in response from google cloud vision OCR,"<p>I am using <a href=""https://cloud.google.com/vision/docs/ocr"" rel=""nofollow noreferrer"">google vision API for OCR</a> to detect Japanese texts in the image. The response from Google contains texts like this: ""text"": ""\u5065\u5eb7\u4fdd\u967a.""
I don't know which ""encoder"" Google is using for encoding japanese texts, UTF-8 or Unicode?</p>",,0,0,,2019-05-24 03:52:27.480 UTC,,2019-05-24 03:52:27.480 UTC,,,,,9625489,1,0,ocr|cjk|google-cloud-vision|encoder,13
How to work Google Vision Api with Php library by using uri?,44794681,How to work Google Vision Api with Php library by using uri?,"<p>I am working on Google Vision Api and I know how to send a request by using any image uri below.</p>

<pre><code>`{
  ""requests"":[

    {
      ""image"":{
        ""source"":{
          ""imageUri"":
            ""https://fallaviblob.blob.core.windows.net/createdblobs/20170601_191635_237.png""
        }
      },
      ""features"":[
        {
          ""type"":""LABEL_DETECTION""
        },
        {
          ""type"":""WEB_DETECTION""
        }
      ]
    }
  ]
}`
</code></pre>

<p>I want to use send request by using Php Client Library. When I look at the sample codes on Google Vision Documentation, it shows only sample for Local images of Remote images that work on Google Cloud Storage. </p>

<p><a href=""https://cloud.google.com/vision/docs/detecting-labels#vision-label-detection-gcs-php"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/detecting-labels#vision-label-detection-gcs-php</a></p>

<p>As you can see my json request, I need to use Php Client library with remote url but I couldn't. </p>

<p>Thank you.</p>",,1,0,,2017-06-28 05:59:44.860 UTC,,2017-06-28 06:05:41.177 UTC,,,,,2834196,1,1,php|google-cloud-storage|google-vision,1580
Force Google Vision API to detect dates / numbers,55019870,Force Google Vision API to detect dates / numbers,"<p>I'm trying to detect handwritten dates using the Google Vision API. Do you know if it is possible to force it to detect dates (DD/MM/YYYY), or at least numbers only to increase reliablity?</p>

<p>The function I use, takes an Image as np.array as input:</p>

<pre><code>def detect_handwritten_text(img):
""""""Recognizes characters using the Google Cloud Vision API.
Args:
    img(np.array) = The Image on which to apply the OCR.

Returns:
    The recognized content of img as string.
""""""

from google.cloud import vision_v1p3beta1 as vision
client = vision.ImageAnnotatorClient()

# Transform np.array image format into vision api readable byte format
sucess, encoded_image = cv.imencode('.png', img)
content = encoded_image.tobytes()

# Configure client to detect handwriting and load picture
image = vision.types.Image(content=content)
image_context = vision.types.ImageContext(language_hints=['en-t-i0-handwrit'])

response = client.document_text_detection(image=image, image_context=image_context)
return response.full_text_annotation.text
</code></pre>",,1,0,,2019-03-06 09:41:37.863 UTC,,2019-03-23 08:57:20.663 UTC,,,,,11096978,1,0,computer-vision|google-vision|handwriting-recognition,32
recognize playing cards in an image,47744054,recognize playing cards in an image,"<p>i'm trying to recognize <a href=""http://www.worldofmunchkin.com/moregoodcards/img/cards.jpg"" rel=""nofollow noreferrer"">munchkin cards</a> from the card game. i've been trying to use a variety of image recognition APIs(google vision api, vize.ai, azure's computer vision api and more), but none of them seem to work ok.<br>
they're able to recognize one of the cards when only one appears in the demo image, but when both appear with another one it fails to identify one or the other.<br>
i've trained the APIs with a set of about 40 different images per card, with different angles, backgrounds and lighting.<br>
i've also tried using ocr(via google vision api) which works only for some cards, probably due to small letters and not much details on some cards. 
Does anyone know of a way i can teach one of these APIs(or another) to read these cards better? or perhaps recognize cards in a different way?    </p>

<p>the outcome should be a user capturing an image while playing the game and have the application understand which cards he has in front of him and return the results.<br>
thank you.</p>",,3,1,,2017-12-10 22:16:19.887 UTC,,2018-11-13 16:51:36.257 UTC,,,,,9081158,1,0,computer-vision|image-recognition,1232
Extract Text from an image using Google Cloud Vision API using cv2 in python,56216376,Extract Text from an image using Google Cloud Vision API using cv2 in python,"<p>We are trying to extract the text from an image using google-cloud-vision API:</p>

<pre><code>import io
import os
from google.oauth2 import service_account
from google.cloud import vision

# The name of the image file to annotate (Change the line below 'image_path.jpg' ******)
path = os.path.join(os.path.dirname(__file__), '3.jpg') # Your image path from current directory 


client = vision.ImageAnnotatorClient()

with io.open(path, 'rb') as image_file:
    content = image_file.read()

image = vision.types.Image(content=content)

response = client.text_detection(image=image)
texts = response.text_annotations
print('Texts:')

for text in texts:
    print(format(text.description))
</code></pre>

<p>In this code, we need to make the API read the image through the 'cv2' function only, instead of using the 'io' function:</p>

<pre><code># Read image file
    with io.open(img_path, 'rb') as image_file:
        content = image_file.read()
</code></pre>

<p>Any suggestion will be helpful</p>",56216832,1,2,,2019-05-20 07:44:29.290 UTC,,2019-05-20 08:19:52.343 UTC,2019-05-20 08:19:52.343 UTC,,10908769,,9934344,1,0,python|python-3.x|google-cloud-platform|google-cloud-vision,28
Using Google Vision annotation in a streaming fashion,52468352,Using Google Vision annotation in a streaming fashion,"<p>Trying to use Google`s CloudVision <a href=""https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate"" rel=""nofollow noreferrer"" title=""annotate"">images.annotate</a> with Akka-HTTP, images coming as streams, http chunked responses. Instead of collecting the whole image, then encoding it with Base64 and only then sending JSON request for annotation, I encode image chunks and send encoded chunks concatenated in the request. </p>

<p>Couldn't find an existing Akka-Streams ready fast implementation for encoding streams with Base64. Fortunately, Base64 is designed to be OK with decoding concatenated encoded parts of the original sequence. But CloudVision doesn't accept that:</p>

<pre><code>base64 --version
#base64 (GNU coreutils) 8.30

ipfs get QmfUQT8FnKY4ZbymvfEMPptFughPgcVi9ahUbrPzhBeavb --output base64_encoded_chunks
ipfs get QmfQpPo5ag4dRtuB9uJLwyc8Z3W3vQr5qMRud4YB2KZFWp --output original_image
base64 -w 0 original_image &gt; base64_encoded_whole

# now both base64 encoded files can be decoded into original file contents:
cat base64_encoded_chunks| base64 -d | shasum
# 7c446d90e78baf9511fbf6e9cfad80139f7b708e  -
cat base64_encoded_whole | base64 -d | shasum 
# 7c446d90e78baf9511fbf6e9cfad80139f7b708e  -
shasum original_image                     
# 7c446d90e78baf9511fbf6e9cfad80139f7b708e  original_image

echo '{""requests"":[{""image"":{""content"": ""' &gt; ./request_prefix
echo '""},""features"":[{""type"":""LABEL_DETECTION""}]}]}' &gt; ./request_suffix

ANNOTATE_URL=https://vision.googleapis.com/v1/images:annotate
API_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
annotate(){ cat request_prefix $1 request_suffix | curl -H ""Content-Type: application/json"" ""$ANNOTATE_URL?key=$API_KEY"" --data-binary @- }

#using the sequence encoded 'in one go' with base64 utility 
#it's accepted by Google Vision, getting a response with labels
annotate base64_encoded_whole
#  ""responses"": [
#    {
#      ""labelAnnotations"": [
#        {
#          ""mid"": ""/m/03q69"",
#          ""description"": ""hair"",
#          ""score"": 0.9692406,
#          ""topicality"": 0.9692406
#        },
#   ...

#using concatenated base64-encoded chunks of the original sequence
#despite the fact that it's decoded exactly to original sequence with base64 utility
#Google Vision can't decode it
annotate base64_encoded_chunks
#{
#  ""error"": {
#    ""code"": 400,
#    ""message"": ""Invalid value at 'requests[0].image.content' (TYPE_BYTES), Base64 decoding failed for \""\n/9j/4AAQSkZJRgABAQEAYABgAA...
</code></pre>

<p>I'm aware of workarounds like ""don't stream that"" or ""adapt a Base64 implementation to Akka-Streams"", but the question is:</p>

<p><strong>[Q]</strong> Is it some limitation/bug of Base64 decoding in CloudVision, or is my way of using Base64 wrong?</p>",,0,3,,2018-09-23 17:01:55.163 UTC,,2018-09-23 17:01:55.163 UTC,,,,,408165,1,0,bash|stream|base64|akka-stream|google-cloud-vision,91
How to perform face recognition on a streaming video using amazon Rekognition?,56313325,How to perform face recognition on a streaming video using amazon Rekognition?,<p>I am streaming video the amazon kinesis from raspberry pi (This is done). Now i want to perform face detection/recognition on that video using amazon Rekognition how to do it explain in detail with links. Thanks</p>,,1,2,,2019-05-26 12:04:41.497 UTC,,2019-05-27 01:18:34.740 UTC,,,,,11438404,1,0,amazon-web-services|amazon-kinesis|amazon-rekognition,19
Azure Custom Vision: Is it possible to store image metadata (NOT tags) in the Custom Vision environment?,56143548,Azure Custom Vision: Is it possible to store image metadata (NOT tags) in the Custom Vision environment?,"<p>I have images with important file metadata (e.g. provenance and processing history) stored locally or in Azure blob storage.</p>

<p>I would like to import (POST) these to the Azure Custom Vision environment (via the API or GUI) (see e.g. <a href=""https://southcentralus.dev.cognitive.microsoft.com/docs/services/Custom_Vision_Training_3.0"" rel=""nofollow noreferrer"">https://southcentralus.dev.cognitive.microsoft.com/docs/services/Custom_Vision_Training_3.0</a>) for training while (i) retaining those image metadata and (ii) being able to retrieve them via (a) the Custom Vision API and (b) the Custom Vision GUI.</p>

<p>An example use case would be to purge images of a certain provenance from the Custom Vision store because of a GDPR-related customer request [Aside: I appreciate that Azure Cognitive Services can anyway use the data for improving their models etc.].</p>

<p>As far as I can tell the only way to reference an image POSTed to Custom Vision is via its UUID. Is there any other way to reference metadata stored with that image or:</p>

<ol>
<li><p>Would that constitute a feature request?</p></li>
<li><p>Could the image metadata be stored inside the image (e.g. JPEG EXIF) (assuming it is possible to retrieve the image itself from the Custom Vision ""environment"", which it may not be)?</p></li>
<li><p>Otherwise, is the only solution to store the returned Custom Vision image UUID in a database elsewhere alongside the required metadata?</p></li>
</ol>

<p>NB In the above, by metadata I do <strong>not</strong> mean tags/labels in the image model-side sense, but rather data-side file metadata.</p>

<p>[Note that Azure Cognitive Services is using stackoverflow for Q&amp;A, so this question is I believe appropriate for stackoverflow.]</p>

<p>Thanks as ever!</p>",56149635,1,0,,2019-05-15 07:11:57.507 UTC,,2019-05-15 12:43:12.550 UTC,,,,,1021819,1,0,azure|azure-cognitive-services|microsoft-custom-vision,28
OCR with Google Cloud Vision python API,44845273,OCR with Google Cloud Vision python API,"<p>I am using the Google Cloud Vision Python API for performing OCR, in order to extract info from a document, like an ID proof. Is there a way to crop the image in such a way that only the part with concentrated text is retained? I tried using cropHint but it simply eliminates the borders. </p>

<p>The function in my code is somewhat like:</p>

<pre><code>def detect_text(path):

    """"""Detects text in the file.""""""

    vision_client = vision.Client()

    with io.open(path, 'rb') as image_file:
        content = image_file.read()

    image = vision_client.image(content=content)

    texts = image.detect_text()
</code></pre>",,1,0,,2017-06-30 11:20:56.660 UTC,,2017-07-03 07:57:05.620 UTC,2017-07-03 07:57:05.620 UTC,,8182876,,8182876,1,0,python|ocr|google-cloud-vision,724
Char's bounding box order of vertices,44635222,Char's bounding box order of vertices,"<p>Google Vision API documentation states that vertices of detected characters will always be in the same order:</p>

<pre><code>// The bounding box for the symbol.
// The vertices are in the order of top-left, top-right, bottom-right,
// bottom-left. When a rotation of the bounding box is detected the rotation
// is represented as around the top-left corner as defined when the text is
// read in the 'natural' orientation.
// For example:
//   * when the text is horizontal it might look like:
//      0----1
//      |    |
//      3----2
//   * when it's rotated 180 degrees around the top-left corner it becomes:
//      2----3
//      |    |
//      1----0
//   and the vertice order will still be (0, 1, 2, 3).
</code></pre>

<p>However sometimes I can see a different order of vertices. Here is an example of two characters from the same image, which have the same orientation:</p>

<pre><code>[x:778 y:316  x:793 y:316  x:793 y:323  x:778 y:323 ]
0----1
|    |
3----2
</code></pre>

<p>and</p>

<pre><code>[x:857 y:295  x:857 y:287  x:874 y:287  x:874 y:295 ]
1----2
|    |
0----3
</code></pre>

<p>Why order of vertices is not the same? and not as in documentation?</p>",44657614,2,0,,2017-06-19 16:06:25.443 UTC,1,2018-01-26 08:22:59.857 UTC,,,,,622894,1,2,google-cloud-platform|ocr|google-cloud-vision,335
I get an JSON decode error when using Python and Google Vision to detect text on PDF file,55714798,I get an JSON decode error when using Python and Google Vision to detect text on PDF file,"<p>I am trying to work with Google Vision and Python.  I am using the sample files but I keep getting the same error message:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Program Files (x86)\Python37-32\lib\site-packages\google\protobuf\jso
n_format.py"", line 416, in Parse
    js = json.loads(text, object_pairs_hook=_DuplicateChecker)
  File ""C:\Program Files (x86)\Python37-32\lib\json\__init__.py"", line 361, in l
oads
    return cls(**kw).decode(s)
  File ""C:\Program Files (x86)\Python37-32\lib\json\decoder.py"", line 338, in de
code
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""C:\Program Files (x86)\Python37-32\lib\json\decoder.py"", line 356, in ra
w_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""sample.py"", line 72, in &lt;module&gt;
    async_detect_document('gs://matr/file_1035.pdf','gs://matr/output/')
  File ""sample.py"", line 59, in async_detect_document
    json_string, vision.types.AnnotateFileResponse())
  File ""C:\Program Files (x86)\Python37-32\lib\site-packages\google\protobuf\jso
n_format.py"", line 418, in Parse
    raise ParseError('Failed to load JSON: {0}.'.format(str(e)))
google.protobuf.json_format.ParseError: Failed to load JSON: Expecting value: li
ne 1 column 1 (char 0).
</code></pre>

<p>I am guessing it has something to do with the resulting JSON file.  It does produce a JSON file but i guess it should print it out to the command line.  Here are the first few lines of the JSON file:</p>

<pre><code>{
    ""inputConfig"": {
        ""gcsSource"": {
            ""uri"": ""gs://python-docs-samples-tests/HodgeConj.pdf""
        },
        ""mimeType"": ""application/pdf""
    },
</code></pre>

<p>I resulting file does load into a JSON object by using </p>

<pre><code>data = json.load(jsonfile)
</code></pre>

<p>I have tried <code>print (json_string)</code> but I only get <code>b'placeholder'</code></p>

<p>How can I get this to work?  I am using Python 3.7.2</p>

<p>My code is below:</p>

<pre><code>def async_detect_document(gcs_source_uri, gcs_destination_uri):
    """"""OCR with PDF/TIFF as source files on GCS""""""
    from google.cloud import vision
    from google.cloud import storage
    from google.protobuf import json_format
    import re
    # Supported mime_types are: 'application/pdf' and 'image/tiff'
    mime_type = 'application/pdf'

    # How many pages should be grouped into each json output file.
    batch_size = 2

    client = vision.ImageAnnotatorClient()

    feature = vision.types.Feature(
        type=vision.enums.Feature.Type.DOCUMENT_TEXT_DETECTION)

    gcs_source = vision.types.GcsSource(uri=gcs_source_uri)
    input_config = vision.types.InputConfig(
        gcs_source=gcs_source, mime_type=mime_type)

    gcs_destination = vision.types.GcsDestination(uri=gcs_destination_uri)
    output_config = vision.types.OutputConfig(
        gcs_destination=gcs_destination, batch_size=batch_size)

    async_request = vision.types.AsyncAnnotateFileRequest(
        features=[feature], input_config=input_config,
        output_config=output_config)

    operation = client.async_batch_annotate_files(
        requests=[async_request])

    print('Waiting for the operation to finish.')
    operation.result(timeout=180)

    # Once the request has completed and the output has been
    # written to GCS, we can list all the output files.
    storage_client = storage.Client()

    match = re.match(r'gs://([^/]+)/(.+)', gcs_destination_uri)
    bucket_name = match.group(1)
    prefix = match.group(2)

    bucket = storage_client.get_bucket(bucket_name=bucket_name)

    # List objects with the given prefix.
    blob_list = list(bucket.list_blobs(prefix=prefix))
    print('Output files:')
    for blob in blob_list:
        print(blob.name)

    # Process the first output file from GCS.
    # Since we specified batch_size=2, the first response contains
    # the first two pages of the input file.
    output = blob_list[0]

    json_string = output.download_as_string()
    response = json_format.Parse(
        json_string, vision.types.AnnotateFileResponse())

    # The actual response for the first page of the input file.
    first_page_response = response.responses[0]
    annotation = first_page_response.full_text_annotation

    # Here we print the full text from the first page.
    # The response contains more information:
    # annotation/pages/blocks/paragraphs/words/symbols
    # including confidence scores and bounding boxes
    print(u'Full text:\n{}'.format(
        annotation.text))

async_detect_document('gs://my_bucket/file_1035.pdf','gs://my_bucket/output/')
</code></pre>",,1,2,,2019-04-16 18:49:01.030 UTC,,2019-04-29 17:16:31.487 UTC,2019-04-16 19:48:35.443 UTC,,2047723,,2047723,1,0,python|json|google-vision,46
working with google cloud vision api,38914432,working with google cloud vision api,"<p>I am trying to use google cloud vision api, I am trying to use the label detection in order to retrieve the tags of images.</p>

<p>First of all, I want to send http request to the api locally to see that all its ok.
In the end I would like to deploy my application to AWS EC2.</p>

<p>My question is: if there is any problem to use vision api if I am using AWS EC2?</p>

<p>I am asking that because I am a little bit confuse what to do with the authentication of vision api, I mean that as I see in vision api documentation there are some ways to authenticate my application, I followed the instructions in the documentation and set the credentials in google cloud console and did like this: </p>

<p>""
The environment variable GOOGLE_APPLICATION_CREDENTIALS is checked. If this variable is specified it should point to a file that defines the credentials. The simplest way to get a credential for this purpose is to create a Service account key in the Google API Console:</p>

<ol>
<li>Go to the API Console Credentials page.</li>
<li>From the project drop-down, select your project.</li>
<li>On the Credentials page, select the Create credentials drop-down, then select Service account key.</li>
<li>From the Service account drop-down, select an existing service account or create a new one.</li>
<li>For Key type, select the JSON key option, then select Create. The file automatically downloads to your computer.</li>
<li>Put the *.json file you just downloaded in a directory of your choosing. This directory must be private (you can't let anyone get access to this), but accessible to your web server code.</li>
<li>Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file downloaded.
""</li>
</ol>

<p>This is the right thing to do in my case?</p>

<p>Thanks very much!</p>",,0,2,,2016-08-12 09:23:50.137 UTC,0,2016-08-12 09:23:50.137 UTC,,,,,6282367,1,0,amazon-web-services|amazon-ec2|google-cloud-vision,465
AWS Rekognition returning invalid signature Angular,50628383,AWS Rekognition returning invalid signature Angular,"<p>Using the AWS SDK for Javascript within Angular5 I'm seeing the following error when running DetectLabels or DetectFaces and returning to promise. When I print the return within the Detect function everything looks correct. The error only pops up when attempting to return the result into the promise. </p>

<pre><code>""core.js:1449 ERROR Error: Uncaught (in promise): 
InvalidSignatureException: The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.
InvalidSignatureException: The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.
</code></pre>

<p>I've confirmed the account seems to be working as expected as the log within the callback prints successfully and the bucket is in the same region as the rekognition. Anyone seen this before?</p>

<pre><code>const rekognition = new Rekognition(
  {
    accessKeyId: this.accessKeyId,
    secretAccessKey: this.secretAccessKey,
    region: this.region,
    signatureCache: false,
    signatureVersion: 'v4'
  }
);

const req = rekognition.detectLabels(params, function(err, data) {
  if (err) {
    console.log(err, err.stack); // an error occurred
    return null;
  }
    // response prints to console successfully
    console.log(JSON.stringify(data, null, '\t'));           

  });

  req.promise().then(data =&gt; {
    console.log(data);    //Throws Exception
  });

}
</code></pre>

<p>**** Workaround (working)</p>

<p>aws.service</p>

<pre><code>rekogDetechLabels(): AWS.Request&lt;Rekognition.DetectLabelsResponse, AWS.AWSError&gt; {

const params = {
  Image: {
    S3Object: {
      Bucket: this.bucket,
      Name: this.fileName
   }
  }
};

const rekognition = new Rekognition(
  {
    accessKeyId: this.accessKeyId,
    secretAccessKey: this.secretAccessKey,
    region: this.region,
    signatureCache: false,
    signatureVersion: 'v4'
  }
);

return rekognition.detectLabels(params, function(err, data) {
  if (err) {
    console.log(err, err.stack); // an error occurred
    return false;
  }
});
</code></pre>

<p>}</p>

<p>app.component</p>

<pre><code>// Label Rekognition
    const req = this.aws.rekogDetechLabels()
      .on('success', response =&gt; {
      console.log(response.data);

      this.labels = (&lt;Rekognition.DetectLabelsResponse&gt;response.data).Labels;
    }).on('error', err =&gt; {
      console.log('Error with Rekognition');
    });
</code></pre>",,0,4,,2018-05-31 15:58:46.140 UTC,,2018-06-02 20:34:56.203 UTC,2018-06-02 20:34:56.203 UTC,,5683318,,5683318,1,0,angular|amazon-web-services|typescript|promise|aws-sdk,95
Tracking movie titles using Google Cloud Vision API,48892816,Tracking movie titles using Google Cloud Vision API,"<p>I'm trying to find a way to crop/obtain offsets of Movie titles via Google Cloud Vision API.</p>

<p>Here's an example image: <a href=""https://imgur.com/A6J2VhA.jpg"" rel=""nofollow noreferrer"">https://imgur.com/A6J2VhA.jpg</a></p>

<p>I've tried to use FACE_DETECTION, LOGO_DETECTON, and event LABEL_DETECTION but I can't seem to get a result for it.</p>

<p>Any ideas?</p>",,1,0,,2018-02-20 19:28:39.227 UTC,,2018-02-22 23:23:56.610 UTC,,,,,9312635,1,0,python|google-cloud-vision|movies,58
Google Cloud Vision OCR API returning incorrect values for bounding box/vertices,46244980,Google Cloud Vision OCR API returning incorrect values for bounding box/vertices,"<p>I'm using the ""TEXT_DETECTION"" option from the Google Cloud Vision API to OCR some images. </p>

<p>The bounding box around individual characters is sometimes accurate and sometimes not, often within the same image.  </p>

<p>Is this a normal side-effect of a probabilistic nature of the vision algorithm, a bug in the Vision API, or of course an issue with how I'm interpreting the response?</p>

<p><a href=""https://i.stack.imgur.com/1yldI.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1yldI.jpg"" alt=""Image annotated with text and bounding boxes from Google Vision OCR API""></a></p>

<p><a href=""https://i.stack.imgur.com/lldVf.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lldVf.jpg"" alt=""The letter &quot;a&quot; with poor bounding box""></a></p>

<p>Here's the portion of the response specific to the letter ""a"" from which I'm extracting the bounding box.</p>

<pre><code>stdClass Object
(
    [property] =&gt; stdClass Object
        (
            [detectedLanguages] =&gt; Array
                (
                    [0] =&gt; stdClass Object
                        (
                            [languageCode] =&gt; en
                        )

                )

        )

    [boundingBox] =&gt; stdClass Object
        (
            [vertices] =&gt; Array
                (
                    [0] =&gt; stdClass Object
                        (
                            [x] =&gt; 419
                            [y] =&gt; 304
                        )

                    [1] =&gt; stdClass Object
                        (
                            [x] =&gt; 479
                            [y] =&gt; 304
                        )

                    [2] =&gt; stdClass Object
                        (
                            [x] =&gt; 479
                            [y] =&gt; 397
                        )

                    [3] =&gt; stdClass Object
                        (
                            [x] =&gt; 419
                            [y] =&gt; 397
                        )

                )

        )

    [text] =&gt; a
)
</code></pre>",,1,0,,2017-09-15 17:38:57.210 UTC,2,2017-09-15 21:19:49.333 UTC,2017-09-15 21:19:49.333 UTC,,4175515,,3092947,1,4,api|bounding-box|vertices|google-cloud-vision,565
Using Google Vision's detect_text on image with text in different directions,42122978,Using Google Vision's detect_text on image with text in different directions,"<p>So what I've recently discovered while playing with Google's Vision API for Python is that the method detect_text will only give me text aligned in a certain direction (probably decided by highest scoring text).  Is there a parameter or request variable I can set to tell it to give me all text regardless of direction?  There isn't much for documentation on anything, and the response parameters they show in walkthroughs don't match what is returned in the EntityAnnotation object I get back from the detect_text API call.</p>",,0,0,,2017-02-08 20:28:13.033 UTC,,2017-02-10 07:52:06.953 UTC,2017-02-10 07:52:06.953 UTC,,7535259,,7535259,1,1,python|google-api-python-client|google-vision,55
Using Cloud Vision REST Api for blur detection instead of Firebase vision api,56001610,Using Cloud Vision REST Api for blur detection instead of Firebase vision api,"<p>In my application I need to detect blurriness of images. </p>

<p>So firstly , is it possible using Google Cloud Vision api which I think is as given here 
<em><a href=""https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate</a></em></p>

<p>Also most of the applications on cloud vision are available to Android development through Firebase ML vision apis with the following dependency</p>

<p><em>com.google.firebase:firebase-ml-vision:18.0.2</em></p>

<p>I did tried FirebaseVision's VisionFaceDetector and even TextRecognizer in my app but it wasn't useful considering the it didn't returned anything like </p>

<p>""joyLikelihood"": enum(Likelihood),
  ""sorrowLikelihood"": enum(Likelihood),
  ""angerLikelihood"": enum(Likelihood),
  ""surpriseLikelihood"": enum(Likelihood),
  ""underExposedLikelihood"": enum(Likelihood),
  <strong>""blurredLikelihood"": enum(Likelihood)</strong>,
  ""headwearLikelihood"": enum(Likelihood)</p>

<p>I know it could not be the best way to achieve the requirement through Cloud Vision or firebase vision api's but any help or way forward is anticipated.</p>

<p><strong>Note :</strong> Plus it Java client Library for Cloud Vision are not supported in Android.</p>

<p><a href=""https://i.stack.imgur.com/Du4st.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Du4st.png"" alt=""enter image description here""></a></p>

<p>Thanks</p>",,0,1,,2019-05-06 08:40:08.587 UTC,,2019-05-06 09:26:56.223 UTC,2019-05-06 09:26:56.223 UTC,,1518273,,1518273,1,0,android|firebase|blur|google-cloud-vision|firebase-mlkit,35
Extract faces from video using AWS Rekognition,47919331,Extract faces from video using AWS Rekognition,"<p>With AWS Rekognition I was able to get faces detected in a mp4 video with the following nodejs,</p>

<pre><code>var AWS = require(""aws-sdk"");                                                                                              
AWS.config.update({                                                                                                        
  region: ""us-east-1""                                                                                                      
});                                                                                                                        

var rekognition = new AWS.Rekognition();                                                                                   



var params = {                                                                                                             
  Video: { /* required */                                                                                                  
    S3Object: {                                                                                                            
      Bucket: 'videobucket',                                                                                          
      Name: 'testvideo.mp4'                                                                                                
    }                                                                                                                      
  },                                                                                                                       
  FaceAttributes: ""ALL"",                                                                                                   
  NotificationChannel: {                                                                                                   
    RoleArn: 'arn:aws:iam::xxx:role/xxx', /* required */                                                     
    SNSTopicArn: 'arn:aws:sns:us-east-1:xxx:alerts' /* required */                                                
  }                                                                                                                        
};                                                                                                                         
rekognition.startFaceDetection(params, function(err, data) {                                                               
  if (err) console.log(err, err.stack); // an error occurred                                                               
  else     console.log(data);           // successful response                                                             
});    
</code></pre>

<p>And was able to get the results with the following cli,</p>

<blockquote>
  <p>aws rekognition get-face-detection --job-id   xxxxxxx</p>
</blockquote>

<p>and outputs the faces in the following json format,</p>

<pre><code>{                                                                                                                          
    ""Faces"": [                                                                                                             
        {                                                                                                                  
            ""Timestamp"": 0,                                                                                                
            ""Face"": {                                                                                                      
                ""BoundingBox"": {                                                                                           
                    ""Width"": 0.029999999329447746,                                                                         
                    ""Top"": 0.2588889002799988,                                                                             
                    ""Left"": 0.29374998807907104,                                                                           
                    ""Height"": 0.052222222089767456                                                                         
                },                                                                                                         
                ""Landmarks"": [                                                                                             
                    {                                                                                                      
                        ""Y"": 0.28277161717414856,                                                                          
                        ""X"": 0.3052537739276886,                                                                           
                        ""Type"": ""eyeLeft""                                                                                  
                    },                                                                                                     
                    {                                                                                                      
                        ""Y"": 0.27957838773727417,                                                                          
                        ""X"": 0.3085327744483948,                                                                           
                        ""Type"": ""eyeRight""    
</code></pre>

<p>How to extract those faces as images and dump them in an s3 bucket?</p>

<p>Thanks</p>",52489330,2,4,,2017-12-21 06:41:35.153 UTC,,2018-09-25 01:22:12.197 UTC,,,,,299462,1,0,amazon-web-services|aws-cli|amazon-rekognition,642
Modify Amazon Rekognition limit detections,52365814,Modify Amazon Rekognition limit detections,"<p>Is possible to modify the maximum detections that Amazon Rekognition textDetection has? It only detects the first 50 occurences, but we need more (at least 60).</p>

<p>If not, do you have any idea of how to make a workaround?</p>

<p>Thanks!</p>",,0,1,,2018-09-17 10:36:44.427 UTC,,2018-09-17 10:36:44.427 UTC,,,,,911420,1,0,amazon-web-services|ocr|amazon-rekognition,81
Multiprocessing multiple images using Rekognition in Python,50829647,Multiprocessing multiple images using Rekognition in Python,"<p>I am trying to detect labels of multiple images using AWS Rekognition in Python.
This process requires around 3 seconds for an image to get labelled. Is there any way I can label these images in parallel? </p>

<p>Since I have restrained using boto3 sessions, please provide the code snippet, if possible.</p>",,1,1,,2018-06-13 05:31:37.140 UTC,,2018-06-13 13:40:23.547 UTC,,,,,7387210,1,0,python|amazon-web-services|boto3|python-multiprocessing|amazon-rekognition,178
Image Selection for Training Visual Recognition,40346408,Image Selection for Training Visual Recognition,"<p>I am training a classifier for recognizing certain objects in an image. I am using the Watson Visual Recognition API but I would assume that the same question applies to other recognition APIs as well.</p>

<p>I've collected 400 pictures of something - e.g. dogs.</p>

<p>Before I train Watson, I can delete pictures that may throw things off. Should I delete pictures of:</p>

<ol>
<li>Multiple dogs</li>
<li>A dog with another animal</li>
<li>A dog with a person</li>
<li>A partially obscured dog</li>
<li>A dog wearing glasses</li>
</ol>

<p>Also, would dogs on a white background make for better training samples?</p>

<p>Watson also takes negative examples. Would cats and other small animals be good negative examples? What else?</p>",,1,0,,2016-10-31 16:28:30.783 UTC,,2016-10-31 19:21:49.427 UTC,,,,,549312,1,2,tensorflow|image-recognition|visual-recognition|watson,166
Camera not showing only blank black i'm using google vision,47250386,Camera not showing only blank black i'm using google vision,"<p>hi guys i need your help. i made  app recognition Text from Camera using google vision i got error when i open my app, my camera doesn't work, it shows me black screen, and textview at bottom. how can i resolve it. </p>

<p>this is my code android manifest </p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
</code></pre>

<p></p>

<pre><code>&lt;uses-permission android:name=""android.permission.CAMERA""/&gt;

&lt;application
    android:allowBackup=""true""
    android:icon=""@mipmap/ic_launcher""
    android:label=""@string/app_name""
    android:roundIcon=""@mipmap/ic_launcher_round""
    android:supportsRtl=""true""
    android:theme=""@style/AppTheme""&gt;
    &lt;activity android:name="".WelcomeActivity""&gt;
        &lt;intent-filter&gt;
            &lt;action android:name=""android.intent.action.MAIN"" /&gt;

            &lt;category android:name=""android.intent.category.LAUNCHER"" /&gt;
        &lt;/intent-filter&gt;
    &lt;/activity&gt;
    &lt;activity
        android:name="".MainActivity""
        android:label=""@string/title_activity_main""
        android:theme=""@style/AppTheme""&gt;
        &lt;action android:name=""android.intent.action.MAIN"" /&gt;

        &lt;category android:name=""android.intent.category.DEFAULT"" /&gt;
    &lt;/activity&gt;


    &lt;meta-data android:name=""com.google.android.gms.vision.DEPENDENCIES""
        android:value=""ocr""/&gt;
&lt;/application&gt;
</code></pre>

<p></p>

<p>this is my mainactivity code </p>

<pre><code>public class MainActivity extends AppCompatActivity {

SurfaceView cameraView;
TextView textView;
CameraSource cameraSource;
final int RequestCameraPermissionID = 1001;


@Override
public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) {
    switch (requestCode) {
        case RequestCameraPermissionID: {
            if (grantResults[0] == PackageManager.PERMISSION_GRANTED) {
                if (ActivityCompat.checkSelfPermission(this, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {
                    return;
                }
                try {
                    cameraSource.start(cameraView.getHolder());
                } catch (IOException e) {
                    e.printStackTrace();
                }

            }
        }
        break;
    }
}

@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.activity_main);

    cameraView = (SurfaceView) findViewById(R.id.surface_view);
    textView = (TextView) findViewById(R.id.text_view);

    TextRecognizer textRecognizer = new TextRecognizer.Builder(getApplicationContext()).build();
    if (!textRecognizer.isOperational()) {
        Log.w(""MainActivity"", ""Detector dependencies are not yet available"");
    } else {

        cameraSource = new CameraSource.Builder(getApplicationContext(), textRecognizer)
                .setFacing(CameraSource.CAMERA_FACING_BACK)
                .setRequestedPreviewSize(1280, 1024)
                .setRequestedFps(2.0f)
                .setAutoFocusEnabled(true)
                .build();
        cameraView.getHolder().addCallback(new SurfaceHolder.Callback() {
            @Override
            public void surfaceCreated(SurfaceHolder surfaceHolder) {

                try {
                    if (ActivityCompat.checkSelfPermission(getApplicationContext(), Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {

                        ActivityCompat.requestPermissions(MainActivity.this,
                                new String[]{Manifest.permission.CAMERA},
                                RequestCameraPermissionID);
                        return;
                    }
                    cameraSource.start(cameraView.getHolder());
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }

            @Override
            public void surfaceChanged(SurfaceHolder surfaceHolder, int i, int i1, int i2) {

            }

            @Override
            public void surfaceDestroyed(SurfaceHolder surfaceHolder) {
                cameraSource.stop();
            }
        });

        textRecognizer.setProcessor(new Detector.Processor&lt;TextBlock&gt;() {
            @Override
            public void release() {

            }

            @Override
            public void receiveDetections(Detector.Detections&lt;TextBlock&gt; detections) {

                final SparseArray&lt;TextBlock&gt; items = detections.getDetectedItems();
                if(items.size() != 0)
                {
                    textView.post(new Runnable() {
                        @Override
                        public void run() {
                            StringBuilder stringBuilder = new StringBuilder();
                            for(int i =0;i&lt;items.size();++i)
                            {
                                TextBlock item = items.valueAt(i);
                                stringBuilder.append(item.getValue());
                                stringBuilder.append(""\n"");
                            }
                            textView.setText(stringBuilder.toString());
</code></pre>

<p>im already debuging etc but still not work 
thanks for help me guys</p>",,0,0,,2017-11-12 15:15:22.250 UTC,,2018-01-12 13:49:42.603 UTC,,,,,8111471,1,2,android|camera|vision|google-vision,111
How to extract name and address from packing labels using regex in python?,46814207,How to extract name and address from packing labels using regex in python?,"<p>I have scanned image and extracted text from image using Google Vision Api. Now i'm facing problem in extracting name and address from scanned text.Through some regex i'm able to detect street code and zipcode from text but not whole address and name.</p>

<pre><code>    def find_between_r(s, first, last):
            try:
                start = s.index(first) + len(first)
                end = s.index(last, start)
                return s[start:end]
            except ValueError:
                return """"
text="""""" 17000 AJHshkjadj dakd ext ESTES RICHMOND VA 23230 On Coll UNIFORM STRAIGHT BILL OF LADING - Original - Not Negotiable - Short Form (EXLA, 3901 WEST BROAD STREET Date 10/12/2017 OBOL No W093556Shippers No P.0. No 16846 very shipments, the letters 'COD appear befo For Payment Bill To Bill being paid by Shipper Consignee ENCANTADA RESORT Ken Smith 407-997-3731 Sp tio 3070 SECRET LAKE DR Ruwes Turiff EXLA 105. KISSIMMEE FL 34747 Shipper WINDWARD DESIGN NO ACCESSORIAL SERVICESADDED WITHOUT PRIOR APPROVAL FROM WINDWARD 941-359-0890 1130 COMMERCE BLVD N SARASOTA FL 34243 ird Is M trial E cy P le # O00 000 0000 NOTE: Liability Limitation for loss or damage on this shipment may be applicable. See 49 U.S.C. 14706 (c)(1XA) and (B No. Pkgs HMI Kind or Peak ange. Description of Articles, Special Marks and Exceptions NMFC Declared Valius TW. (Sub Com) | Chass/Rate Ohk22 CT PATIO FURNITURE 1400 200.0 22 1400 Quote# 4867496 APPOINTMENT CHARGE LIFTGATE DELIVERY CHARGE Rade doled val jeclared Excess WARNING Additional dam Mc LiabiRef IOS the d rges Advanced S Received $. to apply NOTE: Wh JOTE: Commodi requnng speci Subject to Sect 7 of Condit this shipmentrequired specific handling marked and to be delivered to the consignee without recourse wning the agreed or declared ith ordinary ignor, the ignor shall sign the property. The agreed or declared value See Sec NMFC 360. follohereby sp Sally stated by the The fibe booxes used thms shipment Innke del shipp 6pecif forth in the box Lake itbour payme freight and all other lawfuland all other repair Consolidated US NMFC | charge the shi PT is byBill of Lacing shall in the prepayment of the charges on the property described hereof. igned, destination RECEIVED. ly determined have been agroced upo icable otherwise the classif and rules (EstaExpros Linehave been castablished by shippes. stThe property described abovein apparent good ords, and codi of packages unknown) rked, ared destined otion said Juleotherwis Ily agreed. property 1y porton desertion and as to cocb party a serty, that every performed thereunder shall be sult all the 1d oCodhi Bill of Lacing the National Motor Freight Cassifit 100-X and also agreed liable or any consequental damages arising from the delaysery dates (Subject of any app! Gold M Service Ageroement) SHIPPER CERTIFICATION CARRIER CERTIFICATION Gignatueits agreement to all o orching to the applicable regulations Express Lines-EXLA ized Signature Date (Dae Iolel who ret TPMLD Colon coDfee &amp; Shipper O Collect On Delivery C.OD, Amount Certified Check Freight Charges are PREPAID unless marked collect CHECK BOX to be paid by { Consignee Consignee Check Accepted IF COLLECT Mark Ig the PLTS STC PC and Loose Place Guaranteed Sticker Here Tsos ITIL TS Elections or AO eControl IDOT- Pro# 000000028388396 PAGE 624318 O489b24AL8""""""

    data=[]
    Ship_Cons = re.findall(r'\b(?=SHIP|Ship|SHIPPER|Shipper|ONSIGNEE|Onsignee|CONSIGNEE|Consignee|FROM|TO).*',value)
    val="" "".join(map(str,Ship_Cons))
    zip_code = re.findall(
                    r""((?=AL|AK|AS|AZ|AR|CA|CO|CT|DE|DC|FM|FL|GA|GU|HI|ID|IL|IN|IA|KS|KY|LA|ME|MH|MD|MA|MI|MN|MS|MO|MT|NE|NV|NH|NJ|NM|NY|""
                    r""NC|ND|MP|OH|OK|OR|PW|PA|PR|RI|SC|SD|TN|TX|UT|VT|VI|VA|WA|WV|WI|WY)[A-Z]{2}[, ])""
                    r""(\d{5}(?:-\d{4})?|\d{4}(?:-\d{4})?|\d{3}(?:\s\d{2})|\d{3}(?:\s\d{1}\s\d{1})""
                    r""|\d{2,5}(?:\s\d{2,5})(?:-\d{4})?)"",val)
                # print(zip_code)
    for item in zip_code:
       data.append("""".join(item))
    address = re.findall(r""\s\d{4}\s|\w*[a-z]\s\w*[a-z]\s\d{4}\s|\s\d{5}\s"",val)
    print(""Address"",address)
    print(print(find_between_r(val,address[0],data[0])))
</code></pre>

<p>I'm getting </p>

<pre><code>SECRET LAKE DR Ruwes Turiff EXLA 105. KISSIMMEE
</code></pre>

<p>as output from above code. How to avoid unncessary value like Turiff EXLA 105. and address and not able to get name also.Can anyone help me to solve this. Thank you</p>",,0,4,,2017-10-18 15:44:43.700 UTC,,2017-10-18 15:44:43.700 UTC,,,,,6461035,1,0,python|regex|google-vision,103
Google Vision FaceTracker shows a black screen,43534783,Google Vision FaceTracker shows a black screen,"<p>Can someone try <a href=""https://github.com/googlesamples/android-vision/tree/master/visionSamples/FaceTracker"" rel=""nofollow noreferrer"">Google VisionAPI FaceTracker</a> and see if it works?</p>

<p>Here's the <a href=""https://developers.google.com/vision/android/face-tracker-tutorial"" rel=""nofollow noreferrer"">official page</a>.</p>

<p>All I get when I try running it is a black screen (after fixing <a href=""https://stackoverflow.com/questions/43532713/googles-visionapi-example-facetracker-camera-permission"">one error</a>). I don't get any errors in the logs either.</p>",43550936,1,0,,2017-04-21 05:45:50.510 UTC,,2017-04-21 20:13:36.157 UTC,2017-05-23 11:54:34.610 UTC,,-1,,7868376,1,0,android|android-camera|google-vision,368
TEXT_DETECTION ignoring / eliminating words,40372942,TEXT_DETECTION ignoring / eliminating words,"<p>I am experimenting with the Google Vision API text detection feature, and trying to perform OCR on text images. The text images are quite clean and it works 80% of the times. The 20% of errors include misinterpreted numbers / characters (fixable), and some words / numbers that simply don't show up (not fixable!).</p>

<p>I followed the best practices page tips (image is 1024x768, 16-bit PNG) with no avail.</p>

<p>Here is an example: this sample page
<a href=""https://storage.googleapis.com/ximian-cloud.appspot.com/sample_page.png"" rel=""nofollow noreferrer"">https://storage.googleapis.com/ximian-cloud.appspot.com/sample_page.png</a></p>

<p>Has a number 177 (Under observations, right of ""RT ARM"") and this is not detected at all by the API ...</p>

<p>I tried:</p>

<ul>
<li>Twice the resolution (2048 x 1536)</li>
<li>BMP 24-bit </li>
<li>BMP 32-bit </li>
<li>All of the above, in grayscale </li>
<li>All of the above, inverted (black background and white letters)</li>
</ul>

<p>No luck ...</p>

<p>Any hint on why this is happening? Is it the API or my image format could use some formatting?</p>",,1,2,,2016-11-02 05:17:35.147 UTC,,2016-11-30 21:13:03.850 UTC,2016-11-06 08:30:14.980 UTC,,322020,,399302,1,2,google-cloud-platform|google-cloud-vision,181
How can I set timeout for Google's Vision API,47361521,How can I set timeout for Google's Vision API,"<p>I'm using Python to make a query to Google's Vision API to obtain labels from an image, but I'm not able to set a timeout in case I don't receive a response within a given time.</p>

<p>I'm using the following code based on <a href=""https://media.readthedocs.org/pdf/gax-python/latest/gax-python.pdf"" rel=""nofollow noreferrer"">Google's Documentation</a> of CallOptions.</p>

<p>This is my code:</p>

<pre><code>class GoogleQuery():

def __init__(self, VisionTools):
    self.client = vision.ImageAnnotatorClient()
    self.QueryOptions = google.gax.CallOptions(timeout=0.1)

... more init fields

def QueryImage(self, frame):
    image = types.Image(content=frame)

    # Make query to Google
    response = self.client.label_detection(image=image, options=self.QueryOptions)
</code></pre>

<p>I have tried passing directly the arguments into the call to Google without success, like this:</p>

<pre><code>    def QueryImage(self, frame):

        # Convert frame to a type compatible with Google API
        image = types.Image(content=frame)

        # Make query to Google
        o1 = CallOptions(timeout = 0.1)
        response = self.client.label_detection(image=image, options=(o1))
</code></pre>",,0,0,,2017-11-18 01:22:03.377 UTC,,2017-11-18 01:22:03.377 UTC,,,,,2142740,1,1,python|google-vision,261
how to get image of scanned barcode in vision libarary android,35651277,how to get image of scanned barcode in vision libarary android,"<p>I successfully implemented vision library by Google sample and it successfully scanned bar codes and returns a string. I also want bar-code image so my question is how to get image of bar-code or a preview image? </p>

<p>Note: Code is based on git hub sample of google vision library.</p>",,0,5,,2016-02-26 12:06:07.727 UTC,,2016-02-27 08:46:37.810 UTC,2016-02-27 08:46:37.810 UTC,,5832311,,5902175,1,3,android|google-play-services|qr-code|android-vision,437
Amazon Rekognition print result gives IndexError,49955157,Amazon Rekognition print result gives IndexError,"<p>I am trying to print out results from an Amazon Rekognition call, but it returns the error: </p>

<blockquote>
  <p>list index out of range: IndexError</p>
  
  <p>Traceback (most recent call last):  File
  ""/var/task/lambda_function.py"", line 57, in lambda_handler</p>
  
  <p>time = response['Persons'][0]['Timestamp']</p>
  
  <p>IndexError: list index out of range</p>
</blockquote>

<p>I put the index[0], I don't really see why it will happen to out of index range. </p>

<p>Can any one help please?</p>

<pre><code>response = get_face_search(jobID)
time = response['Persons'][0]['Timestamp']
print(time)

#below is the format:
--------------------------------------------
{
'JobStatus': 'IN_PROGRESS'|'SUCCEEDED'|'FAILED',
'StatusMessage': 'string',
'NextToken': 'string',
'VideoMetadata': {
    'Codec': 'string',
    'DurationMillis': 123,
    'Format': 'string',
    'FrameRate': ...,
    'FrameHeight': 123,
    'FrameWidth': 123
},
'Persons': [
    {
        'Timestamp': 123,
        'Person': {
            'Index': 123,
            'BoundingBox': {
                'Width': ...,
                'Height': ...,
                'Left': ...,
                'Top': ...
            },
            'Face': {
                'BoundingBox': {
                    'Width': ...,
                    'Height': ...,
                    'Left': ...,
                    'Top': ...
                },
                'AgeRange': {
                    'Low': 123,
                    'High': 123
                },
                'Smile': {
                    'Value': True|False,
                    'Confidence': ...
                    },
                ],

                'Pose': {
                    'Roll': ...,
                    'Yaw': ...,
                    'Pitch': ...
                },
                'Quality': {
                    'Brightness': ...,
                    'Sharpness': ...
                },
                'Confidence': ...
            }
        },
        'FaceMatches': [
            {
                'Similarity': ...,
                'Face': {
                    'FaceId': 'string',
                    'BoundingBox': {
                        'Width': ...,
                        'Height': ...,
                        'Left': ...,
                        'Top': ...
                    },
                    'ImageId': 'string',
                    'ExternalImageId': 'string',
                    'Confidence': ...
                }
            },
        ]
    },
]
}
</code></pre>",,1,1,,2018-04-21 11:12:01.593 UTC,1,2018-04-22 00:23:28.023 UTC,2018-04-22 00:14:54.350 UTC,,174777,,7028560,1,0,python|amazon-web-services|aws-lambda|amazon-rekognition,133
Face tracker with 3D filters in android,50035205,Face tracker with 3D filters in android,"<p>My application requires a face tracking with 3D filters module just like snapchat. After all searching i am able to track faces with all landmarks using <a href=""https://github.com/googlesamples/android-vision"" rel=""nofollow noreferrer"">Google Vision Api</a> . Although i am able to set some 2D filters on faces but could not find a way to apply 3D filters. Does anyone have some solution to my problem??</p>",,0,2,,2018-04-26 05:07:19.413 UTC,1,2018-05-24 05:47:08.837 UTC,2018-05-24 05:47:08.837 UTC,,9446581,,7772528,1,0,android|augmented-reality|android-augmented-reality,75
Unable to import Google Vision API with Angular,52541577,Unable to import Google Vision API with Angular,"<p>I am trying to create an angular project with Google Vision, but angular refuses to compile it. Here's my app.component.ts file</p>

<pre><code>import { Component } from '@angular/core';
import * as vision from '@google-cloud/vision';

@Component({
  selector: 'app-root',
  templateUrl: './app.component.html',
  styleUrls: ['./app.component.scss']
})
export class AppComponent {
  title = 'Hello';
    ngOnInit() {
        const client = new vision.ImageAnnotatorClient();        
    }
}
</code></pre>

<p>And here's the error I am getting when I build the application.</p>

<pre><code>ERROR in ./node_modules/@grpc/grpc-js/build/src/channel.js
Module not found: Error: Can't resolve '../../package' in 'f:\temp\project\node_modules\@grpc\grpc-js\build\src'
</code></pre>

<p>Any help would be greatly appreciated, thanks in advance.</p>",,1,4,,2018-09-27 16:39:19.103 UTC,,2018-09-27 17:53:42.327 UTC,,,,,10425386,1,1,angular|google-vision,202
Microsoft Face API - faceId value for the same person is different with each API call,44893985,Microsoft Face API - faceId value for the same person is different with each API call,"<p>I'm using the Microsoft Face API to track people in front of a webcam by sending a screenshot from the camera to the API every second or so</p>

<p>If a particular person is in front of the camera for multiple API calls, the API should return the same faceId for that person in each response, but it is returning a new faceId for that person instead. This makes it impossible for me to know whether there is a new person in front of the camera, or a different person</p>

<p>This was not the case a couple of weeks ago, it's just something which has started happening recently</p>

<p>The parameters that I'm sending are...</p>

<pre><code>returnFaceId:true,returnFaceLandmarks:false,returnFaceAttributes:age,gender
</code></pre>

<p>... the gender and age detection are working fine, it's just the faceId that I'm having problems with</p>

<p>Is there a limit to how many faceIds it'll assign per month or something? I can't find any reference to a limit in the documentation</p>",,1,0,,2017-07-03 21:19:45.730 UTC,,2017-07-13 05:36:22.437 UTC,2017-07-03 21:26:59.427 UTC,,138378,,138378,1,1,microsoft-cognitive|face-api,526
Library not found after installing Socket.IO-swift,47436472,Library not found after installing Socket.IO-swift,"<p>I am using the Socket IO-Swift library in my current project. After installing the Socket.IO-Client-Swift pod file into my project, my FMDB library path is not found, showing this error:</p>

<pre><code>library not found for -lAFNetworking
</code></pre>

<p>I'm using the following pods in my podfile: </p>

<pre><code>pod 'Socket.IO-Client-Swift', '~&gt; 12.1.0'
use_frameworks!

pod 'AFNetworking', '~&gt; 3.1.0'

 pod 'CocoaLumberjack', '~&gt; 3.2.0'

 pod 'MBProgressHUD', '~&gt; 1.0.0'

 pod 'SDWebImage', '~&gt; 4.0.0'

 pod 'Reachability', '~&gt; 3.2’

 pod 'IQKeyboardManager', '~&gt; 4.0.9’

 pod 'CrittercismSDK', '~&gt; 5.6.8’

 pod 'SocketIO’, '~&gt; 0.0.1’

 pod 'GPUImage’, '~&gt; 0.1.7’

 pod 'iCarousel’, '~&gt; 1.8.3’

 pod 'TwitterKit'

 pod 'GoogleMaps'

 pod 'GooglePlaces'

 pod 'GooglePlacePicker'

 pod 'Google/SignIn'

 pod 'Firebase/Core’

 pod 'Firebase/Crash'

 pod 'Fabric'

 pod 'Crashlytics'

 pod 'AWSCloudWatch'
 pod 'AWSCognito'
 pod 'AWSCognitoIdentityProvider'
 pod 'AWSDynamoDB'
 pod 'AWSEC2'
 pod 'AWSIoT'
 pod 'AWSKinesis'
 pod 'AWSLambda'
 pod 'AWSLex'
 pod 'AWSMachineLearning'
 pod 'AWSPinpoint'
 pod 'AWSPolly'
 pod 'AWSRekognition'
 pod 'AWSS3'
 pod 'AWSSES'
 pod 'AWSSimpleDB'
 pod 'AWSSNS'
 pod 'AWSSQS'
</code></pre>",,0,8,,2017-11-22 13:54:24.290 UTC,,2017-11-23 15:04:32.217 UTC,2017-11-23 15:04:32.217 UTC,,1025702,,4030535,1,0,ios|swift|xcode,137
Not able to load JS library with Webpack,47296283,Not able to load JS library with Webpack,"<p>I'm trying to authenticate google vision api with electron and it seems like the google vision api can't find the required libraries so it gives me these errors. </p>

<p><a href=""https://i.stack.imgur.com/Q8XzR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q8XzR.png"" alt=""enter image description here""></a></p>

<p>I added the node module using </p>

<pre><code>npm install --save @google-cloud/vision
</code></pre>

<p>and this is how I'm trying to initialize it</p>

<pre><code>var vision = require('@google-cloud/vision');

var visionClient = vision({
    keyFileName: './file.json',
    projectId: 'project-name'
});
</code></pre>",,0,3,,2017-11-14 22:27:09.137 UTC,,2017-11-15 03:37:08.170 UTC,2017-11-15 03:37:08.170 UTC,,1770483,,1770483,1,0,reactjs|webpack|electron,49
Send graphQL mutation based on DynamoDb stream with NodeJS,55151128,Send graphQL mutation based on DynamoDb stream with NodeJS,"<p>I'm trying to update a GraphQL subscription when a DynamoDb table receives a new row. I got the following code working with only the RekognitionId, but I'm not trying to send the entire NewImage object, and I cannot make it work. I get all sorts of type problems, but with no real information to solve it with. The most telling was: </p>

<p><code>""Error: GraphQL error: Variable 'input' has an invalid value. Expected type 'Map' but was 'String'. Variables for input objects must be an instance of type 'Map'.""</code></p>

<p>Unfortunately, I can't find a single reference to a GraphQL type called ""map"", so it's probably scrambled.</p>

<p>Does anyone have any experience of this? This is my Lambda function, like I said it worked with only RekognitionId formatted as a dynamoDb semi-json-string <code>{""S"": ""id""}</code></p>

<pre><code>global.WebSocket = require('ws');
require('es6-promise').polyfill();
require('isomorphic-fetch');
const AWS = require('aws-sdk');
const aws_exports = require('./aws-exports').default;
const AWSAppSyncClient = require('aws-appsync').default;

exports.handler = async (event, context, callback) =&gt; {
    if(!event.Records.length){
        console.log('no records');
        return false;
    }
    const AUTH_TYPE = require('aws-appsync/lib/link/auth-link').AUTH_TYPE;
    const url = aws_exports.ENDPOINT;
    const region = aws_exports.REGION;
    const type = AUTH_TYPE.AMAZON_COGNITO_USER_POOLS;

    AWS.config.update({
        region: aws_exports.REGION,
        credentials: new AWS.Credentials({
            accessKeyId: aws_exports.AWS_ACCESS_KEY_ID,
            secretAccessKey: aws_exports.AWS_SECRET_ACCESS_KEY
        })
    });
    const cognitoIdentityServiceProvider = new AWS.CognitoIdentityServiceProvider({ apiVersion: '2016-04-18' });

    const initiateAuth = async ({ clientId, username, password }) =&gt; cognitoIdentityServiceProvider.initiateAuth({
        AuthFlow: 'USER_PASSWORD_AUTH',
        ClientId: clientId,
        AuthParameters: {
            USERNAME: username,
            PASSWORD: password,
        },
    }).promise();
    const { AuthenticationResult } = await initiateAuth({
        clientId: '*******',
        username: '*******',
        password: '*******',
    });
    const accessToken = AuthenticationResult &amp;&amp; AuthenticationResult.AccessToken;

    // Import gql helper and craft a GraphQL query
    const gql = require('graphql-tag');
    const query = gql(`
    mutation faceAdded($input: UpdateFaceInput!) {
        updateFace(input: $input) { 
            RekognitionId
            Emotions {
                Confidence
                Type
            }
            AgeRange
            Beard
            Mustache
            Gender
            Eyeglasses
        }
    }`);
    // Set up Apollo client
    const client = new AWSAppSyncClient({
        url: url,
        region: region,
        auth: {
            type: type,
            jwtToken: accessToken,
        },
        disableOffline: true      //Uncomment for AWS Lambda
    });
    return runMutationInClient(client, query, event.Records[0], (res) =&gt; {
        return res;
    });
}

const runMutationInClient = async (client, query, RekognitionObj, callback) =&gt; {
    await client.hydrated().then( async (client) =&gt; {
        console.log('Running mutation in Lambda: ', query, 'RekognitionId: ', RekognitionObj);

        var inputObj = AWS.DynamoDB.Converter.unmarshall(RekognitionObj.dynamodb.NewImage)

        await client.mutate({
            mutation: query,
            variables: {
                input: {
                    RekognitionId: {""S"": inputObj.RekognitionId},
                    Emotion: {""L"": []}
                }
            },
            fetchPolicy: 'no-cache'
        }).then(function logData(data) {
            console.log('LAMBDA results of query: ', data);
            callback(data);
        })
        .catch((error)=&gt;{
            console.log('LAMBDA ERROR:', error);
            callback(error);
        });

    });
};
</code></pre>",,0,0,,2019-03-13 21:01:35.117 UTC,,2019-03-13 22:27:00.987 UTC,2019-03-13 22:27:00.987 UTC,,174777,,1832489,1,1,node.js|amazon-web-services|aws-lambda|amazon-dynamodb|graphql,54
How to Authenticate Google Vision/Cloud Using ENV Variable in Ruby on Rails,50159443,How to Authenticate Google Vision/Cloud Using ENV Variable in Ruby on Rails,"<p>My app is hosted on Heroku, so I'm trying to figure out how to use the JSON Google Cloud provides (to authenticate) as an environment variable, but so far I can't get authenticated.</p>

<p>I've searched Google and Stack Overflow and the best leads I found were: </p>

<p><a href=""https://stackoverflow.com/questions/42937145/google-vision-api-authentication-on-heroku"">Google Vision API authentication on heroku</a></p>

<p><a href=""https://stackoverflow.com/questions/35144135/how-to-upload-a-json-file-with-secret-keys-to-heroku"">How to upload a json file with secret keys to Heroku</a></p>

<p>Both say they were able to get it to work, but they don't provide code that I've been able to get work. Can someone please help me? I know it's probably something stupid.</p>

<p>I'm currently just trying to test the service in my product model leveraging this sample code from Google. Mine looks like this:</p>

<pre><code>def self.google_vision_labels
  # Imports the Google Cloud client library
  require ""google/cloud/vision""

  # Your Google Cloud Platform project ID
  project_id = ""foo""

  # Instantiates a client
  vision = Google::Cloud::Vision.new project: project_id

  # The name of the image file to annotate
  file_name = ""http://images5.fanpop.com/image/photos/27800000/FOOTBALL-god-sport-27863176-2272-1704.jpg""

  # Performs label detection on the image file
  labels = vision.image(file_name).labels

  puts ""Labels:""
  labels.each do |label|
    puts label.description
  end
 end
</code></pre>

<p>I keep receiving this error,</p>

<p><code>RuntimeError: Could not load the default credentials. Browse to
https://developers.google.com/accounts/docs/application-default-credentials for more information</code></p>

<p>Based on what I've read, I tried placing the JSON contents in secrets.yml (I'm using the Figaro gem) and then referring to it in a Google.yml file based on the answer in this SO question.</p>

<p>In <code>application.yml</code>, I put (I overwrote some contents in this post for security):</p>

<pre><code>GOOGLE_APPLICATION_CREDENTIALS: {
  ""type"": ""service_account"",
  ""project_id"": ""my_project"",
  ""private_key_id"": ""2662293c6fca2f0ba784dca1b900acf51c59ee73"",
  ""private_key"": ""-----BEGIN PRIVATE KEY-----\n #keycontents \n-----END PRIVATE KEY-----\n"",
  ""client_email"": ""foo-labels@foo.iam.gserviceaccount.com"",
  ""client_id"": ""100"",
  ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",
  ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",
  ""auth_provider_x509_cert_url"": 
  ""https://www.googleapis.com/oauth2/v1/certs"",
  ""client_x509_cert_url"": 
  ""https://www.googleapis.com/robot/v1/metadata/x509/get-product-labels%40foo.iam.gserviceaccount.com""
}
</code></pre>

<p>and in <code>config/google.yml</code>, I put:</p>

<pre><code>GOOGLE_APPLICATION_CREDENTIALS = ENV[""GOOGLE_APPLICATION_CREDENTIALS""]
</code></pre>

<p>also, tried:</p>

<pre><code>GOOGLE_APPLICATION_CREDENTIALS: ENV[""GOOGLE_APPLICATION_CREDENTIALS""]
</code></pre>

<p>I have also tried changing these variable names in both files instead of <code>GOOGLE_APPLICATION_CREDENTIALS</code> with <code>GOOGLE_CLOUD_KEYFILE_JSON</code> and <code>VISION_KEYFILE_JSON</code> based on this Google page.</p>

<p>Can someone please, please help me understand what I'm doing wrong in referencing/creating the environmental variable with the JSON credentials? Thank you!</p>",,0,2,,2018-05-03 16:04:24.417 UTC,,2018-05-03 16:04:24.417 UTC,,,,,994581,1,0,ruby-on-rails|ruby|google-cloud-platform|google-vision,257
"Azure Face API, python SDK attribute url",52818392,"Azure Face API, python SDK attribute url","<p>I'm using the Python SDK snippet provided by <a href=""https://docs.microsoft.com/en-us/azure/cognitive-services/face/tutorials/faceapiinpythontutorial"" rel=""nofollow noreferrer"">Azure docs.</a> </p>

<pre><code>BASE_URL =""https://eastus.api.cognitive.microsoft.com/face/v1.0/
CF.BaseUrl.set(BASE_URL)
</code></pre>

<p>I want to return face attributes, The docs <a href=""https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236"" rel=""nofollow noreferrer"">referenced here</a> suggest that adding </p>

<pre><code>/detect[&amp;returnFaceAttributes=age,gender]
</code></pre>

<p>To the Base URl will return age and gender attributes. It's throwing me an error, am I missing something? </p>

<p>This is my first time using Azure Face API. </p>",52831626,1,1,,2018-10-15 13:56:25.743 UTC,,2018-10-16 08:59:55.900 UTC,2018-10-16 08:59:55.900 UTC,,7005159,,9010692,1,0,python|azure|microsoft-cognitive|face-api,173
Google Vision API detects labels for only public images on GCP,47534024,Google Vision API detects labels for only public images on GCP,"<p>I am storing images on Google Cloud Storage and using Google Vision APIs to detect labels of those images. I use the same account and credentials for both purposes.
I am using the sample program given at:
 '<a href=""https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/vision/cloud-client/detect/detect.py"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/vision/cloud-client/detect/detect.py</a>' </p>

<p>I can successfully detect labels for the local images and images on internet which are publicly accessible.
When I use the following with a image stored in a bucket on my GCP storage, the program does not detect any labels unless I mark the data (image) as public.</p>

<p>e.g.</p>

<p>When it is private:</p>

<pre><code># ./detect.py labels-uri 
'https://www.googleapis.com/download/storage/v1/b/mybucket/o/Penguins.jpg?
generation=1510548912343529&amp;alt=media'
Labels:
</code></pre>

<p>When I mark it as 'public':</p>

<pre><code># ./detect.py labels-uri 
'https://www.googleapis.com/download/storage/v1/b/mybucket/o/Penguins.jpg?
generation=1510548912343529&amp;alt=media'
Labels:
penguin
bird
king penguin
flightless bird
beak
organism
</code></pre>

<p>I was expecting, since I am using the same credentials for the vision and storage API access, it should even work on my private images.</p>

<p>Can you help?</p>",47538253,1,0,,2017-11-28 14:30:23.347 UTC,,2017-11-28 18:10:39.170 UTC,,,,,8653998,1,0,google-cloud-storage|google-cloud-vision,314
AWS Rekognition limits on collections,45245748,AWS Rekognition limits on collections,"<p>If I was using <strong>IndexFaces</strong>, you need to supply a image and a collection id that will then add the faces in the image to the collection id specified. Lets say I gave a collection id on a collection that contains one million faces, which is the limit of collections in <code>AWS</code> <code>Rekognition</code>. Therefore adding more faces to this collection would throw an error (I think) cause then this would surpass the limit of one million faces in the collection. So I was wondering what error would be thrown by <strong>IndexFaces</strong> and/or how to tell on AWS rekognition the number of faces in my collection? I have listed the error list below for <strong>IndexFaces</strong> in case it helps.</p>

<blockquote>
  <p>AccessDeniedException You are not authorized to perform the action.</p>
  
  <p>ImageTooLargeException The input image size exceeds the allowed limit. For more information, see Limits in Amazon Rekognition.</p>
  
  <p>InternalServerError Amazon Rekognition experienced a service issue. Try your call again.</p>
  
  <p>InvalidImageFormatException The provided image format is not supported.</p>
  
  <p>InvalidParameterException Input parameter violated a constraint. Validate your parameter before calling the API operation again.</p>
  
  <p>InvalidS3ObjectException Amazon Rekognition is unable to access the S3 object specified in the request.</p>
  
  <p>ProvisionedThroughputExceededException The number of requests exceeded your throughput limit. If you want to increase this limit, contact Amazon Rekognition.</p>
  
  <p>ResourceNotFoundException Collection specified in the request is not found.</p>
  
  <p>ThrottlingException Amazon Rekognition is temporarily unable to process the request. Try your call again.</p>
</blockquote>",,1,1,,2017-07-21 19:55:18.503 UTC,,2017-11-22 10:55:51.367 UTC,2017-07-22 19:56:01.493 UTC,,7015400,,8342082,1,1,amazon-web-services|amazon-rekognition,433
Microsoft Face API - 400 Request Body is invalid,44465669,Microsoft Face API - 400 Request Body is invalid,"<p>I am using the Microsoft Face API to build a Facial recognition desktop app using Electron. I can right now detect a face and create a person group, but run into this error when I try and add a person to my person group:</p>

<pre><code>{""error"":{""code"":""BadArgument"",""message"":""Request body is invalid.""}}, 
</code></pre>

<p>which is marked as Error 400. Bad request on my console. </p>

<p>This is the <a href=""https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f3039523c"" rel=""nofollow noreferrer"">API</a> page on how to use this request: </p>

<p>Here is my code, obviously something is wrong with the Data field, but when I use the exact same data in the westCentralUS test server, it is successful. I have tried using and omitting the optional userData field, with a string and an image file. </p>

<pre><code>function createPerson() {

var params = {
        // Request parameters
    };

    $.ajax({
        url: ""https://westcentralus.api.cognitive.microsoft.com/face/v1.0/persongroups/students/persons"",
        beforeSend: function(xhrObj){
            // Request headers
            xhrObj.setRequestHeader(""Content-Type"",""application/json"");
            xhrObj.setRequestHeader(""Ocp-Apim-Subscription-Key"",apiKey);
        },
        type: ""POST"",
        // Request body
        data: { name: ""John"",}
    })
    .done(function(data) {
        alert(""success"");
    })
    .fail(function() {
        alert(""error"");
    });
}
</code></pre>",44467648,1,0,,2017-06-09 19:46:54.570 UTC,1,2017-06-12 09:21:57.837 UTC,2017-06-12 09:21:57.837 UTC,,5993438,,8134558,1,3,jquery|azure|electron|face-recognition|microsoft-cognitive,855
How to convert python code character to human readable code,52219324,How to convert python code character to human readable code,"<p>I am currently using Google Vision API in python to detect Chinese character in an image, but I found google will return python source code (Such as \xe7\x80\x86\xe7\xab\x91) instead of some human-readable string.</p>

<p>How can I convert it to human-readable text with utf-8 format?</p>

<hr>

<p>Thanks all of your answer, may be I post my code is more easily for all of you.
Here is my code, basically I try to convert the whole json return from GOOGLE Vision and save in a json file, however, it hasn't success. </p>

<p>try:
    code = requests.post('<a href=""https://vision.googleapis.com/v1/images:annotate?key="" rel=""nofollow noreferrer"">https://vision.googleapis.com/v1/images:annotate?key=</a>'+GOOGLE_API_KEY, data=params,headers=headers)</p>

<pre><code>resultText = code.text.encode(""utf-8"")
outputFileName = image_path.split('.',1)[0]
outputDataFile = open(outputFileName+"".json"", ""w"")
outputDataFile.write(json.dumps(resultText))
outputDataFile.close()
</code></pre>

<p>except requests.exceptions.ConnectionError:
    print('Request error')</p>

<p>Thank you</p>",,3,2,,2018-09-07 09:17:07 UTC,,2018-09-10 02:14:55.777 UTC,2018-09-10 01:19:06.467 UTC,,10329812,,10329812,1,1,python|google-vision,56
Face Tracker CameraSource Android: How to brighten front camera quality?,41766196,Face Tracker CameraSource Android: How to brighten front camera quality?,"<p>Face Tracker app based on <a href=""https://github.com/googlesamples/android-vision/tree/master/visionSamples/FaceTracker"" rel=""nofollow noreferrer"">Google Vision Face Tracker</a>. By default, Face Tracker use rear/back camera, but I want to detect faces with front camera.</p>

<p>This is the code for CameraSourcePreview that google vision provide:</p>

<pre><code>package com.google.android.gms.samples.vision.face.facetracker.ui.camera;

import android.content.Context;
import android.content.res.Configuration;
import android.util.AttributeSet;
import android.util.Log;
import android.view.SurfaceHolder;
import android.view.SurfaceView;
import android.view.ViewGroup;

import com.google.android.gms.common.images.Size;
import com.google.android.gms.vision.CameraSource;

import java.io.IOException;

public class CameraSourcePreview extends ViewGroup {
    private static final String TAG = ""CameraSourcePreview"";

    private Context mContext;
    private SurfaceView mSurfaceView;
    private boolean mStartRequested;
    private boolean mSurfaceAvailable;
    private CameraSource mCameraSource;

    private GraphicOverlay mOverlay;

    public CameraSourcePreview(Context context, AttributeSet attrs) {
        super(context, attrs);
        mContext = context;
        mStartRequested = false;
        mSurfaceAvailable = false;

        mSurfaceView = new SurfaceView(context);
        mSurfaceView.getHolder().addCallback(new SurfaceCallback());
        addView(mSurfaceView);
    }

    public void start(CameraSource cameraSource) throws IOException {
        if (cameraSource == null) {
            stop();
        }

        mCameraSource = cameraSource;

        if (mCameraSource != null) {
            mStartRequested = true;
            startIfReady();
        }
    }

    public void start(CameraSource cameraSource, GraphicOverlay overlay) throws IOException {
        mOverlay = overlay;
        start(cameraSource);
    }

    public void stop() {
        if (mCameraSource != null) {
            mCameraSource.stop();
        }
    }

    public void release() {
        if (mCameraSource != null) {
            mCameraSource.release();
            mCameraSource = null;
        }
    }

    private void startIfReady() throws IOException {
        if (mStartRequested &amp;&amp; mSurfaceAvailable) {
            mCameraSource.start(mSurfaceView.getHolder());
            if (mOverlay != null) {
                Size size = mCameraSource.getPreviewSize();
                int min = Math.min(size.getWidth(), size.getHeight());
                int max = Math.max(size.getWidth(), size.getHeight());
                if (isPortraitMode()) {
                    // Swap width and height sizes when in portrait, since it will be rotated by
                    // 90 degrees
                    mOverlay.setCameraInfo(min, max, mCameraSource.getCameraFacing());
                } else {
                    mOverlay.setCameraInfo(max, min, mCameraSource.getCameraFacing());
                }
                mOverlay.clear();
            }
            mStartRequested = false;
        }
    }

    private class SurfaceCallback implements SurfaceHolder.Callback {
        @Override
        public void surfaceCreated(SurfaceHolder surface) {
            mSurfaceAvailable = true;
            try {
                startIfReady();
            } catch (IOException e) {
                Log.e(TAG, ""Could not start camera source."", e);
            }
        }

        @Override
        public void surfaceDestroyed(SurfaceHolder surface) {
            mSurfaceAvailable = false;
        }

        @Override
        public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {
        }
    }

    @Override
    protected void onLayout(boolean changed, int left, int top, int right, int bottom) {
        int width = 640;
        int height = 480;
        if (mCameraSource != null) {
            Size size = mCameraSource.getPreviewSize();
            if (size != null) {
                width = size.getWidth();
                height = size.getHeight();
            }
        }

        // Swap width and height sizes when in portrait, since it will be rotated 90 degrees
        if (isPortraitMode()) {
            int tmp = width;
            width = height;
            height = tmp;
        }

        final int layoutWidth = right - left;
        final int layoutHeight = bottom - top;

        // Computes height and width for potentially doing fit width.
        int childWidth = layoutWidth;
        int childHeight = (int)(((float) layoutWidth / (float) width) * height);

        // If height is too tall using fit width, does fit height instead.
        if (childHeight &gt; layoutHeight) {
            childHeight = layoutHeight;
            childWidth = (int)(((float) layoutHeight / (float) height) * width);
        }

        for (int i = 0; i &lt; getChildCount(); ++i) {
            getChildAt(i).layout(0, 0, childWidth, childHeight);
        }

        try {
            startIfReady();
        } catch (IOException e) {
            Log.e(TAG, ""Could not start camera source."", e);
        }
    }

    private boolean isPortraitMode() {
        int orientation = mContext.getResources().getConfiguration().orientation;
        if (orientation == Configuration.ORIENTATION_LANDSCAPE) {
            return false;
        }
        if (orientation == Configuration.ORIENTATION_PORTRAIT) {
            return true;
        }

        Log.d(TAG, ""isPortraitMode returning false by default"");
        return false;
    }
}
</code></pre>

<p>I call camera source with this method:</p>

<pre><code>private void startCameraSource() {

        // check that the device has play services available.
        int code = GoogleApiAvailability.getInstance().isGooglePlayServicesAvailable(
                getApplicationContext());
        if (code != ConnectionResult.SUCCESS) {
            Dialog dlg =
                    GoogleApiAvailability.getInstance().getErrorDialog(this, code, RC_HANDLE_GMS);
            dlg.show();
        }

        if (mCameraSource != null) {
            try {
                mPreview.start(mCameraSource, mGraphicOverlay);
            } catch (IOException e) {
                Log.e(TAG, ""Unable to start camera source."", e);
                mCameraSource.release();
                mCameraSource = null;
            }
        }
    }
</code></pre>

<p>Face Tracker front camera still too dark compare with default phone camera app.</p>

<p>How to brighten front camera in face tracker google vision? Is it related with surface view?</p>

<pre><code>&lt;com.google.android.gms.samples.vision.face.facetracker.ui.camera.CameraSourcePreview
    android:id=""@+id/preview""
    android:layout_width=""match_parent""
    android:layout_height=""0dp""
    android:layout_weight=""1.00""
    android:weightSum=""1""&gt;

&lt;com.google.android.gms.samples.vision.face.facetracker.ui.camera.GraphicOverlay
    android:id=""@+id/faceOverlay""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""
    android:layout_weight=""0.79"" /&gt;

&lt;/com.google.android.gms.samples.vision.face.facetracker.ui.camera.CameraSourcePreview&gt;
</code></pre>",,1,0,,2017-01-20 14:41:35.453 UTC,1,2018-07-08 07:13:28.103 UTC,2017-01-20 14:47:13.200 UTC,,4670082,,4670082,1,3,android|android-studio|camera|google-vision|front-camera,692
Google Vision API does not recognize glasses 100%,44195115,Google Vision API does not recognize glasses 100%,"<p>I have one project that integrates with Google vision APIs.</p>

<p>I found that some photo images with wearing glasses, the Google Vision APIs can not detect at all. For my case I need to proof that every photos uploaded must not contain any glasses. <a href=""https://i.stack.imgur.com/JWY5Y.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JWY5Y.jpg"" alt=""enter image description here""></a></p>

<p>This image, it seems that the Google Vision API can not detect wearing glasses at all.</p>",,0,2,,2017-05-26 06:34:21.020 UTC,,2017-05-31 02:48:08.073 UTC,2017-05-31 02:48:08.073 UTC,,5231007,,1976804,1,0,google-cloud-vision,96
Mongoosejs virtual populate,43882577,Mongoosejs virtual populate,"<p>I have a circle model in my project:</p>

<pre><code>var circleSchema = new Schema({
//circleId: {type: String, unique: true, required: true},
patientID: {type: Schema.Types.ObjectId, ref: ""patient""},
circleName: String,
caregivers: [{type: Schema.Types.ObjectId}],
accessLevel: Schema.Types.Mixed
});

circleSchema.virtual('caregiver_details',{
    ref: 'caregiver',
    localField: 'caregivers',
    foreignField: 'userId'
});
</code></pre>

<p>caregiver schema:</p>

<pre><code>var cargiverSchema = new Schema({
    userId: {type: Schema.ObjectId, unique: true},  //objectId of user document
    detailId: {type: Schema.ObjectId, ref: ""contactDetails""},
    facialId: {type: Schema.ObjectId, ref: ""facialLibrary""}, //single image will be enough when using AWS rekognition
    circleId: [{type: Schema.Types.ObjectId, ref: ""circle""}],           //multiple circles can be present array of object id
});
</code></pre>

<p>Sample Object:</p>

<pre><code>{ 
    ""_id"" : ObjectId(""58cf4832a96e0e3d9cec6918""), 
    ""patientID"" : ObjectId(""58fea8ce91f54540c4afa3b4""), 
    ""circleName"" : ""circle1"", 
    ""caregivers"" : [
        ObjectId(""58fea81791f54540c4afa3b3""), 
        ObjectId(""58fea7ca91f54540c4afa3b2"")
    ], 
    ""accessLevel"" : {
        ""location\"""" : true, 
        ""notes"" : false, 
        ""vitals"" : true
    }
}
</code></pre>

<p>I have tried virtual populate for mongoosejs but I am unable to get it to work.
This seems to be the exact same problem: <a href=""https://github.com/Automattic/mongoose/issues/4585"" rel=""noreferrer"">https://github.com/Automattic/mongoose/issues/4585</a></p>

<pre><code>circle.find({""patientID"": req.user._id}).populate('caregivers').exec(function(err, items){
        if(err){console.log(err); return next(err) }
        res.json(200,items);
    });
</code></pre>

<p>I am only getting the object id's in the result. It is not getting populated.</p>",43901938,1,10,,2017-05-10 02:16:12.047 UTC,1,2018-03-26 16:06:51.483 UTC,2017-05-10 11:07:33.783 UTC,,2236931,,2236931,1,8,node.js|mongoose|mongoose-populate,4562
I want to use PDF/TIFF Document Text Detection service from google cloud,52614091,I want to use PDF/TIFF Document Text Detection service from google cloud,"<p>Google cloud vision api is very powerful and now they have support for pdf format, but the documentation is getting me confused, can someone pls guide a noob how to set up and process a pdf file using vision api. </p>

<p>kind of like starter tutorial</p>

<p>ref: <a href=""https://cloud.google.com/vision/docs/pdf#vision-web-detection-gcs-protocol"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/pdf#vision-web-detection-gcs-protocol</a></p>

<p>The confusion is how to pass the command arguments, and send my sample file and retrieve the results in csv or json format </p>

<p>Should i use my windows command line or cloudshell on google cloud </p>

<p>There is good starter reference for other services , if you look at this <a href=""https://cloud.google.com/vision/docs/ocr#vision-detect-labels-gcloud"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/ocr#vision-detect-labels-gcloud</a>, they have clearly given commands to use in different envs</p>",,1,0,,2018-10-02 18:13:49.423 UTC,,2018-10-03 04:18:10.580 UTC,2018-10-03 04:18:10.580 UTC,,10163720,,10163720,1,0,google-cloud-vision,330
Vertical Character Recognition,53940989,Vertical Character Recognition,"<p>I have tried couple of services like google vision,ML Kit &amp; Azure but none were able to give me correct text detection output for vertically placed characters ex.<br>
V<br>
E<br>
R<br>
T<br>
I<br>
C<br>
A<br>
L<br></p>

<p>do you have any OCR that can help me solve this case ?.</p>",,0,0,,2018-12-27 06:59:31.150 UTC,,2018-12-27 07:52:16.043 UTC,2018-12-27 07:52:16.043 UTC,,6043316,,6043316,1,0,mobile|google-cloud-platform|ocr|vision|firebase-mlkit,60
CameraSource.takephoto rotate,39161387,CameraSource.takephoto rotate,"<p>Using Google Vision <code>CameraSource.takephoto</code> i want to take photo to detect face, but photos are rotated. I had tried to rotate it using my legacy code</p>

<pre><code>val ei = ExifInterface(imageUri.getPath())
val orientation = ei.getAttributeInt(ExifInterface.TAG_ORIENTATION, ExifInterface.ORIENTATION_UNDEFINED)

Log.d(""BitmapProcessor"", ""orientation $orientation"")

when (orientation) {
    ExifInterface.ORIENTATION_ROTATE_90 -&gt; rotateImage(img, 90f)
    ExifInterface.ORIENTATION_ROTATE_180 -&gt; rotateImage(img, 180f)
    ExifInterface.ORIENTATION_ROTATE_270 -&gt; rotateImage(img, 270f)
    else -&gt; return
}
</code></pre>

<p>but <code>ExifInterface.getAttribureInt</code> always return 0 so image keep wrong rotation </p>",,0,0,,2016-08-26 08:07:42.250 UTC,,2016-08-26 08:07:42.250 UTC,,,,,2172227,1,2,android|google-vision,170
Restrict Google Cloud Vision Api filtering by countries and / or languages,54847537,Restrict Google Cloud Vision Api filtering by countries and / or languages,<p>Is it possibile restrict the search of Google vision api filtering by specific countries or geographic zones ? By language ?</p>,,0,0,,2019-02-24 00:13:41.377 UTC,,2019-02-24 00:13:41.377 UTC,,,,,2026238,1,0,google-api|google-api-client,14
"Watson Visual Recognition error ""Invalid image data. Supported formats are JPG, PNG, and GIF"" when uploading image from Android Studio",38811303,"Watson Visual Recognition error ""Invalid image data. Supported formats are JPG, PNG, and GIF"" when uploading image from Android Studio","<p>I have been wrecking my brain over this for a while and would really appreciate if someone who have some insight into this problem could help me out!</p>

<p>I am trying to upload an image to Watson's Visual Recognition API using POST from Android Studio (by taking a picture using a camera).</p>

<p>I have managed to
- save image after taking a picture with a camera
- show it as a bitmap image on the app</p>

<p>and I am trying to upload the file to the Watson API, but I keep getting this error</p>

<blockquote>
  <p>""description"": ""Invalid image data. Supported formats are JPG, PNG,
  and GIF.""</p>
</blockquote>

<p>I would really appreciate if anyone could provide some insight to what I am doing wrong here. Thanks in advance!</p>

<p>I am using HttpUrlConnection and DataOutputStream to POST right now and the code is as follows:</p>

<p>imgName and imgPath are all correctly identified, and name=""images_file"" is how Watson Visual Recognition API requests name to be</p>

<pre><code>public void uploadImage(){
    HttpURLConnection conn = null;
    DataOutputStream dos = null;
    String lineEnd = ""\r\n"";
    String twoHyphens = ""--"";
    String boundary = ""*****"";
    int bytesRead, bytesAvailable, bufferSize;
    byte[] buffer;
    int maxBufferSize = 1 * 1024 * 1024;
    try{

        StrictMode.ThreadPolicy policy = new StrictMode.ThreadPolicy.Builder().permitAll().build();

        StrictMode.setThreadPolicy(policy);

        URL url = new URL(""https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classify?api_key=APIKEY&amp;version=2016-05-20"");
        conn= (HttpURLConnection) url.openConnection();
        FileInputStream fileInputStream = new FileInputStream(file);
        conn.setDoInput(true);
        conn.setDoOutput(true);
        conn.setUseCaches(false);
        conn.setRequestMethod(""POST"");
        conn.setRequestProperty(""Connection"", ""Keep-Alive"");
        conn.setRequestProperty(""ENCTYPE"", ""multipart/form-data"");
        conn.setRequestProperty(""Content-Type"", ""multipart/form-data;boundary="" + boundary);
        conn.setRequestProperty(""images_file"", imgPath);

        dos = new DataOutputStream(conn.getOutputStream());

        dos.writeBytes(twoHyphens + boundary + lineEnd);
        dos.writeBytes(""Content-Disposition: form-data; name=\""images_file\""; filename = \"""" + imgName + ""\"""" + lineEnd);
        dos.writeBytes(lineEnd);


        bytesAvailable = fileInputStream.available();
        bufferSize = Math.min(bytesAvailable, maxBufferSize);
        buffer = new byte[bufferSize];

        bytesRead = fileInputStream.read(buffer, 0, bufferSize);
        while (bytesRead &gt; 0) {

            dos.write(buffer, 0, bufferSize);
            bytesAvailable = fileInputStream.available();
            bufferSize = Math.min(bytesAvailable, maxBufferSize);
            bytesRead = fileInputStream.read(buffer, 0, bufferSize);

        }

        // send multipart form data necesssary after file data...
        dos.writeBytes(lineEnd);
        dos.writeBytes(twoHyphens + boundary + twoHyphens + lineEnd);

        // Responses from the server (code and message)
        serverResponseCode = conn.getResponseCode();
        String serverResponseMessage = conn.getResponseMessage();

        Log.i(""uploadFile"", ""HTTP Response is : ""
                + serverResponseMessage + "": "" + serverResponseCode);

        if(serverResponseCode == 200){

            BufferedReader br = new BufferedReader(new InputStreamReader((conn.getInputStream())));
            StringBuilder sb = new StringBuilder();
            String output;
            while ((output = br.readLine()) !=null) {
                sb.append(output);
            }
            Log.d(""debugging"", sb.toString());

            runOnUiThread(new Runnable() {
                public void run() {

                    String msg = ""File Upload Completed.\n\n See uploaded file here : \n\n""
                            +"" http://www.androidexample.com/media/uploads/""
                            +imgName;


                    Toast.makeText(getApplicationContext(), ""File Upload Complete."",
                            Toast.LENGTH_SHORT).show();


                }
            });
        }

        //close the streams //
        fileInputStream.close();
        dos.flush();
        dos.close();

    } catch (MalformedURLException ex) {

        ex.printStackTrace();

        runOnUiThread(new Runnable() {
            public void run() {
                Toast.makeText(getApplicationContext(), ""MalformedURLException"",
                        Toast.LENGTH_SHORT).show();
            }
        });

        Log.e(""Upload file to server"", ""error: "" + ex.getMessage(), ex);
    } catch (Exception e) {

        e.printStackTrace();

        runOnUiThread(new Runnable() {
            public void run() {
                Toast.makeText(getApplicationContext(), ""Got Exception : see logcat "",
                        Toast.LENGTH_SHORT).show();
            }
        });
        Log.e(""Debugging"", ""Exception : ""
                + e.getMessage(), e);
    }
    Log.d(""Debugging"", ""responseCode:"" + serverResponseCode);
}
</code></pre>",38818105,2,0,,2016-08-07 05:48:38.787 UTC,,2016-08-08 23:59:25.420 UTC,2016-08-08 00:21:26.810 UTC,,2253918,,2253918,1,0,android|android-studio|ibm-watson|visual-recognition,421
Microsoft Facial Recognition with Amazon s3 bucket?,51609428,Microsoft Facial Recognition with Amazon s3 bucket?,"<p>I'd like to know whether we can use <strong>Amazon S3</strong> and <strong>Microsoft Face API</strong> together. The use case that we would like to implement is that the image taken from Android after the preliminary checks are done should be matched with the person's image that is pre-stored in S3 bucket. I understand that there is something called <strong>PersonGroup</strong> or <strong>LargePersonGroup</strong> which are the list of known people. This needs to be initialized at the start and has a capacity of <strong>1,000,000</strong>, this I would like to omit because I want to check the picture taken directly with the image that is stored in S3, which I can get directly on the basis of Key. </p>

<p>Any suggestions? </p>",,0,0,,2018-07-31 08:59:34.240 UTC,,2018-07-31 10:51:37.633 UTC,2018-07-31 09:14:46.933 UTC,,2442831,,1503786,1,0,java|amazon-web-services|microsoft-cognitive|face-recognition,43
How to use AWS Video Rekognition with an unauthenticated identity?,49890225,How to use AWS Video Rekognition with an unauthenticated identity?,"<p>i have followed the steps of the documentation but i received:</p>

<p><code>User: arn:aws:sts::xxxxxxxxxxxx:assumed-role/CognitoRkUnauth_Role/CognitoIdentityCredentials is not authorized to perform: iam:PassRole on resource: arn:aws:iam::xxxxxxxxxx:role/CognitoRkUnauth_Role</code></p>

<p>The code fails en <code>NotificationChannel</code>. Without this i received the jobId correctly</p>

<pre><code>var params = {
      Video: {
        S3Object: {
          Bucket: 'mybucket',
          Name: 'myvideoa1.mp4'
        }
      },
      ClientRequestToken: 'LabelDetectionToken',
      MinConfidence: 70,
      NotificationChannel: {
        SNSTopicArn: 'arn:aws:sns:us-east-1:xxxxxxxx:RekognitionVideo',
        RoleArn: 'arn:aws:iam::xxxxxx:role/CognitoRkUnauth_Role'
      },
      JobTag: ""DetectingLabels""
    }
</code></pre>

<p>I set configuration to CognitoRkUnauth_Role instead of a iam user. Translation worked doing this.</p>

<p>In <code>RoleArn</code> I created another Role but it fails too.</p>

<p>I am not the root user.</p>

<p>I know I need to give more information but if someone can guide me, i will start again the configuration.</p>

<p>I am beginner in aws and i dont understand several things at all.</p>

<p><em>(english is not my first language)</em></p>",49992388,1,0,,2018-04-18 02:30:15.670 UTC,,2018-04-24 02:04:22.687 UTC,,,,,4172591,1,0,amazon-web-services|amazon-cognito,41
Check if android OCR is downloaded or not,53517832,Check if android OCR is downloaded or not,"<p>I have used OCR, from google vision api, where I used the app to detect text from screen using text recognizer API. But, as the document says, it needs to download the necessary files from the internet and it says that it downloads when the app is downloaded from play store</p>

<pre><code> &lt;meta-data
        android:name=""com.google.android.gms.vision.DEPENDENCIES""
        android:value=""ocr"" /&gt;
</code></pre>

<p>Is there any way how I can check if the necessary files are downloaded or not? If not, I want to start the download, once the user gets connected to wifi.</p>",,0,4,,2018-11-28 10:56:50.233 UTC,,2018-11-28 10:56:50.233 UTC,,,,,8237551,1,0,android|android-vision,47
Managing Azure Face API images,46078769,Managing Azure Face API images,"<p>I'm playing with the Azure Face API and enjoying it very much.</p>

<p>I was wondering - where do the images I upload via the API (for example - in order to create a Person Group) stored? Can I view them or download them?</p>

<p>Thanks!</p>",,1,0,,2017-09-06 15:09:37.620 UTC,,2018-04-03 18:06:55.897 UTC,,,,,927420,1,1,azure-cognitive-services,194
How to send application/octet-steam image data to Microsoft Face API in Node.js?,49711906,How to send application/octet-steam image data to Microsoft Face API in Node.js?,"<p>I want to send Octet-Stream binary data to Microsoft Face API in Nodejs. I have a base64 encoded image data. I want to send it to the Face API. I'm using the following code:</p>

<pre><code>var dataURItoBuffer = function (dataURL, callback) {
    var buff = new Buffer(dataURL.replace(/^data:image\/(png|gif|jpeg);base64,/, ''), 'base64');
    callback(buff);
};

var sendImageToMicrosoftDetectEndPoint = function (imageData, callback) {
    console.log('Entered helper');
    dataURItoBuffer(imageData, function (buff) {
        request.post({
            url: keyConfig.microsoftDetectURL,
            headers: {
                'Content-Type': 'application/octet-stream',
                'Ocp-Apim-Subscription-Key': keyConfig.microsoftApiKey
            },
            data: buff
        }, function (err, httpResponse, body) {
            console.log(body);
        });
    })
}
</code></pre>

<p>But it gives me this response:</p>

<pre><code>{
  ""error"": {
     ""code"":""InvalidImageSize"",
     ""message"":""Image size is too small.""
   }
}
</code></pre>",49712432,1,0,,2018-04-07 20:54:46.540 UTC,,2018-04-07 22:10:07.527 UTC,2018-04-07 22:10:07.527 UTC,,1623249,,5154565,1,0,javascript|microsoft-cognitive,547
How to use AWS Rekognition to Compare Face in Swift 3,46483447,How to use AWS Rekognition to Compare Face in Swift 3,"<p>I've been trying to use the AWSRekognition SDK in order to compare face. However, Amazon has no Documentation on how to integrate their SDK with iOS. They have links that show how to work with Recognition (<a href=""http://%20%20https://docs.aws.amazon.com/rekognition/latest/dg/what-is.html"" rel=""nofollow noreferrer"">Developer Guide</a>) with examples only in Java and very limited.</p>

<p>I wanted to know if anyone knows how to integrate AWS Rekognition in Swift 3. How to Initialize it and make a request with an image, receiving a response with the labels. </p>

<p>I have AWS Signatures AccessKey, SecretKey, AWS Region, Service Name. also Body </p>

<pre><code>{
  ""SourceImage"": {
    ""S3Object"": {
      ""Bucket"": ""bucketName"",
      ""Name"": ""ios/sample.jpg""
    }
  },
  ""TargetImage"": {
    ""S3Object"": {
      ""Bucket"": ""buketName"",
      ""Name"": ""ios/target.JPG""
    }
  }
}
</code></pre>

<p>how can I initialize Rekognition and build a Request.</p>

<p>Thanks you!</p>",46494066,1,0,,2017-09-29 07:02:37.440 UTC,,2018-12-28 06:33:21.017 UTC,2017-09-29 07:08:25.550 UTC,,8368982,,8368982,1,2,ios|amazon-web-services|swift3|amazon-rekognition,1075
Adding face to an existing face,54881537,Adding face to an existing face,"<p>What is the best practice in <code>Azure FACE API</code> to add person face to the trained person.</p>

<p>Is it advisable to add face again and again ? Using,</p>

<pre><code>_faceServiceClient.AddPersonFaceAsync(GroupId, PersonId, stream);
</code></pre>",,0,3,,2019-02-26 08:50:02.750 UTC,,2019-02-26 11:38:06.957 UTC,2019-02-26 11:38:06.957 UTC,,4881193,,4881193,1,0,azure|azure-cognitive-services|face-api,20
Google cloud vision not accepting base64 encoded images python,45695542,Google cloud vision not accepting base64 encoded images python,"<p>I'm having a problem with base64 encoded images sent to Google Cloud Vision. Funny thing is that if I send the image via URI, it works fine, so I suspect there is something wrong the way I'm encoding.</p>

<p>Here's the deal:</p>

<pre><code>from google.cloud import vision
import base64
client = vision.ImageAnnotatorClient()
image_path ='8720911950_91828a2aeb_b.jpg'
with open(image_path, 'rb') as image:
    image_content = image.read()
    content = base64.b64encode(image_content)   
    response = client.annotate_image({'image': {'content': content}, 'features': [{'type': vision.enums.Feature.Type.LABEL_DETECTION}],})
    print(response)
</code></pre>

<p>The response I get always is:</p>

<pre><code>error {
  code: 3
  message: ""Bad image data.""
}
</code></pre>

<p>If I try using URI instead:</p>

<pre><code>response = client.annotate_image({'image': {'source': {'image_uri': 'https://farm8.staticflickr.com/7408/8720911950_91828a2aeb_b.jpg'}}, 'features': [{'type': vision.enums.Feature.Type.LABEL_DETECTION}],})
</code></pre>

<p>Response is ok...</p>

<pre><code>label_annotations {
  mid: ""/m/0168g6""
  description: ""factory""
  score: 0.7942917943000793
}
label_annotations {
  mid: ""/m/03rnh""
  description: ""industry""
  score: 0.7761002779006958
}
</code></pre>

<p>I've followed the <a href=""https://cloud.google.com/vision/docs/base64"" rel=""nofollow noreferrer"">recommended way to encode</a> from Google</p>

<p>Any idea what is wrong here?</p>",45696640,1,8,,2017-08-15 14:56:10.763 UTC,2,2017-08-15 15:54:57.237 UTC,2017-08-15 15:11:10.417 UTC,,14637,,526801,1,4,python|google-cloud-platform|google-cloud-vision,1543
"In a Jupyter Notebook in Watson Studio , how to refer to a file uploaded to the ""assets""?",49881417,"In a Jupyter Notebook in Watson Studio , how to refer to a file uploaded to the ""assets""?","<p>in Watson Studio I am writing code in a Jupyter Notebook to use a Watson Visual Recognition custom model.
It works ok with external images.
I haven't been able yet to refer to an image I have uploaded to the Assets of my project. 
The url of the asset gets to a full page not the image only: 
<a href=""https://dataplatform.ibm.com/projects/2f4b89d9-b93a-4c98-a327-9b863a467b7c/data-assets/ed16c385-e09e-4bcb-bfab-67ee864538e4/?context=data"" rel=""nofollow noreferrer"">https://dataplatform.ibm.com/projects/2f4b89d9-b93a-4c98-a327-9b863a467b7c/data-assets/ed16c385-e09e-4bcb-bfab-67ee864538e4/?context=data</a></p>

<p>Thank you </p>",,1,1,,2018-04-17 14:56:51.020 UTC,,2018-05-08 03:07:00.867 UTC,,,,,3779503,1,1,jupyter-notebook|watson-studio,497
JSON parsing error when trying to use Face API,55665919,JSON parsing error when trying to use Face API,"<p>I'm trying to send photo from Imgur via URL adress to Microsoft Face API and get ID of face from Json response but when I try to run the code, I always get JSON parsing error. I have no idea what I am doing wrong. </p>

<p>I tried to make this request via Postman and everything is working fine there but in c# it just won't work. </p>

<p>Can you help me please?</p>

<pre><code>    static void TryFunction()
    {
        string host = ""https://westcentralus.api.cognitive.microsoft.com/face/v1.0/detect?returnFaceId=true"";
        string subscriptionKey = ""..."";

        body = new System.Object[] { new { url = @""https://i.imgur.com/... .png"" } };
        var requestBody = JsonConvert.SerializeObject(body);

        using (var client = new HttpClient())
        using (var request = new HttpRequestMessage())
        {
            client.DefaultRequestHeaders.Add(""Ocp-Apim-Subscription-Key"", subscriptionKey);
            request.Method = HttpMethod.Post;
            request.RequestUri = new Uri(host);
            request.Content = new StringContent(requestBody, Encoding.UTF8, ""application/json"");

            var response = client.SendAsync(request).Result;
            var jsonResponse = response.Content.ReadAsStringAsync().Result;

            dynamic json = JsonConvert.DeserializeObject(jsonResponse);
            Console.WriteLine(jsonResponse);
        }
    }
</code></pre>

<blockquote>
  <p>{""error"": {""code"":""BadArgument"", ""message"":""JSON parsing error.""}}</p>
</blockquote>

<p>The C# request body looks like this:</p>

<pre><code>[{""url"":""https://i.imgur.com/... .png""}]
</code></pre>

<p>Whereas the Postman request body looks like this:</p>

<pre><code>{ ""url"": ""https://i.imgur.com/... .png"" }
</code></pre>",55666330,1,2,,2019-04-13 13:20:37.650 UTC,,2019-04-13 14:04:27.030 UTC,2019-04-13 14:04:27.030 UTC,,3181933,,11355812,1,0,c#|json|face-api,41
Getting Json Reponse from AWS Rekognition,51361560,Getting Json Reponse from AWS Rekognition,"<p>I am working on AWS Rekognition(using Java API) which recognize celebrity from a video stored in S3 and list them on the console. (<a href=""https://docs.aws.amazon.com/rekognition/latest/dg/celebrities-procedure-image.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/rekognition/latest/dg/celebrities-procedure-image.html</a>).</p>

<p>It is working fine and gives the detected celebrity on the console. But I want the complete JSON response which it gets in ""RecognizeCelebritiesResult"". Amazon internally parsing JSON response to respective POJO and giving us getter/setter and different function to operate.</p>

<p>I myself want to parse the JSON or just want to save whole JSON response in the file. Where will I get that whole JSON??</p>",,0,0,,2018-07-16 12:11:31.327 UTC,,2018-07-16 12:11:31.327 UTC,,,,,10088072,1,0,java|json|amazon-web-services|amazon-rekognition,129
Pass Camera Image to Google Vision API,54896032,Pass Camera Image to Google Vision API,"<p>I am absolutely stuck with the following Activity, where I am trying to make a Camera Picture, save it and then send it to the Google Vision API in order to process it further. </p>

<p>Making the Image works fine, it is saved to the Gallery; the API Request is also working fine, as I don´t get any error message. However, the Toast which contains the Image Analysis is just don´t showing up and I absolutely don´t know why. I am very sure, that the API Request isn´t failing as I could see responses such as ""400"" before in the Logcat; however, now it isn´t just responding at all. </p>

<p>I try to make the picture, save it to the device, open it and pass it as an Inputstream to the Google Vision API.</p>

<p>Any hints or help would be very much appreciated, Thanks in advance! </p>

<pre><code>import android.content.Intent;
import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.net.Uri;
import android.os.Bundle;
import android.os.Environment;
import android.os.StrictMode;
import android.provider.MediaStore;
import android.support.design.widget.FloatingActionButton;
import android.support.design.widget.Snackbar;
import android.support.v4.content.FileProvider;
import android.support.v7.app.AppCompatActivity;
import android.support.v7.widget.Toolbar;
import android.util.Log;
import android.view.View;
import android.widget.ImageView;
import android.widget.Toast;

import com.google.api.client.extensions.android.json.AndroidJsonFactory;
import com.google.api.client.http.javanet.NetHttpTransport;
import com.google.api.services.vision.v1.Vision;
import com.google.api.services.vision.v1.VisionRequestInitializer;
import com.google.api.services.vision.v1.model.AnnotateImageRequest;
import com.google.api.services.vision.v1.model.BatchAnnotateImagesRequest;
import com.google.api.services.vision.v1.model.BatchAnnotateImagesResponse;
import com.google.api.services.vision.v1.model.FaceAnnotation;
import com.google.api.services.vision.v1.model.Feature;
import com.google.api.services.vision.v1.model.Image;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.InputStream;
import java.text.SimpleDateFormat;
import java.util.Arrays;
import java.util.Date;
import java.util.List;


public class VisionAPIActivity extends AppCompatActivity {

    ImageView imageView2;
    ImageView imageView1;
    static final int REQUEST_TAKE_PHOTO = 1;
    String currentPhotoPath;
    final Uri photoURI = null;


    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_vision_api);
        Toolbar toolbar = findViewById(R.id.toolbar);
        if (android.os.Build.VERSION.SDK_INT &gt; 9) {
            StrictMode.ThreadPolicy policy = new StrictMode.ThreadPolicy.Builder().permitAll().build();
            StrictMode.setThreadPolicy(policy);
        }
        setSupportActionBar(toolbar);

        FloatingActionButton fab = findViewById(R.id.fab);
        fab.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                Snackbar.make(view, ""Replace with your own action"", Snackbar.LENGTH_LONG)
                        .setAction(""Action"", null).show();
            }
        });


    }

    public void imageClick(View view) throws FileNotFoundException {
        imageView2 = findViewById(R.id.imageView2);
        open();
    }

    public void open() throws FileNotFoundException {
        Intent takePictureIntent = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);
        startActivityForResult(takePictureIntent, REQUEST_TAKE_PHOTO);
        if (takePictureIntent.resolveActivity(getPackageManager()) != null) {
            // Create the File where the photo should go
            File photoFile = null;
            try {
                photoFile = createImageFile();
            } catch (IOException ex) {
                // Error occurred while creating the File
                Toast.makeText(this,
                        ""Photo file can't be created, please try again"",
                        Toast.LENGTH_SHORT).show();
            }
            // Continue only if the File was successfully created
            if (photoFile != null) {
                 final Uri photoURI = FileProvider.getUriForFile(this,
                        ""xxx.xxxx.xxxx.provider"",
                        photoFile);
                takePictureIntent.putExtra(MediaStore.EXTRA_OUTPUT, photoURI);
                Toast.makeText(this,
                        ""Photo URI has been created"",
                        Toast.LENGTH_SHORT).show();

               // run();
            }
            Vision.Builder visionBuilder = new Vision.Builder(
                    new NetHttpTransport(),
                    new AndroidJsonFactory(),
                    null);

            visionBuilder.setVisionRequestInitializer(
                    new VisionRequestInitializer(""xxx""));
            final Vision vision = visionBuilder.build();

            Log.i(""log-"", ""passed VisionBuilder Initialisation"");

            // TODO:
            // Convert photo to byte array
            ImageView mImageView;
            mImageView = findViewById(R.id.imageView1);
            InputStream ims = getContentResolver().openInputStream(Uri.fromFile(photoFile));
            // just display image in imageview
            mImageView.setImageBitmap(BitmapFactory.decodeStream(ims));

            byte[] photoData = new byte[0];
            Log.i(""log-"", ""Content of Photo Data"" + photoData);

            Image inputImage = new Image();
            inputImage.encodeContent(photoData);
            Feature desiredFeature = new Feature();
            desiredFeature.setType(""FACE_DETECTION"");
            AnnotateImageRequest request = new AnnotateImageRequest();
            request.setImage(inputImage);
            Log.i(""log-"", ""Content of inputImage"" + inputImage);
            request.setFeatures(Arrays.asList(desiredFeature));
            BatchAnnotateImagesRequest batchRequest =
                    new BatchAnnotateImagesRequest();

            batchRequest.setRequests(Arrays.asList(request));
            BatchAnnotateImagesResponse batchResponse =
                    null;
            try {
                batchResponse = vision.images().annotate(batchRequest).execute();
                List&lt;FaceAnnotation&gt; faces = batchResponse.getResponses()
                        .get(0).getFaceAnnotations();

                // Count faces
                int numberOfFaces = faces.size();
                Log.i(""log-"", ""number Of Faces"" + numberOfFaces);

                // Get joy likelihood for each face
                String likelihoods = """";
                for (int i = 0; i &lt; numberOfFaces; i++) {
                    likelihoods += ""\n It is "" +
                            faces.get(i).getJoyLikelihood() +
                            "" that face "" + i + "" is happy"";
                }

                // Concatenate everything
                final String message =
                        ""This photo has "" + numberOfFaces + "" faces"" + likelihoods;

                // Display toast on UI thread
                runOnUiThread(new Runnable() {
                    @Override
                    public void run() {
                        Toast.makeText(getApplicationContext(),
                                message, Toast.LENGTH_LONG).show();
                    }
                });
            }
            catch (IOException e) {
                e.printStackTrace();
            }


        }
        return;
    }

    private File createImageFile() throws IOException {
        // Create an image file name
        String timeStamp = new SimpleDateFormat(""yyyyMMdd_HHmmss"").format(new Date());
        String imageFileName = ""JPEG_"" + timeStamp + ""_"";
        File storageDir = getExternalFilesDir(Environment.DIRECTORY_PICTURES);
        File image = File.createTempFile(
                imageFileName,  /* prefix */
                "".jpg"",         /* suffix */
                storageDir      /* directory */
        );

        // Save a file: path for use with ACTION_VIEW intents
        currentPhotoPath = image.getAbsolutePath();
        return image;
    }
    protected void onActivityResult(int requestCode,int resultCode,Intent data){
        super.onActivityResult(requestCode,resultCode,data);
        Bitmap bitmap=(Bitmap)data.getExtras().get(""data"");
        // imageView2.setImageBitmap(bitmap);
    }
}
</code></pre>",,0,0,,2019-02-26 23:55:54.217 UTC,,2019-02-26 23:55:54.217 UTC,,,,,8623502,1,0,java|android|android-activity|google-vision|android-fileprovider,67
Google Vision API returns invalid authentication credentials,48219196,Google Vision API returns invalid authentication credentials,"<p>I just developed my system (PHP) using Google Vision API, but when I deployed it to the server (Amazon Elastic Beanstalks) the result showed me that.
<br>""<strong>error code:401 message:Request had invalid authentication credentials. Expected OAuth 2 access token, login cookie or other valid authentication credential</strong>"" </p>

<p>For more information 
<br> - I try to add GOOGLE_APPLICATION_CREDENTIALS to AWS EBS environment variables ---> It's not work.
<br>- I try to create client ID and client secret on OAuth consent screen ---> I don't know how to apply it.</p>",,0,1,,2018-01-12 03:35:17.583 UTC,3,2018-01-12 03:35:17.583 UTC,,,,,1831412,1,1,php|amazon-web-services|amazon-elastic-beanstalk|google-cloud-vision,202
Authorize Google Cloud Vision API to Google Storage image,35660357,Authorize Google Cloud Vision API to Google Storage image,"<p>From Node, I am attempting to use Google Cloud Vision API to analyze an image stored in Google Storage. I have successfully base64 encoded an image and uploaded it but would like to speed up the process by executing against files I have in Google Storage. My Request is so,</p>

<p>{ ""requests"":[ { ""image"":{ ""source"": { ""gcsImageUri"" : ""gs://mybucket/10001.jpg"" } }, ""features"":[ { ""type"":""LABEL_DETECTION"", ""maxResults"":10 } ] } ] }</p>

<p>but I receive this error from my call,</p>

<p>{ ""responses"": [ { ""error"": { ""code"": 7, ""message"": ""image-annotator::User lacks permission.: ACCESS_DENIED: Anonymous callers do not have storage.objects.get access to object mybucket/10001.jpg."" } } ] }</p>

<p>I am not using any google SDK or node_modules for this request, just a Browser API Key and the http module. Is there some permissions I have to set within Cloud Storage to allow Vision API access to the objects in the bucket? If so what would that be, I am new to Google Cloud Platform but have extensive experience with AWS IAM roles.</p>

<p>Thanks,
VIPER</p>",,2,0,,2016-02-26 19:45:21.463 UTC,,2017-05-03 07:01:25.100 UTC,,,,,5987837,1,0,google-cloud-storage|google-cloud-platform|google-vision,1587
Google Vision API: Handle native crash,43765499,Google Vision API: Handle native crash,"<p>Anybody knows how can I handle exceptions coming from native method calls?</p>

<p>I'm running the <a href=""https://github.com/googlesamples/android-vision/tree/master/visionSamples/barcode-reader"" rel=""nofollow noreferrer"">Barcode Reader example from Google Vision API</a>, it works very well reading some 2d - pdf417 codes, but in some cases it crashes with a native exception attempting to use <code>NewStringUTF</code> like this:</p>

<pre><code>    art/runtime/java_vm_ext.cc:410] JNI DETECTED ERROR IN APPLICATION: input is not valid Modified UTF-8: illegal start byte 0x90
    art/runtime/java_vm_ext.cc:410]     in call to NewStringUTF
    art/runtime/java_vm_ext.cc:410]     from com.google.android.gms.vision.barcode.internal.NativeBarcode[] com.google.android.gms.vision.barcode.internal.NativeBarcodeDetector.recognizeNative(int, int, byte[], com.google.android.gms.vision.barcode.internal.NativeOptions)
    art/runtime/java_vm_ext.cc:410] ""Thread-4533"" prio=5 tid=19 Runnable
    art/runtime/java_vm_ext.cc:410]   | group=""main"" sCount=0 dsCount=0 obj=0x12c8e0a0 self=0x7f849f4200
    art/runtime/java_vm_ext.cc:410]   | sysTid=9051 nice=0 cgrp=default sched=0/0 handle=0x7f7b7bf440
    art/runtime/java_vm_ext.cc:410]   | state=R schedstat=( 1663092757 5060164 94 ) utm=165 stm=1 core=2 HZ=100
    art/runtime/java_vm_ext.cc:410]   | stack=0x7f7b6bd000-0x7f7b6bf000 stackSize=1037KB
    art/runtime/java_vm_ext.cc:410]   | held mutexes= ""mutator lock""(shared held)
    art/runtime/java_vm_ext.cc:410]   native: #00 pc 00000000004897a8  /system/lib64/libart.so (_ZN3art15DumpNativeStackERNSt3__113basic_ostreamIcNS0_11char_traitsIcEEEEiP12BacktraceMapPKcPNS_9ArtMethodEPv+200)
    art/runtime/java_vm_ext.cc:410]   native: #01 pc 0000000000458644  /system/lib64/libart.so (_ZNK3art6Thread4DumpERNSt3__113basic_ostreamIcNS1_11char_traitsIcEEEEP12BacktraceMap+224)
    art/runtime/java_vm_ext.cc:410]   native: #02 pc 000000000030c9e4  /system/lib64/libart.so (_ZN3art9JavaVMExt8JniAbortEPKcS2_+1004)
    art/runtime/java_vm_ext.cc:410]   native: #03 pc 000000000030d29c  /system/lib64/libart.so (_ZN3art9JavaVMExt9JniAbortVEPKcS2_St9__va_list+116)
    art/runtime/java_vm_ext.cc:410]   native: #04 pc 0000000000141f9c  /system/lib64/libart.so (_ZN3art11ScopedCheck6AbortFEPKcz+144)
    art/runtime/java_vm_ext.cc:410]   native: #05 pc 000000000014a1d0  /system/lib64/libart.so (_ZN3art11ScopedCheck5CheckERNS_18ScopedObjectAccessEbPKcPNS_12JniValueTypeE.constprop.116+11084)
    art/runtime/java_vm_ext.cc:410]   native: #06 pc 0000000000153418  /system/lib64/libart.so (_ZN3art8CheckJNI12NewStringUTFEP7_JNIEnvPKc+468)
    art/runtime/java_vm_ext.cc:410]   native: #07 pc 0000000000005774  /data/data/com.google.android.gms/files/com.google.android.gms.vision/barcode/libs/arm64-v8a/libbarhopper.so (_ZN9barhopper9JniObject14SetStringFieldEPKcRKNSt3__112basic_stringIcNS3_11char_traitsIcEENS3_9allocatorIcEEEE+168)
    art/runtime/java_vm_ext.cc:410]   native: #08 pc 00000000000036f8  /data/data/com.google.android.gms/files/com.google.android.gms.vision/barcode/libs/arm64-v8a/libbarhopper.so (???)
    art/runtime/java_vm_ext.cc:410]   native: #09 pc 0000000000004c8c  /data/data/com.google.android.gms/files/com.google.android.gms.vision/barcode/libs/arm64-v8a/libbarhopper.so (Java_com_google_android_gms_vision_barcode_internal_NativeBarcodeDetector_recognizeNative+84)
</code></pre>

<p>I would like to handle the crash in order to catch it and give some feedback to the user.</p>",43766165,2,0,,2017-05-03 16:29:56.850 UTC,1,2019-05-22 07:07:41.607 UTC,,,,,1623501,1,2,android|google-play-services|google-vision|android-vision,680
QR Code Scanner App works perfectly EXCEPT in Nougat,55859968,QR Code Scanner App works perfectly EXCEPT in Nougat,"<p>Been a while since I last asked a question here. Googled, Stack-Overflowed, etc - feels like no one else had this problem.</p>

<p>Anyway, I created an app for a client that reads a QR Code from their employees' ID to time them in or out; a bundy clock with a twist. I am not very good with Android programming but I am good with programming in general; my background is in Visual Basic and Java. I used Google's Vision API for the camera and it works beautifully. I used API-17 since the only available tablet I have for testing runs on Jelly Bean.</p>

<p>So, my client buys a tablet for the app and it runs on Nougat 7.0. For some reason, the SurfaceView just sits there. It can ""see"" but it does not read the QR Code. I tap it to auto-focus, even change the preferred camera (that I built into my code), and it still does not read any QR code. I installed my app in devices running Kitkat, Lollipop, Marshmallow, Oreo, even Pie and they all work EXCEPT the one running Nougat! I haven't tried installing it in another device with Nougat though and I might do that after this posting - maybe it's the device itself? It's a Huawei Mediapad M3 Lite and runs on its own custom OS based on Nougat so it could also be that.</p>

<p>But, just in case I missed something, maybe someone with a lot more expertise on Android programming here can help shed some light on this mystery.</p>

<p>UPDATE: I installed my app in a Nougat-7.1 device and it worked. I reinstalled the app in the Mediapad M3 Lite, still NOT working. I am inclined to believe that this issue might have something to do with Nougat-7.0 or the Custom OS of Huawei (EMUI 5.1.3).</p>",55860328,1,0,,2019-04-26 02:20:02.133 UTC,,2019-04-26 03:13:38.080 UTC,2019-04-26 02:37:42.593 UTC,,4776205,,4776205,1,0,android|qr-code|android-7.0-nougat,26
Add Confidence Score to Azure Cognitive Services Web App,54981232,Add Confidence Score to Azure Cognitive Services Web App,"<p>I created an Azure Web App with Microsoft Computer Vision to Tag images I upload and write a Description. I followed this tutorial: <a href=""https://github.com/Microsoft/computerscience/blob/master/Labs/Azure%20Services/Azure%20Storage/Azure%20Storage%20and%20Cognitive%20Services%20(MVC).md#Exercise1"" rel=""nofollow noreferrer"">https://github.com/Microsoft/computerscience/blob/master/Labs/Azure%20Services/Azure%20Storage/Azure%20Storage%20and%20Cognitive%20Services%20(MVC).md#Exercise1</a></p>

<p>The app tags the images, but I can not see the Confidence Score for the tags. Anyone had this problem before or do you have any tips on how to add the confidence scores? Help is appreciated</p>

<p>Best regards,
Daniel</p>",,1,1,,2019-03-04 10:25:34.537 UTC,,2019-03-04 17:21:00.223 UTC,,,,,11147611,1,-3,azure|computer-vision|microsoft-cognitive|azure-cognitive-services,47
Android Camera: Failed to connect to service,33482245,Android Camera: Failed to connect to service,"<p>I am trying to develop a face tracking app using the Google Vision API (<a href=""https://developers.google.com/android/reference/com/google/android/gms/vision/package-summary"" rel=""nofollow"">API doc</a>)</p>

<p><br>This is my manifest:</p>

<pre><code>&lt;uses-permission android:name=""android.permission.CAMERA""/&gt;
&lt;uses-feature android:name=""android.hardware.camera.front"" android:required=""true""/&gt;
&lt;meta-data android:name=""com.google.android.gms.vision.DEPENDENCIES"" android:value=""face""&gt;&lt;/meta-data&gt;

    *activities*
</code></pre>

<p><br>This is my code:</p>

<pre><code>private void startCameraSource() {
    try {
        mCameraSource.start(mPreviewHolder);
    } catch (IOException e) {
        //Error handling
        Toast.makeText(this, ""Could not start camera!"", Toast.LENGTH_LONG).show();
    }
}
</code></pre>

<p><br>This is the error in Logcat:</p>

<pre><code>Caused by: java.lang.RuntimeException: Fail to connect to camera service
E/AndroidRuntime:     at android.hardware.Camera.&lt;init&gt;(Camera.java:520)
E/AndroidRuntime:     at android.hardware.Camera.open(Camera.java:361)
E/AndroidRuntime:     at com.google.android.gms.vision.CameraSource.zzEu(Unknown Source)
E/AndroidRuntime:     at com.google.android.gms.vision.CameraSource.start(Unknown Source)
</code></pre>

<p><br>Why does this happen (...on an Xperia Z3 compact 5.1)?</p>

<p><br>
<strong>UPDATE:</strong></p>

<p>I spotted a new error. I think it might be the reason why my code is not working.<br> How can I resolve this problem?</p>

<pre><code>W/ServiceManager: Permission failure: android.permission.CAMERA from uid=10241 pid=26845
E/CameraService: Permission Denial: can't use the camera pid=26845, uid=10241
</code></pre>",33492907,1,2,,2015-11-02 16:23:40.607 UTC,,2015-11-03 06:43:40.117 UTC,2015-11-03 06:31:51.610 UTC,,4427841,,4427841,1,2,android|android-camera,1938
What training data is used for Google Cloud Vision API,55982785,What training data is used for Google Cloud Vision API,<p>I am trying to find out what training data set is used for training the Google Cloud Vision API. Do any of you know where the data is from and if it is accessible?</p>,,0,1,,2019-05-04 13:00:09.113 UTC,,2019-05-04 13:00:09.113 UTC,,,,,11451942,1,-1,google-cloud-vision,23
How to make bounding box around text-areas in an image? (Even if text is skewed!!),54821969,How to make bounding box around text-areas in an image? (Even if text is skewed!!),"<p>I am trying to detect and grab text from a screenshot taken from any consumer product's ad.</p>

<p>My code works at a certain accuracy but fails to make bounding boxes around the skewed text area. </p>

<p>Recently I tried <strong>Google Vision API</strong> and it makes bounding boxes around almost every possible text area and detects text in that area with great accuracy. I am curious about how can I achieve the same or similar! </p>

<p>My test image: </p>

<p><a href=""https://i.stack.imgur.com/IxBds.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IxBds.png"" alt=""enter image description here""></a></p>

<p>Google Vision API after bounding boxes: </p>

<p><a href=""https://i.stack.imgur.com/TnH1z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TnH1z.png"" alt=""enter image description here""></a></p>

<p>Thank you in advance:)</p>",,1,5,,2019-02-22 07:19:57.137 UTC,,2019-02-22 08:57:26.043 UTC,2019-02-22 07:33:19.997 UTC,,1932557,,10926598,1,0,opencv|imagemagick|bounding-box|google-vision|python-tesseract,277
Time synchronizing on Raspberry Pi,49526718,Time synchronizing on Raspberry Pi,"<p>I am getting an error related to time synchronization on Raspberry Pi.The error asks me to check and correct iat and exp values.What are these values and how to correct them?</p>

<p>PS:I googled and found about ntp i used it  to correct the time on raspberry pi but I am getting the same error . I am getting the error while trying to access Google Cloud vision API</p>",,0,6,,2018-03-28 05:26:34.107 UTC,,2018-03-28 05:26:34.107 UTC,,,,,9557546,1,1,google-cloud-platform|raspberry-pi3|ntp|google-cloud-vision,78
Adding a Geo Django project to AWS Lambda using Zappa. Getting OSError: cannot open shared object file: No such file or directory,54307715,Adding a Geo Django project to AWS Lambda using Zappa. Getting OSError: cannot open shared object file: No such file or directory,"<p>I am trying to deploy my Geo-Django app to Zappa
1st I got</p>

<pre><code>django.core.exceptions.ImproperlyConfigured: Could not find the GDAL library 
(tried ""gdal"", ""GDAL"", ""gdal2.2.0"", ""gdal2.1.0"", ""gdal2.0.0"", ""gdal1.11.0"", 
""gdal1.10.0"", ""gdal1.9.0""). Is GDAL installed? If it is, try setting 
GDAL_LIBRARY_PATH in your settings.
</code></pre>

<p>Then I followed this <a href=""https://github.com/Miserlou/Zappa/issues/985"" rel=""nofollow noreferrer"">link</a> and added the below</p>

<p>I set these environment variables in my AWS Lambda console:</p>

<pre><code>""LD_LIBRARY_PATH"": ""/tmp/code/lib/"",
""PROJ_LIB"": ""/tmp/code/lib/proj4/"",
</code></pre>

<p>and in my (Django) app's settings file, I set:</p>

<pre><code>GDAL_LIBRARY_PATH = ""/tmp/code/lib/libgdal.so.20.1.3""
GEOS_LIBRARY_PATH = ""/tmp/code/lib/libgeos_c.so.1""
</code></pre>

<p>Now I am getting the error</p>

<pre><code>OSError: /tmp/code/lib/libgdal.so.20.1.3: cannot open shared object file: No such file or directory
</code></pre>

<p>How can I fix this ?</p>

<blockquote>
  <p>Summary of what I have done </p>
</blockquote>

<pre><code>$ pip install zappa
$ zappa init
$ zappa deploy prod
</code></pre>

<p>Below is my zappa_settings.json</p>

<pre><code>{
    ""prod"": {
        ""aws_region"": ""us-east-1"",
        ""django_settings"": ""Cool.settings"",
        ""profile_name"": ""default"",
        ""project_name"": ""cool"",
        ""runtime"": ""python3.6"",
        ""s3_bucket"": ""coolplaces-t47c5adgt"",
        ""extra_permissions"": [{
            ""Effect"": ""Allow"",
            ""Action"": [""rekognition:*""],
            ""Resource"": ""*""
        }]
    }
} 
</code></pre>",54340617,2,0,,2019-01-22 11:51:43.177 UTC,,2019-01-24 06:39:04.487 UTC,2019-01-24 06:39:04.487 UTC,,9663800,,9663800,1,1,django|python-3.x|aws-lambda|zappa,89
"Google Cloud Vision API -"" image-annotator::Malformed request.: Image processing error""",39616361,"Google Cloud Vision API -"" image-annotator::Malformed request.: Image processing error""","<p>I am getting an error while querying Google Vision API:</p>

<pre><code> {
      ""responses"" : [ {
        ""error"" : {
          ""code"" : 3,
          ""message"" : ""image-annotator::Malformed request.: Image processing error!""
        }
      } ]
    }
</code></pre>

<p>I have passed a pdf file which contains images and then extracted image using <code>pdfbox</code> to create <code>AnnotateImageRequest</code> list</p>

<pre><code>List&lt;AnnotateImageRequest&gt; visionRequests = new ArrayList&lt;&gt;();
PDDocument document = PDDocument.load(pdfDatastream);
for (PDPage page : document.getPages()) {
    PDResources resources = page.getResources();
    for (COSName xObjectName : resources.getXObjectNames()) {
        PDXObject pdxObject = resources.getXObject(xObjectName);
            if (pdxObject instanceof PDImageXObject) {
                byte[] imageArray = Base64.encodeBase64(IOUtils.toByteArray(((PDImageXObject) pdxObject).createInputStream()));
                System.out.println(""image &gt;&gt;""+imageArray.length);
                Image image = new Image();
                image.encodeContent(imageArray);

                Feature feature = new Feature();
                feature.setType(""TEXT_DETECTION"");

                AnnotateImageRequest annotateImageRequest = new AnnotateImageRequest();
                annotateImageRequest.setImage(image);
                annotateImageRequest.setFeatures(Arrays.asList(feature));
                visionRequests.add(annotateImageRequest);
            }
    }
}
</code></pre>

<p>And passed the list created above to vision service :</p>

<pre><code>BatchAnnotateImagesResponse visionSrvcResponse = visionSrvc.images().annotate(new BatchAnnotateImagesRequest().setRequests(visionRequests)).execute();
System.out.println(visionSrvcResponse.toPrettyString());
</code></pre>

<p>I have also tried removing the base64 encoding of image bytearray, but still get the same error listed on the top.The bytearray length is ""<em>774800</em>""</p>

<p><strong>Is there something which I am missing because when I multipart an image to the servlet and pass the bytearray obtained from the inputstream it works fine.</strong></p>

<p>I am running the application on Tomcat V8</p>

<p>dependecies used : </p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.apache.tika&lt;/groupId&gt;
    &lt;artifactId&gt;tika-core&lt;/artifactId&gt;
    &lt;version&gt;1.13&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.tika&lt;/groupId&gt;
    &lt;artifactId&gt;tika-parsers&lt;/artifactId&gt;
    &lt;version&gt;1.13&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;com.google.apis&lt;/groupId&gt;
    &lt;artifactId&gt;google-api-services-vision&lt;/artifactId&gt;
    &lt;version&gt;v1-rev24-1.22.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>",,1,3,,2016-09-21 12:18:01.530 UTC,,2016-09-22 07:50:15.973 UTC,2016-09-21 14:21:15.270 UTC,,4162192,,4162192,1,0,pdfbox|apache-tika|google-cloud-vision,595
Google Vision API Text Recognizer is not working,47570243,Google Vision API Text Recognizer is not working,"<p>I have used Google vision API to read text from any object like newspaper or text in wall. I have tried same sample from Google developer website but my Text Recognizer always return false on <code>IsOperational</code> function. am tested on Blackberry keyone and also tested on Moto x play its working fine.</p>

<blockquote>
  <p>Gradle file : <code>compile 'com.google.android.gms:play-services-vision:11.0.4'</code></p>
</blockquote>

<p>Can anyone help me on this. Thanks in Advance</p>

<pre><code>TextRecognizer textRecognizer = new TextRecognizer.Builder(context).build();
textRecognizer.setProcessor(new OcrDetectorProcessor(mGraphicOverlay, OcrCaptureActivity.this));
if(!textRecognizer.isOperational()) { 
  Log.e(""TextRecog"",""Not Operational""); IntentFilter lowstorageFilter = new 
  IntentFilter(Intent.ACTION_DEVICE_STORAGE_LOW); boolean hasLowStorage = registerReceiver(null, lowstorageFilter) != null;
}
</code></pre>",,3,4,,2017-11-30 09:42:34.173 UTC,,2018-07-09 08:45:32.660 UTC,2018-03-22 04:23:21.143 UTC,,4092401,,4591135,1,3,android|ocr|google-vision|vision-api,1860
Reliable MRZ (Machine Readable Zone) cloud API,55072291,Reliable MRZ (Machine Readable Zone) cloud API,"<p>I'm looking for a cloud based solution to read the Machine Readable Zone from IDs or Passports to implement in our backend.</p>

<p>I tried some generic OCR solutions such as: </p>

<ol>
<li><strong>Amazon Rekognition</strong></li>
<li><strong>Google Vision</strong></li>
<li><strong>Microsoft Computer Vision</strong></li>
<li><strong>Teserract</strong> 3.0 / 4.0 (experimental)</li>
</ol>

<p>None of these provide accurate (sometimes not at all MRZ recognition)</p>

<p>I also tried some other tools specialized in MRZ OCR:</p>

<ol>
<li><strong>BlinkID</strong> from MicroBlink (which is very good but doesn't have a cloud solution)</li>
<li><strong>Accurascan</strong> (provides cloud solution but less accurate than BlinkID)</li>
<li><strong>Abbyy</strong> (too slow, 10~ seconds per request)</li>
</ol>

<p>Can you recommend me a good cloud solution for MRZ OCR of documents?</p>",,2,0,,2019-03-08 23:13:11.023 UTC,,2019-03-22 16:11:24.947 UTC,2019-03-08 23:20:26.740 UTC,,5369823,,5369823,1,0,computer-vision|ocr|tesseract|amazon-rekognition|microblink,119
How can I call or emulate Google Mobile Vision API in Google Cloud?,46194151,How can I call or emulate Google Mobile Vision API in Google Cloud?,"<p>Background: Building on <a href=""https://stackoverflow.com/questions/36309709/does-google-cloud-vision-ocr-support-bar-code-reading"">Does google cloud vision OCR support bar code reading?</a>, I <em>want</em> to do offline analysis of PDF417 barcodes using Google Mobile Vision (Barcode API) but accessible from Google Cloud.</p>

<p>Is there any way to call, emulate or otherwise access the Google Mobile Vision API off-Android/off-iOS?</p>",,1,0,,2017-09-13 09:44:42.557 UTC,,2018-01-12 17:36:41.260 UTC,2017-09-16 23:18:58.010 UTC,,322020,,1021819,1,1,android|google-app-engine|google-cloud-platform|google-cloud-vision|pdf417,263
Error in Request to Google Video Intelligence API,46332861,Error in Request to Google Video Intelligence API,"<p>I have been trying to send a request to the Google Video Intelligence API for SAFE_SEARCH_DETECTION (in node.js), but I keep running into the same error:  </p>

<blockquote>
  <p>ERROR: { Error: Request contains an invalid argument.
      at /Users/paulsteenkiste/node_modules/grpc/src/node/src/client.js:569:15
    code: 3,
    metadata: Metadata { _internal_repr: {} },
    note: 'Exception occurred in retry method that was not classified as transient' }</p>
</blockquote>

<p>I tried to dive into that client.js file listed in the error, but it was not very illuminating. Here is the code that yields this error:</p>

<pre><code>const firebase = require('firebase');

firebase.initializeApp({
    serviceAccount: ""./service-account.json"",
    apiKey: ""&lt;API key&gt;"",
    databaseURL: ""&lt;My Database&gt;""
});

// Imports the Google Cloud Video Intelligence library
const Video = require('@google-cloud/video-intelligence');

// Instantiates a client
const video = Video();

firebase.auth().signInWithEmailAndPassword(""&lt;My email&gt;"", ""&lt;My password&gt;"")
    .then(function(user) {

    // The GCS filepath of the video to analyze
    const gcsUri = '&lt;File location&gt;';

    const request = {
      inputUri: gcsUri,
      features: [""SAFE_SEARCH_DETECTION""]
    };

    // Human-readable likelihoods
    const likelihoods = ['UNKNOWN', 'VERY_UNLIKELY', 'UNLIKELY', 'POSSIBLE', 'LIKELY', 'VERY_LIKELY'];

    // Detects unsafe content
    video.annotateVideo(request)
      .then((results) =&gt; {
        const operation = results[0];
        console.log('Waiting for operation to complete...');
        return operation.promise();
      })
      .then((results) =&gt; {
        // Gets unsafe content
        const safeSearchResults = results[0].annotationResults[0].safeSearchAnnotations;
        console.log('Safe search results:');
        safeSearchResults.forEach((result) =&gt; {
          console.log(`Time: ${result.timeOffset / 1e6}s`);
          console.log(`\tAdult: ${likelihoods[result.adult]}`);
          console.log(`\tSpoof: ${likelihoods[result.spoof]}`);
          console.log(`\tMedical: ${likelihoods[result.medical]}`);
          console.log(`\tViolent: ${likelihoods[result.violent]}`);
          console.log(`\tRacy: ${likelihoods[result.racy]}`);
        });
      })
      .catch((err) =&gt; {
        console.error('ERROR:', err);
      });
    })
    .catch(function(error) {
        var errorCode = error.code;
        var errorMessage = error.message;
        console.log(errorMessage);
    });
</code></pre>

<p>(Note that this is essentially copied and pasted from Google's docs at <a href=""https://cloud.google.com/video-intelligence/docs/analyze-safesearch"" rel=""nofollow noreferrer"">https://cloud.google.com/video-intelligence/docs/analyze-safesearch</a>). The service-account.json is the file I downloaded when I created the service account, and it is in the same folder as the above file. I do not think it is necessary to do that firebase authentication, but I wanted to make sure that wasn't the issue. I have enabled the API and have full access to the project, so neither of those are the issue.</p>

<p>I believe the problem is coming from the service account somehow, but whatever I try the same error shows up. Some of the things that I have tried:</p>

<ol>
<li>Setting GOOGLE_APPLICATION_CREDENTIALS from the terminal</li>
<li>Giving that service account ""Suite Domain-wide Delegation""</li>
<li>Making the file public</li>
<li>Doing the ""gsutil"" command recommended by the answer here: <a href=""https://stackoverflow.com/questions/44441055/permission-denied-when-making-request-to-gcp-video-intelligence-api"">Permission Denied When Making Request to GCP Video Intelligence API</a></li>
</ol>

<p>Any ideas as to what the problem is?</p>",,1,0,,2017-09-20 22:49:46.793 UTC,,2018-02-19 21:20:12.783 UTC,,,,,8643532,1,1,firebase-realtime-database|google-cloud-platform|video-intelligence-api,197
How to darken the area around the field to be scanned?,51417691,How to darken the area around the field to be scanned?,"<p>How to darken the area around the field to be scanned?</p>

<p>I would like the view as in the <a href=""https://i.stack.imgur.com/5hXb3.png"" rel=""nofollow noreferrer"">example</a>. I have a frame around, but I do not know how to dim the area around. I use google vision.</p>

<p>[<img src=""https://i.stack.imgur.com/5hXb3.png"" alt="""">]</p>",,1,0,,2018-07-19 08:18:15.793 UTC,,2018-07-19 09:40:21.530 UTC,2018-07-19 09:16:01.203 UTC,,4578794,,9548627,1,0,android|android-studio|mobile|layout|graphics,31
Google cloud Vision API gives different results to drag and drop example,47415374,Google cloud Vision API gives different results to drag and drop example,"<p>I'm using the google vision (OCR) API (feature ""TEXT_DETECTION"") and I've noticed differences in the results. </p>

<p>The results given using <a href=""https://cloud.google.com/vision/docs/drag-and-drop"" rel=""nofollow noreferrer"">Google's drag and drop page</a><br>
differ to the results returned from an android app which calls the same API.  The android example code is provided by Google on <a href=""https://github.com/GoogleCloudPlatform/cloud-vision/tree/master/android"" rel=""nofollow noreferrer"">github</a> (I have modified it to use text detection rather than labels API)</p>

<p>Using the same image, drag and drop returns perfect results but the android app which calls the same API has several mistakes and doesn't even attempt part of the text. </p>

<p>Can someone tell me why this is, am I missing some kind of setting? </p>

<p>** Also using android play services, gives incorrect results - It would be my preference to use play services</p>",,1,0,,2017-11-21 14:21:00.657 UTC,,2017-11-21 14:32:42.487 UTC,2017-11-21 14:30:24.240 UTC,,400320,,400320,1,1,android|google-vision,324
"Use Google Cloud Vision in Codeigniter, Cannot Find Class ImageAnnotatorClient",55447535,"Use Google Cloud Vision in Codeigniter, Cannot Find Class ImageAnnotatorClient","<p>I want to implement the google cloud vision textDetection using a google cloud vision.</p>

<p>I have install the composer from google cloud vision to the thirdparty vendor in codeignier.</p>

<p><strong>What my setup in construct is :</strong></p>

<pre><code>include APPPATH . 'third_party/vendor/autoload.php';
        require_once APPPATH.'third_party/vendor/google/cloud-vision/src/V1/ImageAnnotatorClient.php';          
</code></pre>

<p><strong>and my function to call the OCR is :</strong></p>

<pre><code>function upload_ocr_image()
    {               
        if (count($_FILES) === 0) { echo 'no image received from unity'; }

        $phone_code = $this-&gt;input-&gt;post('phone_code', true);
        $phone_number = $this-&gt;input-&gt;post('phone_number', true);

        $allowedType = array(IMAGETYPE_GIF,IMAGETYPE_JPEG,IMAGETYPE_PNG);       
        $imgType = exif_imagetype($_FILES['ocr_image']['tmp_name']);

        if(!in_array($imgType,$allowedType))
        {
            echo ""Images Type Error. Images Type Only : GIF , JPEG, PNG"";
            exit;
        }
        else
        {

            //upload original size front end slider
            $config['upload_path'] = './assets/ocr_image/';
            $config['allowed_types'] = '*';
            $config['file_name'] = $phone_code.$phone_number."".jpg"";
            $config['overwrite'] = TRUE;
            $config['max_size'] = '8096';
            $config['max_width']  = '8000';
            $config['max_height']  = '8000';


            $this-&gt;load-&gt;library('upload', $config);

            $this-&gt;upload-&gt;initialize($config);
            if(!$this-&gt;upload-&gt;do_upload(""ocr_image""))
            {
                echo ""Maximum File Size Only 2 Mb Or Max Width = 2000 , Height = 2000"";
                exit;
            }
            else
            {
                $img_data = $this-&gt;upload-&gt;data();

                // Authenticating with a keyfile path.

                $imageAnnotator = new ImageAnnotatorClient([
                    'credentials' =&gt; base_url().'assets/google_cloud_vision/keyfile.json'
                ]);

                # annotate the image
                $response = $imageAnnotator-&gt;textDetection($img_data['full_path']);
                $texts = $response-&gt;getTextAnnotations();

                printf('%d texts found:' . PHP_EOL, count($texts));
                foreach ($texts as $text) {
                    print($text-&gt;getDescription() . PHP_EOL);

                    # get bounds
                    $vertices = $text-&gt;getBoundingPoly()-&gt;getVertices();
                    $bounds = [];
                    foreach ($vertices as $vertex) {
                        $bounds[] = sprintf('(%d,%d)', $vertex-&gt;getX(), $vertex-&gt;getY());
                    }
                    print('Bounds: ' . join(', ',$bounds) . PHP_EOL);
                }

                $imageAnnotator-&gt;close();

            }
        }
    }
</code></pre>

<p>But before process the text detection i have run into an error :</p>

<blockquote>
  <p><b>Fatal error</b>:  Class 'ImageAnnotatorClient' not found</p>
</blockquote>

<p>Which is this line : </p>

<p><strong>$imageAnnotator = new ImageAnnotatorClient([</strong></p>

<p>What could possible cause the error ? From the construct above i already include or require_once the Path to the Class.</p>

<p>Is there something that i have missed in here ?</p>

<p>Thank You</p>",55464616,1,2,,2019-04-01 02:53:31.547 UTC,,2019-04-01 22:47:07.480 UTC,,,,,6786634,1,0,php|codeigniter|google-cloud-vision,58
GMVTextLineFeature returns null when initialised with the image in iOS,51859539,GMVTextLineFeature returns null when initialised with the image in iOS,"<p>I have initialised the <code>textlinedetector</code> as below</p>

<pre><code>self.textDetector = [GMVDetector detectorOfType:GMVDetectorTypeText options:nil]; 
</code></pre>

<p>Since I want only the line and not the whole block, I'm directly accessing <code>GMVTextLineFeature</code> and the input image is of the type <code>UIImage</code>   directly from the camera preview. </p>

<pre><code>NSArray&lt;GMVTextLineFeature *&gt; *features = [self.textDetector featuresInImage:[_Result originalImage] options:nil]; 
</code></pre>

<p>But the above array is nil. </p>

<pre><code>   [myOperation setCompletionBlock:^{


                for (GMVTextLineFeature *textLine in features) {

                    NSLog(@""value of each element: %@"", textLine.value);

                    _Result.text = textLine.value;
                }



        [self finishDetection];
    }];

    [_operationQueue addOperation:myOperation];
</code></pre>

<p>My concern is that my project is in gradle and GoogleVision is built in cocoapods. So I manually copied the framework files into my project and linked it in <code>frameworks and libraries</code>. I also linked the resource files for the frameworks, where it contains all the conv config files under <code>copy resource bundles</code>. </p>

<p>Yet the <code>feature</code> object is <code>nil</code>. I have also clean built the project multiple times. Since I'm new to iOS, I'm unable to figure out whether it is theproblem with cocoapods to gradle or  the way it is implemented. But this is how it is implemented in the demo app <a href=""https://cocoapods.org/pods/GoogleMobileVision"" rel=""nofollow noreferrer"">TextDetectorDemo</a>. I'm using xcode 9.4. </p>

<p>Any insight or any workarounds will be much appreciated. </p>

<p>Thanks in advance.   </p>",,1,0,,2018-08-15 13:21:34.537 UTC,0,2018-08-22 15:25:32.817 UTC,2018-08-15 13:46:16.773 UTC,,8834241,,8834241,1,1,ios|gradle|cocoapods|google-vision|google-ios-vision,83
Can we train google vision OCR to read printed complex mathematical expressions?,42345567,Can we train google vision OCR to read printed complex mathematical expressions?,"<p>Google Vision OCR API is unable to read mathematical expressions, can we train it to  read the complex mathematical expressions? If yes, please let us know the procedure. If not, can you please suggest some other OCRs which can serve the purpose? We need them as API, which we can integrate with our app. </p>

<p>Thank you in Advance. </p>",,1,2,,2017-02-20 13:15:43.290 UTC,,2017-02-20 14:21:13.470 UTC,,,,,7593116,1,2,math|ocr|mathematical-expressions|google-vision,874
Increase the Maximum preview resolution for Fotoapparat,55739476,Increase the Maximum preview resolution for Fotoapparat,"<p>Currently the maximum preview resolution supported by Fotoapparat library for the frame processor is set to 1280x720 but Im using Google Cloud vision API and the recommended resolution can go up to 1600x1200 as mentioned in <a href=""https://cloud.google.com/vision/docs/supported-files"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/supported-files</a></p>

<p>How can I increase the maximum preview resolution to suit my needs for firing the appropriate detection request?</p>",,0,0,,2019-04-18 05:09:48.780 UTC,,2019-04-18 05:09:48.780 UTC,,,,,237403,1,0,android|google-cloud-vision|fotoapparat,17
Can we use Microsoft Emotion API in our Android Apps,42375271,Can we use Microsoft Emotion API in our Android Apps,"<p>Can we use Microsoft Emotion API in our Android Apps, considering the fact that it's still in its 'Preview' mode, can we create our own customized app using the code of EMOTION API to recognize the moods of users in our own app?</p>",,1,1,,2017-02-21 18:34:38.970 UTC,,2017-02-21 18:41:17.257 UTC,,,,,3443158,1,-2,java|android|emotion,262
Google Cloud Vision API returning nothting for Type = TEXT_DETECTION,38280779,Google Cloud Vision API returning nothting for Type = TEXT_DETECTION,"<p>I'm working on an OCR android application using Google Cloud Vision API</p>

<p>For testing I've used the sample application which is provided by Google</p>

<p><a href=""https://github.com/GoogleCloudPlatform/cloud-vision/tree/master/android/CloudVision"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/cloud-vision/tree/master/android/CloudVision</a></p>

<p>I've tested it for type ""LABEL_DETECTION"" and it works fine</p>

<p>I've updated this sample application to work for type ""TEXT_DETECTION"" instead of ""LABEL_DETECTION""</p>

<p>I've tested it using this image and it return ""nothing"" result</p>

<p>[ocr_image]</p>

<p><a href=""https://i.stack.imgur.com/igtRe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/igtRe.png"" alt=""enter image description here""></a></p>

<p>Appreciate if anyone knows what is the issue</p>

<p>Thanks in advance</p>",38511016,1,0,,2016-07-09 10:20:19.703 UTC,,2016-07-21 17:46:21.830 UTC,2016-07-09 10:24:05.877 UTC,,5642279,,4650010,1,0,android|google-cloud-platform|google-cloud-vision,578
Google Vision API: Support for negative colors on QRCode detector,34925092,Google Vision API: Support for negative colors on QRCode detector,"<p>I was working on a QRCode reader within my app using the Vision API. I realized the API can't detect negative colors. My customer has thousand of cards with white QRCodes on a blue background.</p>

<p>Attached is an example of 2 qrcodes. The first works perfectly (default colors). The second one doesn't (white on blue). Some commercial QRCode readers are able to read it but I couldn't discover how they do it.</p>

<p>I really want to avoid using third party libs and apps to do this. So far, I'm using the NEGATIVE effect on the camera. But this is a workaround that I want to avoid too.</p>

<p>Invert the bitmap on the fly is very slow and is out of question.</p>

<p>I read somewhere that detection of negative colors is optional on the QRCode specification and it seems that Google Vision API doesn't support it. Suggestions?</p>

<p>Thank you for your attention.</p>

<p><a href=""http://i.stack.imgur.com/I7q55.png"" rel=""nofollow"">QRCode Working</a></p>

<p><a href=""http://i.stack.imgur.com/dt8Af.png"" rel=""nofollow"">QRCode Not Working</a></p>",,0,6,,2016-01-21 13:25:12.557 UTC,,2016-01-21 13:25:12.557 UTC,,,,,2300034,1,3,android|google-vision|android-vision,495
How to authenticate Google Cloud Vision via HTTP Request,50657314,How to authenticate Google Cloud Vision via HTTP Request,"<p>I am trying to use Google Cloud Vision via a rest http request using c#. As described <a href=""https://cloud.google.com/vision/docs/auth"" rel=""nofollow noreferrer"">here</a>, I tried to authenticate with the api key as a parameter:</p>

<pre><code>string uri = ""https://vision.googleapis.com/v1/images:annotate?key="" + API_KEY;
HttpRequestMessage request = new HttpRequestMessage(HttpMethod.Post, uri);
[...]
response = await client.SendAsync(request);
</code></pre>

<p>However, I always get a 403 PERMISSION DENIED Code:</p>

<blockquote>
  <p>Cloud Vision API has not been used in project XXXXX before or it is
  disabled. Enable it by visiting
  <a href=""https://console.developers.google.com/apis/api/vision.googleapis.com/overview?project=XXXXXXX"" rel=""nofollow noreferrer"">https://console.developers.google.com/apis/api/vision.googleapis.com/overview?project=XXXXXXX</a>
  then retry. If you enabled this API recently, wait a few minutes for
  the action to propagate to our systems and retry.</p>
</blockquote>

<p>Of course I checked, The API is activated, enabled and there are no API restrictions:
<a href=""https://i.stack.imgur.com/wesXo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wesXo.png"" alt=""API Key: No restrictions""></a></p>

<p>Since there seemed to exist <a href=""https://github.com/GoogleCloudPlatform/google-cloud-ruby/issues/706"" rel=""nofollow noreferrer"">some problems</a> with that authentication method, especially with cloud vision, I tried authentication via an access token of a service account that I created. I gave the service account full access just to be sure that there is no issue with the rights of the service account: </p>

<pre><code>string uri = ""https://vision.googleapis.com/v1/images:annotate"";
HttpRequestMessage request = new HttpRequestMessage(HttpMethod.Post, uri);
request.Headers.Add(""Authorization"", ""Bearer "" + ACCESS_TOKEN);
response = await client.SendAsync(request);
</code></pre>

<p>Still, same error message. Same goes with curl:</p>

<pre><code>curl -k -X POST -H ""Content-Type:application/json"" 
-d ""@{REQUEST_AS_JSON_OR_PATH_TO_JSON-FILE}"" 
https://vision.googleapis.com/v1/images:annotate?key={API_KEY}
</code></pre>

<p>What am I missing out?</p>",50661027,1,0,,2018-06-02 13:15:44.487 UTC,1,2018-06-02 20:56:41.353 UTC,2018-06-02 20:53:34.340 UTC,,6539634,,6539634,1,0,c#|rest|authentication|google-cloud-platform|google-cloud-vision,312
Analysing URL's using Google Cloud Vision - Python,36729360,Analysing URL's using Google Cloud Vision - Python,"<p>Is there anyway I can analyse URL's using Google Cloud Vision. I know how to analyse images that I store locally, but I can't seem to analyse jpg's that exist on the internet:</p>

<pre><code>import argparse
import base64
import httplib2
from googleapiclient.discovery import build
import collections
import time
import datetime
import pyodbc

time_start = datetime.datetime.now()

def main(photo_file):
    '''Run a label request on a single image'''

    API_DISCOVERY_FILE = 'https://vision.googleapis.com/$discovery/rest?version=v1'
    http = httplib2.Http()

    service = build('vision', 'v1', http, discoveryServiceUrl=API_DISCOVERY_FILE, developerKey=INSERT API KEY HERE)

    with open(photo_file, 'rb') as image:
        image_content = base64.b64encode(image.read())
        service_request = service.images().annotate(
            body={
                'requests': [{
                    'image': {
                        'content': image_content
                    },
                    'features': [{
                        'type': 'LOGO_DETECTION',
                        'maxResults': 10,
                    }]
                }]
            })
        response = service_request.execute()

        try:
            logo_description = response['responses'][0]['logoAnnotations'][0]['description']
            logo_description_score = response['responses'][0]['logoAnnotations'][0]['score']
            print logo_description
            print logo_description_score 
        except KeyError:
            print ""logo nonexistent"" 
            pass

        print time_start

if __name__ == '__main__':
    main(""C:\Users\KVadher\Desktop\image_file1.jpg"")
</code></pre>

<p>Is there anyway I can analyse a URL and get an answer as to whether there are any logo's in them? </p>",,2,0,,2016-04-19 20:59:39.270 UTC,,2016-04-20 13:44:22.457 UTC,2016-04-19 21:25:41.323 UTC,,881229,,5080273,1,-1,python|google-cloud-vision,321
Getting Empty Response from Google Vision API,38417738,Getting Empty Response from Google Vision API,"<p>I am testing some features of the Google Vision API and getting Empty response for images which I have clicked from my camera(5MP camera). However when I download any image from web for Example, an image of a delivery guy (with the plain background such as white) I get a meaningful response with labels. Both the sets of images are present on my local disk. Below is the code which I have written by taking reference from google's documentation,</p>

<pre><code>public class ImageAnalyzer {

final static String APPLICATION_NAME =""My_APP/1.0"";
final static String IMAGE_PATH = ""E:/Vision/SampleImages/IMAG0013.jpg"";
final static int maxResults =3;

private Vision vision;

public ImageAnalyzer(Vision vision){
    this.vision=vision;
}
  /**
   * Connects to the Vision API using Application Default Credentials.
   */
  public static Vision getVisionService() throws IOException, GeneralSecurityException {
    GoogleCredential credential =
        GoogleCredential.getApplicationDefault().createScoped(VisionScopes.all());
    JsonFactory jsonFactory = JacksonFactory.getDefaultInstance();
    return new Vision.Builder(GoogleNetHttpTransport.newTrustedTransport(), jsonFactory, credential)
            .setApplicationName(APPLICATION_NAME)
            .build();
  }

  public Map&lt;String, List&lt;String&gt;&gt; labelImage(String imagePath){

      final Map&lt;String,List&lt;String&gt;&gt; labels = new HashMap&lt;String, List&lt;String&gt;&gt;();

      final Path path = Paths.get(imagePath);  

      try {

        final byte[] raw = Files.readAllBytes(path);

        /*final AnnotateImageRequest request =new AnnotateImageRequest().setImage(new Image().encodeContent(raw))
        .setFeatures(ImmutableList.of(new Feature().setType(""LABEL_DETECTION"").setMaxResults(3)
                , new Feature().setType(""LOGO_DETECTION"").setMaxResults(3)));*/

        AnnotateImageRequest request =
                new AnnotateImageRequest()
                    .setImage(new Image().encodeContent(raw)).setFeatures(ImmutableList.of(
                new Feature()
                    .setType(""LABEL_DETECTION"")
                    .setMaxResults(maxResults),
                new Feature()
                    .setType(""LOGO_DETECTION"")
                    .setMaxResults(1),
                new Feature()
                    .setType(""TEXT_DETECTION"")
                    .setMaxResults(maxResults),
                new Feature()
                    .setType(""LANDMARK_DETECTION"")
                    .setMaxResults(1)));


        final Vision.Images.Annotate annotate = vision.images().annotate(
                new BatchAnnotateImagesRequest().
                setRequests(ImmutableList.of(request)));

        final BatchAnnotateImagesResponse batchResponse = annotate.execute();
        //assert batchResponse.getResponses().size() == 1;

        System.out.println(""Size of searches""+batchResponse.getResponses().size());

        //final AnnotateImageResponse response = batchResponse.getResponses().get(0);
        if (batchResponse.getResponses().get(0).getLabelAnnotations() != null) {

            final List&lt;String&gt; label = new ArrayList&lt;String&gt;();
            for (EntityAnnotation ea: batchResponse.getResponses().get(0).getLabelAnnotations()) {
                label.add(ea.getDescription());
            }
            labels.put(""LABEL_ANNOTATION"", label);
        }

        if (batchResponse.getResponses().get(0).getLandmarkAnnotations() != null) {
            final List&lt;String&gt; landMark = new ArrayList&lt;String&gt;();
            for (EntityAnnotation ea : batchResponse.getResponses().get(0).getLandmarkAnnotations()) {
                landMark.add(ea.getDescription());
            }
            labels.put(""LANDMARK_ANNOTATION"", landMark);
        }

        if (batchResponse.getResponses().get(0).getLogoAnnotations() != null) {
            final List&lt;String&gt; logo = new ArrayList&lt;String&gt;();
            for (EntityAnnotation ea : batchResponse.getResponses().get(0).getLogoAnnotations()) {
                logo.add(ea.getDescription());
            }
            labels.put(""LOGO_ANNOTATION"", logo);
        }

        if (batchResponse.getResponses().get(0).getTextAnnotations() != null) {

            List&lt;String&gt; text = new ArrayList&lt;String&gt;();
            for (EntityAnnotation ea : batchResponse.getResponses().get(0).getTextAnnotations()) {
                text.add(ea.getDescription());
            }
            labels.put(""TEXT_ANNOTATION"", text);
        }


        return labels;

    } catch (IOException e) {
        e.printStackTrace();
    }
      return null;

  }

  public static void printAnnotations(final List&lt;EntityAnnotation&gt; entityAnnotations) {

      if(entityAnnotations!=null &amp;&amp; !entityAnnotations.isEmpty()) {
          for (final EntityAnnotation entityAnnotation : entityAnnotations) {
              final String desc = entityAnnotation.getDescription();
              final Float score = entityAnnotation.getScore();
              System.out.println(desc+""     ""+score);
          }

      }
  }

  public static void main(String[] args) throws IOException, GeneralSecurityException {
        // TODO Auto-generated method stub
        final ImageAnalyzer analyzer = new ImageAnalyzer(getVisionService());
        final Map&lt;String, List&lt;String&gt;&gt; labels = analyzer.labelImage(IMAGE_PATH);

        for (Entry&lt;String, List&lt;String&gt;&gt; entry : labels.entrySet()) {

             final String key = entry.getKey();
             final List&lt;String&gt; value = entry.getValue();
             if(value!=null &amp;&amp; !value.isEmpty()) {

             System.out.println(""Printing for key""+key);
             for (final String myLebel : value) {

                 System.out.println("" ""+myLebel);
               }

             }
        }

       //System.out.println(System.getenv(""GOOGLE_APPLICATION_CREDENTIALS""))            
    }
</code></pre>

<p>}</p>

<p>Can anyone help me out?</p>",,1,2,,2016-07-17 03:50:20.613 UTC,,2016-07-18 16:56:14.373 UTC,2016-07-18 16:10:55.703 UTC,,5231007,,2206366,1,1,java|image-recognition|google-cloud-vision,780
Android Google Vision API TextRecognizer isOperational always returns false on some devices,46630157,Android Google Vision API TextRecognizer isOperational always returns false on some devices,"<p>I want to try one of google vision api on android. I added this line in gradle file</p>

<pre><code>compile 'com.google.android.gms:play-services-vision:11.0.4'
</code></pre>

<p>also in manifest file I added this</p>

<pre><code> &lt;meta-data
            android:name=""com.google.android.gms.vision.DEPENDENCIES""
            android:value=""ocr"" /&gt;
</code></pre>

<p>And here's the code snippet </p>

<pre><code>TextRecognizer textRecognizer = new TextRecognizer.Builder(this).build();
        try {
            if (!textRecognizer.isOperational()) {
                new AlertDialog.
                        Builder(this).
                        setMessage(""Text recognizer could not be set up on your device"").show();
                return;
            }
}
</code></pre>

<p>So here <code>textRecognizer.isOperational()</code> always returns false. I looked to other questions and found nothing that helped me. (Device storage left 4 gb, so it's not storage issue). What I'm missing? UPDATE: I'm using LG G Flex 2 And tested same code on Samsung J7 (2017) and it works perfectly. So why G FLex2 fails?</p>",,0,0,,2017-10-08 10:40:35.640 UTC,1,2017-10-13 06:48:43.517 UTC,2017-10-13 06:48:43.517 UTC,,3334375,,3334375,1,2,java|android|google-api|ocr|vision-api,402
How to add Google vision dependencies to my .aar?,43657393,How to add Google vision dependencies to my .aar?,"<p>I am creating an android library (.aar) that is using the Google android vision Gradle dependencies for OCRing. But I am unable to figure out how should I can add the Gradle dependency to the .aar File.</p>

<p>I don't want to add Google dependency separately while using my .aar because my library project already contains the same.</p>

<p>I have tried one solution by pushing the .aar file to local maven then using the same in the application but in that case I was still unable to find the Google Vision classes to use.</p>

<p>Thanks.</p>",,1,0,,2017-04-27 12:09:31.567 UTC,,2017-04-27 14:12:12.723 UTC,2017-04-27 14:12:12.723 UTC,user3572586,,user3572586,,1,0,java|android|vision|google-vision,209
Does Google Cloud Vision API support face recognition or face identification?,41803160,Does Google Cloud Vision API support face recognition or face identification?,"<p>I am looking for a Google Cloud API that can do both face recognition and identification. My understanding is that the Google Cloud Vision API will support only face detection, but not recognition.</p>

<p>Is there any Google Cloud API that can do face recognition?</p>",,3,1,,2017-01-23 09:41:35.747 UTC,2,2017-07-04 13:36:16.673 UTC,2017-07-04 13:36:16.673 UTC,,81019,,6458601,1,4,google-cloud-platform|google-cloud-vision,4039
AWS Rekognition FaceSearch - no notification of job completion,51880350,AWS Rekognition FaceSearch - no notification of job completion,"<p>Has anyone successfully completed a FaceSearch?</p>

<p>I submitted a Face Search with the .Net API around 8am Eastern 8/13/2018 and my Queue has not yet been notified that the job is complete.  The HttpStatusCode of the response from StartFaceSearch was: OK</p>

<p>I have a Queue subscribed to the Topic that I requested to be notified at.  I published a test message to the Topic and the Queue did pick it up.</p>

<p>Here is the code... (identifiers redacted)</p>

<pre><code>            // upload the video to Amazon
            IAmazonS3 client = new AmazonS3Client();
            // 1. Put object-specify only key name for the new object.
            var putRequest1 = new PutObjectRequest
            {
                BucketName = ""mybucket.com"",
                Key = dr[""video_id""].ToString(),
                FilePath = Path.Combine(videoPath, dr[""video_path""].ToString()),
                ContentType = ""video/mp4""
            };
            PutObjectResponse response1 = await client.PutObjectAsync(putRequest1);

            // upload the image to Amazon
            byte[] image = (byte[])(dr[""photobinary""]);
            MemoryStream stream = new MemoryStream(image);

            using (Amazon.S3.Transfer.TransferUtility tranUtility =
              new Amazon.S3.Transfer.TransferUtility(client))
            {
                tranUtility.Upload(stream, ""bucket.com"", dr[""SSN""].ToString());
            }
            stream.Close();

            // create a Rekognition Collection
            IAmazonRekognition rekClient = new AmazonRekognitionClient();

            try
            {
                // try to delete in case it already exists
                rekClient.DeleteCollection(new DeleteCollectionRequest
                {
                    CollectionId = dr[""ssn""].ToString()
                });
            }
            catch { }

            var response = rekClient.CreateCollection(new CreateCollectionRequest
            {
                CollectionId = dr[""ssn""].ToString()
            });

            string collectionArn = response.CollectionArn;
            int statusCode = response.StatusCode;

            var facesResponse = rekClient.IndexFaces(new IndexFacesRequest
            {
                CollectionId = dr[""ssn""].ToString(),
                DetectionAttributes = new List&lt;string&gt;
                {

                },
                ExternalImageId = dr[""ssn""].ToString(),
                Image = new Image
                {
                    S3Object = new Amazon.Rekognition.Model.S3Object
                    {
                        Bucket = ""bucket.com"",
                        Name = dr[""ssn""].ToString()
                    }
                }
            });

            List&lt;FaceRecord&gt; faceRecords = facesResponse.FaceRecords;
            string orientationCorrection = facesResponse.OrientationCorrection;

            var faceSearchResponse = rekClient.StartFaceSearch(new StartFaceSearchRequest
            {
                CollectionId = dr[""ssn""].ToString(),
                Video = new Video
                {
                    S3Object = new Amazon.Rekognition.Model.S3Object
                    {
                        Bucket = ""videobucket.com"",
                        Name = dr[""video_id""].ToString()
                    }
                },
                NotificationChannel = new NotificationChannel
                {
                    SNSTopicArn = ""arn:xxx"",
                    RoleArn = ""arn:xxx""
                }
            });
</code></pre>",51880906,1,0,,2018-08-16 15:14:20.563 UTC,,2018-08-16 15:46:51.983 UTC,,,,,2132098,1,0,amazon-rekognition,32
Unable to cv2.imwrite in django static folder from django views,49936444,Unable to cv2.imwrite in django static folder from django views,"<p>I am unable to save the image in <code>static</code> but can save in the same rekognition directory. How do I save a snapshot from camera to static folder? </p>

<p>My OpenCV code is running in post method of <code>views.py</code>.</p>

<p>In <code>views.py</code>:</p>

<pre><code>cap = cv2.VideoCapture(0)
ret, img = cap.read()  # Read the image from webcam
cv2.imwrite('static/video_snapshot.jpg', img)
</code></pre>

<p><a href=""https://i.stack.imgur.com/yue4a.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yue4a.jpg"" alt=""Tree of the directory""></a>  </p>",,0,3,,2018-04-20 07:35:30.157 UTC,,2018-04-21 10:44:02.593 UTC,2018-04-21 10:44:02.593 UTC,,3962537,,8040193,1,0,python|django|opencv|static,83
crop landmarks part of the face using Google vision API,42363426,crop landmarks part of the face using Google vision API,<p>I am trying to crop out the face from an image using google vision api. I understood how to get the landmarks from their sample code. But I don't know how to crop the face using those landmarks. Can someone provide some pseudo code or guide me on how to proceed further thanks.</p>,,1,1,,2017-02-21 09:27:43.093 UTC,,2017-02-21 18:58:42.537 UTC,,,,,7597694,1,1,java|android|google-vision,467
How to set full screen in Android Vision API?,48529675,How to set full screen in Android Vision API?,"<p>I want to create a barcode scanner that uses full screen.
I used google's vision API samples that can be found <a href=""https://github.com/googlesamples/android-vision"" rel=""nofollow noreferrer"">here</a></p>

<p>This is my result:</p>

<p><img src=""https://i.stack.imgur.com/L0p5o.jpg"" alt=""image""></p>

<p>I want to make a preview of the camera to the full height, how can I do it?</p>",48530941,1,0,,2018-01-30 20:06:58.653 UTC,,2018-09-30 18:47:26.380 UTC,2018-01-31 11:17:05.557 UTC,,1401510,,4788738,1,0,java|android,242
Why would Computer Vision recognize more text when submitting a subset of the image?,51389440,Why would Computer Vision recognize more text when submitting a subset of the image?,"<p>So I am attempting to use Azure Computer Vision OCR to recognize text in a jpg image.  The image is about 2000x3000 pixels and is a picture of a contract.  I want to get all the text and the bounding boxes.  The image DPI is over 300 and it's quality is very clear.  I noticed that a lot of text was being skipped so I cropped a section of the image and submitted that instead.  This time it recognized text that it did not recognize before.  Why would it do this?  If the quality of the image never changed and the image was within the bounds of the resolution requirements, why is it skipping texts?</p>",,0,7,,2018-07-17 20:09:15.840 UTC,,2018-07-17 20:09:15.840 UTC,,,,,1973080,1,0,computer-vision|ocr,31
Google vision API changes app pictures with its own pictures,49335721,Google vision API changes app pictures with its own pictures,"<p>I am making an app using Google Vision API for face detection. As the pictures show, my app displays two images normally without Google Vision, but with it Google Vision automatically changes the two images with its own.</p>

<p>The images are stored inside the drawable folder, stored in an SQLite database through their id (for example, R.drawable.crown_flowers) and fetched at runtime from the database.</p>

<p>The code that performs the face detection itself is not responsible for the behavior. Simply having </p>

<pre><code>compile 'com.google.android.gms:play-services-vision:9.8.0'
</code></pre>

<p>in the build.gradle file causes this behavior, even if the library is not referenced anywhere in the actual code. I have tried using a more recent version of the library (11.8.0) but to no avail.</p>

<p><a href=""https://i.stack.imgur.com/yC4tq.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yC4tq.jpg"" alt=""Without Google Vision""></a></p>

<p><a href=""https://i.stack.imgur.com/W5Xm6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W5Xm6.jpg"" alt=""With Google Vision""></a></p>

<p><strong>Edit:</strong> doing some debugging I found that the problem is with the SQLite database. If I reference the pictures only by their drawable id without fetching them from the database, the app works correctly. The problem is that this app is for a school project and I am required to use SQLite. This is code for the database:</p>

<p>From the DatabaseConnector class:</p>

<pre><code>public class DatabaseConnector extends SQLiteOpenHelper {


    public DatabaseConnector(Context context, String name, SQLiteDatabase.CursorFactory factory, int version) {
        super(context, name, factory, version);
    }

    public void queryData(String sqlString) {
        SQLiteDatabase database = getWritableDatabase();
        database.execSQL(sqlString);
    }

    public void insertData(String name, int imgUrl) {
        SQLiteDatabase database = getWritableDatabase();
        String sqlString = ""INSERT INTO FILTER VALUES (NULL, ?, ?);"";

        SQLiteStatement statement = database.compileStatement(sqlString);
        statement.bindString(1, name);
        statement.bindLong(2, imgUrl);
        statement.execute();
    }

    public Cursor getData(String sqlString) {
        SQLiteDatabase database = getReadableDatabase();

        return database.rawQuery(sqlString, null);
    }
}
</code></pre>

<p>This is where data is inserted in the database inside the Activity class:</p>

<pre><code>private void connectDatabase() {
    databaseConnector = new DatabaseConnector(this, ""FilterDB.sqlite"", null, 1);

    databaseConnector.queryData(""CREATE TABLE IF NOT EXISTS FILTER (Id INTEGER PRIMARY KEY AUTOINCREMENT, name VARCHAR, image INTEGER);"");


    if (databaseConnector.getData(""SELECT * FROM FILTER"").getCount() &lt; 2) {
        databaseConnector.insertData(""Primavera"", R.drawable.crown_flowers);
        databaseConnector.insertData(""Desir"", R.drawable.sparkle);
    }
}
</code></pre>

<p>And this is where the images are fetched from the database:</p>

<pre><code>Cursor cursor = ((HomepageActivity)mActivity).databaseConnector.getData(""SELECT * FROM FILTER"");

    if (cursor.moveToFirst() &amp;&amp; position &gt; 16) {
        mFilters.clear();
        do {
            int id = cursor.getInt(0);
            String name = cursor.getString(1);
            int imgId = (int)cursor.getLong(2);

            mFilters.add(new OverFilter(id, name, imgId));

        } while (cursor.moveToNext());
    }
</code></pre>

<p>Just for clarity, this is the OverFilter class:</p>

<pre><code>public class OverFilter {

    private String mName;
    private int mImage;
    private int mId;

    public OverFilter(int mId, String mName, int mImage) {
        this.mName = mName;
        this.mImage = mImage;
        this.mId = mId;
    }

    public String getName() {
        return mName;
    }

    public void setName(String mName) {
        this.mName = mName;
    }

    public int getImage() {
        return mImage;
    }

    public void setImage(int mImage) {
        this.mImage = mImage;
    }

    public int getId() {
        return mId;
    }

    public void setId(int mId) {
        this.mId = mId;
    }
}
</code></pre>

<p>mFilter is just an ArrayList of OverFilter.</p>",,1,3,,2018-03-17 11:58:13.160 UTC,,2018-03-17 13:52:00.157 UTC,2018-03-17 13:17:23.747 UTC,,5406318,,5406318,1,0,android|google-vision,23
"Google Vision, ImageAnnotatorClient error: code: 3, message: ""Bad image data.""",55929206,"Google Vision, ImageAnnotatorClient error: code: 3, message: ""Bad image data.""","<p>Getting <code>Bad image data</code> when trying to detect text in a part (numpy array rectangle out) of an opencv image. When I try to convert the resulting base64 string online it works without issues <a href=""https://codebeautify.org/base64-to-image-converter"" rel=""nofollow noreferrer"">(here for example)</a></p>

<pre><code>im = Image.fromarray(narray.astype(""uint8""))
rawBytes = io.BytesIO()
im.save(rawBytes, ""PNG"")
rawBytes.seek(0)  # return to the start of the file
image = vision.types.Image(content = b64encode(rawBytes.read()))
resp = client.text_detection(image = image)
</code></pre>

<p>I found <a href=""https://stackoverflow.com/questions/51735897/google-cloud-vision-api-error-code-3-message-bad-image-data"">this</a> on the topic but why would I need to do that?</p>

<p>Any ideas what I could be doing wrong?</p>",,0,0,,2019-04-30 22:03:09.200 UTC,,2019-04-30 22:03:09.200 UTC,,,,,4512646,1,1,python-3.x|image|base64|google-cloud-vision,18
@google-cloud/vision npm package Issue,55217560,@google-cloud/vision npm package Issue,"<p>Unable to install <strong>@google-cloud/vision</strong> npm package on Raspberry Pi.</p>

<h3>npm-log:</h3>

<pre><code>54 error git clone git@github.com:google-cloud/vision Cloning into bare repository '/root/.npm/_git-remotes/git-github-com-google-cloud-vision-41bc45c9'...
54 error git clone git@github.com:google-cloud/vision Warning: Permanently added the RSA host key for IP address '192.30.253.113' to the list of known hosts.
54 error git clone git@github.com:google-cloud/vision Permission denied (publickey).
54 error git clone git@github.com:google-cloud/vision fatal: Could not read from remote repository.
54 error git clone git@github.com:google-cloud/vision
54 error git clone git@github.com:google-cloud/vision Please make sure you have the correct access rights
54 error git clone git@github.com:google-cloud/vision and the repository exists.
55 silly lockFile 41bc45c9-t-github-com-google-cloud-vision git@github.com:google-cloud/vision
56 silly lockFile 41bc45c9-t-github-com-google-cloud-vision git@github.com:google-cloud/vision
57 silly lockFile d5a09ed3-google-cloud-vision google-cloud/vision
58 silly lockFile d5a09ed3-google-cloud-vision google-cloud/vision
59 error addLocal Could not install google-cloud/vision
60 error Error: ENOENT: no such file or directory, stat 'google-cloud/vision'
61 error If you need help, you may report this *entire* log,
61 error including the npm and node versions, at:
61 error     &lt;http://github.com/npm/npm/issues&gt;
62 error System Linux 4.14.79-v7+
63 error command ""/usr/bin/node"" ""/usr/bin/npm"" ""install"" ""@google-cloud/vision""
64 error cwd /home/pi/voiceplatformtesting
65 error node -v v8.11.1
66 error npm -v 1.4.21
67 error path google-cloud/vision
68 error syscall stat
69 error code ENOENT
70 error errno -2
71 verbose exit [ -2, true ]
</code></pre>",,0,0,,2019-03-18 08:51:36.617 UTC,,2019-03-18 09:24:16.887 UTC,2019-03-18 09:24:16.887 UTC,,7404943,,11183728,1,0,npm-install|google-cloud-vision,32
How do I import google visions api into eclipse?,49711924,How do I import google visions api into eclipse?,<p>I am trying to use the google vision api in a java project in eclipse. I've looked for tutorials and looked over the steps that google provide but I am still lost. Can someone explain the steps i need to take in order to import the api?</p>,,0,0,,2018-04-07 20:55:58.837 UTC,1,2018-04-07 20:55:58.837 UTC,,,,,7274855,1,1,java|eclipse|google-api|google-vision,49
Issue getting AWS Rekognition to read images when placed in S3 bucket,51479671,Issue getting AWS Rekognition to read images when placed in S3 bucket,"<p>Have gotten really stuck trying to get AWS Rekognition to label images I upload to S3. I am still learning how to get the roles and acceess right (I have added 'all' Rekognition services as inline policies to all the Roles I have in IAM for this app I'm building to get some hands-on experience with AWS.</p>

<p>Below is all the code (apologies for the messy code - still learning). 
Further below that is the output from the tests I'm running in Lambda. </p>

<p>Could someone please help to suggest what I am doing wrong and how I could make some adjustments to get Rekognition to be able to scan the image and use list out what is in the image (eg; person, tree, car, etc).
Thanks in advance!!!</p>

<pre><code>'use strict';

let aws = require('aws-sdk');
let s3 = new aws.S3({ apiVersion: '2006-03-01' });
let rekognition =  new aws.Rekognition();
s3.bucket = 'arn:aws:s3:::XXXXXXX/uploads';

exports.handler = function(event, context) {

// Get the object from the event and show its content type
const eventn = event.Records[0].eventName;
const filesize = event.Records[0].s3.object.size;
const bucket = event.Records[0].s3.bucket.name;
const key = decodeURIComponent(event.Records[0].s3.object.key.replace(/\+/g, ' '));
var eventText = JSON.stringify(event, null, 2);
console.log('print this out --&gt;' + eventText);
console.log('bucket name --&gt; ' + s3.bucket);
var filesizemod = ""-"";
if (typeof filesize == ""number"") {
    if      (filesize&gt;=1000000000) {filesizemod=(filesize/1000000000).toFixed(2)+' GB';}
    else if (filesize&gt;=1000000)    {filesizemod=(filesize/1000000).toFixed(2)+' MB';}
    else if (filesize&gt;=1000)       {filesizemod=(filesize/1000).toFixed(2)+' KB';}
    else                           {filesizemod=filesize+' bytes';}
} else if (typeof filesize !== 'undefined' &amp;&amp; filesize) {
    filesizemod = filesize;
}

var Rekparams = {
    Image: {
        S3Object: {Bucket: s3.bucket, Name: key }},
    MaxLabels: 10,
    MinConfidence: 0.0
};

console.log(""s3object is = "" + JSON.stringify(Rekparams));

var request = rekognition.detectLabels(Rekparams, function(err, data) {
    if(err){
        var errorMessage =  'Error in [rekognition-image-assessment].\r' + 
                            '   Function input ['+JSON.stringify(event, null, 2)+'].\r' +  
                            '   Error ['+err+'].';
        // Log error
        console.log(errorMessage, err.stack); 
        return(errorMessage, null);
    }
    else{
        console.log(""i get to here!!!! ****"")
        console.log('Retrieved Labels ['+JSON.stringify(data)+']');

        console.log(""i have got all the labels i need!!"")

        // Return labels as a JavaScript object that can be passed into the 
        // subsequent lambda function.
        return(null, Object.assign(data, event));
    }
});

 console.log(""not in label getting function!!"")
// Call detectLabels
//var request = rekognition.detectLabels(Rekparams);
//var request1 = rekognition.detectLabels(bucket, key);
//var labels = JSON.stringify(request1);

//console.log('Retrieved Labels ['+JSON.stringify(data)+']');

//DetectLabelsRequest request = new DetectLabelsRequest()
         // .withImage(new Image().withS3Object(new S3Object().withName(key).withBucket(s3.bucket))).withMaxLabels(10).withMinConfidence(75F);

var subjecttext=""Myfirstapp -&gt; New image uploaded"";
var eventText2 = ""\n\nFile: "" + key + ""\nSize: "" 
    + filesizemod 
    + ""\n\nPlease see my S3 bucket for images.""
    + ""\nThis is what is in the image:""
    + request;

var sns = new aws.SNS();
var params = {
    Message: eventText2, 
    Subject: subjecttext,  
    TopicArn: ""arn:aws:sns:XXXXXX""
};
sns.publish(params, context.done);
};
</code></pre>

<p>Test output from Lambda. Also note my S3 bucket is in the same region as my Lambda function:</p>

<pre><code>Response:
{
  ""ResponseMetadata"": {
    ""RequestId"": ""a08afc8a-d2a4-5a8a-a435-af4503295913""
  },
  ""MessageId"": ""5f1c516b-c52f-5aa1-8af3-02a414a2c938""
}

Request ID:
""1b17d85f-8e77-11e8-a89d-e723ca75e0cf""

Function Logs:
 ""1970-01-01T00:00:00.000Z"",
      ""requestParameters"": {
        ""sourceIPAddress"": ""127.0.0.1""
      },
      ""s3"": {
        ""configurationId"": ""testConfigRule"",
        ""object"": {
          ""eTag"": ""0123456789abcdef0123456789abcdef"",
          ""key"": ""HappyFace.jpg"",
          ""sequencer"": ""0A1B2C3D4E5F678901"",
          ""size"": 1024
        },
        ""bucket"": {
          ""ownerIdentity"": {
            ""principalId"": ""EXAMPLE""
          },
          ""name"": ""sourcebucket"",
          ""arn"": ""arn:aws:s3:::mybucket""
        },
        ""s3SchemaVersion"": ""1.0""
      },
      ""responseElements"": {
        ""x-amz-id-2"": ""EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH"",
        ""x-amz-request-id"": ""EXAMPLE123456789""
      },
      ""awsRegion"": ""us-east-1"",
      ""eventName"": ""ObjectCreated:Put"",
      ""userIdentity"": {
        ""principalId"": ""EXAMPLE""
      },
      ""eventSource"": ""aws:s3""
    }
  ]
}
2018-07-23T12:51:24.864Z    1b17d85f-8e77-11e8-a89d-e723ca75e0cf    bucket name --&gt; arn:aws:s3:::XXXXXXXX/uploads
2018-07-23T12:51:24.865Z    1b17d85f-8e77-11e8-a89d-e723ca75e0cf    s3object is = {""Image"":{""S3Object"":{""Bucket"":""arn:aws:s3:::XXXXXXX/uploads"",""Name"":""HappyFace.jpg""}},""MaxLabels"":10,""MinConfidence"":0}
2018-07-23T12:51:25.427Z    1b17d85f-8e77-11e8-a89d-e723ca75e0cf    not in label getting function!!
2018-07-23T12:51:25.925Z    1b17d85f-8e77-11e8-a89d-e723ca75e0cf    Error in [rekognition-image-assessment].
   Function input [{
  ""Records"": [
    {
      ""eventVersion"": ""2.0"",
      ""eventTime"": ""1970-01-01T00:00:00.000Z"",
      ""requestParameters"": {
        ""sourceIPAddress"": ""127.0.0.1""
      },
      ""s3"": {
        ""configurationId"": ""testConfigRule"",
        ""object"": {
          ""eTag"": ""0123456789abcdef0123456789abcdef"",
          ""key"": ""HappyFace.jpg"",
          ""sequencer"": ""0A1B2C3D4E5F678901"",
          ""size"": 1024
        },
        ""bucket"": {
          ""ownerIdentity"": {
            ""principalId"": ""EXAMPLE""
          },
          ""name"": ""sourcebucket"",
          ""arn"": ""arn:aws:s3:::mybucket""
        },
        ""s3SchemaVersion"": ""1.0""
      },
      ""responseElements"": {
        ""x-amz-id-2"": ""EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH"",
        ""x-amz-request-id"": ""EXAMPLE123456789""
      },
      ""awsRegion"": ""us-east-1"",
      ""eventName"": ""ObjectCreated:Put"",
      ""userIdentity"": {
        ""principalId"": ""EXAMPLE""
      },
      ""eventSource"": ""aws:s3""
    }
  ]
}].
   Error [ValidationException: 1 validation error detected: Value 'arn:aws:s3:::XXXXXX/uploads' at 'image.s3Object.bucket' failed to satisfy constraint: Member must satisfy regular expression pattern: [0-9A-Za-z\.\-_]*]. ValidationException: 1 validation error detected: Value 'arn:aws:s3:::XXXXXXXX/uploads' at 'image.s3Object.bucket' failed to satisfy constraint: Member must satisfy regular expression pattern: [0-9A-Za-z\.\-_]*
    at Request.extractError (/var/runtime/node_modules/aws-sdk/lib/protocol/json.js:48:27)
    at Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:105:20)
    at Request.emit (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:77:10)
    at Request.emit (/var/runtime/node_modules/aws-sdk/lib/request.js:683:14)
    at Request.transition (/var/runtime/node_modules/aws-sdk/lib/request.js:22:10)
    at AcceptorStateMachine.runTo (/var/runtime/node_modules/aws-sdk/lib/state_machine.js:14:12)
    at /var/runtime/node_modules/aws-sdk/lib/state_machine.js:26:10
    at Request.&lt;anonymous&gt; (/var/runtime/node_modules/aws-sdk/lib/request.js:38:9)
    at Request.&lt;anonymous&gt; (/var/runtime/node_modules/aws-sdk/lib/request.js:685:12)
    at Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:115:18)
END RequestId: 1b17d85f-8e77-11e8-a89d-e723ca75e0cf
REPORT RequestId: 1b17d85f-8e77-11e8-a89d-e723ca75e0cf  Duration: 1309.41 ms    Billed Duration: 1400 ms    Memory Size: 128 MB Max Memory Used: 36 MB  
</code></pre>",,1,0,,2018-07-23 13:11:55.637 UTC,,2019-05-21 18:26:29.763 UTC,,,,,9577899,1,0,amazon-web-services|amazon-s3|amazon-sns|amazon-rekognition|amazon-mobile-hub,170
Android OCR project,41872763,Android OCR project,"<p>I am working in OCR android Application.
Now I can take images and extract word easily using google vision API
but the result is not <strong>100%</strong> according to the <strong>angle</strong> of capturing the image.
and <strong>illumination</strong>.
So I tried to make some image processing techniques on image before extracting text. but I search a lot but I can't deiced what is the best image processing technique to use.<strong>(blurring,filtering)</strong> to smooth image and improve its quality.
<strong>So</strong>  if there is any libraries or guide line to follow up with it in this subject.
<strong>How</strong>  to improve image quality before extracting text</p>

<p>I testes this libraries for OCR operation<p>
<a href=""https://github.com/tesseract-ocr/tesseract"" rel=""nofollow noreferrer"">Tesseract</a><p>
<a href=""https://developers.google.com/vision/"" rel=""nofollow noreferrer"">Google Mobile Vision APIs</a></p>",41897787,1,0,,2017-01-26 11:45:40.217 UTC,,2017-02-06 18:38:48.520 UTC,2017-01-30 08:44:05.373 UTC,,6177391,,6177391,1,-1,android|image-processing|ocr|tesseract|google-vision,431
How to run a ktor application inside AWS lambda?,48923406,How to run a ktor application inside AWS lambda?,"<p>I don't find a way to use a ktor application inside an AWS lambda...</p>

<p>That is, instead of starting an embedded server or using an external server as described in <a href=""http://ktor.io/servers/engine.html"" rel=""nofollow noreferrer"">http://ktor.io/servers/engine.html</a>, I just need to ""execute"" the pipeline.</p>

<p>I suppose this is more or less like the TestEngine but I am not so familiar with the ktor framework to be sure</p>

<p>Note :</p>

<p>I have already found examples to run one kotlin function per lambda (the best tutorial IMHO being <a href=""https://aws.amazon.com/fr/blogs/machine-learning/use-amazon-rekognition-to-build-an-end-to-end-serverless-photo-recognition-system/"" rel=""nofollow noreferrer"">https://aws.amazon.com/fr/blogs/machine-learning/use-amazon-rekognition-to-build-an-end-to-end-serverless-photo-recognition-system/</a>).</p>

<p>The problem is I dont want to manage one lambda per function (I want one microservice per lambda, the microservice being responsible for multiple tightly coupled operations)</p>",,1,0,,2018-02-22 09:09:07.297 UTC,,2018-02-22 10:31:54.553 UTC,,,,,1545567,1,0,amazon-web-services|aws-lambda|ktor,524
Aws Lambda function returns confidence 99% to anything i upload,53578143,Aws Lambda function returns confidence 99% to anything i upload,"<p>I am doing some thing wrong over here, while comparing two images in different S3 Bucket.</p>

<p>Even though, I am comparing images of male and female it would give 99% confidence</p>

<p>or am i missing something in the declaration yet</p>

<p>Maybe This line is causing a problem                                          </p>

<blockquote>
  <p>key_target = ""targett/"" + key</p>
</blockquote>

<p>Or my event code is error prone this is where i have mentioned my source bucket ,even though i have mentioned it in lambda function for testing below. What else do i need to correct so that it will return the confidence within the rang specified</p>

<pre><code>from __future__ import print_function

import boto3
from decimal import Decimal
import json
import urllib

print('Loading function')

rekognition = boto3.client('rekognition')
#iot = boto3.client('iot-data')


 # --------------- Helper Functions to call Rekognition APIs ------------------

def compare_faces(bucket, key, key_target, threshold=75):
response = rekognition.compare_faces(
    SourceImage={
        ""S3Object"": {
            ""Bucket"": 'dacss',
            ""Name"": 'obama.jpg',
        }
    },
    TargetImage={
        ""S3Object"": {
            ""Bucket"": 'targett',
            ""Name"": 'michelle.jpg',
        }
    },
    SimilarityThreshold=threshold,
)
return response['SourceImageFace'], response['FaceMatches']

 # --------------- Main handler ------------------


def lambda_handler(event, context):
print(""Received event: "" + json.dumps(event, indent=2))
bucket = event['Records'][0]['s3']['bucket']['name']
key = urllib.unquote_plus(event['Records'][0]['s3']['object'] 
['key'].encode('utf8'))
key_target = ""targett/"" + key
try:
    response = compare_faces(bucket, key, key_target)
    print(response)
#       mypayload = json.dumps(response)
#      iotResponse = iot.publish(
 #         topic=""rekognition/result"",
 #        qos=1,
 #       payload=mypayload)
  #  print(iotResponse)
   # return iotResponse
    print(response)
    return response
 except Exception as e:
    print(e)
    print(""Error processing object {} from bucket {}. "".format(key, 
 bucket) 
 +
          ""Make sure your object and bucket exist and your bucket is in 
 the 
 same region as this function."")
    raise e


 ---------------output-----------------
  Response:
[
{
""BoundingBox"": {
  ""Width"": 0.7813892960548401,
  ""Top"": 0.15193353593349457,
  ""Left"": 0.1047489121556282,
  ""Height"": 0.8365015387535095
  },
  ""Confidence"": 99.99993896484375
 },
 []
 ]
</code></pre>",53581345,2,4,,2018-12-02 07:03:18.837 UTC,,2018-12-02 15:58:14.337 UTC,2018-12-02 11:18:57.333 UTC,,10599727,,10599727,1,-3,python|amazon-web-services|amazon-s3|aws-lambda,58
"Google Cloud Shell is using project=cloud-devshell-dev instead of my actual project, can't find enabled APIs",44184869,"Google Cloud Shell is using project=cloud-devshell-dev instead of my actual project, can't find enabled APIs","<p>I created a GCP project to play around with the video-intelligence API. I enabled the API on my project and launched a Cloud Shell.</p>

<p>I then copied the code from <a href=""https://github.com/GoogleCloudPlatform/python-docs-samples/tree/master/video/cloud-client/faces"" rel=""nofollow noreferrer"">github</a> and followed the README instructions.</p>

<p>However, when I try to run <code>faces.py</code> I get this error message:</p>

<blockquote>
  <p>StatusCode.PERMISSION_DENIED, Google Cloud Video Intelligence API has
  not been used in project cloud-devshell-dev before or it is disabled.
  Enable it by visiting
  <a href=""https://console.developers.google.com/apis/api/videointelligence.googleapis.com/overview?project=cloud-devshell-dev"" rel=""nofollow noreferrer"">https://console.developers.google.com/apis/api/videointelligence.googleapis.com/overview?project=cloud-devshell-dev</a>
  then retry</p>
</blockquote>

<p><strong>Why is it pointing to <code>project=cloud-devshell-dev</code> and not to my <code>videointel</code> project?</strong></p>

<p>If I <code>gcloud config list</code> I can see the correct project and service account. Baffled.</p>",44192942,1,0,,2017-05-25 15:58:58.380 UTC,,2017-05-26 03:05:10.033 UTC,,,,,469449,1,4,python|google-app-engine|gcp|google-cloud-shell,1594
Image is not uploaded in PHP using Swift,38748027,Image is not uploaded in PHP using Swift,"<p>I am trying to use Google Vision API and upload an image using their API to get analysis. I am using this php code:</p>

<pre><code>&lt;?php

include(""./includes/common.php"");
include_once(""creds.php""); // Get $api_key
$cvurl = ""https://vision.googleapis.com/v1/images:annotate?key="" . $api_key;
$type = ""LABEL_DETECTION"";

//echo ""Item is: "" . $item;

//Did they upload a file...

$item = $_GET[item];



if($_FILES['photo']['name'])
{
}else{
echo ""you did not upload image"".
}
</code></pre>

<p>It always show ""you did not upload image"". And here's my Swift function where I upload the image:</p>

<pre><code>func UploadRequest(img: UIImage, item: String)
    {

        let url = NSURL(string: ""http://myurlhere"")

        let request = NSMutableURLRequest(URL: url!)
        request.HTTPMethod = ""POST""

        let boundary = generateBoundaryString()

        //define the multipart request type

        request.setValue(""multipart/form-data; boundary=\(boundary)"", forHTTPHeaderField: ""Content-Type"")



        let image_data = UIImagePNGRepresentation(img)


        if(image_data == nil)
        {
            return
        }


        let body = NSMutableData()

        let fname = ""image.png""
        let mimetype = ""image/png""




        //define the data post parameter

        body.appendData(""--\(boundary)\r\n"".dataUsingEncoding(NSUTF8StringEncoding)!)
        body.appendData(""Content-Disposition:form-data; name=\""test\""\r\n\r\n"".dataUsingEncoding(NSUTF8StringEncoding)!)
        body.appendData(""hi\r\n"".dataUsingEncoding(NSUTF8StringEncoding)!)



        body.appendData(""--\(boundary)\r\n"".dataUsingEncoding(NSUTF8StringEncoding)!)
        body.appendData(""Content-Disposition:form-data; name=\""file\""; filename=\""\(fname)\""\r\n"".dataUsingEncoding(NSUTF8StringEncoding)!)
        body.appendData(""Content-Type: \(mimetype)\r\n\r\n"".dataUsingEncoding(NSUTF8StringEncoding)!)
        body.appendData(image_data!)
        body.appendData(""\r\n"".dataUsingEncoding(NSUTF8StringEncoding)!)


        body.appendData(""--\(boundary)--\r\n"".dataUsingEncoding(NSUTF8StringEncoding)!)



        request.HTTPBody = body



        let session = NSURLSession.sharedSession()


        let task = session.dataTaskWithRequest(request) {
            (
            let data, let response, let error) in

            guard let _:NSData = data, let _:NSURLResponse = response  where error == nil else {
                //EZLoadingActivity.hide(success: false, animated: true)
                print(""error"")
                return
            }

            let dataString = NSString(data: data!, encoding: NSUTF8StringEncoding)
            print(dataString)
            //EZLoadingActivity.hide(success: true, animated: true)
            self.dismissViewControllerAnimated(true, completion: nil)
        }

        task.resume()


    }
</code></pre>

<p>When I do <code>print_r($_FILES)</code>, I get:</p>

<pre><code>Array
(
    [file] =&gt; Array
        (
            [name] =&gt; image.png
            [type] =&gt; image/png
            [tmp_name] =&gt; /tmp/phplSB2dc
            [error] =&gt; 0
            [size] =&gt; 864781
        )

)
</code></pre>",38748206,2,3,,2016-08-03 15:36:43.720 UTC,,2017-02-12 09:59:08.617 UTC,2016-08-03 15:43:49.943 UTC,,1299645,,1299645,1,1,php|swift,392
AWS rekognition InvalidParameterException,55514812,AWS rekognition InvalidParameterException,"<p>I got aws rekognition invalid parameter Exception, if I upload small lower resolution image.</p>

<p>see below error</p>

<pre><code>https://rekognition.us-west-2.amazonaws.com` resulted in a `400 Bad Request` response: {""__type"":""InvalidImageFormatException"",""Message"":""Request has invalid image format""} InvalidImageFormatException (client): Request has invalid image format - {""__type"":""InvalidImageFormatException"",""Message"":""Request has invalid image format""}' GuzzleHttp\Exception\ClientException: Client error: `POST https://rekognition.us-west-2.amazonaws.com` resulted in a `400 Bad Request` response: {""__type"":""InvalidImageFormatException"",""Message"":""Request has invalid image format""} in /var/www/vhosts/harvest.io.farm/aws/vendor/guzzlehttp/guzzle/src/Exception/RequestException.php:111 Stack trace: #0 /var/www/vhosts/harvest.io.farm/aws/vendor/guzzlehttp/guzzle/src/Middleware.php(65): GuzzleHttp\Exception\RequestException::create(Object(Guz in /var/www/vhosts/harvest.io.farm/aws/vendor/aws/aws-sdk-php/src/WrappedHttpHandler.php on line 192
</code></pre>

<p>And my code is</p>

<pre><code>$result = $recognizationClient-&gt;indexFaces([
            'CollectionId' =&gt; getCollectionName( $txtCID ), // REQUIRED
            'Image' =&gt; [ // REQUIRED
                'S3Object' =&gt; [
                    'Bucket' =&gt; $bucket,
                    'Name' =&gt; $actual_image_name            
                ],
            ],
        ]);
</code></pre>",,0,3,,2019-04-04 11:26:49.673 UTC,,2019-04-04 11:26:49.673 UTC,,,,,9006793,1,2,php|amazon-web-services|amazon-rekognition,29
How to use Json in async local response,55585979,How to use Json in async local response,"<p>I am new to Google Vision, and I want create code to receive an asynchronous response. For example, create a JSON file to response and later load the JSON file and continue with de recognizer.</p>

<p>I am trying to use some code from Google, but when I try to read the JSON file, it's not working like in synchronous mode.</p>

<p>This is how I save the response to a JSON file:</p>

<pre class=""lang-py prettyprint-override""><code>with open(path_source_image, 'rb') as image_file:
    tmp_image_opened = image_file.read()

tpm_image_opened_to_vision = vision.types.Image(content=tmp_image_opened)
tpm_vision_client = vision.ImageAnnotatorClient()
tpm_response_from_vision = tpm_vision_client.document_text_detection(image=tpm_image_opened_to_vision)
</code></pre>

<p>This is how I try to read and use the JSON file:</p>

<pre class=""lang-py prettyprint-override""><code>with open('proyecto/2_MILE.json') as fp:
    document = json.load(fp)

for page in document.pages:
    for block in page.blocks:
        for paragraph in block.paragraphs:
            for word in paragraph.words:
                assembled_word = assemble_word(word)
                if (assembled_word == 'LEIDY'):
                    return word.bounding_box.vertices[0]
</code></pre>

<p>but it does not work, it says </p>

<blockquote>
<pre><code>in find_word_location
    for page in document.pages:
AttributeError: 'dict' object has no attribute 'pages'
</code></pre>
</blockquote>",,0,2,,2019-04-09 05:55:44.720 UTC,,2019-04-09 07:27:56.350 UTC,2019-04-09 07:27:56.350 UTC,,18771,,11332604,1,0,python|json|google-cloud-vision,22
How to set GOOGLE_APPLICATION_CREDENTIALS in spring boot app,46921518,How to set GOOGLE_APPLICATION_CREDENTIALS in spring boot app,"<p>I am attempting to use the google vision library in java. The steps specify that I need to setup my auth credentials in order to start using the <a href=""https://developers.google.com/identity/protocols/application-default-credentials"" rel=""nofollow noreferrer"">this</a> library . I was able to generate my json property file from API Console Credentials page and I placed it in my spring boot app in the resources folder.</p>

<p>I think updated my application.properties file to include the value like so:</p>

<pre><code>GOOGLE_APPLICATION_CREDENTIALS=datg-avatar-generator-9dc9155cd5bd.json
</code></pre>

<p>I'm also setting my property source in my controller like so:</p>

<pre><code>@PropertySource(""${GOOGLE_APPLICATION_CREDENTIALS}"")
</code></pre>

<p>However, after doing that I'm still getting an error saying:</p>

<pre><code>java.io.IOException: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.
</code></pre>",,1,3,,2017-10-24 23:28:08.367 UTC,,2018-08-04 11:25:34.393 UTC,2017-10-24 23:45:11.630 UTC,,7467246,,4735549,1,1,java|spring|google-api|google-oauth2,1123
How can I count cars in an image with Google Cloud Vision API?,51354969,How can I count cars in an image with Google Cloud Vision API?,"<p>I need a count of all the <strong>cars</strong> included in a image with the Google Cloud Vision API in Python.
 I take only the labels of the image right now.</p>

<pre><code>import io
import os
from google.cloud import vision
from google.cloud.vision import types
client = vision.ImageAnnotatorClient();
file_name = os.path.join(
    os.path.dirname(__file__),
    'car.jpg');
with io.open(file_name,'rb') as image_file:
    content = image_file.read();
image = types.Image(content=content)
response = client.label_detection(image=image)
print(response)
</code></pre>",,1,2,,2018-07-16 05:15:46.423 UTC,,2018-07-16 14:56:12.297 UTC,2018-07-16 08:07:24.027 UTC,,5587356,,10086273,1,1,python|google-cloud-vision,191
Google Vision batch OCR,53564632,Google Vision batch OCR,"<p>How feasible would it be to extract text from a large dataset of jpeg images (say, 100,000 of them) with Google Cloud Vision? In past questions, respondents have pointed to the <a href=""https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate"" rel=""nofollow noreferrer"">images.annotate method</a>, but given that the maximum number of images per request is just 16, I'm concerned about its runtime.</p>",,0,0,,2018-11-30 20:41:26.083 UTC,,2018-11-30 21:16:47.890 UTC,2018-11-30 21:16:47.890 UTC,,10729564,,10729564,1,0,google-cloud-vision,60
How to set timeout for the request Vision.Images.Annotate,44910023,How to set timeout for the request Vision.Images.Annotate,"<p>I am using below code to call google cloud vision api. not able to find out how can I set response timeout for the request in case I do not get response within a set timeout.</p>

<pre><code>Vision.Images.Annotate annotateRequest =
                vision.images().annotate(batchAnnotateImagesRequest);
// Due to a bug: requests to Vision API containing large images fail when GZipped.
annotateRequest.setDisableGZipContent(true);
Log.d(TAG, ""created Cloud Vision request object, sending request"");

BatchAnnotateImagesResponse response = annotateRequest.execute();
</code></pre>",,1,1,,2017-07-04 15:47:48.027 UTC,,2018-09-05 09:59:09.640 UTC,2017-07-05 02:04:22.937 UTC,,1402846,,2281965,1,1,java|google-apis-explorer,211
How to authorize google cloud vision API android,49376836,How to authorize google cloud vision API android,"<p>I am implementing google cloud vision API from this <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">link</a>.</p>

<p>Below is my code where I am getting exception:</p>

<pre><code>private void callCloudVision(final Bitmap bitmap) throws IOException {
    new AsyncTask&lt;Object, Void, String&gt;() {
        @Override
        protected String doInBackground(Object... params) {
            try {
                List&lt;AnnotateImageRequest&gt; requests = new ArrayList&lt;&gt;();
                ByteArrayOutputStream bos = new ByteArrayOutputStream();
                bitmap.compress(Bitmap.CompressFormat.PNG, 0 /*ignored for PNG*/, bos);
                byte[] bitmapdata = bos.toByteArray();
                ByteArrayInputStream bs = new ByteArrayInputStream(bitmapdata);
                ByteString imgBytes = ByteString.readFrom(bs);
                Image img = Image.newBuilder().setContent(imgBytes).build();
                Feature feat = Feature.newBuilder().setType(Feature.Type.WEB_DETECTION).build();
                AnnotateImageRequest request = AnnotateImageRequest.newBuilder().addFeatures(feat).setImage(img).build();
                requests.add(request);

                ***//Getting exception here at the time of execution...***
                try (ImageAnnotatorClient client = ImageAnnotatorClient.create()) {
                    BatchAnnotateImagesResponse response = client.batchAnnotateImages(requests);
                    List&lt;AnnotateImageResponse&gt; responses = response.getResponsesList();
                    for (AnnotateImageResponse res : responses) {
                        if (res.hasError()) {
                            System.out.printf(""Error: %s\n"", res.getError().getMessage());
                            return """";
                        }
                        System.out.println(""\nPages with matching images: Score\n=="");
                        for (WebDetection.WebPage page : annotation.getPagesWithMatchingImagesList()) {
                            System.out.println(page.getUrl() + "" : "" + page.getScore());
                        }
                    }
                }

                return """";

            }  catch (Exception e) {
                Log.d(""LOG_TAG"", ""Request failed: "" + e.getMessage());
                return ""Cloud Vision API request failed."";
            }

        }

        protected void onPostExecute(String result) {

        }
    }.execute();
    }
</code></pre>

<p>I am getting error </p>

<blockquote>
  <p>java.io.IOException: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See <a href=""https://developers.google.com/accounts/docs/application-default-credentials"" rel=""nofollow noreferrer"">https://developers.google.com/accounts/docs/application-default-credentials</a> for more information.</p>
</blockquote>

<p>My graddle</p>

<pre><code>implementation 'com.android.support.constraint:constraint-layout:1.0.2'
compile fileTree(dir: 'libs', include: ['*.jar'])
compile 'com.android.support:appcompat-v7:26.+'
**compile 'com.google.cloud:google-cloud-vision:1.21.0'**
compile group: 'com.google.apis', name: 'google-api-services-sqladmin', version: 'v1beta4-rev52-1.23.0'
compile('com.google.api-client:google-api-client-android:1.22.0') {
    exclude module: 'httpclient'
    exclude group: 'com.google.guava'
}
compile('com.google.http-client:google-http-client-gson:1.20.0') {
    exclude module: 'httpclient'
    exclude group: 'com.google.guava'
}
</code></pre>",49379966,1,1,,2018-03-20 05:22:15.523 UTC,,2018-03-20 09:01:41.627 UTC,2018-03-20 05:45:08.467 UTC,,4458897,,4458897,1,2,java|android|google-cloud-platform|vision,272
Android Progress Bar with percentage of image upload,51381522,Android Progress Bar with percentage of image upload,"<p>I am using google vision api in my code in Android studio and I'm sending it image from camera or gallery. Uploading image takes time, so I want the users of my app to have feedback about what's going on. Progress Bar with real time percentage of image upload is what I need. Here is my code for uploading image:</p>

<pre><code>@SuppressLint(""StaticFieldLeak"")
private void callCloudVision(final Bitmap bitmap) throws IOException {

    getWindow().setFlags(WindowManager.LayoutParams.FLAG_NOT_TOUCHABLE,
            WindowManager.LayoutParams.FLAG_NOT_TOUCHABLE);

    // Do the real work in an async task, because we need to use the network anyway
    new AsyncTask&lt;Object, Integer, String&gt;() {

        @Override
        protected String doInBackground(Object... params) {
            try {
                HttpTransport httpTransport = AndroidHttp.newCompatibleTransport();
                JsonFactory jsonFactory = GsonFactory.getDefaultInstance();


                VisionRequestInitializer requestInitializer =
                        new VisionRequestInitializer(CLOUD_VISION_API_KEY) {
                            /**
                             * We override this so we can inject important identifying fields into the HTTP
                             * headers. This enables use of a restricted cloud platform API key.
                             */
                            @Override
                            protected void initializeVisionRequest(VisionRequest&lt;?&gt; visionRequest)
                                    throws IOException {
                                super.initializeVisionRequest(visionRequest);

                                String packageName = getPackageName();
                                visionRequest.getRequestHeaders().set(ANDROID_PACKAGE_HEADER, packageName);

                                String sig = PackageManagerUtils.getSignature(getPackageManager(), packageName);

                                visionRequest.getRequestHeaders().set(ANDROID_CERT_HEADER, sig);
                            }
                        };

                Vision.Builder builder = new Vision.Builder(httpTransport, jsonFactory, null);
                builder.setVisionRequestInitializer(requestInitializer);

                Vision vision = builder.build();

                BatchAnnotateImagesRequest batchAnnotateImagesRequest =
                        new BatchAnnotateImagesRequest();
                batchAnnotateImagesRequest.setRequests(new ArrayList&lt;AnnotateImageRequest&gt;() {{
                    AnnotateImageRequest annotateImageRequest = new AnnotateImageRequest();

                    // Add the image
                    Image base64EncodedImage = new Image();
                    // Convert the bitmap to a JPEG
                    // Just in case it's a format that Android understands but Cloud Vision
                    ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();
                    bitmap.compress(Bitmap.CompressFormat.JPEG, 100, byteArrayOutputStream);
                    byte[] imageBytes = byteArrayOutputStream.toByteArray();

                    // Base64 encode the JPEG
                    base64EncodedImage.encodeContent(imageBytes);
                    annotateImageRequest.setImage(base64EncodedImage)   ;

                    // add the features we want
                    annotateImageRequest.setFeatures(new ArrayList&lt;Feature&gt;() {{
                        Feature labelDetection = new Feature();
                        labelDetection.setType(""TEXT_DETECTION"");
                        add(labelDetection);
                    }});

                    // Add the list of one thing to the request
                    add(annotateImageRequest);
                }});


                Vision.Images.Annotate annotateRequest =
                        vision.images().annotate(batchAnnotateImagesRequest);
                // Due to a bug: requests to Vision API containing large images fail when GZipped.
                annotateRequest.setDisableGZipContent(true);
                Log.d(TAG, ""created Cloud Vision request object, sending request"");


                BatchAnnotateImagesResponse response = annotateRequest.execute();
                ByteArrayOutputStream out = new ByteArrayOutputStream();

                return convertResponseToString(response);

            } catch (GoogleJsonResponseException e) {
                Log.d(TAG, ""failed to make API request because "" + e.getContent());
            } catch (IOException e) {
                Log.d(TAG, ""failed to make API request because of other IOException "" +
                        e.getMessage());
            }
            getWindow().clearFlags(WindowManager.LayoutParams.FLAG_NOT_TOUCHABLE);
            return ""Cloud Vision API request failed. Check logs for details."";

        }

        @Override
        protected void onProgressUpdate(Integer... values) {
            super.onProgressUpdate(values);

        }


        protected void onPostExecute(String result) {
            openOcrResultActivity(result);        
            getWindow().clearFlags(WindowManager.LayoutParams.FLAG_NOT_TOUCHABLE);
        }
    }.execute();
}
</code></pre>

<p>I searched for few days and I know that in <code>callCloudVision</code> function I need to call <code>publishProgress()</code>. <code>publishProgress</code> will trigger<code>onProgressUpdate(i)</code> and there I will get percent of upload as parameter (i) and increment <code>progressbar</code>. Something like this:  </p>

<pre><code>MyProgressBar.incrementProgressBy((values[0] * 2));
</code></pre>

<p>But I don't know how to get percentage of uploaded image in callCloudVision function.</p>

<p>Please help</p>",,0,2,,2018-07-17 12:33:05.583 UTC,,2018-07-18 11:40:38.783 UTC,2018-07-18 11:40:38.783 UTC,,8934776,,8934776,1,0,android|image-uploading|android-progressbar,231
PDF/TIFF Document Text Detection,52274071,PDF/TIFF Document Text Detection,"<p>I am currently trying to use Google's cloud vision API for my project. The problem is that Google cloud vision API for document text detection accepts only Google Cloud Services URI as input and output destination. But I have all my projects, data in Amazon S3 server which cant be directly used with this API.</p>

<p>Points to be noted:-  </p>

<ol>
<li>All data should be in kept in <strong>S3</strong> only.     </li>
<li>I can't change my cloud storage to <strong>GCS</strong> now.  </li>
<li>I can't download files from <strong>S3</strong> and upload to <strong>GCS</strong> manually.The number
of files that are incoming per day is more than 1000 and less than
100,000.  </li>
<li>Even if I could automate downloading and uploading of the pdf, this
would serve as a bottleneck for the entire project, since I would have to deal
with concurrency issues and memory management.</li>
</ol>

<p>Is there any workaround to make this API work with S3 URI? I am in need of your help. </p>

<p>Thank You</p>",,1,1,,2018-09-11 10:40:16.270 UTC,,2019-01-25 11:32:39.560 UTC,2018-09-11 10:56:53.040 UTC,,1403604,,10346853,1,0,python|google-cloud-platform|ocr|python-3.6|google-cloud-vision,219
How do I interpret a web entity with a null score?,56399486,How do I interpret a web entity with a null score?,"<p>When I make a web detection request to the Google Vision API, I get back a bunch of web entities. The last entity in the list has no score -- that is, 'score' isn't 0, it just returns null. I can't find any Google documentation explaining what a null score means. </p>

<p>I've only seen this happen consistently for one image (so far).</p>

<p>Example of a normal WebEntity which has description, entityId, and score:</p>

<pre><code>{
  ""description"": ""Car"",
  ""entityId"": ""/m/11afgkh"",
  ""score"": ""0.24221982""
}
</code></pre>

<p>Actual WebEntity that I get:</p>

<pre><code>{
  ""description"": ""Unbreakable"",
  ""entityId"": ""/g/11f_s_bdwj"",
}
</code></pre>

<p>How should a null score be interpreted? Also... I know this is off topic, but what is the entityId even used for? I can't find much documentation on either of these other than the comments in the code:</p>

<pre><code> /**
   * Overall relevancy score for the entity. Not normalized and not comparable across different
   * image queries.
   * @return value or {@code null} for none
   */
  public java.lang.Float getScore() {
    return score;
  }
</code></pre>

<pre><code>  /**
   * Opaque entity ID.
   * @return value or {@code null} for none
   */
  public java.lang.String getEntityId() {
    return entityId;
  }

</code></pre>",,0,0,,2019-05-31 17:32:41.990 UTC,,2019-05-31 17:32:41.990 UTC,,,,,4036346,1,0,null|google-cloud-vision,7
How to configure google vision api with tika parser,51767916,How to configure google vision api with tika parser,"<p>I am trying to parse images using the Apache tika-parser in python, but sometimes I get content as ""none"". But when I try the same image with Google the vision API it gives me a good response.</p>

<p>Is it possible to integrate tika with Google vision API? If yes then how using python?</p>",,0,2,,2018-08-09 13:07:28.060 UTC,,2018-08-09 18:54:27.307 UTC,2018-08-09 18:54:27.307 UTC,,214143,,4948038,1,4,python-3.x|apache|apache-tika|google-vision,62
"Parse complex JSON files ""undefined""",48704050,"Parse complex JSON files ""undefined""","<p>I got ""undefined"" while trying to Parse this JSON file: </p>

<pre><code> {
""responses"": [
    {
      ""labelAnnotations"": [
        {
          ""mid"": ""/m/01yrx"",
          ""description"": ""cat"",
          ""score"": 0.9926739,
          ""topicality"": 0.9926739
        },
        {
          ""mid"": ""/m/01l7qd"",
          ""description"": ""whiskers"",
          ""score"": 0.9639658,
          ""topicality"": 0.9639658
        },
        {
          ""mid"": ""/m/083jv"",
          ""description"": ""white"",
          ""score"": 0.9582038,
          ""topicality"": 0.9582038
        },
        {
          ""mid"": ""/m/0k0pj"",
          ""description"": ""nose"",
          ""score"": 0.9425352,
          ""topicality"": 0.9425352
        },
        {
          ""mid"": ""/m/06z04"",
          ""description"": ""skin"",
          ""score"": 0.92025506,
          ""topicality"": 0.92025506
        }
      ]
    }
  ]
}
</code></pre>

<p>This file is th result from a XMLHttpRequest from the google vision API and This is what i'm doing to print ""description"" field:</p>

<pre><code>e.onload=function(){
  var i= JSON.parse(e.response);
  value = i.responses[0][""description""];
  alert(value);
};
</code></pre>",48704140,1,4,,2018-02-09 10:42:40.443 UTC,,2018-02-09 10:47:19.770 UTC,,,,,6731250,1,1,javascript|json,52
How to detect TEXT in an image more than 50words?,50554306,How to detect TEXT in an image more than 50words?,"<p>according to AWS Rekognition Terms:</p>

<blockquote>
  <p>A word is one or more ISO basic latin script characters that are not
  separated by spaces. DetectText can detect up to 50 words in an image.</p>
</blockquote>

<p>How can I detect text in an image it has more than 50words?</p>

<p>Give me a clue... thanks.</p>",,1,0,,2018-05-27 16:39:00.867 UTC,,2018-09-21 23:32:16.733 UTC,2018-06-01 02:45:55.077 UTC,,174777,,5439049,1,1,amazon-web-services|amazon-rekognition,230
How to detect text/logo-details from an image of any consumer product?,54625126,How to detect text/logo-details from an image of any consumer product?,"<p>I am trying to detect name of any consumer product from an image of its packaging.For eg- <a href=""https://i.stack.imgur.com/8fnUE.jpg"" rel=""nofollow noreferrer"">Maggie</a> (I want to detect- Maggie happiness is homemade) <a href=""https://i.stack.imgur.com/cweis.jpg"" rel=""nofollow noreferrer"">Kellogg's</a></p>

<p>I have tried applying image prepossessing(e.g- erosion, open, close etc.) and then supplying that pre-processed image to pytesseract(OCR). I am planning to use Image-Magic tool if it can do any help.  </p>

<p>Would just pre-processing of an image be enough, if not then what should I do?(Any code, software anything) </p>

<p>PS- I dont want to use Google Vision or anything similar API </p>",54637373,1,1,,2019-02-11 06:39:23.577 UTC,,2019-02-11 19:02:40.787 UTC,,,,,10926598,1,0,image|imagemagick|text-recognition|python-tesseract,31
"How to upload a video to Microsoft service using python by sending a ""POST"" request with application/octet-stream content type",44981942,"How to upload a video to Microsoft service using python by sending a ""POST"" request with application/octet-stream content type","<p>I want to use Microsoft Azure emotional API to analyse the local video, but how to upload a video to Microsoft service using python by sending a ""POST"" request with application/octet-stream content type together with the data read from a video file.</p>",,1,2,,2017-07-08 02:30:10.407 UTC,,2017-07-11 09:29:56.187 UTC,,,,,8270098,1,1,python|api|azure|video,209
Best way to reference a keyfile in nodejs,53281993,Best way to reference a keyfile in nodejs,"<p>I did some searching, but my terms ""keyfile reference secure"" and various others turned up too many results, so here I am. If this has been asked before, I'd be happy to reference that. </p>

<p>I have a nodejs project that uses the <a href=""https://github.com/googleapis/nodejs-vision#readme"" rel=""nofollow noreferrer"">Google Vision API/SDK</a> and that sample project uses the <code>google-cloud</code> module and then uses this kind of structure to get the key file (reference <a href=""https://github.com/googlecodelabs/cloud-nodejs/blob/8d2578696549ccc4b2cf84cdae83bed6b23fda3e/step-3-book-cover-images/books.js"" rel=""nofollow noreferrer"">here</a></p>

<pre><code>  var gcloud = require('google-cloud');

  var datastore = gcloud.datastore({
    projectId: config.projectId,
    keyFilename: config.keyFilename
  });
</code></pre>

<p>However, when using the <code>@google-cloud/vision</code> module directly, you can also use the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable pointing to a file containing the key file and it ""just works"" as long as you export the environment variable or set the variable and initiate the command like so:</p>

<pre><code>GOOGLE_APPLICATION_CREDENTIALS=/home/jomama/somefolder/keyfile.json node
</code></pre>

<p>So, considering the project will be tracked with git, my questions are:</p>

<ol>
<li>What is the code level advantage of using either approach in the above?</li>
<li>What security issues need to be addressed in either approach?</li>
<li>Any other considerations I'm missing?</li>
</ol>

<p>Granted I don't want my keyfile stored in git but I still want the project tracked there. </p>",,0,0,,2018-11-13 13:23:06.513 UTC,,2018-11-13 13:23:06.513 UTC,,,,,3143112,1,1,git|google-cloud-platform|private-key,22
How to accurately extract the email and unit number string text from OCR content?,43844506,How to accurately extract the email and unit number string text from OCR content?,"<p>I've used google cloud vision OCR to extract business card email string text from <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">here</a> and used the below regular expression to try to extract but without much good results. Any better suggestions to increase the performance?</p>

<pre><code>function extract_emails($str){
    // This regular expression extracts all emails from a string:
    $regexp = '/([a-z0-9_\.\-])+\@(([a-z0-9\-])+\.)+([a-z0-9]{2,4})+/i';
    preg_match_all($regexp, $str, $m);

    return isset($m[0]) ? $m[0] : array();
}

$Email = extract_emails($gcv_response);

if (!empty($Email))
{
    $Email = reset($Email); 
}
else
{
    $Email = 'NULL';
}
</code></pre>

<p>OCR text 1:  ""ALGEN MARINE PTE LTD Specialist in Fire Protection and Safety Engineering Philip Cheng Assistant Sales Manager 172 Tuas South Avenue 2, West Point Bizhub, Singapore 637191 Email: philip @algen.comsg Website: www.algen.comsg Tel: (65) 6898 2292 Fax: (65) 6898 2202 (65) 6898 2813 HP : (65) 9168 9799"" </p>

<p>Result from the running the above code = NULL; Desired output: philip@algen.comsg</p>

<p>OCR text 2: ""Allan Lim Yee Chian Chief Executive Officer Alpha Biofuels (S) Pte Ltd LHCCBNFLN FR2 a mobile 9790 3063 tel 6264 6696 fax 6260 2082 C#01-05, 2 Tuas South Ave 2 Singapore 637601 tang. Steve. Eric@alphabiofuels.sg www.alphabiofuels.sg"" </p>

<p>Result from the running the above code = NULL; Desired output: tang.Steve.Eric@alphabiofuels.sg;</p>",,1,11,,2017-05-08 09:46:40.373 UTC,,2018-07-17 12:18:45.957 UTC,2017-05-08 10:10:00.070 UTC,,694831,,694831,1,0,php|regex|ocr|google-vision,110
How to specify the order of text detection,46380748,How to specify the order of text detection,"<p>I started using Google Vision API recently and have confronted a problem.
Chat-bot I've been working is a bill-recognition bot. So, it should scan the bill left-to-right downwards the image. I do all manipulations with recognized text after.
My text detection code is following:</p>

<pre><code>const gcsPath = ""https://api.telegram.org/file/bot427731672:AAHC6nkvnnSqYKHYaeZVSWY7itB0BGgv2Kw/"" + photo.file_path;
            vision.textDetection({ source: {imageUri : gcsPath } })
            .then((results) =&gt; {
                const detections = results[0].textAnnotations;
                console.log('Text:');
                detections.forEach((text) =&gt; console.log(text.description));
                });
</code></pre>

<p>The console output often has no structure relatively to the image i.e for some image it can be left-to-right, for the other right-to-left. 
My question is, how do I set the hints, so the detection always has the direction?</p>",,0,2,,2017-09-23 14:41:55.807 UTC,,2017-09-23 14:41:55.807 UTC,,,,,5615628,1,0,node.js|google-cloud-vision,40
TypeError when using Google Vision API in Firebase Cloud Functions,48343919,TypeError when using Google Vision API in Firebase Cloud Functions,"<p>I'm trying to get a sample for the use of the Google Vision API within Firebase Cloud Functions to work but it fails.</p>

<p>I'm using the unmodified sample provided on Github: <a href=""https://github.com/firebase/functions-samples/tree/master/moderate-images"" rel=""nofollow noreferrer"">https://github.com/firebase/functions-samples/tree/master/moderate-images</a></p>

<p><strong>EDIT:</strong></p>

<p>Here is my source file:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>/**
 * Copyright 2016 Google Inc. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for t`he specific language governing permissions and
 * limitations under the License.
 */
'use strict';

const functions = require('firebase-functions');
const mkdirp = require('mkdirp-promise');
const gcs = require('@google-cloud/storage')();
const vision = require('@google-cloud/vision')();
const spawn = require('child-process-promise').spawn;
const path = require('path');
const os = require('os');
const fs = require('fs');

/**
 * When an image is uploaded we check if it is flagged as Adult or Violence by the Cloud Vision
 * API and if it is we blur it using ImageMagick.
 */
exports.blurOffensiveImages = functions.storage.object().onChange(event =&gt; {
  const object = event.data;
  const file = gcs.bucket(object.bucket).file(object.name);

  // Exit if this is a move or deletion event.
  if (object.resourceState === 'not_exists') {
    return console.log('This is a deletion event.');
  }

  // Check the image content using the Cloud Vision API.
  return vision.detectSafeSearch(file).then(data =&gt; {
    const safeSearch = data[0];
    console.log('SafeSearch results on image', safeSearch);

    if (safeSearch.adult || safeSearch.violence) {
      return blurImage(object.name, object.bucket, object.metadata);
    }
  });
});

/**
 * Blurs the given image located in the given bucket using ImageMagick.
 */
function blurImage(filePath, bucketName, metadata) {
  const tempLocalFile = path.join(os.tmpdir(), filePath);
  const tempLocalDir = path.dirname(tempLocalFile);
  const bucket = gcs.bucket(bucketName);

  // Create the temp directory where the storage file will be downloaded.
  return mkdirp(tempLocalDir).then(() =&gt; {
    console.log('Temporary directory has been created', tempLocalDir);
    // Download file from bucket.
    return bucket.file(filePath).download({destination: tempLocalFile});
  }).then(() =&gt; {
    console.log('The file has been downloaded to', tempLocalFile);
    // Blur the image using ImageMagick.
    return spawn('convert', [tempLocalFile, '-channel', 'RGBA', '-blur', '0x8', tempLocalFile]);
  }).then(() =&gt; {
    console.log('Blurred image created at', tempLocalFile);
    // Uploading the Blurred image.
    return bucket.upload(tempLocalFile, {
      destination: filePath,
      metadata: {metadata: metadata} // Keeping custom metadata.
    });
  }).then(() =&gt; {
    console.log('Blurred image uploaded to Storage at', filePath);
    fs.unlinkSync(tempLocalFile);
    console.log('Deleted local file', filePath);
  });
}</code></pre>
</div>
</div>
</p>

<p>I've done the following steps:</p>

<ul>
<li>Created a working Firebase project</li>
<li>Activated the Vision API and the billing for the project</li>
<li>Initialized the Firebase Functions localy on my PC</li>
<li>Installed needed npm modules with <code>npm install</code></li>
<li>Tried to deploy with <code>firebase deploy</code></li>
</ul>

<p>Then i got this error:</p>

<pre><code>i  deploying functions
i  functions: ensuring necessary APIs are enabled...
+  functions: all necessary APIs are enabled
i  functions: preparing functions directory for uploading...

Error: Error occurred while parsing your function triggers.

TypeError: require(...) is not a function
    at Object.&lt;anonymous&gt; (C:\Users\xxxxxx\FirebaseTest\functions\index.js:21:47)
    at Module._compile (module.js:570:32)
    at Object.Module._extensions..js (module.js:579:10)
    at Module.load (module.js:487:32)
    at tryModuleLoad (module.js:446:12)
    at Function.Module._load (module.js:438:3)
    at Module.require (module.js:497:17)
    at require (internal/module.js:20:19)
    at C:\Users\Tobias\AppData\Roaming\npm\node_modules\firebase-tools\lib\triggerParser.js:18:11
    at Object.&lt;anonymous&gt; (C:\Users\xxxxxx\AppData\Roaming\npm\node_modules\firebase-tools\lib\triggerParser.js:38:3)
</code></pre>

<p>So he is complaining about this line:</p>

<pre><code>const vision = require('@google-cloud/vision')();
</code></pre>

<p>My package.json looks like this:</p>

<pre><code>{
  ""name"": ""functions"",
  ""description"": ""Cloud Functions for Firebase"",
  ""scripts"": {
    ""serve"": ""firebase serve --only functions"",
    ""shell"": ""firebase experimental:functions:shell"",
    ""start"": ""npm run shell"",
    ""deploy"": ""firebase deploy --only functions"",
    ""logs"": ""firebase functions:log""
  },
  ""dependencies"": {
    ""@google-cloud/storage"": ""^1.5.2"",
    ""@google-cloud/vision"": ""^0.14.0"",
    ""child-process-promise"": ""^2.2.1"",
    ""firebase-admin"": ""^5.8.1"",
    ""firebase-functions"": ""^0.8.1"",
    ""mkdirp"": ""^0.5.1"",
    ""mkdirp-promise"": ""^5.0.1""
  },
  ""private"": true
}
</code></pre>

<p><strong>Nice to know:</strong> Other attempts, for example to try out Cloud Storage triggers in other functions, are working pretty well with my Firebase project. Only the Vision API gives me that much trouble.</p>

<p>Can someone please give me a hint what went wrong with my setup?</p>

<p>Thank you!</p>",48345310,2,2,,2018-01-19 15:01:49.703 UTC,,2018-01-19 20:28:27.207 UTC,2018-01-19 16:34:33.753 UTC,,9240621,,9240621,1,2,javascript|node.js|firebase|google-cloud-functions|google-cloud-vision,529
Android Studio - How can I trigger an intent when an 'if' is fulfilled,56023874,Android Studio - How can I trigger an intent when an 'if' is fulfilled,"<p>I've managed to get a qr scanner powered by google vision working and placing the qrcode into a text view on the same activity.</p>

<p>The end goal is to have the url in the qrcode open up in a webview in another activity (QRWebActivity) as soon as a qrcode is detected.</p>

<p>At this stage I was able to move the qrcode into a string and push across and open in the webview using intents activated by sendMessage2 on button click.</p>

<p>But I really want to find a way to have it just automatically open the QRWebActivity and send the webview to the qrCode on 'if(qrCodes.size()!=0).</p>

<p>Any help would be amazing.</p>

<p>Really sorry if I'm not using the right terminology, I just don't know what I'm doing but really keen to finish this app by the end of the week for release and I'm so close.</p>

<pre><code>        barcodeDetector.setProcessor(new Detector.Processor&lt;Barcode&gt;() {
            @Override
            public void release() {

            }

            @Override
            public void receiveDetections(Detector.Detections&lt;Barcode&gt; detections) {
                final SparseArray&lt;Barcode&gt; qrCodes = detections.getDetectedItems();

                if(qrCodes.size()!=0)
                {
                    textView.post(new Runnable() {
                        @Override
                        public void run() {
                            Vibrator vibrator = (Vibrator)getApplicationContext().getSystemService(Context.VIBRATOR_SERVICE);
                            vibrator.vibrate(1000);
                            textView.setText(qrCodes.valueAt(0).displayValue);
                        }

                    });
                }
            }
        });

    }
    public void sendMessage2 (View view)
    {
        String qrmessage = textView.getText().toString();

        Intent intent2 = new Intent(view.getContext(),QRWebActivity.class);
        intent2.putExtra(""EXTRA_QRMESSAGE"",qrmessage);
        startActivity(intent2);
    }
}
</code></pre>

<p>If I could just simulate pressing the button and triggering 'sendMessage2' when qrCodes !=0 that would do me... even though I'm sure there's a more elegant way.</p>",56032880,2,3,,2019-05-07 13:34:37.757 UTC,,2019-05-08 02:52:20.863 UTC,2019-05-07 23:30:19.130 UTC,,11464950,,11464950,1,0,java|android,49
The Comprehend class in the boto3 client does not work,49182513,The Comprehend class in the boto3 client does not work,"<p>I am attempting to use boto3 client (v.1.4.8) to access the AWS comprehend service to evaluate small user-defined strings. But when I attempt to use the client, it doesn't work.</p>

<p>The <a href=""http://boto3.readthedocs.io/en/docs/reference/services/comprehend.html#client"" rel=""nofollow noreferrer"">documentation</a>.</p>

<p>The code I use:</p>

<pre><code>client = boto3.client('comprehend')
</code></pre>

<p>The exception I'm being thrown:</p>

<blockquote>
<pre><code>UnknownServiceError: Unknown service: 'comprehend'.
</code></pre>
  
  <p>Valid service names are: acm, apigateway, application-autoscaling,
  appstream, athena, autoscaling, batch, budgets, ce, clouddirectory,
  cloudformation, cloudfront, cloudhsm, cloudhsmv2, cloudsearch,
  cloudsearchdomain, cloudtrail, cloudwatch, codebuild, codecommit,
  codedeploy, codepipeline, codestar, cognito-identity, cognito-idp,
  cognito-sync, config, cur, datapipeline, dax, devicefarm,
  directconnect, discovery, dms, ds, dynamodb, dynamodbstreams, ec2,
  ecr, ecs, efs, elasticache, elasticbeanstalk, elastictranscoder, elb,
  elbv2, emr, es, events, firehose, gamelift, glacier, glue, greengrass,
  health, iam, importexport, inspector, iot, iot-data, kinesis,
  kinesisanalytics, kms, lambda, lex-models, lex-runtime, lightsail,
  logs, machinelearning, marketplace-entitlement,
  marketplacecommerceanalytics, meteringmarketplace, mgh, mobile, mturk,
  opsworks, opsworkscm, organizations, pinpoint, polly, pricing, rds,
  redshift, rekognition, resourcegroupstaggingapi, route53,
  route53domains, s3, sdb, servicecatalog, ses, shield, sms, snowball,
  sns, sqs, ssm, stepfunctions, storagegateway, sts, support, swf, waf,
  waf-regional, workdocs, workspaces, xray</p>
</blockquote>

<p>I'm guessing there has to be something going on that i'm not aware of</p>",,1,1,,2018-03-08 21:10:14.970 UTC,,2018-03-09 04:58:51.680 UTC,2018-03-08 21:21:24.860 UTC,,1190388,,9379091,1,0,amazon-web-services|boto3,243
Find input/output tensors' name in Tensorboard,51241387,Find input/output tensors' name in Tensorboard,"<p>I have run this keras model based on mobilenet: <a href=""https://gist.github.com/giacomobartoli/eb45ab61b43e5e47ea2a60113f9352ef"" rel=""nofollow noreferrer"">https://gist.github.com/giacomobartoli/eb45ab61b43e5e47ea2a60113f9352ef</a>
The output is a frozen graph called mobilenetv1.pb.</p>

<p>Now, I want to compile this model on the new <a href=""https://aiyprojects.withgoogle.com/vision/"" rel=""nofollow noreferrer"">Google Vision Kit</a>.</p>

<p>In order to do that, I need to know the input and output tensor's name of my frozen graph (mobilenetv1.pb).
So checking TensorBoard I have the following graph:</p>

<p><a href=""https://i.stack.imgur.com/0kepe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0kepe.png"" alt=""enter image description here""></a></p>

<p>Each node contains different input/output tensors. This is not a problem.
The point is: amongst all the nodes, which should I consider for compiling this frozen graph on the vision kit?</p>

<p>In other words, I need to run this script:</p>

<pre><code>./bonnet_model_compiler.par \
--frozen_graph_path=mobilenetv1.pb \
--output_graph_path=mobilenetv1.binaryproto \
--input_tensor_name=INPUT_TENSOR_NAME \
--output_tensor_names=OUTPUT_TENSOR_NAME \
--input_tensor_size=256
</code></pre>

<p>I just need to understand which INPUT_TENSOR_NAME and OUTPUT_TENSOR_NAME are in my graph.</p>",,1,0,,2018-07-09 08:45:46.880 UTC,,2018-08-21 16:42:46.113 UTC,,,,,1662332,1,1,tensorflow|computer-vision|tensorboard,274
Is it possible to use faceAPI in linux,50893836,Is it possible to use faceAPI in linux,"<p>I am new to microsoft face API. Is it possible to use it in linux environment?</p>

<p>I could not find any documentation about it in their website.</p>

<p>Thanks.</p>",50900774,1,0,,2018-06-17 04:56:20.897 UTC,,2018-06-17 21:54:16.447 UTC,,,,,5527743,1,0,face-api,423
Generate com.google.android.gms.vision.barcode.Barcode from string rawValue result,40816309,Generate com.google.android.gms.vision.barcode.Barcode from string rawValue result,"<p>I am using google vision for barcode scanning. I want to keep the history of every scanned result so i am storing 'format', 'rawValue' in to sqlite database. So when I want to display the history item again, I want to conver this 'rawValue"" into 'com.google.android.gms.vision.barcode.Barcode' object so that I can get the values directly.</p>

<p>So is there any way to do so ? where we pass 'rawValue' and get 'com.google.android.gms.vision.barcode.Barcode' object ?</p>

<p>Please help me if anybody knows because I am not able to find any hint or example of it.</p>",,0,2,,2016-11-26 08:08:04.287 UTC,,2016-11-26 08:08:04.287 UTC,,,,,792480,1,0,android|barcode|google-vision,184
Amazon Rekognition API - IndexFaces prompting an error for external image id - When 'externalImageId' has folder structure in it,49558215,Amazon Rekognition API - IndexFaces prompting an error for external image id - When 'externalImageId' has folder structure in it,"<p>I am trying to invoke IndexFaces API but getting an error :</p>

<pre><code>*""exception"":""com.amazonaws.services.rekognition.model.AmazonRekognitionException"",
""message"": ""1 validation error detected: Value 'postman/postworld/postman_female.jpg' at 'externalImageId' failed to satisfy constraint: Member must satisfy regular expression pattern: [a-zA-Z0-9_.\\-:]+ (Service: AmazonRekognition; Status Code: 400; Error Code: ValidationException; Request ID: 3ac46c4d-3358-11e8-abd5-d5fb3ad03e33)"",
""path"": ""/enrolluser""*
</code></pre>

<p>I was able to upload my file successfully into S3 using the so called ""folder structure""of S3 . But when I am trying to read the same file for IndexFaces , then it's prompting an error related to éxternalImageId'.</p>

<p>Here is the snapshot from the S3 of my uploaded file :</p>

<pre><code>http://xxxxxx.s3.amazonaws.com/postman/postworld/postman_automated.jpg
</code></pre>

<p>If I get rid of folder structure and directly dump the file , like :</p>

<pre><code>http://xxxxxx.s3.amazonaws.com/postman_automated.jpg
</code></pre>

<p>then the IndexFaces API is passing it successfully .</p>

<p>Can you please suggest how to pass the externalImageId when I do have the 'folder structure'? Currently I am passing the externalImageId through my java code like :</p>

<pre><code>enrolledFileName = userName +""/""+myWorldName+""/""+enrolledFileName;
System.out.println(""The FILE NAME MANIPULATED IS:""+enrolledFileName);
String generateAmazonFaceId = amazonRekognitionManagerObj.addToCollectionForEnrollment(collectionName, bucketName, enrolledFileName);
System.out.println(""The Generated FaceId is:""+generateAmazonFaceId);**strong text**
</code></pre>

<p>Above code internally calls :</p>

<pre><code>Image image=new Image().withS3Object(new S3Object().withBucket(bucketName)
                              .withName(fileName));

 IndexFacesRequest indexFacesRequest = new IndexFacesRequest()
                  .withImage(image)
                  .withCollectionId(collectionName)
                  .withExternalImageId(fileName)
                  .withDetectionAttributes(""ALL"");
</code></pre>",,0,2,,2018-03-29 14:05:59.970 UTC,,2018-03-29 14:05:59.970 UTC,,,,,9538876,1,1,amazon-s3|amazon|amazon-rekognition,250
Unable to read FNC1 character at the first position of a GS1 128 Barcode using Zbar library?,39121975,Unable to read FNC1 character at the first position of a GS1 128 Barcode using Zbar library?,"<p>I have developed an application for barcode decoding in android using Google vision Library for GS1 data matrix and Zbar Library for GS1 128 barcode Unable to read FNC1 character at the first position of a GS1 128 Barcode using Zbar library.</p>

<p>The Zbar library is unable to display any sign of FNC1 character at the start of the Barcode!</p>

<p>Any Solutions. . . .</p>

<p>Instant Help is appreciable . . .</p>

<p>Below is my ZBar Scanner Activity</p>

<pre><code> @SuppressWarnings(""deprecation"")
 public class ZBarFirstScannerActivity extends AppCompatActivity{

//TextView tv;
ImageView iv;
LinearLayout ll;
private Camera mCamera;
private CameraPreview mPreview;
private Handler autoFocusHandler;
private ImageScanner scanner;
private boolean barcodeScanned = false;
private boolean previewing = true;
TextView tv;

static {
    System.loadLibrary(""iconv"");
}
static {
    System.loadLibrary(""zbarjni"");
}



public void onCreate(Bundle savedInstanceState)
{
    super.onCreate(savedInstanceState);



    setContentView(R.layout.barcode_capture1d);


    tv = (TextView) findViewById(R.id.textVertical);
    tv.setRotation(90);

    initToolbar();


    autoFocusHandler = new Handler();
    mCamera = getCameraInstance();
    // Instance barcode scanner

    scanner = new ImageScanner();
    scanner.setConfig(0, Config.X_DENSITY, 1);
    scanner.setConfig(0, Config.Y_DENSITY, 1);
    scanner.setConfig(Symbol.CODE128, Config.ENABLE,1);
    scanner.setConfig(Symbol.EAN13, Config.ENABLE,1);

    mPreview = new CameraPreview(this, mCamera, previewCb, autoFocusCB);
    FrameLayout preview = (FrameLayout)findViewById(R.id.cameraPreview);
    preview.addView(mPreview);


}

private void initToolbar() {

    final Toolbar toolbar = (Toolbar) findViewById(R.id.toolbar);
    setSupportActionBar(toolbar);
    final ActionBar actionBar = getSupportActionBar();

    if (actionBar != null) {


        actionBar.setHomeButtonEnabled(true);
        actionBar.setHomeAsUpIndicator(ContextCompat.getDrawable(this, R.drawable.abc_ic_ab_back_mtrl_am_alpha));

        actionBar.setDisplayHomeAsUpEnabled(true);
    }
}
/** A safe way to get an instance of the Camera object. */
public static Camera getCameraInstance()
{
    Camera c = null;
    try
    {
        c = Camera.open();
    } catch (Exception e)
    {
        //nada
    }
    return c;
}

private void releaseCamera()
{
    if (mCamera != null)
    {
        previewing = false;
        mCamera.setPreviewCallback(null);
        mCamera.release();
        mCamera = null;
    }
}

PreviewCallback previewCb = new PreviewCallback()
{
    public void onPreviewFrame(byte[] data, Camera camera)
    {
        Camera.Parameters parameters = camera.getParameters();
        Size size = parameters.getPreviewSize();

        Image barcode = new Image(size.width, size.height, ""Y800"");
        barcode.setData(data);

        int result = scanner.scanImage(barcode);
        if (result != 0)
        {
            previewing = false;
            mCamera.setPreviewCallback(null);
            mCamera.stopPreview();
            SymbolSet syms = scanner.getResults();
            for (Symbol sym : syms)
            {
                barcodeScanned = true;

                Intent returnIntent = new Intent();
                returnIntent.putExtra(""BARCODE"", sym.getData());
                setResult(MainActivity.BAR_CODE_TYPE_128,returnIntent);
                releaseCamera();
                finish();
                break;
            }
        }
    }
};

// Mimic continuous auto-focusing
AutoFocusCallback autoFocusCB = new AutoFocusCallback()
{
    public void onAutoFocus(boolean success, Camera camera)
    {
        autoFocusHandler.postDelayed(doAutoFocus, 3000);
    }
};

private Runnable doAutoFocus = new Runnable()
{
    public void run()
    {
        if (previewing)
            mCamera.autoFocus(autoFocusCB);
    }
};

public void onPause() {
    super.onPause();
    releaseCamera();
}

public void onResume(){
    super.onResume();
    new ZBarFirstScannerActivity();

}

@Override
public void onBackPressed() {

    releaseCamera();

    finish();
}

@Override
public boolean onOptionsItemSelected(MenuItem item) {
    int id = item.getItemId();

    if (id == android.R.id.home) {
        onBackPressed();
        return true;
    }
    return super.onOptionsItemSelected(item);
}
}
</code></pre>

<p>Below is my Google Scanner Activity</p>

<pre><code>public final class GoogleScannerActivity extends AppCompatActivity {
private static final String TAG = ""Barcode-reader"";

// intent request code to handle updating play services if needed.
private static final int RC_HANDLE_GMS = 9001;

// permission request codes need to be &lt; 256
private static final int RC_HANDLE_CAMERA_PERM = 2;

// constants used to pass extra data in the intent
public static final String AutoFocus = ""AutoFocus"";
public static final String UseFlash = ""UseFlash"";
public static final String BarcodeObject = ""Barcode"";
Bitmap bmp;
FileOutputStream fos = null;
private Camera c;

Switch aSwitch;
private CameraSource mCameraSource;
private CameraSourcePreview mPreview;
private GraphicOverlay&lt;BarcodeGraphic&gt; mGraphicOverlay;

// helper objects for detecting taps and pinches.
private ScaleGestureDetector scaleGestureDetector;
private GestureDetector gestureDetector;

/**
 * Initializes the UI and creates the detector pipeline.
 */
@Override
public void onCreate(Bundle icicle) {
    super.onCreate(icicle);
    setContentView(R.layout.barcode_capture2d);
    initToolbar();

    ActivitySource.caller = this;
    mPreview = (CameraSourcePreview) findViewById(R.id.preview);
    mGraphicOverlay = (GraphicOverlay&lt;BarcodeGraphic&gt;) findViewById(R.id.graphicOverlay);

    boolean autoFocus = true;
    boolean useFlash = false;

    // Check for the camera permission before accessing the camera.  If the
    // permission is not granted yet, request permission.
    int rc = ActivityCompat.checkSelfPermission(this, Manifest.permission.CAMERA);
    if (rc == PackageManager.PERMISSION_GRANTED) {
        createCameraSource(autoFocus, useFlash);
    } else {
        requestCameraPermission();
    }

    gestureDetector = new GestureDetector(this, new CaptureGestureListener());
    scaleGestureDetector = new ScaleGestureDetector(this, new ScaleListener());

    /*Snackbar.make(mGraphicOverlay, ""Tap to capture. Pinch/Stretch to zoom"",
            Snackbar.LENGTH_LONG)
            .show();*/
}

private void initToolbar() {

    final Toolbar toolbar = (Toolbar) findViewById(R.id.toolbar);
    setSupportActionBar(toolbar);
    final ActionBar actionBar = getSupportActionBar();

    if (actionBar != null) {


        actionBar.setHomeButtonEnabled(true);
        actionBar.setHomeAsUpIndicator(ContextCompat.getDrawable(this, R.drawable.abc_ic_ab_back_mtrl_am_alpha));

        actionBar.setDisplayHomeAsUpEnabled(true);
    }
}

private Camera.Size getBestPreviewSize(int width, int height, Camera.Parameters parameters){
    Camera.Size bestSize = null;
    List&lt;Camera.Size&gt; sizeList = parameters.getSupportedPreviewSizes();

    bestSize = sizeList.get(0);

    for(int i = 1; i &lt; sizeList.size(); i++){
        if((sizeList.get(i).width * sizeList.get(i).height) &gt;
                (bestSize.width * bestSize.height)){
            bestSize = sizeList.get(i);
        }
    }
    return bestSize;
}
/**
 * Handles the requesting of the camera permission.  This includes
 * showing a ""Snackbar"" message of why the permission is needed then
 * sending the request.
 */
private void requestCameraPermission() {
    Log.w(TAG, ""Camera permission is not granted. Requesting permission"");

    final String[] permissions = new String[]{Manifest.permission.CAMERA};

    if (!ActivityCompat.shouldShowRequestPermissionRationale(this,
            Manifest.permission.CAMERA)) {
        ActivityCompat.requestPermissions(this, permissions, RC_HANDLE_CAMERA_PERM);
        return;
    }

    final Activity thisActivity = this;

    View.OnClickListener listener = new View.OnClickListener() {
        @Override
        public void onClick(View view) {
            ActivityCompat.requestPermissions(thisActivity, permissions,
                    RC_HANDLE_CAMERA_PERM);
        }
    };

    Snackbar.make(mGraphicOverlay, R.string.permission_camera_rationale,
            Snackbar.LENGTH_INDEFINITE)
            .setAction(R.string.ok, listener)
            .show();
}

@Override
public boolean onTouchEvent(MotionEvent e) {
    boolean b = scaleGestureDetector.onTouchEvent(e);

    boolean c = gestureDetector.onTouchEvent(e);

    return b || c || super.onTouchEvent(e);
}

/**
 * Creates and starts the camera.  Note that this uses a higher resolution in comparison
 * to other detection examples to enable the barcode detector to detect small barcodes
 * at long distances.
 *
 * Suppressing InlinedApi since there is a check that the minimum version is met before using
 * the constant.
 */
@SuppressLint(""InlinedApi"")
private void createCameraSource(boolean autoFocus, boolean useFlash) {
    Context context = getApplicationContext();

    // A barcode detector is created to track barcodes.  An associated multi-processor instance
    // is set to receive the barcode detection results, track the barcodes, and maintain
    // graphics for each barcode on screen.  The factory is used by the multi-processor to
    // create a separate tracker instance for each barcode.

    BarcodeDetector barcodeDetector = new BarcodeDetector.Builder(context).setBarcodeFormats(Barcode.CODE_128 | Barcode.DATA_MATRIX | Barcode.QR_CODE).build();
    BarcodeTrackerFactory barcodeFactory = new BarcodeTrackerFactory(mGraphicOverlay);
    barcodeDetector.setProcessor(
            new MultiProcessor.Builder&lt;&gt;(barcodeFactory).build());

    if (!barcodeDetector.isOperational()) {
        // Note: The first time that an app using the barcode or face API is installed on a
        // device, GMS will download a native libraries to the device in order to do detection.
        // Usually this completes before the app is run for the first time.  But if that
        // download has not yet completed, then the above call will not detect any barcodes
        // and/or faces.
        //
        // isOperational() can be used to check if the required native libraries are currently
        // available.  The detectors will automatically become operational once the library
        // downloads complete on device.
        Log.w(TAG, ""Detector dependencies are not yet available."");

        // Check for low storage.  If there is low storage, the native library will not be
        // downloaded, so detection will not become operational.
        IntentFilter lowstorageFilter = new IntentFilter(Intent.ACTION_DEVICE_STORAGE_LOW);
        boolean hasLowStorage = registerReceiver(null, lowstorageFilter) != null;

        if (hasLowStorage) {
            Toast.makeText(this, R.string.low_storage_error, Toast.LENGTH_LONG).show();
            Log.w(TAG, getString(R.string.low_storage_error));
        }
    }

    // Creates and starts the camera.  Note that this uses a higher resolution in comparison
    // to other detection examples to enable the barcode detector to detect small barcodes
    // at long distances.
    CameraSource.Builder builder = new CameraSource.Builder(getApplicationContext(), barcodeDetector)
            .setFacing(CameraSource.CAMERA_FACING_BACK)
            .setRequestedPreviewSize(1100, 844)
            .setRequestedFps(15.0f);
    // make sure that auto focus is an available option
    if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.ICE_CREAM_SANDWICH) {
        builder = builder.setFocusMode(
                autoFocus ? Camera.Parameters.FOCUS_MODE_CONTINUOUS_PICTURE : null);
    }

    mCameraSource = builder
            .setFlashMode(useFlash ? Camera.Parameters.FLASH_MODE_TORCH : null)
            .build();
}


/**
 * Restarts the camera.
 */
@Override
protected void onResume() {
    super.onResume();
    startCameraSource();
}

/**
 * Stops the camera.
 */
@Override
protected void onPause() {
    super.onPause();
    if (mPreview != null) {
        mPreview.stop();
    }
}

/**
 * Releases the resources associated with the camera source, the associated detectors, and the
 * rest of the processing pipeline.
 */
@Override
protected void onDestroy() {
    super.onDestroy();
    if (mPreview != null) {
        mPreview.release();
    }
}


@Override
public void onRequestPermissionsResult(int requestCode,
                                       @NonNull String[] permissions,
                                       @NonNull int[] grantResults) {
    if (requestCode != RC_HANDLE_CAMERA_PERM) {
        Log.d(TAG, ""Got unexpected permission result: "" + requestCode);
        super.onRequestPermissionsResult(requestCode, permissions, grantResults);
        return;
    }

    if (grantResults.length != 0 &amp;&amp; grantResults[0] == PackageManager.PERMISSION_GRANTED) {
        Log.d(TAG, ""Camera permission granted - initialize the camera source"");
        // we have permission, so create the camerasource
        boolean autoFocus = getIntent().getBooleanExtra(AutoFocus,false);
        boolean useFlash = getIntent().getBooleanExtra(UseFlash, false);
        createCameraSource(autoFocus, useFlash);
        return;
    }

    Log.e(TAG, ""Permission not granted: results len = "" + grantResults.length +
            "" Result code = "" + (grantResults.length &gt; 0 ? grantResults[0] : ""(empty)""));

    DialogInterface.OnClickListener listener = new DialogInterface.OnClickListener() {
        public void onClick(DialogInterface dialog, int id) {
            finish();
        }
    };

    AlertDialog.Builder builder = new AlertDialog.Builder(this);
    builder.setTitle(""Multitracker sample"")
            .setMessage(R.string.no_camera_permission)
            .setPositiveButton(R.string.ok, listener)
            .show();
}

/**
 * Starts or restarts the camera source, if it exists.  If the camera source doesn't exist yet
 * (e.g., because onResume was called before the camera source was created), this will be called
 * again when the camera source is created.
 */
private void startCameraSource() throws SecurityException {
    // check that the device has play services available.
    int code = GoogleApiAvailability.getInstance().isGooglePlayServicesAvailable(
            getApplicationContext());
    if (code != ConnectionResult.SUCCESS) {
        Dialog dlg =
                GoogleApiAvailability.getInstance().getErrorDialog(this, code, RC_HANDLE_GMS);
        dlg.show();
    }

    if (mCameraSource != null) {
        try {
            mPreview.start(mCameraSource, mGraphicOverlay);
        } catch (IOException e) {
            Log.e(TAG, ""Unable to start camera source."", e);
            mCameraSource.release();
            mCameraSource = null;
        }
    }
}

/**
 * onTap is called to capture the oldest barcode currently detected and
 * return it to the caller.
 *
 * @param rawX - the raw position of the tap
 * @param rawY - the raw position of the tap.
 * @return true if the activity is ending.
 */

private boolean onTap(float rawX, float rawY) {
    //TODO: use the tap position to select the barcode.
    BarcodeGraphic graphic = mGraphicOverlay.getFirstGraphic();
    Barcode barcode = null;
    if (graphic != null) {
        barcode = graphic.getBarcode();
        if (barcode != null) {
            Intent data = new Intent();
            data.putExtra(BarcodeObject, barcode);
            setResult(CommonStatusCodes.SUCCESS, data);
            finish();
        }
        else {
            Log.d(TAG, ""barcode data is null"");
        }
    }
    else {
        Log.d(TAG,""no barcode detected"");
    }
    return barcode != null;
}

private class CaptureGestureListener extends GestureDetector.SimpleOnGestureListener {

    @Override
    public boolean onSingleTapConfirmed(MotionEvent e) {

        return onTap(e.getRawX(), e.getRawY()) || super.onSingleTapConfirmed(e);
    }
}

private class ScaleListener implements ScaleGestureDetector.OnScaleGestureListener {

    /**
     * Responds to scaling events for a gesture in progress.
     * Reported by pointer motion.
     *
     * @param detector The detector reporting the event - use this to
     *                 retrieve extended info about event state.
     * @return Whether or not the detector should consider this event
     * as handled. If an event was not handled, the detector
     * will continue to accumulate movement until an event is
     * handled. This can be useful if an application, for example,
     * only wants to update scaling factors if the change is
     * greater than 0.01.
     */
    @Override
    public boolean onScale(ScaleGestureDetector detector) {
        return false;
    }

    /**
     * Responds to the beginning of a scaling gesture. Reported by
     * new pointers going down.
     *
     * @param detector The detector reporting the event - use this to
     *                 retrieve extended info about event state.
     * @return Whether or not the detector should continue recognizing
     * this gesture. For example, if a gesture is beginning
     * with a focal point outside of a region where it makes
     * sense, onScaleBegin() may return false to ignore the
     * rest of the gesture.
     */
    @Override
    public boolean onScaleBegin(ScaleGestureDetector detector) {
        return true;
    }

    /**
     * Responds to the end of a scale gesture. Reported by existing
     * pointers going up.
     * &lt;p/&gt;
     * Once a scale has ended, {@link ScaleGestureDetector#getFocusX()}
     * and {@link ScaleGestureDetector#getFocusY()} will return focal point
     * of the pointers remaining on the screen.
     *
     * @param detector The detector reporting the event - use this to
     *                 retrieve extended info about event state.
     */
    @Override
    public void onScaleEnd(ScaleGestureDetector detector) {
        mCameraSource.doZoom(detector.getScaleFactor());
    }
}

@Override
public boolean onOptionsItemSelected(MenuItem item) {
    int id = item.getItemId();

    if (id == android.R.id.home) {
        onBackPressed();
        return true;
    }
    return super.onOptionsItemSelected(item);
}
}
</code></pre>",,1,0,,2016-08-24 11:22:32.067 UTC,1,2016-09-19 14:13:39.930 UTC,2016-08-26 11:07:12.660 UTC,,5502638,,5502638,1,0,android|barcode-scanner|zbar-sdk|code128,937
Elasticsearch - rank match fields across indices,51863232,Elasticsearch - rank match fields across indices,"<p>I have one index called <code>transcriptions</code> with documents containing a field <code>raw_data</code> that contains blobs of text returned from the Google vision image transcription API. I have another index <code>people</code> with documents containing a <code>full_name</code> field (i.e. ""John Smith""). I'd like to run a query to return the top 5 <code>people</code> documents matched on <code>full_name</code> to the <code>raw_data</code> field of a given <code>transcription</code> document. Can anyone help me out with this or point me in the right direction?</p>",,0,0,,2018-08-15 17:03:56.770 UTC,,2018-08-15 17:03:56.770 UTC,,,,,2738246,1,0,elasticsearch,16
Reading from Realm in iOS with Swift interferes with the Xcode debugger?,51294008,Reading from Realm in iOS with Swift interferes with the Xcode debugger?,"<p>I have an iOS app that reads a QR code, and after reading the tag, it is processed using Realm as DB.</p>

<p>Everything works fine using the Google Vision MLKit.</p>

<p>I am migrating the QR library to use the Apple Vision Framework and I am facing a strange behavior.</p>

<p>The initial symptom is as follows:
- The QR code is read and reported correctly, then the processing of the scanned tag does not continue. (The tag code is a regular 24 bytes String. It all works fine using Google Vision)</p>

<p>I dug a bit using the Xcode debugger and here is where I face the problem (it seems to be related to Realm, but it only fails when using the Vision Framework). </p>

<p>This is the funky code, where the debugger fails (and given that here is where the tag processing from the Vision Framework handler is received, I suspect there is somehow a relationship between the way the Vision handler works and the Realm operation):</p>

<pre><code>print(tagNumber)
let result = self.realm.objects(BizObjects.self).filter(""tagNumber = %@  AND deleted = 0"", tagNumber)
print(""Count: \(result.count)"")
</code></pre>

<ul>
<li>I have breakpoints in lines 2 and 3 of this code.</li>
<li>I scan a QR code and the tagNumber is reported correctly (and printed in line 1)</li>
<li>Once the debugger stopped in the first breakpoint, I click ""step' and then the second breakpoint is ignored, the processing of the tagNumber is not performed, but the app returns to the point where I can scan again.</li>
</ul>

<p>I restarted Xcode and rebooted my Mac.   Still the same strange behavior.</p>

<p>I am using Xcode Version 9.4 (9F1027a), and Swift 4.1</p>

<p>Any ideas of what may be happening here?</p>",,0,1,,2018-07-11 20:52:36.170 UTC,,2018-07-11 20:52:36.170 UTC,,,,,1550223,1,0,ios|swift|xcode|debugging|realm,33
Invalid Syntax in build.py,47647693,Invalid Syntax in build.py,"<p>I'm trying to build an application using this - <a href=""https://github.com/awslabs/amazon-rekognition-video-analyzer/"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-rekognition-video-analyzer/</a>
Unfortunately, i'm getting a syntaxerror in the build.py file.</p>

<pre><code>def write_dir_to_zip(src, zf):
'''Write a directory tree to an open ZipFile object.'''
abs_src = os.path.abspath(src)
for dirname, subdirs, files in os.walk(src):
    for filename in files:
        absname = os.path.abspath(os.path.join(dirname, filename))
        arcname = absname[len(abs_src) + 1:]
        print 'zipping %s as %s' % (os.path.join(dirname, filename),
                                    arcname)
        zf.write(absname, arcname)
</code></pre>

<p>The error i'm getting is as follows </p>

<pre><code> File ""build.py"", line 26
 arcname)) zf.write(absname, arcname)
          ^
 SyntaxError: invalid syntax
</code></pre>

<p>Thanks in advance!</p>",,0,1,,2017-12-05 06:50:58.607 UTC,,2017-12-05 06:54:05.960 UTC,2017-12-05 06:54:05.960 UTC,,2225682,,6310141,1,1,python|amazon-web-services|amazon-rekognition,100
How to improve OCR results with Google Vision API and Python?,47924385,How to improve OCR results with Google Vision API and Python?,"<p>I am working with Google Vision API and Python to apply <code>text_detection</code> which is an OCR function of Google Vision API which detects the text on the image and returns it as an output. My original image is the following:</p>

<p><a href=""https://i.stack.imgur.com/3zxSU.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3zxSU.jpg"" alt=""enter image description here""></a></p>

<p>I have used the following different algorithms:</p>

<p>1) Apply <code>text_detection</code> to the original image</p>

<p>2) Enlarge the original image by 3 times and then apply <code>text_detection</code></p>

<p>3) Apply <code>Canny</code>, <code>findContours</code>, <code>drawContours</code> on a mask (with OpenCV) and then <code>text_detection</code> to this</p>

<p>4) Enlarge the original image by 3 times, apply <code>Canny</code>, <code>findContours</code>, <code>drawContours</code> on a mask (with <code>OpenCV</code>) and then <code>text_detection</code> to this</p>

<p>5) Sharpen the original image and then apply <code>text_detection</code></p>

<p>6) Enlarge the original image by 3 times, sharpen the image and then apply <code>text_detection</code></p>

<p>The ones which fare the best are (2) and (5). On the other hand, (3) and (4) are probably the worse among them.</p>

<p><strong>The major problem is that <code>text_detection</code> does not detect in most cases the minus sign especially the one of '-1.00'.</strong> 
Also, I do not know why, sometimes it does not detect '-1.00' itself at all which is quite surprising as it does not have any significant problem with the other numbers.</p>

<p><strong>What do you suggest me to do to detect accurately the minus sign and in general the numbers?</strong></p>

<p>(Keep in mind that I want to apply this algorithm to different boxes so the numbers may not be at the same position as in this image)</p>",,3,0,,2017-12-21 11:50:35.823 UTC,1,2018-01-09 12:33:04.140 UTC,2018-01-09 12:33:04.140 UTC,,9024698,,9024698,1,1,python|opencv|ocr|google-vision,1716
"Is it possible to display a 3d object outside a Plane, and without a tap?",54437624,"Is it possible to display a 3d object outside a Plane, and without a tap?","<p>I'm developing an android app capable of recognizing texts (doing it with Google Vision).</p>

<p>My goal is to wrap the text recognized with an AR (I'm using ARcore) rectangle as soon as it corresponds to a char sequence.
The problem I'm facing is that the text I want to recognize is on a small piece of metal.
It makes it impossible to detect a Plane on it ---> impossible to place the 3D rectangle.</p>

<p>I was wondering if with the coordinates I get from the text detected (I get either the 4 corner points or <code>getboundingbox()</code>) it is possible to create a custom Plane on the metal item in order to display my rectangle.</p>

<p>I've already tried different ways of doing it, and I can't do it.</p>

<pre><code>ArFragment fragment;

 Session session = fragment.getArSceneView().getSession();
                float[] pos = {0, 0, -1};
                float[] rotation = {0, 0, 0, 1};
                Anchor anchor = session.createAnchor(new Pose(pos, rotation));
                placeObject(fragment, anchor, Uri.parse(""model.sfb""));


private void placeObject(ArFragment arFragment, Anchor anchor, Uri uri) {
        ModelRenderable.builder()
                .setSource(arFragment.getContext(), uri)
                .build()
                .thenAccept(modelRenderable -&gt; addNodeToScene(arFragment, anchor, modelRenderable))
                .exceptionally(throwable -&gt; {
                            Toast.makeText(arFragment.getContext(), ""Error:"" + throwable.getMessage(), Toast.LENGTH_LONG).show();
                            return null;
                        }
                );
    }


private void addNodeToScene(ArFragment arFragment, Anchor anchor, ModelRenderable renderable) {
        AnchorNode anchorNode = new AnchorNode(anchor);
        TransformableNode node = new TransformableNode(arFragment.getTransformationSystem());
        node.setRenderable(renderable);
        node.setParent(anchorNode);
        arFragment.getArSceneView().getScene().addChild(anchorNode);
        node.select();
    }
</code></pre>",,1,0,,2019-01-30 09:49:49.293 UTC,0,2019-02-01 16:26:13.350 UTC,2019-01-30 10:15:10.193 UTC,,10862130,,9136723,1,0,android|ocr|arcore|plane|3d-model,68
How to determine authentication method while using Google Cloud Platform client libraries locally,48527260,How to determine authentication method while using Google Cloud Platform client libraries locally,"<p>I'm currently able to run a local python script that calls the Google vision API using the <a href=""https://googlecloudplatform.github.io/google-cloud-python/"" rel=""nofollow noreferrer"">python client library</a> (specifically, I'm using the <code>google-cloud-vision</code> package).  However, I'm curious about how it's authenticating.  In the python script that I'm running locally I do not provide any authentication information.  From reading the below posts, it seems that a common way to authenticate when running locally is to set an environment variable to the path of a .JSON key file (i.e <code>export GOOGLE_APPLICATION_CREDENTIALS = path/to/JSON/key/file</code>), however, I don't recall doing this and if I run <code>printenv</code>, I do not have an environment variable called GOOGLE_APPLICATION_CREDENTIALS.  </p>

<p>The below posts provide great details about different ways to authenticate using the client libraries locally, but <strong>how can I see/determine exactly how my program is being authenticated?  Is there a way to query for this?</strong></p>

<p><a href=""https://cloud.google.com/vision/docs/auth#top_of_page"" rel=""nofollow noreferrer"">""Authenticating to the Cloud Vision API""</a>...including the <a href=""https://cloud.google.com/vision/docs/auth#application_default_credentials"" rel=""nofollow noreferrer"">""Application Default Credentials""</a> part of the above page</p>

<p><a href=""https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances?hl=en_US&amp;_ga=2.257948290.-809205911.1510850812&amp;_gac=1.170601876.1516657719.EAIaIQobChMIh-WMosbs2AIVh7fACh1PGwf9EAAYASAAEgLwRvD_BwE#clientlib"" rel=""nofollow noreferrer"">""Authenticating Applications With a Client Library""</a> section of Creating and Enabling Service Accounts for Instances</p>

<p><a href=""https://cloud.google.com/docs/authentication/production#providing_credentials_to_your_application"" rel=""nofollow noreferrer"">""Providing Credentials to Your Application""</a> section of ""Setting Up Authentication for Server to Server Production Capabilities"" page</p>

<p><a href=""https://cloud.google.com/docs/authentication/getting-started#setting_the_environment_variable"" rel=""nofollow noreferrer"">""Setting the Environment Variable""</a> Section of ""Getting Started With Authentication"" page:</p>

<p>Python client libraries <a href=""https://developers.google.com/api-client-library/python/start/get_started"" rel=""nofollow noreferrer"">""Getting Started""</a> page:</p>

<p><a href=""https://cloud.google.com/video-intelligence/docs/common/auth#top_of_page"" rel=""nofollow noreferrer"">""Authenticating to a Cloud API Service""</a></p>",,1,0,,2018-01-30 17:28:46.997 UTC,,2018-02-08 15:43:23.477 UTC,2018-01-30 17:50:53.117 UTC,,4521204,,4521204,1,0,python|authentication|google-cloud-platform|gcloud-python|vision-api,323
Multiple Google Vision OCR requests at once?,53585778,Multiple Google Vision OCR requests at once?,"<p>According to the <a href=""http://%20https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate"" rel=""nofollow noreferrer"">Google Vision documentation</a>, the maximum number of image files per request is 16. <a href=""https://cloud.google.com/vision/quotas"" rel=""nofollow noreferrer"">Elsewhere</a>, however, I'm finding that the maximum number of requests per minute is as high as 1800. Is there any way to submit that many requests in such a short period of time from a single machine? I'm using curl on a Windows laptop, and I'm not sure how to go about submitting a second request before waiting for the first to finish almost a minute later (if such a thing is possible).</p>",,1,1,,2018-12-02 23:52:05.673 UTC,,2018-12-27 02:39:41.290 UTC,,,,,10729564,1,0,google-cloud-vision,67
How to draw a moving line on android camera?,39466171,How to draw a moving line on android camera?,"<p>I'd like to draw a moving line on a the camera for waiting the end of a process. For the moment I have this piece of code :</p>

<pre><code> public class OcrGraphic extends GraphicOverlay.Graphic {

    OcrGraphic(GraphicOverlay overlayt) {
        super(overlay);
        postInvalidate();
    }

    @Override
    public void draw(Canvas canvas) {
         //draw line
    }
}
</code></pre>

<p>The issue is that I don't want to draw the line only once but I want to create something like a thread in the purpose of moving the line (up and down) during a long process.</p>

<p>Here is the XML I have : </p>

<pre><code>    &lt;com.google.android.gms.samples.vision.ocrreader.ui.camera.CameraSourcePreview
        android:id=""@+id/preview""
        android:layout_width=""match_parent""
        android:layout_height=""match_parent""&gt;

        &lt;com.google.android.gms.samples.vision.ocrreader.ui.camera.GraphicOverlay
            android:id=""@+id/graphicOverlay""
            android:layout_width=""match_parent""
            android:layout_height=""match_parent"" /&gt;
        &lt;View
            android:layout_width=""5dp""
            android:layout_height=""1dp""
            android:background=""#FF0000""
            android:id=""@+id/line""
            android:layout_alignParentTop=""true""
            android:layout_alignParentLeft=""true""
            android:layout_alignParentStart=""true""
            android:layout_marginTop=""65dp""
            android:minHeight=""5dp"" /&gt;

    &lt;/com.google.android.gms.samples.vision.ocrreader.ui.camera.CameraSourcePreview&gt;
</code></pre>

<p>The 'camereSourcePreview' tag comes from the google vision API.</p>",,1,0,,2016-09-13 09:07:25.083 UTC,,2016-09-13 21:28:09.270 UTC,2016-09-13 21:28:09.270 UTC,,5505361,,5505361,1,3,android|camera,560
"Microsoft Azure Face API returns ""com.microsoft.projectoxford.face.rest.ClientException: Image size is too small.""",49014560,"Microsoft Azure Face API returns ""com.microsoft.projectoxford.face.rest.ClientException: Image size is too small.""","<p>When I send a photo to the Microsoft Azure Face API (<a href=""https://azure.microsoft.com/en-gb/services/cognitive-services/face/"" rel=""nofollow noreferrer"">the api</a>, <a href=""https://docs.microsoft.com/en-us/azure/cognitive-services/face/tutorials/faceapiinjavaforandroidtutorial#feedback"" rel=""nofollow noreferrer"">tutorial</a>), I am receiving </p>

<blockquote>
  <p>com.microsoft.projectoxford.face.rest.ClientException: Image size is too small.</p>
</blockquote>

<p>But when I am debugging the application and when I inspect the following code</p>

<pre><code>faceServiceClient.detect( params[0], false, false, expectedFaceAttributes )
</code></pre>

<p>actually it IS working and I CAN get the result, but only the first time. If I press ""inspect"" one more time I will again receiving the above mentioned error message.</p>

<p>P.S. I tried using different images and the behaviors is the same. I would appreciate any help.</p>

<p>The application: </p>

<pre><code>public class MainActivity extends AppCompatActivity {

    Bitmap bmpImage;
    TextView txtResult;

    ByteArrayInputStream bs;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);

        txtResult = (TextView) findViewById(R.id.txtResult);
        Button btnMicrosoft = (Button) findViewById(R.id.btnMicrosoft);

        btnMicrosoft.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                getFaces();
                makeMicrosoftCall();
            }
        });

    }

    private void getFaces() {
        bmpImage = BitmapFactory.decodeResource(this.getResources(), R.drawable.face_1);

    }

    private void makeMicrosoftCall() {
        imgEncoding();
        GetEmotionCall emotionCall = new GetEmotionCall();
        emotionCall.execute(bs);

    }

    public void imgEncoding()
    {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        bmpImage.compress(Bitmap.CompressFormat.JPEG, 100, baos);
        bs = new ByteArrayInputStream(baos.toByteArray());
    }

    // asynchronous class which makes the api call in the background
    private class GetEmotionCall extends AsyncTask&lt;InputStream, String, Face[]&gt; {
        GetEmotionCall() {
        }


        @Override
        protected void onPreExecute() {
            super.onPreExecute();
            txtResult.setText(""Getting results..."");
        }

        // this function is called when the api call is made
        @Override
        protected Face[] doInBackground(InputStream... params) {
            FaceServiceClient faceServiceClient = new FaceServiceRestClient(""https://westcentralus.api.cognitive.microsoft.com/face/v1.0"", ""*******************"");

            // the only attribute wanted it emotional state
            FaceServiceClient.FaceAttributeType[] expectedFaceAttributes = new FaceServiceClient.FaceAttributeType[]{FaceServiceClient.FaceAttributeType.Emotion};

            try {
                //THE PROBLEMATIC AREA
                return faceServiceClient.detect( params[0], false, false, expectedFaceAttributes );      

            } catch (ClientException e) {
                Log.e(""ClientException"", e.toString());
                return null;
            } catch (IOException e) {
                Log.e(""IOException"", e.toString());
                e.printStackTrace();
                return null;
            }
        }
    }

}
</code></pre>",,0,1,,2018-02-27 17:26:15.537 UTC,,2018-02-27 17:26:15.537 UTC,,,,,9355244,1,1,android|azure|azure-analysis-services|face-api,177
Can Cross Account Access EC2 Role be Created?,46609930,Can Cross Account Access EC2 Role be Created?,"<p>This can be divided in 3 Parts:</p>

<p>Q1) For a Piece of Code Running on EC2 Instance Ec2 Role Super Seeds AWS Config Credentials , because code could not access s3 bucket and awsr rkognition collections , but when I used AWS Configure on instance and added access and secret key I could access resources via aws cli.
But when I ran it through my code it gave an error which on debugged showed aws role arn stating access denied.</p>

<p>Q2) I Deployed an Application On Account <em>1111111</em> With AWS Codestar 
It Requires following Resources:
1) AWS S3 
2) AWS Rekognition</p>

<p>Now the Scenario is that the bucket and Collection of AWS Rekognition are on Different AWS Account <em>2222222</em>.</p>

<p>AWS Code Star Automatically Assigns A role to EC2 Instance and if I Remove it codestar breaks so role that has that accounts code deploy must be kept.</p>

<p>What can done in this scenario i know <a href=""http://docs.aws.amazon.com/AmazonS3/latest/dev/example-walkthroughs-managing-access-example2.html"" rel=""nofollow noreferrer"">S3 Bucket can be given cross account access</a> but what about Rekognition</p>

<p>Can Some One Tell Me Exactly What IAM Settings needs to applied on what accounts if someone wants to achieve this.</p>

<p>Thanks</p>",,1,0,,2017-10-06 16:16:19.833 UTC,,2017-10-06 16:45:19.223 UTC,,,,,6077057,1,0,amazon-web-services|amazon-ec2|amazon-iam,321
Google API for tags extraction from youtube videos,41501511,Google API for tags extraction from youtube videos,"<p>I have extracted tags from the given image using Clarifai and Google Vision APIs. Similar thing I want to achieve for videos. </p>

<p>Can anyone suggest, if there are any APIs available to do so.</p>

<p>Thanks!</p>",,1,0,,2017-01-06 08:08:51.050 UTC,,2017-01-13 18:21:55.753 UTC,,,,,3110257,1,0,video|youtube|tagging,44
Amazon Rekognition Compare Photos Access Denied Exception,53043003,Amazon Rekognition Compare Photos Access Denied Exception,"<p>I want to compare two photos. When I connected to AWS I try to connect to <code>AmazonRekognitionClient</code>:</p>

<pre><code>CompareFacesResult result = new 
AmazonRekognitionClient(credentialsProvider).compareFaces(request);
</code></pre>

<p>But have this error:</p>

<blockquote>
  <p>Caused by: com.amazonaws.services.rekognition.model.AccessDeniedException: User: arn:aws:sts::475877890857:assumed-role/wsirstpp-20181028230251-unauthRole/CognitoIdentityCredentials is not authorized to perform: rekognition:CompareFaces (Service: AmazonRekognition; Status Code: 400; Error Code: AccessDeniedException; Request ID: 7f665a07-db54-11e8-8773-2de830a9e39f)</p>
</blockquote>

<p>Full code for getting information about photos:</p>

<pre><code>ByteBuffer image1 = ByteBuffer.wrap(imageSource);
ByteBuffer image2 = ByteBuffer.wrap(imageTarget);

CompareFacesRequest request = new CompareFacesRequest()
    .withSourceImage(new Image().withBytes(image1))
    .withTargetImage(new Image().withBytes(image2))
    .withSimilarityThreshold(70 F);

CognitoCachingCredentialsProvider credentialsProvider = new CognitoCachingCredentialsProvider(
    context,
    ""us-east-2:My_pool_Id"", // Identity pool ID
    Regions.US_EAST_2 // Region
);

CompareFacesResult result = new AmazonRekognitionClient(credentialsProvider).compareFaces(request);

List &lt; CompareFacesMatch &gt; faceMatches = result.getFaceMatches();

for (CompareFacesMatch match: faceMatches) {
    Float similarity = match.getSimilarity();
    Log.d(TAG, ""run: similarity:"" + similarity.toString());
}
</code></pre>

<p>What is wrong? What I did wrong in this code?</p>",53317587,2,0,,2018-10-29 10:01:35.510 UTC,,2018-11-15 10:41:35.243 UTC,2018-10-29 11:14:18.393 UTC,,5104748,,8498598,1,0,compare|amazon|amazon-rekognition,78
oauth2client.client.ApplicationDefaultCredentialsError,55164321,oauth2client.client.ApplicationDefaultCredentialsError,"<p>I am trying to run the code in <a href=""https://github.com/DexterInd/GoogleVisionTutorials"" rel=""nofollow noreferrer"">GoogleVisionTutorials</a>. I have run the following terminal command correctly:</p>

<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=filename.json
</code></pre>

<p>However I am getting the following credential errors:</p>

<pre><code>Traceback (most recent call last):

File ""camera-vision-logo.py"", line 52, in &lt;module&gt;
    main()
  File ""camera-vision-logo.py"", line 26, in main
    credentials = GoogleCredentials.get_application_default()
  File ""/usr/local/lib/python2.7/dist-packages/oauth2client/client.py"", line 1271, in get_application_default
    return GoogleCredentials._get_implicit_credentials()
  File ""/usr/local/lib/python2.7/dist-packages/oauth2client/client.py"", line 1256, in _get_implicit_credentials
    credentials = checker()
  File ""/usr/local/lib/python2.7/dist-packages/oauth2client/client.py"", line 1207, in _implicit_credentials_from_files
    credentials_filename = _get_environment_variable_file()
  File ""/usr/local/lib/python2.7/dist-packages/oauth2client/client.py"", line 1355, in _get_environment_variable_file
    ' environment variable) does not exist!')
oauth2client.client.ApplicationDefaultCredentialsError: File vision2-234508-f73783d1ef52.json (pointed by GOOGLE_APPLICATION_CREDENTIALS environment variable) does not exist!
</code></pre>",,2,0,,2019-03-14 13:54:14.987 UTC,,2019-03-20 02:04:12.293 UTC,2019-03-14 17:37:16.627 UTC,,4420967,,11203430,1,1,python|linux|raspberry-pi|google-cloud-vision,59
"Node JS API: Return JSON to user, after service is done.",42222036,"Node JS API: Return JSON to user, after service is done.","<p>I'm currently using node.js to create a post-upload API, to upload an image, which is processed by the Watson Visual Recognition Service. This returns a JSON, which is currently logged to the console. 
<strong>Is there a way to send this JSON back to the user, after the process is done?</strong> 
I'm a total newbie to Node.js, so I really appreciate your help. </p>

<p>This is my code:  </p>

<pre><code>// initialising ...  

app.post( '/detectFaces', avatarUpload, ( req, res ) =&gt; { 

    avatarUpload( req, res, ( err ) =&gt; {

        if ( err || !req.file )
            return res.send({ error: 'invalid_file' })

        console.log( req.file );

        var path = req.file.path;
        var name = req.file.filename;  

        var params = { 
            images_file: fs.createReadStream(path)
        };   

        //Call to the visual recognition Service
        visual_recognition.detectFaces(params, function(err, res){ 
            if(err) 
                console.log(err); 
            else  
                console.log(JSON.stringify(res, null, 2));   

        });  

        //The JSON of the visual Rec Service should send here. 
        res.send({ 'status' : 'check', url: 'uploads' + '/' + filename }) 


    }) 
    var params = { 
            images_file: fs.createReadStream(path)
        };    


}); 


app.listen(3000, function () {
  console.log('Upload Server listening on port 3000');
});
</code></pre>",42222065,1,0,,2017-02-14 09:15:02.427 UTC,,2017-02-14 09:47:40.767 UTC,2017-02-14 09:47:40.767 UTC,,157247,,2197319,1,-1,javascript|json|node.js|watson,80
Having issue with google vision api PDF conversion,56016618,Having issue with google vision api PDF conversion,"<p>I am implementing google vision API to convert pdf to text. I am at the end everything works fine but getting an error at the end </p>

<p>I have used </p>

<p><a href=""https://github.com/GoogleCloudPlatform/php-docs-samples/blob/master/vision/src/detect_pdf_gcs.php"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/php-docs-samples/blob/master/vision/src/detect_pdf_gcs.php</a></p>

<p>Output file is showing Output files: </p>

<blockquote>
  <p>OCR_PDF_TEST_OUTPUT/output-1-to-2.json</p>
</blockquote>

<p>But after that getting </p>

<blockquote>
  <p>Call to a member function downloadAsString() on null</p>
</blockquote>

<p>.</p>

<pre><code>$jsonString = $firstObject-&gt;downloadAsString();
    $firstBatch = new AnnotateFileResponse();
    $firstBatch-&gt;mergeFromJsonString($jsonString);
</code></pre>",,0,0,,2019-05-07 06:21:17.843 UTC,,2019-05-07 07:07:40.337 UTC,2019-05-07 07:07:40.337 UTC,,1491414,,11462893,1,0,php|api|vision,13
Google Vision API - Split OCR Results to Different Lines?,48670839,Google Vision API - Split OCR Results to Different Lines?,"<p>I'm trying to use the Google Vision API in C# for an image with text on multiple lines. I want each line to be a separate string, but the API puts it all into 1 string. </p>

<p>I tried filtering by capitals at the beginning, but some lines have capitals at the beginning of each word, so it's not always just at the beginning of each line.</p>

<p>How can I change it so that it takes in each line separately? Since all the lines are in the same place in the image each time, could I crop it using C# to get each line individually?</p>

<p>Thanks :)</p>",,1,3,,2018-02-07 18:24:50.580 UTC,,2018-02-07 23:36:43.210 UTC,,,,,6185049,1,0,c#|api|google-vision|vision-api,667
Google cloud vision OCR reads the same letter more than once,56362468,Google cloud vision OCR reads the same letter more than once,"<p>I'm using google cloud vision OCR to extract text from receipt images and came across this weird issue where the OCR reads the same letter twice but with different coordinates.
To visualize the issue, I draw rectangles around each letter using the coordinates returned from the API:
This is the part of the image with the issue:</p>

<p><img src=""https://i.ibb.co/Rj4wzy5/Screen-Shot-2019-05-29-at-4-56-04-PM.png"" alt=""example""></p>

<p>As you can see, there are overlapping rectangles on the 'M' and the 'a'.
The result is something like this:
   '''MMaay 10, 2019'''</p>

<p>Why this is happening?
Is there a way to fix it?</p>

<p>I tried to change the image format from bmp to png. The only difference is that the overlapping rectangle is moved from the 'a' to the 'y'.</p>",,0,0,,2019-05-29 14:05:06.103 UTC,,2019-05-29 14:05:06.103 UTC,,,,,5367057,1,1,ocr|google-cloud-vision,8
AWS image processing,50035928,AWS image processing,"<p>I am working on a project where I need to take a picture of a surface using my phone and then analyze the surface for defects and marks.</p>

<p>I want to take the image and then send it to the cloud for analysis.
Does AWS-Rekognition provide such a service to analyze the defects I want to study?
Or Would I need to write a custom code using opencv or something?</p>",,1,1,,2018-04-26 06:11:04.473 UTC,,2018-04-26 06:48:59.563 UTC,,,,,3138711,1,0,amazon-web-services|amazon-rekognition,77
JNI DETECTED ERROR IN APPLICATION: input is not valid Modified UTF-8: illegal start byte 0x9c NativeBarcode,46375016,JNI DETECTED ERROR IN APPLICATION: input is not valid Modified UTF-8: illegal start byte 0x9c NativeBarcode,"<p>I get a problem with a Google Vision API.</p>

<p>I try to get the PDF417 from idcard and get this exception:</p>

<pre><code>09-22 18:52:08.701 19216-
20545/com.google.android.gms.samples.vision.barcodereader A/art: 
art/runtime/java_vm_ext.cc:470] JNI DETECTED ERROR IN APPLICATION: input is 
not valid Modified UTF-8: illegal start byte 0x9c
09-22 18:52:08.701 19216-
20545/com.google.android.gms.samples.vision.barcodereader A/art: 
art/runtime/java_vm_ext.cc:470]     input: '0x30 0x33 0x34 0x35 0x39 0x33 
0x33 0x36 0x34 0x39 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 
0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 
0xc0 0x80 0x50 0x75 0x62 0x44 0x53 0x4b 0x5f 0x31 0xc0 0x80 0xc0 0x80 0xc0 
0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0x31 0x38 0x30 0x37 
0x34 0x36 0x39 0x38 0x30 0x30 0x38 0x30 0x34 0x34 0x39 0x34 0x39 0x38 0x53 
0x41 0x41 0x56 0x45 0x44 0x52 0x41 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 
0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 
0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0x53 0x41 0x4c 0x41 0x4d 0x41 0x4e 0x43 
0x41 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 
0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0x44 
0x49 0x45 0x47 0x4f 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 
0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 
0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80 0xc0 0x80
09-22 18:52:08.701 19216-
20545/com.google.android.gms.samples.vision.barcodereader A/art: 
art/runtime/java_vm_ext.cc:470]     in call to NewStringUTF
09-22 18:52:08.701 19216-
20545/com.google.android.gms.samples.vision.barcodereader A/art: 
art/runtime/java_vm_ext.cc:470]     from 
com.google.android.gms.vision.barcode.internal.NativeBarcode[]com.google.android.gms.vision.barcode.internal.NativeBarcodeDetector.recognizeN
ative(int, int, byte[],com.google.android.gms.vision.barcode.internal.NativeBarcodeDetector$NativeOptions)
09-22 18:52:08.701 19216-20545/com.google.android.gms.samples.vision.barcodereader A/art: art/runtime/java_vm_ext.cc:470] ""Thread-2"" prio=5 tid=18 Runnable
</code></pre>

<p>My code is equals to the example in :</p>

<pre><code>try {
    mDetector.receiveFrame(outputFrame);
} catch (Throwable t) {
    t.printStackTrace();
} finally {
    mCamera.addCallbackBuffer(data.array());
}
</code></pre>

<p>the example is available at: <a href=""https://github.com/googlesamples/android-vision"" rel=""nofollow noreferrer"">https://github.com/googlesamples/android-vision</a></p>

<p>Any idea?</p>",,0,0,,2017-09-23 01:05:12.233 UTC,,2018-01-11 11:34:57.303 UTC,,,,,8658771,1,1,android|google-vision,599
post binary data to Microsoft face api and return 400 bad request,45696336,post binary data to Microsoft face api and return 400 bad request,"<p>I have converted an image into byte array and try to post it through microsoft face api, but I have been receiving Http400 bad request. I am not sure if this problem is caused by the headers or binary data I created.  I did manage to post an image uri to it in similar manners and it works just fine. </p>

<pre><code>    HttpClient client = new HttpClient();
    client.BaseAddress = new Uri(baseUrl);
    client.DefaultRequestHeaders.TryAddWithoutValidation(""content-type"", ""application/octet-stream"");
    client.DefaultRequestHeaders.Add(""Ocp-Apim-Subscription-Key"", 
                                    ""MY-KEY"");
    HttpRequestMessage request = new HttpRequestMessage(HttpMethod.Post, ""/face/v1.0/detect"");
    request.Content = new ByteArrayContent(bytes);
    HttpResponseMessage response = await client.SendAsync(request);
    return response;
</code></pre>

<p>This is the request and for some reason the content-type is not there. Could some one help to explain? Thanks</p>

<pre><code>{Method: POST, RequestUri: 'https://eastus2.api.cognitive.microsoft.com/face/v1.0/detect', Version: 2.0, Content: System.Net.Http.ByteArrayContent, Headers:{  Ocp-Apim-Subscription-Key: MY-Key  Content-Length: 141600}}
</code></pre>

<p>This is the api reference <a href=""https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236"" rel=""nofollow noreferrer"">Documentation</a></p>",,0,0,,2017-08-15 15:38:58.387 UTC,1,2017-08-15 15:38:58.387 UTC,,,,,5569571,1,1,c#|.net|api|httpclient,127
CORS Issues with Google Vision API Calls,49912384,CORS Issues with Google Vision API Calls,"<p>I am making a call to Google's Vision API using Ajax. I have completed billing and received an API key. However once implemented, I am getting errors like this:</p>

<p>""Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'null' is therefore not allowed access. The response had HTTP status code 403.""</p>

<p>I have tried using solutions I found online like setting the request header to ""Access-Control-Allow-Origin: *"" and using a Chrome Extension. If anybody can help that would be excellent.</p>

<pre><code>var request = {     
    ""requests"": [{
              ""image"": {
                ""content"": url,
              },
              ""features"": [{
                  ""type"": ""WEB_DETECTION"",
                  ""maxResults"": 1
               }]
            }]
     }

$.ajax({
     method: 'POST',
     url: 'https://vision.googleapis.com/v1/images:annotate?key=' + key,
     contentType: 'application/json',
     data: JSON.stringify(request),
     processData: false,
     beforeSend: function(req) {
         req.setRequestHeader('Access-Control-Allow-Origin', '*');
         console.log(req);
     },
     success: function(data){
         console.log(""Data: "" + data);
         var webData = data.responses[0].webAnnotations[0];
         console.log(""Web Data: "" + webData);
     },
     error: function (data, textStatus, errorThrown) {
         console.log('error: ' + errorThrown);
     }
}); 
</code></pre>

<p></p>",,1,16,,2018-04-19 03:46:14.037 UTC,,2018-04-25 12:35:16.503 UTC,2018-04-25 12:35:16.503 UTC,,6915507,,9660917,1,1,javascript|google-cloud-platform|google-cloud-vision,253
Inconsistent Google OCR text detection,53039190,Inconsistent Google OCR text detection,"<p>I am using Goggle vision 'documentTextDetection' for one of my project. My aim is to detect text from images, while checking I get the impression that am getting inconsistent text extraction for same images(ie different link, but image is same) and getting different results. </p>

<p>I am using '@google-cloud/vision'(<a href=""https://github.com/googleapis/nodejs-vision"" rel=""nofollow noreferrer"">https://github.com/googleapis/nodejs-vision</a>) node npm for the same. Also noticed that some of the characters are mismatching in the results </p>

<p>eg: In most of the cases '0' is recognizing as O(<a href=""https://samsung-nudge.s3.eu-central-1.amazonaws.com/4.jpeg"" rel=""nofollow noreferrer"">https://samsung-nudge.s3.eu-central-1.amazonaws.com/4.jpeg</a>), 5 as S (<a href=""https://samsung-nudge.s3.eu-central-1.amazonaws.com/4.jpeg"" rel=""nofollow noreferrer"">https://samsung-nudge.s3.eu-central-1.amazonaws.com/4.jpeg</a>), / as I (<a href=""https://samsung-nudge.s3.eu-central-1.amazonaws.com/1.jpeg"" rel=""nofollow noreferrer"">https://samsung-nudge.s3.eu-central-1.amazonaws.com/1.jpeg</a>), etc </p>

<pre><code>let imageurl= 'https://samsung-nudge.s3.eu-central-1.amazonaws.com/barcode1540752102759.jpeg'
client
.documentTextDetection(imageurl)
.then(results =&gt; {
       console.log('results', JSON.stringify(results[0].textAnnotations))
})
.catch(err =&gt; {
    console.error('GOOGLE VISION ERROR:', err);
    reject(err)
});
</code></pre>

<p><strong>Same image giving different results</strong> </p>

<p><a href=""https://samsung-nudge.s3.eu-central-1.amazonaws.com/barcode1540752102759.jpeg"" rel=""nofollow noreferrer"">https://samsung-nudge.s3.eu-central-1.amazonaws.com/barcode1540752102759.jpeg</a></p>

<blockquote>
  <p>MODEL:\nRH60H8138WZ\nPOWER:\n230V/ 50Hz\nCOMPRESSOR:\n2007 -
  000029\nMODEL CODE:\nRH6OH8138WZ/SS\nSERIAL NO:\n07KH43AG300046M\n</p>
</blockquote>

<p><a href=""https://samsung-nudge.s3.eu-central-1.amazonaws.com/m.jpeg"" rel=""nofollow noreferrer"">https://samsung-nudge.s3.eu-central-1.amazonaws.com/m.jpeg</a></p>

<blockquote>
  <p>MODEL:\nRH60H8138WZ\nPOWER:\n230V/ 50Hz\nCOMPRESSOR:\n2007 -
  000029\nMODEL CODE:\nRH60H8138WZ/SS\nSERIAL NO:\n07KH43AG300046M\n</p>
</blockquote>

<p>Please let me know why am getting inconsistent responses? Also let me know anything I can do to improve the results. </p>",,0,0,,2018-10-29 05:23:38.397 UTC,,2018-10-29 05:23:38.397 UTC,,,,,2244411,1,0,ocr|google-vision,32
"Bounding box (left, top, height, width) to PHP x1, x2, y1, y2 coordinates?",53737055,"Bounding box (left, top, height, width) to PHP x1, x2, y1, y2 coordinates?","<p>I am using AWS Rekognition to detect faces in an image. When a face is detected it outputs bound box information so that you can use it to draw one on the image. However, these are left, top, height, and width and the numbers are decimal floats.</p>

<p>Here is an example of the output:</p>

<pre><code>{ ""BoundingBox"": { ""Width"": 0.06649632751941680908203125, ""Height"": 0.102198123931884765625, ""Left"": 0.52286112308502197265625, ""Top"": 0.2651510536670684814453125 }
</code></pre>

<p>And to draw the boxes on the image I do this:</p>

<pre><code>                list($width, $height, $type, $attr) = getimagesize($destinationPath . $im . '.png');
                    $white = imagecolorallocate($im, 255, 255, 255);
                    imagerectangle($im, $result['FaceDetails']['0']['BoundingBox']['Top'] * $width, $result['FaceDetails']['0']['BoundingBox']['Left'] * $height, $result['FaceDetails']['0']['BoundingBox']['Height'] * $width, $result['FaceDetails']['0']['BoundingBox']['Width'] * $height, $white);
                    imagepng($im, $destinationPath . $im . '.png', 9);
</code></pre>

<p>However, the box never matches the face. Is there an easier way to convert these variables or calculate them? I have looked everywhere and could really use some guidance.</p>",,1,0,,2018-12-12 06:11:25.403 UTC,,2018-12-12 07:05:52.997 UTC,,,,,7716139,1,0,php|laravel|amazon-web-services|amazon-rekognition,85
Passing base64 to google vision gives 400 error,54061460,Passing base64 to google vision gives 400 error,"<p>I am grabbing frames from the webcam, converting each image bitmap into a base64 string then passing that to the Google vision API. When i do this i am catching an error but it only logs as true. Im new to react and am struggling to see what i am missing. </p>

<pre><code>  grabFrame() {
    let mediaStreamTrack = this.state.mediaStream.getVideoTracks()[0];
    let imageCapture = new window.ImageCapture(mediaStreamTrack);

    return imageCapture.grabFrame();
  }

  uploadFrame() {
    this.grabFrame()
    .then(function(bitmapImage) {
      var canvas = document.createElement(""canvas"")
      canvas.width = bitmapImage.width;
      canvas.height = bitmapImage.height;

      let context = canvas.getContext(""2d"")
      context.drawImage(bitmapImage, 0, 0);

      let base64Image = canvas.toDataURL(""image/png"")

      const request = new vision.Request({
        image: new vision.Image({
          base64: base64Image,
        }),
        features: [ new vision.Feature('FACE_DETECTION') ]
      })

      vision.annotate(request)
      .then((response) =&gt; {
        console.log(`Response: ${response}`)
      })
      .catch((error) =&gt; {
        console.log(`Error: ${error}`)      &gt;&gt;&gt;&gt;      ""Error: true""
      });
    }).catch((error) =&gt; {
      console.log('grabFrame() error: ', error)
    });
  }
</code></pre>

<p>In the console, all I can see is <code>POST https://vision.googleapis.com/v1/images:annotate?key=xxxxxxxxxxxxxxxxxxx 400</code></p>

<p>Logging <code>base64Image</code> gives <code>data:image/png;base64,iVBORw0KGgoAA...</code></p>

<p>Am I missing something?</p>",,0,2,,2019-01-06 12:24:11.997 UTC,,2019-01-06 12:24:11.997 UTC,,,,,6734301,1,0,reactjs|google-vision,13
Blocktype BARCODE : can Cloud Vision read bar codes and how?,49450500,Blocktype BARCODE : can Cloud Vision read bar codes and how?,"<p>This is my first question here so I'll try to be as relevant as possible.
I am interested in using Cloud Vision to process some documents, as I need OCR capabilities. I also happen to need bar code reading, which I currently have implemented using ZXing.</p>

<p>I stumbled upon the BARCODE blocktype in the OCR ( <a href=""https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate#BlockType"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate#BlockType</a> ) but I did not manage to produce such a block, even with an image containing <em>only</em> a bar code.</p>

<p>Hence the question: is the feature implemented, and if so, how can we get it to work ? Thank you for your time !</p>

<p><strong>Note:</strong></p>

<p>I have seen those related questions:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/36309709/does-google-cloud-vision-ocr-support-bar-code-reading"">Does google cloud vision OCR support bar code reading?</a></li>
<li><a href=""https://stackoverflow.com/questions/46194151/how-can-i-call-or-emulate-google-mobile-vision-api-in-google-cloud"">How can I call or emulate Google Mobile Vision API in Google Cloud?</a></li>
</ul>

<p>But they do not satify me as I need both the barcode reading and the OCR, and  I am doing work on backend only, no user involved.</p>

<p><strong>Edit:</strong></p>

<p>I have tried for example with <a href=""https://www.flickr.com/photos/89357270@N00/2672281295/in/photolist-55998r-8S2hai-5rS4RR-9ajtPc-dcLBtm-QdFMGv-cRmwsm-j1ufZK-7KVuqs-4s1hnQ-BSW5AE-T5dyYk-75zzRB-6cezRo-7t3XJv-hrU5VH-qpNd5j-8LkuxN-81nU8f-hzM87o-bjzFJk-9RSFHj-XfrcwL-ccoDSs-65oNUC-DvVakW-9Hb6VY-F9c92B-7pzDDu-nk2fMA-5WW3Bq-bE8JcG-a19FW3-3gFyZq-h4KL2P-6u8Nfe-bX64rT-kc9ifb-5qFzJd-t5C5H-im2U2R-4h7G2J-85Rit1-5T59Tb-bALAzp-8KSNA7-5J3vhW-9daxMy-7umGQm-6So7Fo"" rel=""nofollow noreferrer"">this image</a> :</p>



<pre class=""lang-bash prettyprint-override""><code>~/Pictures                                                                                                                                                                                                                                                                    
» gcloud ml vision detect-text barcode.jpg | grep -i ""blocktype""
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",

~/Pictures                                                                                                                                                                                                                                                                    
» gcloud ml vision detect-document barcode.jpg | grep -i ""blocktype""
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
            ""blockType"": ""TEXT"",
</code></pre>

<p>I also tried with <a href=""https://imgur.com/a/TBWEp"" rel=""nofollow noreferrer"">this one</a></p>

<pre class=""lang-bash prettyprint-override""><code>~/Pictures                                                                                                                                                                                                                                                                
»gcloud ml vision detect-text barcode.png                      
{
  ""responses"": [
    {}
  ]
}

~/Pictures                                                                                                                                                                                                                                                                    
» gcloud ml vision detect-document barcode.png
{
  ""responses"": [
    {}
  ]
}
</code></pre>",49521236,1,0,,2018-03-23 13:13:51.747 UTC,2,2019-02-18 22:25:37.857 UTC,2018-03-28 05:20:12.027 UTC,,9540386,,9540386,1,3,ocr|barcode|google-cloud-vision,335
"Is there any option on Google Cloud Vision API, to detect and return a table (Rows and Column with headers) from a scanned Image?",50133223,"Is there any option on Google Cloud Vision API, to detect and return a table (Rows and Column with headers) from a scanned Image?",<p>We are using Google Cloud Vision APIs to extract Invoice fields. We would like to know whether the APIs support detection of table of data? Or do we have to write custom code to detect tables?</p>,,1,0,,2018-05-02 11:10:01.343 UTC,,2018-05-08 12:59:32.400 UTC,2018-05-07 14:11:14.597 UTC,,4370109,,9729939,1,1,detection|google-cloud-vision,684
Prevent app crash due to Google native library error,47158230,Prevent app crash due to Google native library error,"<p>I have an app uploaded to Google Play that was working fine, but suddenly some users reported me a strange error. I have been investigating and it's due to an <a href=""https://github.com/googlesamples/android-vision/issues/269"" rel=""nofollow noreferrer"">internal error of the google vision inside gms library</a>. This is the code that causes the problem:</p>

<pre><code>TextRecognizer textRecognizer = new TextRecognizer.Builder(context)
                .build();
</code></pre>

<p>This error only occurs on some devices. So how can I handle this error at runtime preventing an app crash?. </p>

<p>Thanks a lot!</p>",,2,2,,2017-11-07 12:38:44.563 UTC,,2017-11-08 13:34:50.827 UTC,2017-11-07 13:23:28.250 UTC,,5107823,,5107823,1,1,android|google-vision|crash,304
"API - Microsoft Cognitive Services Face, how to return attributes",38336213,"API - Microsoft Cognitive Services Face, how to return attributes","<p>In swift I'm using the Microsoft Cognitive Services Face API function <code>detectWithData</code> and trying to use <code>returnFaceAttributes</code> which calls for <code>[AnyObject]!</code>. I need help with what to enter into the Array.</p>

<p>According to <a href=""https://dev.projectoxford.ai/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236"" rel=""nofollow"">this link</a> I assumed <code>[""age"", ""gender""]</code> would work but I receive an error saying:</p>

<blockquote>
  <p>unrecognized selector sent to instance 0x7f9b96043df0</p>
</blockquote>

<p>And using <code>[MPOFaceAttributeTypeAge, MPOFaceAttributeTypeGender]</code> gives an error: </p>

<blockquote>
  <p>Value of type 'MPOFaceAttributeTypeAge' does not conform to expected element type 'AnyObject'</p>
</blockquote>

<p>For some reason typing ""true"" in the array give me the age attribute but all other attributes show as nil.  </p>

<p>I can't find any examples using swift online.  Any advice or pointing me in the right direction would be appreciated.</p>

<pre><code>@IBAction func battleBtn(sender: UIButton){
    if !hasChoosenTop || !hasChoosenBottom{
        showErrorAlert()
    } else{
        if let firstImg = topImg.image, let firstImgData = UIImageJPEGRepresentation(firstImg, 0.8), let secondImg = bottomImg.image, let secondImgData = UIImageJPEGRepresentation(secondImg, 0.8){
            FaceService.instance.client.detectWithData(firstImgData, returnFaceId: true, returnFaceLandmarks: false, returnFaceAttributes: [MPOFaceAttributeTypeAge, MPOFaceAttributeTypeGender], completionBlock: { (face: [MPOFace]!, err: NSError!) in
                if err == nil {
                    var topFace: String?
                    topFace = face[0].faceId
                    var top = face[0].attributes.age
                    print(""my faceId: \(topFace)"")
                    print(""my faceId: \(top)"")

                }
            })
        }
    }
}
</code></pre>

<p><a href=""http://i.stack.imgur.com/n1tCL.png"" rel=""nofollow"">screenshot of error</a> </p>",,1,2,,2016-07-12 18:15:44.560 UTC,0,2016-07-13 18:13:37.300 UTC,2016-07-13 07:05:03.967 UTC,,6075275,,6075275,1,1,ios|swift|microsoft-cognitive,347
Why is my amazon rekognition program throwing this error?,55113529,Why is my amazon rekognition program throwing this error?,"<p>Following a tutorial, doing everything exactly as in the video, can anybody see what's wrong? Hoping to figure this out as it is a very interesting concept. I think it is related to the recognition client/ location, but this is my first aws project so there is a lot of uncharted territory for me.</p>

<p>Thanks for the help!</p>

<p>Im getting this error: </p>

<pre><code>com.amazonaws.services.rekognition.model.AmazonRekognitionException: The security token included in the request is invalid. (Service: AmazonRekognition; Status Code: 400; Error Code: UnrecognizedClientException; Request ID: 678e90e3-4466-11e9-8116-e94cae3aa352)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)
        at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)
        at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)
        at com.amazonaws.services.rekognition.AmazonRekognitionClient.doInvoke(AmazonRekognitionClient.java:3430)
        at com.amazonaws.services.rekognition.AmazonRekognitionClient.invoke(AmazonRekognitionClient.java:3397)
        at com.amazonaws.services.rekognition.AmazonRekognitionClient.invoke(AmazonRekognitionClient.java:3386)
        at com.amazonaws.services.rekognition.AmazonRekognitionClient.executeDetectLabels(AmazonRekognitionClient.java:1168)
        at com.amazonaws.services.rekognition.AmazonRekognitionClient.detectLabels(AmazonRekognitionClient.java:1140)
        at com.amazonaws.samples.DetectLabels.main(DetectLabels.java:35)
</code></pre>

<p>Here is my code which I tried</p>

<pre><code>    package com.amazonaws.samples;

    //Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
    //PDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-rekognition-developer-guide/blob/master/LICENSE-SAMPLECODE.)

    //package com.amazonaws.samples;
    import com.amazonaws.services.rekognition.AmazonRekognition;
    import com.amazonaws.services.rekognition.AmazonRekognitionClientBuilder;
    import com.amazonaws.services.rekognition.model.AmazonRekognitionException;
    import com.amazonaws.services.rekognition.model.DetectLabelsRequest;
    import com.amazonaws.services.rekognition.model.DetectLabelsResult;
    import com.amazonaws.services.rekognition.model.Image;
    import com.amazonaws.services.rekognition.model.Label;
    import com.amazonaws.services.rekognition.model.S3Object;
    import java.util.List;

    public class DetectLabels {

     public static void main(String[] args) throws Exception {

        String photo = ""19756622_10211667570038052_5508425252837325962_n.jpg"";
        String bucket = ""211proj"";


        AmazonRekognition rekognitionClient = AmazonRekognitionClientBuilder.standard().withRegion(""us-west-2"").build();

        DetectLabelsRequest request = new DetectLabelsRequest()
             .withImage(new Image()
             .withS3Object(new S3Object()
             .withName(photo).withBucket(bucket)))
             .withMaxLabels(10)
             .withMinConfidence(75F);

        try {
           DetectLabelsResult result = rekognitionClient.detectLabels(request);
           List &lt;Label&gt; labels = result.getLabels();

           System.out.println(""Detected labels for "" + photo);
           for (Label label: labels) {
              System.out.println(label.getName() + "": "" + label.getConfidence().toString());
           }
        } catch(AmazonRekognitionException e) {
           e.printStackTrace();
        }
     }
    }
</code></pre>",,0,0,,2019-03-12 03:06:51.020 UTC,,2019-03-12 03:26:33.273 UTC,2019-03-12 03:26:33.273 UTC,,10429035,,11168589,1,0,java|amazon-web-services,28
What is the daily limit on the number of images Watson Visual Recognition.,45790003,What is the daily limit on the number of images Watson Visual Recognition.,<p>What is the daily limit on the number of images that could be processed using Watson Visual Recognition. Free plan on the doc shows 250. Can we upload more on a Standard plan ??</p>,45790833,2,0,,2017-08-21 05:42:35.433 UTC,,2017-08-21 22:15:48.030 UTC,,,,,2623476,1,1,image|max|limit|visual-recognition,123
Using local <image file> for microsoft custom vision,55555575,Using local <image file> for microsoft custom vision,"<p>I'm not sure how to go about this, but I need help in getting my microsoft custom vision to work. I'm using javascript to link my html document to custom vision but I don't know how to use a local image file I have in the same folder as my html and js files, could anybody assist me with any codes?</p>

<pre><code>        },
        type: ""POST"",
        // Request body
        data: ""{body}"",
    })
    .done(function(data) {
        alert(""success"");
    })
    .fail(function() {
        alert(""error"");
    });
});
</code></pre>

<p>The instructions tell me to change {body} to </p>",,1,3,,2019-04-07 04:17:24.427 UTC,,2019-04-08 09:24:36.913 UTC,2019-04-07 05:02:36.017 UTC,,3443050,,11323311,1,0,javascript|microsoft-custom-vision,39
How to uploading duplicate tags at some picture for azure custom vision?,55330723,How to uploading duplicate tags at some picture for azure custom vision?,"<p>I have a question for azure custom vision. I have a custom vision project for object detection. 
And I use the python SDK to create the project (see that: <a href=""https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/python-tutorial-od"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/python-tutorial-od</a>). 
But I found something wrong in the process of uploading. 
For example, there is a picture that has 3 persons in this picture. So I tag 3 same class “person” in this picture. But after uploading, I just found 1 ""person"" tagged in this picture at the custom vision website. 
But the other class is fine, such as can also have ""person"", ""car"", and ""scooter"" at this picture. It looks like that can only have single same class at the picture. </p>

<p>I tried to use python SDK (see that: <a href=""https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/python-tutorial-od"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/python-tutorial-od</a>) to upload my picture and tag information.</p>

<pre><code>A0_tag = trainer.create_tag(project.id, ""A0"")
A1_tag = trainer.create_tag(project.id, ""A1"")
A2_tag = trainer.create_tag(project.id, ""A2"")

A0_image_regions={
""0001.jpg"":[0.432291667,0.28125,0.080729167,0.09765625],
""0001.jpg"":[0.34765625,0.385742188,0.131510417,0.135742188],
""0001.jpg"":[0.479166667,0.385742188,0.130208333,0.135742188],
""0003.jpg"":[0.19921875,0.158203125,0.083333333,0.099609375]
}
</code></pre>

<p>The above code can see that I uploaded three ""A0"" class in 0001.jpg. But in the GUI interface on the website, I can only see that one ""A0"" class exists above 0001.jpg finally. Is there anything solution that can solve this problem?</p>",,3,0,,2019-03-25 02:59:26.563 UTC,,2019-04-15 09:17:25.820 UTC,,,,,11252652,1,1,python|azure|microsoft-custom-vision,46
google cloud vision image properties,55024058,google cloud vision image properties,"<p>I am developing an application that uses the <strong>Google vision API</strong> and I have a question about the colors properties.</p>

<p>Is the color shown in the properties with the highest percentage is the dominant color? And how that works ? </p>

<p>Because the color with highest percentage is not accurate.</p>",,1,0,,2019-03-06 13:18:17.357 UTC,,2019-03-09 20:13:15.780 UTC,2019-03-07 01:28:46.307 UTC,,322020,,11159916,1,0,colors|google-api|google-cloud-vision|google-vision,43
Using ARCore and Vision SDK together Android,53799283,Using ARCore and Vision SDK together Android,<p>Working on an idea of detecting the text using the google vision sdk and then placing the corresponding ar node on the screen. This detection and placing ar node has to be on same activity and not two different activities. Is it possible to combine both (arcore and Vision) on same activity.</p>,,0,0,,2018-12-16 03:52:42.230 UTC,,2018-12-16 03:52:42.230 UTC,,,,,9654684,1,1,android|arcore|google-vision|text-recognition|android-augmented-reality,40
Can I use Google API without Google Compute Engine? (Cloud SDK),44961470,Can I use Google API without Google Compute Engine? (Cloud SDK),"<p><a href=""https://www.youtube.com/watch?v=chk2rRjSn5o"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=chk2rRjSn5o</a>
<a href=""https://www.youtube.com/watch?v=nMY0qDg16y4&amp;t=491s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=nMY0qDg16y4&amp;t=491s</a></p>

<p>These series of videos use Google Compute Engine to access Google Cloud vision API.</p>

<p>But, I only want to use Google cloud vision API on my local computer.</p>

<p>I found Cloud SDK is an option but I don't understand how it works.
Does it link my computer to Google Compute Engine?
Then, do I still have to pay for the Engine?</p>

<p>To Sum up the question.</p>

<p>Is there any way that I only pay for Google Cloud Vision API without paying for Google Compute Engine?</p>",44961911,1,0,,2017-07-07 02:06:20.180 UTC,,2017-07-07 05:38:08.747 UTC,,,,,8257461,1,0,google-api|google-compute-engine|google-cloud-vision,331
"Given two points, find if a third point is in the above or below the formed line",55599305,"Given two points, find if a third point is in the above or below the formed line","<p>I have consumed the google cloud vision api to recognize a document with a table, but sometimes the image will be a little rotated, im triyng to get the value using the <code>boundingPoly</code> of the key i want, but how do i get it if it's not on the same <code>'y'</code>.</p>

<p>I was thinking of making a 'line' above and below the <code>boundingBox</code> and finding if the point is between that, but i dont know how to do it.</p>

<pre class=""lang-php prettyprint-override""><code>[18] =&gt; Array
(
  [description] =&gt; Date
  [boundingPoly] =&gt; Array
  (
    [vertices] =&gt; Array
    (
      [0] =&gt; Array
      (
        [x] =&gt; 698
        [y] =&gt; 289
      )
      [1] =&gt; Array
      (
        [x] =&gt; 729
        [y] =&gt; 289
      )
      [2] =&gt; Array
      (
        [x] =&gt; 729
        [y] =&gt; 301
      )
      [3] =&gt; Array
      (
        [x] =&gt; 698
        [y] =&gt; 301
      )
    )
  )
)
</code></pre>

<pre class=""lang-php prettyprint-override""><code>[66] =&gt; Array
(
  [description] =&gt; 25/03/2019
  [boundingPoly] =&gt; Array
  (
    [vertices] =&gt; Array
    (
      [0] =&gt; Array
      (
        [x] =&gt; 1007
        [y] =&gt; 290
      )
      [1] =&gt; Array
      (
        [x] =&gt; 1131
        [y] =&gt; 290
      )
      [2] =&gt; Array
      (
        [x] =&gt; 1131
        [y] =&gt; 307
      )
      [3] =&gt; Array
      (
        [x] =&gt; 1007
        [y] =&gt; 307
      )
    )
  )
)
</code></pre>",55600652,1,1,,2019-04-09 18:41:42.830 UTC,,2019-04-10 20:57:56.660 UTC,2019-04-10 20:57:56.660 UTC,,7195491,,7195491,1,1,php|math|vector|google-cloud-vision,42
AWS Rekognition PHP SDK gives invalid image encoding error,41166264,AWS Rekognition PHP SDK gives invalid image encoding error,"<p>I am using the PHP SDK to upload a local file (not S3) to be parsed in AWS Rekognition. However, the image blob will not work and I get the message: <code>InvalidImageFormatException: ""Invalid image encoding""</code>.</p>

<p>I've tried multiple images (<a href=""http://docs.aws.amazon.com/fr_fr/rekognition/latest/dg/API_DetectLabels.html"" rel=""nofollow noreferrer"">the docs say JPEGs and PNGs are accepted</a>), but none work.</p>

<p>My code is:</p>

<pre><code>$client = new RekognitionClient($credentials);

$im = file_get_contents('/app/image1.png');
$imdata = base64_encode($im);

$result = $client-&gt;detectLabels(
    [
       'Image' =&gt; [
          'Bytes' =&gt; $imdata,
       ]
    ]
);
</code></pre>

<p>Am I encoding it correctly? The <a href=""http://docs.aws.amazon.com/fr_fr/rekognition/latest/dg/API_Image.html"" rel=""nofollow noreferrer"">docs</a> are quite vague.</p>

<p>I've found SO questions about 'No Image Content', but none about invalid format.</p>

<p>Any ideas? Thanks!</p>",41167237,3,6,,2016-12-15 14:12:11.830 UTC,1,2017-06-22 07:51:50.110 UTC,2016-12-15 14:21:49.980 UTC,,1008563,,1008563,1,1,php|amazon-web-services|amazon-rekognition,1536
How to implement Vision API with 'Service account' in the iOS app?,53931530,How to implement Vision API with 'Service account' in the iOS app?,"<p>I'm trying to implement Google Vision API for the my app via REST. 
<a href=""https://cloud.google.com/vision/docs/pdf"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/pdf</a></p>

<p>Is there any examples or any suggestions how to do this?   </p>

<p>Documentation says that they require service account token but can't find any examples how to get service account token from iOS app side.
I've tried via GTMAppAuth but getting 403 error</p>

<p>I was able to generate this token from my mac machine and all worked, but token has limited life time and after 3-4 hours it expiries </p>",53955055,2,1,,2018-12-26 11:34:44.610 UTC,,2019-01-17 13:52:15.680 UTC,2018-12-26 11:41:00.370 UTC,,10834878,,10834878,1,1,ios|objective-c|iphone|swift|google-cloud-vision,131
Is there a way to mantain the position of ScrollView in react-native when using conditional rendering?,55447296,Is there a way to mantain the position of ScrollView in react-native when using conditional rendering?,"<p>Im using react-native-camera with a group of buttons that take pictures, Since I want to fill my app with more content in one single screen then Im using a ScrollView component. Whenever I press a button to take a picture I render the camera component, but when going back to my main view the ScrollView resets its position. For this Im using conditional rendering. I render the camera or the main view with content. Is there an easy or proper way to achieve this?</p>

<p>So, I know there is a way to scroll to a defined position with scrollview including animation, but that might work with listeners for each button that I have for the camera. I still dont know whats the best option for this.</p>

<pre><code>import React, { Component } from 'react';
import { StyleSheet, Text, TouchableOpacity, View, Button, ScrollView } from 'react-native';
import { RNCamera } from 'react-native-camera';

export default class BadInstagramCloneApp extends Component {
  constructor(props){
    super(props);
    this.state = {
      pictureType: null,
      isVisible: false,
      value1: null,
      value2: null
    }
  }

  render() {
    return (
        &lt;View style={styles.subcontainer}&gt;
          {this.state.isVisible === true
              ?
                &lt;View style={styles.container}&gt;
                  &lt;RNCamera
                      ref={ref =&gt; {
                        this.camera = ref;
                      }}
                      style={styles.preview}
                      type={RNCamera.Constants.Type.back}
                      flashMode={RNCamera.Constants.FlashMode.on}
                      permissionDialogTitle={'Permission to use camera'}
                      permissionDialogMessage={'We need your permission to use your camera phone'}
                      onGoogleVisionBarcodesDetected={({ barcodes }) =&gt; {
                        console.log(barcodes);
                      }}
                  /&gt;
                  &lt;View style={{ flex: 0, flexDirection: 'row', justifyContent: 'center' }}&gt;
                    &lt;TouchableOpacity onPress={this.takePicture.bind(this)} style={styles.capture}&gt;
                      &lt;Text style={{ fontSize: 14 }}&gt; SNAP &lt;/Text&gt;
                    &lt;/TouchableOpacity&gt;
                  &lt;/View&gt;
                &lt;/View&gt;
              :
                &lt;ScrollView&gt;
                  &lt;Button title='PHOTO 1' onPress={() =&gt; this.initTakingPicture(""A"")}/&gt;
                  &lt;Text&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam enim ex,
                    vehicula quis rhoncus et, tristique vitae est. Nullam nec odio est.
                    Nunc diam dolor, sagittis sed scelerisque nec, fringilla at tortor.
                    Donec vitae nibh risus. Integer et cursus ante.
                    Aliquam sodales elementum nisl, a bibendum ipsum sollicitudin eget.
                    Praesent non molestie augue. Curabitur at dui nunc.
                    Nulla facilisi. Vestibulum ante ipsum primis in faucibus orci
                    luctus et ultrices posuere cubilia Curae; Donec sodales lacus et ex
                    posuere elementum. Pellentesque egestas eros ut elit viverra, maximus volutpat diam
                    rhoncus. Proin sed lacinia quam. Nunc a leo ullamcorper, accumsan lectus vitae,
                    tristique mauris. Duis non eros quis nisi efficitur laoreet non sit amet neque.
                    Aenean egestas, urna eget ultricies pretium, ligula ex euismod libero, ac dignissim sem velit vitae ex. Proin consequat quam sed tellus ornare, feugiat porta sapien lacinia. Integer scelerisque auctor lorem mattis lobortis. Sed consequat at nibh vel consequat. Ut ullamcorper aliquet commodo. Nam elementum sed elit ut aliquet. Proin urna est, ullamcorper in elit ut, tristique eleifend nisl.

                    Nulla facilisi. Duis in mollis urna. Cras lacus lectus, vulputate ut eleifend at, egestas eu arcu. Cras ornare nibh a euismod vestibulum. Vivamus facilisis sem non est dictum, ac porta massa venenatis. Duis non dolor fringilla, fringilla dui quis, feugiat arcu. Nullam in turpis id augue consectetur volutpat et eget arcu. Praesent tincidunt sit amet ligula ac bibendum.

                    Ut mollis vitae ex sed pellentesque. In et viverra leo, eu pulvinar velit. Nunc a maximus sem. Nunc venenatis turpis eu accumsan rutrum. Proin lacinia velit et ex venenatis, sed convallis nisi sodales. Aenean placerat dapibus ultrices. Aenean et interdum mauris. Etiam quis ante tincidunt, dapibus orci in, finibus velit. Cras lorem nibh, commodo at posuere ac, porta sodales massa. Nulla mollis cursus eros, a rhoncus magna posuere at. Fusce orci augue, sodales eget ligula at, lacinia vulputate lectus.

                    Cras rhoncus augue sed eleifend sagittis. Proin fermentum ut ligula eu faucibus. Sed ullamcorper urna lacus, eget venenatis felis aliquet at. Ut orci turpis, porttitor tempus sem quis, fringilla porta ante. Mauris bibendum enim purus, et congue ipsum cursus quis. Nunc quis aliquam erat. Phasellus id turpis at dui iaculis laoreet. Quisque sed tincidunt lacus. Ut efficitur, sapien id lacinia congue, lorem tortor dictum magna, ac eleifend lectus ligula non diam. Nulla enim orci, faucibus et sagittis a, mattis nec felis. Nunc ex mauris, ornare eget tellus vitae, sollicitudin fermentum sapien. Donec mollis nec nunc laoreet ultricies. Suspendisse imperdiet quam non molestie pellentesque. Maecenas facilisis urna eget tortor viverra cursus. Pellentesque lacinia lacinia turpis, eget lobortis sapien.
                  &lt;/Text&gt;
                  &lt;Button title='PHOTO 2' onPress={() =&gt; this.initTakingPicture(""B"")}/&gt;
                  &lt;Button title='SHOW RESULTS' onPress={this.showResults}/&gt;
                &lt;/ScrollView&gt;
          }
        &lt;/View&gt;
    );
  }

  showResults = () =&gt; {
    console.log('VALOR1: ' + this.state.value1);
    console.log('VALOR2: ' + this.state.value2);
  }

  takePicture = async function() {
    if (this.camera) {
      const options = { quality: 0.5, base64: true };
      const data = await this.camera.takePictureAsync(options);
      console.log(data.uri);
      let fieldToSave = ""value1"" // Fallback
      if (this.state.pictureType === ""A"") {
        // Operation you need to do for pictureType A
        fieldToSave = ""value1""
        //FIELTOSAVE DEBE OPTIMIZARSE Y GUARDAR SU VALOR EN LOCAL STATE
        //NO HACE FALTA CAMBIAR PARAMETROSS A ENVIAR PORQUE TOMAN EL MISMO NOMBRE
      } else if (this.state.pictureType === ""B"") {
        // Operation you need to do for pictureType B
        fieldToSave = ""value2""
      }

      this.setState({
        isVisible: false,
        pictureType: null,
        [fieldToSave]: data.uri
      });
    }
  };

  initTakingPicture = (pictureType) =&gt; {
    this.setState({
      isVisible: true,
      pictureType: pictureType
    })
  }
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    flexDirection: 'column',
    backgroundColor: 'black'
  },
  subcontainer: {
    flex: 1,
    flexDirection: 'column',
  },
  preview: {
    flex: 1,
    justifyContent: 'flex-end',
    alignItems: 'center',
  },
  capture: {
    flex: 0,
    backgroundColor: '#fff',
    borderRadius: 5,
    padding: 15,
    paddingHorizontal: 20,
    alignSelf: 'center',
    margin: 20,
  },
});

</code></pre>

<p>Just in case, I used some Lorem Ipsum to fill all the content and make the ScrollView scroll to see the problem itself.</p>

<p>I expect the ScrollView itself to maintain its position, not resetting after re-render</p>",55604825,1,1,,2019-04-01 02:10:33.690 UTC,,2019-04-10 04:38:38.487 UTC,,,,,5921192,1,0,javascript|react-native|react-native-camera,82
Receiving invalid image format error with NodeJS Rekognition api call,41127382,Receiving invalid image format error with NodeJS Rekognition api call,"<p>I'm trying to make a call to the Amazon Rekognition service with NodeJS. The call is going through but I receive an <code>InvalidImageFormatException</code> error in which it says:</p>

<blockquote>
  <p>Invalid Input, input image shouldn't be empty.</p>
</blockquote>

<p>I'm basing my code off an S3 example:</p>

<pre><code>var AWS = require('aws-sdk');
var rekognition = new AWS.Rekognition({region: 'us-east-1'});

//Create a bucket and upload something into it

var params = {
    Image: {
        S3Object: {
            Bucket: ""MY-BUCKET-NAME"",
            Name: ""coffee.jpg""
        }
    },
    MaxLabels: 10,
    MinConfidence: 70.0
};

var request = rekognition.detectLabels(params, function(err, data) {
    if(err){
        console.log(err, err.stack); // an error occured
    } 
    else{
        console.log(data);               // successful response
    }
});
</code></pre>

<p>The documentation states that the service only accepts PNG or JPEG images but I can't figure out what is going on.</p>",,0,2,,2016-12-13 17:42:56.933 UTC,,2016-12-13 18:43:22.500 UTC,2016-12-13 18:43:22.500 UTC,,3473158,,1387593,1,0,node.js|amazon-s3,300
Google vision API algorithm,43041575,Google vision API algorithm,"<p>I was wondering how the google cloud vision works behind the scenes. What kind of algorithms are used for processing the images? Is there some texts explaining this? </p>

<p>Thanks to all</p>",,2,0,,2017-03-27 08:37:24.900 UTC,,2018-01-16 10:38:04.907 UTC,,,,,7772899,1,0,google-cloud-vision,572
Google Cloud Vision API giving inaccurate text detection result,43687962,Google Cloud Vision API giving inaccurate text detection result,"<p>I am using Google Cloud Vision API for OCR purpose. I am able to connect to the API and getting JSON result back as expected. What baffles me is that while the <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/</a> url correctly detects the text in the image, the API call often returns inaccurate text data for the same image. Pl. let me know what could be the case. Sample code is attached.</p>

<pre><code>        String url = ""https://vision.googleapis.com/v1/images:annotate?key=mykey"";
        URL obj = new URL(url);
        HttpsURLConnection con = (HttpsURLConnection) obj.openConnection();
        BufferedImage img = ImageIO.read(new File(""F://image.jpg""));
        ByteArrayOutputStream baos=new ByteArrayOutputStream();
        String fileext = ""jpg"";
        ImageIO.write(img, fileext, baos );
        baos.flush();
        byte[] imageInByte=baos.toByteArray();
        baos.close();
        String imgstr =  java.util.Base64.getEncoder().encodeToString(imageInByte);
        //add reuqest header
        con.setRequestMethod(""POST"");
        con.setRequestProperty(""Content-Type"", ""application/json; charset=UTF-8"");
        con.setRequestProperty(""Accept-Language"", ""en-US,en;q=0.5"");
</code></pre>",,1,2,,2017-04-28 19:52:47.897 UTC,,2017-06-27 20:53:39.227 UTC,,,,,2784296,1,0,google-cloud-vision,660
"Google Vision, use SceneKit to create face masks",48286385,"Google Vision, use SceneKit to create face masks","<p>I am developing an iOS application in Objective-C, I need to apply on a detected face( using google vision's system) a .scn file. After set up a SceneView with a 3D object stored in a Node... I need to a apply it to a face: </p>

<pre><code>  SCNVector3 currentPosition = [self.currentView unprojectPoint:(SCNVector3Make(newLeftEyePosition.x, newLeftEyePosition.y , ?))];
  self.ship.position = currentPosition;
</code></pre>

<p>The problem is the '?', because the value I put as parameter, should be calculated using the distance between the user's face and the device... but how should I get it ? an is what I am trying to do correct, or there is anything wrong in the reasoning? </p>",,0,2,,2018-01-16 17:05:25.087 UTC,,2018-01-17 11:08:04.723 UTC,2018-01-17 11:08:04.723 UTC,,7426374,,8863432,1,0,ios|objective-c|scenekit|scnvector3,127
Extracting JSON data Xamarin,48733647,Extracting JSON data Xamarin,"<p>I am working a Xamarin.Forms App that uses the Azure Face API. With this API you retrieve a JSON response (See Below). </p>

<p>I want to extract the gender of the person in the image but am having trouble with it as I am  very new to this.</p>

<p>I extract the full JSON response into a string but I would like to be able to extract data such as the 'Gender' or the 'Age' of the person in the image.</p>

<pre><code>[{""faceId"":""9448dfe4-afb6-4557-94fe-010fc439ff36"",""faceRectangle"":{""top"":635,""left"":639,""width"":789,""height"":789},""faceAttributes"":{""smile"":0.187,""headPose"":{""pitch"":0.0,""roll"":-1.6,""yaw"":-7.9},""gender"":""male"",""age"":34.6,""facialHair"":{""moustache"":0.5,""beard"":0.6,""sideburns"":0.6},""glasses"":""NoGlasses"",""emotion"":{""anger"":0.0,""contempt"":0.69,""disgust"":0.0,""fear"":0.0,""happiness"":0.187,""neutral"":0.12,""sadness"":0.002,""surprise"":0.0},""blur"":{""blurLevel"":""low"",""value"":0.15},""exposure"":{""exposureLevel"":""overExposure"",""value"":0.85},""noise"":{""noiseLevel"":""medium"",""value"":0.42},""makeup"":{""eyeMakeup"":false,""lipMakeup"":false},""accessories"":[],""occlusion"":{""foreheadOccluded"":false,""eyeOccluded"":false,""mouthOccluded"":false},""hair"":{""bald"":0.02,""invisible"":false,""hairColor"":[{""color"":""brown"",""confidence"":1.0},{""color"":""black"",""confidence"":0.95},{""color"":""other"",""confidence"":0.22},{""color"":""blond"",""confidence"":0.11},{""color"":""gray"",""confidence"":0.05},{""color"":""red"",""confidence"":0.04}]}}}]
</code></pre>

<p>This is how I set the JSON data to a string.</p>

<pre><code> string contentString = await response.Content.ReadAsStringAsync();
</code></pre>",,2,0,,2018-02-11 16:23:57.840 UTC,,2018-02-11 16:35:32.377 UTC,2018-02-11 16:35:10.030 UTC,,1163423,,6221300,1,2,c#|json|azure|xamarin|xamarin.forms,97
Insert multi nested object with lists and other properties to database,45775358,Insert multi nested object with lists and other properties to database,"<p>I create a web service and I have to insert multi nested object to the database. Can I insert all the objects at the same time or should I add each object individually one by one? </p>

<p>It seems it's not optimal way. I implemented Onion Architecture in my solution and I add each object by other service. Is this a correct way? The object which I want to insert is AwsRekognitionResponse. I would like to know which way is the most optimal and correct. </p>

<pre><code>public class ResponseMetadata
{
    public string RequestId { get; set; }
    public int RetryAttempts { get; set; }
    public int HTTPStatusCode { get; set; }
}

public class Emotion
{
    public decimal Confidence { get; set; }
    public string Type { get; set; }
}

public class Landmark
{
    public decimal X { get; set; }
    public decimal Y { get; set; }
    public string Type { get; set; }
}

public class MouthOpen
{
    public decimal Confidence { get; set; }
    public bool Value { get; set; }
}

public class Sunglasses
{
    public decimal Confidence { get; set; }
    public bool Value { get; set; }
}

public class BoundingBox
{
    public decimal Height { get; set; }
    public decimal Width { get; set; }
    public decimal Top { get; set; }
    public decimal Left { get; set; }
}

public class FaceDetail
{
    public Emotion[] Emotions { get; set; }
    public Landmark[] Landmarks { get; set; }
    public MouthOpen MouthOpen { get; set; }
    public Sunglasses Sunglasses { get; set; }
    public BoundingBox BoundingBox { get; set; }
    public decimal Confidence { get; set; }
}

public class AwsRekognitionResponse
{
    public string OrientationCorrection { get; set; }
    public ResponseMetadata ResponseMetadata { get; set; }
    public List&lt;FaceDetail&gt; FaceDetails { get; set; }
}
</code></pre>",,0,1,,2017-08-19 19:05:53.333 UTC,,2017-08-19 21:13:37.030 UTC,2017-08-19 21:13:37.030 UTC,,1315502,,8488769,1,1,c#|asp.net|database|entity-framework,24
Is there a way to make a realtime object detection using microsoft cognitive service face api?,51543592,Is there a way to make a realtime object detection using microsoft cognitive service face api?,"<p>Im experimenting a way to make the samples of microsoft cognitive services face api for real time and i cant find a way, i have tried mixing up codes from fotoapparat so that it would make a rectangle on faces detected and then from identificationtask from microsoft cognitive example like this but it wont seem to work. </p>

<pre><code>public class RealtimeCameraActivity extends Activity {

    boolean detected;
    String mPersonGroupId;

    private final PermissionsDelegate permissionsDelegate = new PermissionsDelegate(this);
    private boolean hasCameraPermission;
    private CameraView cameraView;
    private RectanglesView rectanglesView;

    private FotoapparatSwitcher fotoapparatSwitcher;
    private Fotoapparat frontFotoapparat;
    private Fotoapparat backFotoapparat;
    FaceListAdapter mFaceListAdapter;
    PersonGroupListAdapter mPersonGroupListAdapter;



    private class IdentificationTask extends AsyncTask&lt;UUID, String, IdentifyResult[]&gt; {
        private boolean mSucceed = true;

        String mPersonGroupId;
        IdentificationTask(String personGroupId) {
            this.mPersonGroupId = personGroupId;
        }

        @Override
        protected IdentifyResult[] doInBackground(UUID... params) {


            // Get an instance of face service client to detect faces in image.
            FaceServiceClient faceServiceClient = SampleApp.getFaceServiceClient();
            try{
                publishProgress(""Getting person group status..."");

                TrainingStatus trainingStatus = faceServiceClient.getLargePersonGroupTrainingStatus(
                        this.mPersonGroupId);     /* personGroupId */
                if (trainingStatus.status != TrainingStatus.Status.Succeeded) {
        //            publishProgress(""Person group training status is "" + trainingStatus.status);
                    mSucceed = false;
                    return null;
                }

       //         publishProgress(""Identifying..."");

                // Start identification.
                return faceServiceClient.identityInLargePersonGroup(
                        this.mPersonGroupId,   /* personGroupId */
                        params,                  /* faceIds */
                        1);  /* maxNumOfCandidatesReturned */
            }  catch (Exception e) {
                mSucceed = false;
       //         publishProgress(e.getMessage());
      //          addLog(e.getMessage());
                return null;
            }
        }

        @Override
        protected void onPreExecute() {
            ;
        }

        @Override
        protected void onProgressUpdate(String... values) {
            // Show the status of background detection task on screen.a

        }

        @Override
        protected void onPostExecute(IdentifyResult[] result) {
            // Show the result on screen when detection is done.
            setUiAfterIdentification(result, mSucceed);
        }
    }



    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_realtime);

        cameraView = (CameraView) findViewById(R.id.cameraView);
        rectanglesView = (RectanglesView) findViewById(R.id.rectanglesView);
        hasCameraPermission = permissionsDelegate.hasCameraPermission();

        if (hasCameraPermission) {
            cameraView.setVisibility(View.VISIBLE);
        } else {
            permissionsDelegate.requestCameraPermission();
        }

        frontFotoapparat = createFotoapparat(LensPosition.FRONT);
        backFotoapparat = createFotoapparat(LensPosition.BACK);
        fotoapparatSwitcher = FotoapparatSwitcher.withDefault(backFotoapparat);

        View switchCameraButton = findViewById(R.id.switchCamera);
        switchCameraButton.setVisibility(
                canSwitchCameras()
                        ? View.VISIBLE
                        : View.GONE
        );
        switchCameraButton.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                switchCamera();
            }
        });


    }

    // Show the result on screen when detection is done.
    private void setUiAfterIdentification(IdentifyResult[] result, boolean succeed) {


        if (succeed) {
            // Set the information about the detection result.

            if (result != null) {

                //android things
                //           try {
                //             Led = RainbowHat.openLedBlue();
                //             Led.setDirection(Gpio.DIRECTION_OUT_INITIALLY_LOW);

                //        } catch (IOException e) {
                //            e.printStackTrace();
                //        }

                mFaceListAdapter.setIdentificationResult(result);





                //          if (logString.contains(""Unknown Person"")) {
                //             setLedValue(false);
                //         } else {
                //            setLedValue(true);
                //        }




                // Show the detailed list of detected faces.
                ListView listView = (ListView) findViewById(R.id.list_identified_faces);
                listView.setAdapter(mFaceListAdapter);
            }
        }

    }







    private boolean canSwitchCameras() {
        return frontFotoapparat.isAvailable() == backFotoapparat.isAvailable();
    }

    private Fotoapparat createFotoapparat(LensPosition position) {


        return Fotoapparat
                .with(this)
                .into(cameraView)
                .lensPosition(lensPosition(position))
                .frameProcessor(
                        FaceDetectorProcessor.with(this)
                                .listener(new FaceDetectorProcessor.OnFacesDetectedListener() {
                                    @Override
                                    public void onFacesDetected(List&lt;Rectangle&gt; faces) {
                                        Log.d(""&amp;&amp;&amp;"", ""Detected faces: "" + faces.size());
                                        rectanglesView.setRectangles(facess);


                                        // Start a background task to identify faces in the image.
                                       List&lt;UUID&gt; faceIds = new ArrayList&lt;&gt;();
                                        for (Face face:  mFaceListAdapter.faces) {
                                           faceIds.add(face.faceId);
                                        }


                                        new RealtimeCameraActivity.IdentificationTask(mPersonGroupId).execute(
                                               faceIds.toArray(new UUID[faceIds.size()]));
                                    }
                                })
                                .build()
                )
                .logger(loggers(
                        logcat(),
                        fileLogger(this)
                ))
                .build();
    }

    void setPersonGroupSelected(int position) {
        if (position &gt; 0) {
            String personGroupIdSelected = mPersonGroupListAdapter.personGroupIdList.get(position);
            mPersonGroupListAdapter.personGroupIdList.set(
                    position, mPersonGroupListAdapter.personGroupIdList.get(0));
            mPersonGroupListAdapter.personGroupIdList.set(0, personGroupIdSelected);
            setPersonGroupSelected(0);
        } else {
            mPersonGroupId = mPersonGroupListAdapter.personGroupIdList.get(0);

        }
    }

    // Background task of face detection.
    private class DetectionTask extends AsyncTask&lt;InputStream, String, Face[]&gt; {
        @Override
        protected Face[] doInBackground(InputStream... params) {
            // Get an instance of face service client to detect faces in image.
            FaceServiceClient faceServiceClient = SampleApp.getFaceServiceClient();
            try{

                // Start detection.
                return faceServiceClient.detect(
                        params[0],  /* Input stream of image to detect */
                        true,       /* Whether to return face ID */
                        false,       /* Whether to return face landmarks */
                        /* Which face attributes to analyze, currently we support:
                           age,gender,headPose,smile,facialHair */
                        null);
            }  catch (Exception e) {
                return null;
            }
        }

        @Override
        protected void onPreExecute() {
        }

        @Override
        protected void onProgressUpdate(String... values) {
            // Show the status of background detection task on screen.
        }

        @Override
        protected void onPostExecute(Face[] result) {

            if (result != null) {
                // Set the adapter of the ListView which contains the details of detected faces.
                mFaceListAdapter = new RealtimeCameraActivity.FaceListAdapter(result);
                ListView listView = (ListView) findViewById(R.id.list_identified_faces);
                listView.setAdapter(mFaceListAdapter);

                if (result.length == 0) {
                    detected = false;
                } else {
                    detected = true;
                }
            } else {
                detected = false;
            }

        }
    }

    private void switchCamera() {
        if (fotoapparatSwitcher.getCurrentFotoapparat() == frontFotoapparat) {
            fotoapparatSwitcher.switchTo(backFotoapparat);
        } else {
            fotoapparatSwitcher.switchTo(frontFotoapparat);
        }
    }


    @Override
    protected void onStart() {
        super.onStart();
        if (hasCameraPermission) {
            fotoapparatSwitcher.start();
        }
    }



    @Override
    protected void onStop() {
        super.onStop();
        if (hasCameraPermission) {
            fotoapparatSwitcher.stop();
        }
    }

    @Override
    public void onRequestPermissionsResult(int requestCode,
                                           @NonNull String[] permissions,
                                           @NonNull int[] grantResults) {
        super.onRequestPermissionsResult(requestCode, permissions, grantResults);
        if (permissionsDelegate.resultGranted(requestCode, permissions, grantResults)) {
            fotoapparatSwitcher.start();
            cameraView.setVisibility(View.VISIBLE);
        }
    }

    private class FaceListAdapter extends BaseAdapter {
        List&lt;Face&gt; faces;

        List&lt;IdentifyResult&gt; mIdentifyResults;



        // Initialize with detection result.
        FaceListAdapter(Face[] detectionResult) {
            faces = new ArrayList&lt;&gt;();
            mIdentifyResults = new ArrayList&lt;&gt;();

            if (detectionResult != null) {
                faces = Arrays.asList(detectionResult);

            }
        }

        public void setIdentificationResult(IdentifyResult[] identifyResults) {
            mIdentifyResults = Arrays.asList(identifyResults);
        }

        @Override
        public boolean isEnabled(int position) {
            return false;
        }

        @Override
        public int getCount() {
            return faces.size();
        }

        @Override
        public Object getItem(int position) {
            return faces.get(position);
        }

        @Override
        public long getItemId(int position) {
            return position;
        }

        @Override
        public View getView(final int position, View convertView, ViewGroup parent) {
            if (convertView == null) {
                LayoutInflater layoutInflater =
                        (LayoutInflater)getSystemService(Context.LAYOUT_INFLATER_SERVICE);
                convertView = layoutInflater.inflate(
                        R.layout.item_face_with_description, parent, false);
            }
            convertView.setId(position);


            if (mIdentifyResults.size() == faces.size()) {
                // Show the face details.
                DecimalFormat formatter = new DecimalFormat(""#0.00"");
                if (mIdentifyResults.get(position).candidates.size() &gt; 0) {

                    String personId =
                            mIdentifyResults.get(position).candidates.get(0).personId.toString();
                    String personName = StorageHelper.getPersonName(
                            personId, mPersonGroupId, RealtimeCameraActivity.this);
                    String identity = ""Person: "" + personName + ""\n""
                            + ""Confidence: "" + formatter.format(
                            mIdentifyResults.get(position).candidates.get(0).confidence);
                    ((TextView) convertView.findViewById(R.id.text_detected_face)).setText(
                            identity);

                } else {

                    ((TextView) convertView.findViewById(R.id.text_detected_face)).setText(
                            ""Unknown person"");

                }
            }

            return convertView;
        }
    }


    // The adapter of the ListView which contains the person groups.
    private class PersonGroupListAdapter extends BaseAdapter {
        List&lt;String&gt; personGroupIdList;

        // Initialize with detection result.
        PersonGroupListAdapter() {
            personGroupIdList = new ArrayList&lt;&gt;();

            Set&lt;String&gt; personGroupIds
                    = StorageHelper.getAllPersonGroupIds(RealtimeCameraActivity.this);

            for (String personGroupId: personGroupIds) {
                personGroupIdList.add(personGroupId);
                if (mPersonGroupId != null &amp;&amp; personGroupId.equals(mPersonGroupId)) {
                    personGroupIdList.set(
                            personGroupIdList.size() - 1,
                            mPersonGroupListAdapter.personGroupIdList.get(0));
                    mPersonGroupListAdapter.personGroupIdList.set(0, personGroupId);
                }
            }
        }

        @Override
        public int getCount() {
            return personGroupIdList.size();
        }

        @Override
        public Object getItem(int position) {
            return personGroupIdList.get(position);
        }

        @Override
        public long getItemId(int position) {
            return position;
        }

        @Override
        public View getView(final int position, View convertView, ViewGroup parent) {
            if (convertView == null) {
                LayoutInflater layoutInflater =
                        (LayoutInflater)getSystemService(Context.LAYOUT_INFLATER_SERVICE);
                convertView = layoutInflater.inflate(R.layout.item_person_group, parent, false);
            }
            convertView.setId(position);

            // set the text of the item
            String personGroupName = StorageHelper.getPersonGroupName(
                    personGroupIdList.get(position), RealtimeCameraActivity.this);
            int personNumberInGroup = StorageHelper.getAllPersonIds(
                    personGroupIdList.get(position), RealtimeCameraActivity.this).size();
            ((TextView)convertView.findViewById(R.id.text_person_group)).setText(
                    String.format(
                            ""%s (Person count: %d)"",
                            personGroupName,
                            personNumberInGroup));

            if (position == 0) {
                ((TextView)convertView.findViewById(R.id.text_person_group)).setTextColor(
                        Color.parseColor(""#3399FF""));
            }

            return convertView;
        }
    }
</code></pre>

<p>how will i do this?</p>",,0,0,,2018-07-26 16:30:02.220 UTC,,2018-07-26 16:30:02.220 UTC,,,,,9864668,1,0,android|real-time|object-detection|microsoft-cognitive|face-api,74
Face rekognition in streaming video returns just one frame for each second,50894208,Face rekognition in streaming video returns just one frame for each second,"<p>I'm trying to run face recognition on live stream via amazon rekogntion and kinesis services. I've configured kinesis video stream for input video, stream processor for recognition and kinesis data stream to get results from the stream processor. All is working good, but I'm getting just one frame for each second in the stream. </p>

<p>I calculate frame timestamp accordignly:
<a href=""https://docs.aws.amazon.com/rekognition/latest/dg/streaming-video-kinesis-output.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/rekognition/latest/dg/streaming-video-kinesis-output.html</a>
by adding the <code>ProducerTimestamp</code> and <code>FrameOffsetInSeconds</code> field values together and get timestamps with defference 1 second. </p>

<p>For instance:</p>

<pre><code>1528993313.0310001
1528993314.0310001
1528993314.0310001
</code></pre>

<p>I use demo app for video streaming from Java Producer SDK
<a href=""https://github.com/awslabs/amazon-kinesis-video-streams-producer-sdk-java.git"" rel=""nofollow noreferrer"">https://github.com/awslabs/amazon-kinesis-video-streams-producer-sdk-java.git</a></p>

<p>Total duration of data from stream processor is correct and equals the video file duration, but as I said I get just on frame for each second.</p>",,1,0,,2018-06-17 06:19:27.430 UTC,,2019-05-28 01:46:11.490 UTC,2018-06-17 06:44:09.857 UTC,,13302,,9859548,1,0,amazon-web-services|amazon-rekognition,108
Google Vision API - tatusCode.RESOURCE_EXHAUSTED,43746016,Google Vision API - tatusCode.RESOURCE_EXHAUSTED,"<p>I am new to the Google Vision API and I would like to conduct a label detection of approx. 10 images and I would like to run the vision quickstart.py file. However when I do this with only 3 images then it is successful. With more than 3 images I am getting the error message below. I know that I would need to change something at my setup, but I do not know what I should change.</p>

<p>Here is my error message: </p>

<pre><code>google.gax.errors.RetryError: GaxError(Exception occurred in retry method 
that was not classified as transient, caused by &lt;_Rendezvous of RPC that 
terminated with (StatusCode.RESOURCE_EXHAUSTED, Insufficient tokens for 
quota 'DefaultGroup' and limit 'USER-100s' of service 
'vision.googleapis.com' for consumer 'project_number: XXX'.)&gt;)
</code></pre>

<p>Does anybody know what I need to do?</p>

<p>Any help would be much appreciated</p>

<p>Cheers,
Andi</p>",,1,0,,2017-05-02 19:45:13.880 UTC,,2017-05-11 08:26:20.613 UTC,,,,,1684315,1,3,gcloud,503
invalid-api-key in Watson Visual Recognition API,38580989,invalid-api-key in Watson Visual Recognition API,"<p>I would like to classify images by calling Watson Visual Recognition APIs.</p>

<p>So, I set my end point as </p>

<pre><code>(a) and sent a request message as 
(b) However, I received an error message as 
(c) How can I resolve this issue?
</code></pre>

<blockquote>
  <p>FYI. - The API Key was generated on IBM Bluemix.</p>
</blockquote>

<p><strong>(a) End Point :</strong> ""<a href=""https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classify"" rel=""nofollow"">https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classify</a>"";</p>

<p><strong>(b) Captured Request Message</strong></p>

<pre><code>reqMessage: {""api_key"":""XXXX"" ,""url"":""http://cfile5.uf.tistory.com/image/1876DE4C4F29F9F13BB066"",
""version"":""2016-05-20"", ""classifirer_ids"":default"",""owners"":""IBM""}
</code></pre>

<p><strong>(c) Captured Response Message</strong></p>

<pre><code>resMessage: {""status"": ""ERROR"", ""statusInfo"": ""invalid-api-key""}
</code></pre>",,1,4,,2016-07-26 04:25:01.487 UTC,,2016-08-02 18:19:11.007 UTC,2016-08-02 18:17:24.320 UTC,,456564,,6637830,1,1,api-key|ibm-watson|visual-recognition,543
Bundler could not find compatible versions for gem even if I change versions,50381609,Bundler could not find compatible versions for gem even if I change versions,"<p>I am trying to update to the latest google vision, and so I add in Gemfile:</p>

<pre><code>gem 'google-cloud-vision', '~&gt; 0.28.0'
</code></pre>

<p>But when I run bundle install, I get the following error:</p>

<blockquote>
  <p>Bundler could not find compatible versions for gem ""faraday"":   In
  Gemfile:
      google-cloud-vision (~> 0.28.0) ruby depends on
        google-cloud-core (~> 1.2) ruby depends on
          google-cloud-env (~> 1.0) ruby depends on
            faraday (~> 0.11) ruby</p>

<pre><code>forecast_io (&gt;= 0) ruby depends on
  faraday (0.9.2)
</code></pre>
</blockquote>

<p>I tried using the latest version of forecast too:</p>

<pre><code>gem 'forecast_io', '~&gt; 2.0', '&gt;= 2.0.2'
</code></pre>

<p>I understand that two different gems require two different versions of faraday. But isn't bundler supposed to resolve this?</p>",,1,0,,2018-05-17 00:11:17.353 UTC,,2018-05-17 10:23:30.823 UTC,,,,,4501354,1,0,ruby,44
Python using local files rather than a url argument,56012355,Python using local files rather than a url argument,"<p>I'm playing around with the google vision ai to learn python and I was wondering how I could get this code to run with a local image as at the moment you have to run ""python code.py URL argument"". How could I make it load a file in a local directory?</p>

<pre><code>import argparse
import io

from google.cloud import vision
from google.cloud.vision import types


def annotate(path):
    """"""Returns web annotations given the path to an image.""""""
    client = vision.ImageAnnotatorClient()

    if path.startswith('http') or path.startswith('gs:'):
        image = types.Image()
        image.source.image_uri = path

    else:
        with io.open(path, 'rb') as image_file:
            content = image_file.read()

        image = types.Image(content=content)

    web_detection = client.web_detection(image=image).web_detection

    return web_detection


    def report(annotations):
        """"""Prints detected features in the provided web annotations.""""""
        if annotations.pages_with_matching_images:
            print('\n{} Pages with matching images retrieved'.format(
                len(annotations.pages_with_matching_images)))

        for page in annotations.pages_with_matching_images:
            print('Url   : {}'.format(page.url))

    if annotations.full_matching_images:
        print('\n{} Full Matches found: '.format(
              len(annotations.full_matching_images)))

        for image in annotations.full_matching_images:
            print('Url  : {}'.format(image.url))

    if annotations.partial_matching_images:
        print('\n{} Partial Matches found: '.format(
              len(annotations.partial_matching_images)))

        for image in annotations.partial_matching_images:
            print('Url  : {}'.format(image.url))

    if annotations.web_entities:
        print('\n{} Web entities found: '.format(
              len(annotations.web_entities)))

        for entity in annotations.web_entities:
            print('Score      : {}'.format(entity.score))
            print('Description: {}'.format(entity.description))

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawDescriptionHelpFormatter)
    path_help = str('The image to detect, can be web URI, '
                    'Google Cloud Storage, or path to local file.')
    parser.add_argument('image_url', help=path_help)
    args = parser.parse_args()

    report(annotate(args.image_url))
</code></pre>

<p>I've tried changing a lot of things but whatever happens it still asks for an argument, is this a stipulation from the google api? I thought it had local support based on the code already but I can't figure out for the life of my what the trigger is. Any time I try to run the could without a URL argument it throws this error my way. </p>

<p>"" usage: visiontest.py [-h] image_url visiontest.py: error: too few arguments"" </p>",,0,8,,2019-05-06 20:57:54.203 UTC,,2019-05-12 09:27:13.413 UTC,2019-05-12 09:27:13.413 UTC,,13302,,8909459,1,1,python|google-vision,52
How to use AWS Rekognition to detect Image Labels and Faces in Swift 3,41348880,How to use AWS Rekognition to detect Image Labels and Faces in Swift 3,"<p>So I've been trying to use the AWSRekognition SDK in order to detect faces and labels in images. However, Amazon has no Documentation on how to integrate their SDK with iOS. They have links that show how to work with Rekognition (Developer Guide) with examples only in Java and very limited.</p>

<p><a href=""http://docs.aws.amazon.com/rekognition/latest/dg/what-is.html"" rel=""nofollow noreferrer"">Amazon Rekognition Developer Guide</a></p>

<p>If you click on their ""iOS Documentation"", it takes you to the general iOS documentation page, with no signs of Rekognition in any section. </p>

<p><a href=""http://docs.aws.amazon.com/mobile/sdkforios/developerguide/"" rel=""nofollow noreferrer"">AWS iOS Developer Guide</a></p>

<p>I wanted to know if anyone knows how to integrate AWS Rekognition in <strong>Swift 3</strong>. How to Initialize it and make a request with an image, receiving a response with the labels. </p>

<p>I already downloaded the <code>AWSRekognition.framework</code> and the <code>AWSCore.framework</code> and added them into my project. Also I have imported both of them in my <code>AppDelegate.swift</code> and initialized my AWS Credentials.</p>

<pre><code>let credentialsProvider = AWSCognitoCredentialsProvider(
        regionType: AWSRegionType.usEast1,
        identityPoolId: ""us-east-1_myPoolID"")
let configuration = AWSServiceConfiguration(
        region: AWSRegionType.usEast1,
        credentialsProvider: credentialsProvider)
AWSServiceManager.default().defaultServiceConfiguration = configuration
</code></pre>

<p>Also I've tried to initialize Rekognition and build a Request:</p>

<pre><code>do {

    let rekognitionClient:AWSRekognition = AWSRekognition(forKey: ""Maybe a Key from AWS?"")

    let request: AWSRekognitionDetectLabelsRequest = try AWSRekognitionDetectLabelsRequest(dictionary: [""image"": UIImage(named:""TestImage"")!, ""maxLabels"":3, ""minConfidence"":90], error: (print(""error"")))
    rekognitionClient.detectLabels(request) { (response:AWSRekognitionDetectLabelsResponse?, error:Error?) in
        if error == nil {
            print(response!)
        }
    }

} catch {
    print(""Error"")
}
</code></pre>

<p>Thanks a lot!</p>",41381595,2,1,,2016-12-27 16:41:35.810 UTC,,2018-01-08 12:56:55.677 UTC,,,,,3116369,1,1,ios|amazon-web-services|swift3|aws-sdk|amazon-rekognition,3173
faceDetector.isOperational() is always returning false,48381832,faceDetector.isOperational() is always returning false,"<p>I am using using google vision API to detect face from bitmap. But it is always returning false. It used to work previously but not now.</p>

<p>Here the code and verisons I am using.</p>

<p><strong>build.gradle</strong></p>

<pre><code>compile 'com.google.android.gms:play-services-vision:10.2.4'
</code></pre>

<p><strong>Manifest</strong></p>

<pre><code>&lt;meta-data
        android:name=""com.google.android.gms.vision.DEPENDENCIES""
        android:value=""face"" /&gt;
</code></pre>

<p><strong>Code</strong></p>

<pre><code> FaceDetector faceDetector = new FaceDetector.Builder(context)
            .setTrackingEnabled(false)
            .setLandmarkType(FaceDetector.ALL_LANDMARKS)
            .build();

    if (faceDetector.isOperational()) {
        ...
        ...
</code></pre>

<p>But <code>faceDetector.isOperational()</code> <strong>always returning false</strong>. I checked in OPPO(5.1.1) and Moto(6.0)</p>

<p>TIA</p>",,1,0,,2018-01-22 12:39:19.460 UTC,,2018-01-22 12:58:06.480 UTC,,,,user8757853,,1,1,android|google-vision|vision-api,470
Google Cloud Vision API - send base64 encoded image in POST request,42970980,Google Cloud Vision API - send base64 encoded image in POST request,"<p>I'm making a little project using the Google Vision API. I want to detect the face of a base64 encoded image that a send to the API in a POST request. My code is based on this tutorial of Google: <a href=""https://cloud.google.com/community/tutorials/make-an-http-request-to-the-cloud-vision-api-from-java"" rel=""nofollow noreferrer"">https://cloud.google.com/community/tutorials/make-an-http-request-to-the-cloud-vision-api-from-java</a>. Apparently it is possible to send a base64 encoded image.</p>

<p>Here is my code:</p>

<pre><code>    File File = new File(args[2]);
    String ImageString;

    FileInputStream fileInputStreamReader = new FileInputStream(File);
    byte[] bytes = new byte[(int)File.length()];
    fileInputStreamReader.read(bytes);
    ImageString = Base64.getEncoder().encodeToString(bytes);

    URL serverUrl = new URL(TARGET_URL + API_KEY); //TARGET_URL = ""https://vision.googleapis.com/v1/images:annotate?""
    URLConnection urlConnection = serverUrl.openConnection();
    HttpURLConnection httpConnection = (HttpURLConnection)urlConnection;

    httpConnection.setRequestMethod(""POST"");
    httpConnection.setRequestProperty(""Content-Type"", ""application/json"");

    httpConnection.setDoOutput(true);

    BufferedWriter httpRequestBodyWriter = new BufferedWriter(new
            OutputStreamWriter(httpConnection.getOutputStream()));

    httpRequestBodyWriter.write
            (""{\""requests\"":  [{ \""features\"":  [ {\""type\"": \""FACE_DETECTION\""""
                    +""}], \""image\"": {\""content\"": \"""" + ImageString + ""\""}]}"");
    httpRequestBodyWriter.close();
</code></pre>

<p>As you see, I replaced the ""content"" field by my string. I just don't know what I am doing wrong.
Thanks in advance.  </p>",,0,2,,2017-03-23 08:42:12.677 UTC,,2017-03-27 16:36:32.967 UTC,2017-03-27 16:36:32.967 UTC,,5231007,,6177563,1,0,java|maven|http|post|google-cloud-vision,741
How do I use vision API in ionic 2?,42713068,How do I use vision API in ionic 2?,"<p>I am a novice at ionic 2. Is there any way I could use google's vision API in my program? If so, could you explain it in simple terms due to the fact that I am generally quite new to the coding scene. Thanks in advance.</p>",,0,1,,2017-03-10 07:53:39.537 UTC,,2017-03-10 08:25:21.947 UTC,2017-03-10 08:25:21.947 UTC,,3214843,,7688942,1,2,ionic2|vision,300
Boto3 InvalidParameterException,41863595,Boto3 InvalidParameterException,"<p>I have some code that calls into AWS's Rekognition service. Sometimes it throws this exception:</p>

<pre><code>An error occurred (InvalidParameterException) when calling the DetectLabels operation: Request has Invalid Parameters
</code></pre>

<p>I can't find the <code>InvalidParameterException</code> anywhere in the documentation or code, though, so I can't write a specific handler for when that occurs. Does anyone know what library module that exception lives in?</p>",41863670,3,0,,2017-01-25 23:29:59.873 UTC,,2018-12-11 01:34:33.617 UTC,2017-05-23 17:46:53.397 UTC,,442945,,442945,1,5,python|python-3.x|boto3,1192
Text Recognition through AWS Rekognition Fails to Detect Majority of Text,47557888,Text Recognition through AWS Rekognition Fails to Detect Majority of Text,"<p>I am using AWS Rekognition to detect text from a pdf that is converted into a jpeg. 
The image that I am using has text that is approximately size 10-12 or a regular letter page. However, The font changes throughout the image several times. </p>

<p>Is my lack of detection and low confidence levels due to having a document where the text changes often? Small Font?</p>

<p>Essentially I'd like to know what kind of image/text do I need to have the best results from a detect text algorithm?  </p>",,1,0,,2017-11-29 16:54:18.870 UTC,,2018-05-17 13:20:16.367 UTC,,,,,7903466,1,0,amazon-web-services|amazon-s3|text-recognition|amazon-rekognition,236
Volley POST request with Raw Image Data,55127193,Volley POST request with Raw Image Data,"<p>I'm trying to use volley to call Azure Computer Vision REST API using POST request to upload image to be analysed. </p>

<p>Here's the API documentation: <a href=""https://southeastasia.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/56f91f2e778daf14a499e1fa"" rel=""nofollow noreferrer"">https://southeastasia.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/56f91f2e778daf14a499e1fa</a></p>

<p>The input is passed within the POST body, and I wanted to send the raw image binary using <code>application/octet-stream</code>.</p>

<p>I convert the image to raw binary using <code>bitmap.compress(...)</code> and <code>byteArrayOutputStream.toByteArray();</code></p>

<p>I have no success so far in sending the raw binary image data, it's giving error 400.</p>

<p>Here's the code:</p>

<pre><code>public void testVollReq(){

   final TextView tv = findViewById(R.id.tv);
   String url = ""https://southeastasia.api.cognitive.microsoft.com/vision/v1.0/describe"";
   final StringBuilder stringBuilder = new StringBuilder();

   JSONObject postParams = new JSONObject();
   try {
       postParams.put(""maxCandidates"",""1"");
   } catch (JSONException e) {
       e.printStackTrace();
   }

   JsonObjectRequest jsonObjectRequest = new JsonObjectRequest(
        Request.Method.POST,
        url,
        null,
        new Response.Listener&lt;JSONObject&gt;() {
            @Override
            public void onResponse(JSONObject response) {
                try {
                    JSONObject descriptionObject = response.getJSONObject(""description"");
                    JSONArray captionsArr = descriptionObject.getJSONArray(""captions"");
                    for (int i = 0; i &lt; captionsArr.length(); i++) {
                        JSONObject captionObject = captionsArr.getJSONObject(i);
                        stringBuilder.append((String) captionObject.get(""text"")).append(""\n"");
                        Toast toast = Toast.makeText(MainActivity.this, ""SUCCESS"", Toast.LENGTH_SHORT);
                        toast.show();
                        tv.setText(stringBuilder.toString());
                    }
                } catch (JSONException e) {
                    Toast toast = Toast.makeText(MainActivity.this, ""FAIL1"", Toast.LENGTH_SHORT);
                    toast.show();
                    e.printStackTrace();
                }
            }
        },
        new Response.ErrorListener() {
            @Override
            public void onErrorResponse(VolleyError error) {
                Toast toast = Toast.makeText(MainActivity.this, ""FAIL2"", Toast.LENGTH_SHORT);
                toast.show();
                tv.setText(error.getMessage());
            }
        })
   {
       @Override
       public Map&lt;String, String&gt; getHeaders() throws AuthFailureError {
           HashMap headers = new HashMap();
           headers.put(""Content-Type"", ""application/octet-stream"");
           headers.put(""Ocp-Apim-Subscription-Key"", ""xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"");
           return headers;
       }

       @Override
       public byte[] getBody() {
           Bitmap bitmap = BitmapFactory.decodeResource(getResources(), R.drawable.test);
           ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();
           bitmap.compress(Bitmap.CompressFormat.JPEG, 100, byteArrayOutputStream);
           byte[] imageData = byteArrayOutputStream.toByteArray();
           return imageData;
       }
   };

   RequestQueue requestQueue = Volley.newRequestQueue(this);
   requestQueue.add(jsonObjectRequest);
}
</code></pre>

<p>There are only few resources in the internet about Volley POST request using <code>application/octet-stream</code>, so I am a little bit lost here...</p>

<p>Any help would be appreciated!</p>",,0,0,,2019-03-12 17:14:26.370 UTC,,2019-03-12 17:22:49.667 UTC,2019-03-12 17:22:49.667 UTC,,6296561,,8531471,1,0,android|azure|post|android-volley,36
Trouble with Google Vision API key,46739009,Trouble with Google Vision API key,"<p>I am trying to use Google Vision API in my WinForms (.NET) project. I have signed up in Google Cloud Platform and enabled Vision API. Having followed Google Cloud standard steps in authorization process I have created and downloaded service key in JSON format. As far as it is concerned, GOOGLE_APPLICATION_CREDENTIALS also have been set to be pointing to key file (JSON file I have mentioned before). All settings are looking good in Google Cloud Platform, in terms of API.</p>

<p>I am wondering why I am getting exception </p>

<blockquote>
  <p>Grpc.Core.RpcException: 'Status(StatusCode=Unauthenticated,Detail=""Getting metadata from plugin failed with error: Exception occured in metadata credentials plugin."")'</p>
</blockquote>

<p>Here is the code of method where exception is thrown:</p>

<pre><code>        var client = ImageAnnotatorClient.Create();
        var image = Image.FromFile(filePath);
        var response = client.DetectText(image); // &lt;- here exception is thrown
</code></pre>

<p>P.S. I have made a significant research on this topic. I know it seems something is wrong with authentication, can't figure out what exactly is wrong though.</p>

<p>P.S.S Should you have any references or tutorials, don't hesitate to provide me with them.</p>",,1,0,,2017-10-13 22:49:10.943 UTC,,2018-01-22 01:55:22.137 UTC,2018-01-22 01:55:22.137 UTC,,1905949,,8773690,1,2,.net|json|google-vision,330
AWS - Does not recognise access key ID and secret access key,47867995,AWS - Does not recognise access key ID and secret access key,"<p>I am using AWS, boto3 and Pycharm to write a very simple program on python that compare one faces from one face image to another face from another face image.
I written the following simple source code:</p>

<pre><code>import boto3

s3 = boto3.resource('s3',
                    aws_access_key_id = ""xxx"",
                    aws_secret_access_key = ""yyy"")

BUCKET = ""eyeglasses-images""
KEY_SOURCE = ""Foucault.jpg""
KEY_TARGET = ""Ricoeur.jpg""


def compare_faces(bucket, key, bucket_target, key_target, threshold=80, region=""eu-west-1""):
    rekognition = boto3.client(""rekognition"", region)
    response = rekognition.compare_faces(
        SourceImage={
            ""S3Object"": {
                ""Bucket"": bucket,
                ""Name"": key,
            }
        },
        TargetImage={
            ""S3Object"": {
                ""Bucket"": bucket_target,
                ""Name"": key_target,
            }
        },
        SimilarityThreshold=threshold,
    )
    return response['SourceImageFace'], response['FaceMatches']


source_face, matches = compare_faces(BUCKET, KEY_SOURCE, BUCKET, KEY_TARGET)

# the main source face
print
""Source Face ({Confidence}%)"".format(**source_face)

# one match for each target face
for match in matches:
    print
    ""Target Face ({Confidence}%)"".format(**match['Face'])
    print
    ""  Similarity : {}%"".format(match['Similarity'])
</code></pre>

<p>However I get the following error:</p>

<pre><code>raise NoCredentialsError
botocore.exceptions.NoCredentialsError: Unable to locate credentials
</code></pre>

<p>(Obviously in the place of xxx and yyy I am using the real keys)</p>

<p>What is the problem and how can I fix this?</p>",,2,2,,2017-12-18 11:56:01.917 UTC,,2017-12-18 12:41:29.353 UTC,2017-12-18 12:14:57.243 UTC,,2244081,,9024698,1,1,python|amazon-web-services|boto3,366
Searching AFINN-165 json by score,48022812,Searching AFINN-165 json by score,"<p>I'm making a emotion-adjusted Youtube search engine which maps a score (read from webcam images by Microsoft Azure Emotion API) to a few words selected in the AFINN-165 list, and then peforms a Youtube search.</p>

<p>The code is written in Node &amp; Express (returns the answer by GET request).</p>

<p>I'm trying to search the JSON by value of a word. Example; When I give the function (5) it would return all words that have a score of five.
The JSON is structured like this:</p>

<pre><code>var data = {
 word: score,
 word: score,
 word: score,
};
</code></pre>

<p>Which I wrap in an array below</p>

<pre><code>function getWordsByScore() {
  var afinnKeys = Object.keys(afinn);
  var afinnArray = [afinn]
  console.log(afinnKeys.length);

 for (var i = 0; i &lt; afinnKeys.length; i++) {
  var word = String(afinnKeys[i]);
  return(afinnArray[0].word);
 }
}
</code></pre>

<p>Somehow I just can't get it to work. I try to get the actual 'word' by creating an array of keys in AfinnKeys. But feeding this word by a forloop to the afinnArray[0] just gives undefined as a return.</p>

<p>I hope someone could help me out. Have been stuck on this for some time now.</p>",48023190,2,0,,2017-12-29 12:43:46.290 UTC,,2017-12-29 13:14:13.690 UTC,,,,,9153046,1,1,javascript|json,36
Google Vision Batch Annotate Images with Java Client Library,45306016,Google Vision Batch Annotate Images with Java Client Library,"<p>I am getting an exception when trying to annotate images via Google Vision using the provided java client google vision.</p>

<p>specifically this code where the batch client.batchAnnotateImages occurs:</p>

<pre><code>public void processOCR(byte[] file) 
{
     List&lt;AnnotateImageRequest&gt; requests = new ArrayList&lt;&gt;();

      ByteString imageByteString = ByteString.copyFrom(file);

      Image img = Image.newBuilder().setContent(imageByteString).build();
      Feature feat = Feature.newBuilder().setType(Type.DOCUMENT_TEXT_DETECTION).build();

      AnnotateImageRequest request = AnnotateImageRequest.newBuilder().addFeatures(feat).setImage(img).build();
      requests.add(request);

      try (ImageAnnotatorClient client = ImageAnnotatorClient.create()) 
      {

        BatchAnnotateImagesResponse response = client.batchAnnotateImages(requests);
        List&lt;AnnotateImageResponse&gt; responses = response.getResponsesList();
        client.close();

        //visionResultsDTO result = new visionResultsDTO();
        String ParagraphText = """";


        for (AnnotateImageResponse res : responses) {
          if (res.hasError()) {
            //throw exception.
            return;
          }

          // For full list of available annotations, see http://g.co/cloud/vision/docs
          TextAnnotation annotation = res.getFullTextAnnotation();
          for (Page page: annotation.getPagesList()) {
            String pageText = """";
            for (Block block : page.getBlocksList()) {
          String blockText = """";
              for (Paragraph para : block.getParagraphsList()) {
                String paraText = """";
                for (Word word: para.getWordsList()) {
                  String wordText = """";
              for (Symbol symbol: word.getSymbolsList()) {
                    wordText = wordText + symbol.getText();
                  }
              paraText = paraText + wordText;
                }
                // Output Example using Paragraph:
                blockText = blockText + paraText;
              }
              pageText = pageText + blockText;
            }
          }
          ParagraphText = annotation.getText();
        //  result.setResultText(ParagraphText);
        }
      } catch (Exception e) 
      {
        // TODO Auto-generated catch block
        e.printStackTrace();
       }
 }
</code></pre>

<p>I am being presented with the following Stack Trace / Error:</p>

<blockquote>
  <p>java.lang.NoSuchMethodError: com.google.common.util.concurrent.MoreExecutors.directExecutor()Ljava/util/concurrent/Executor;
      at com.google.api.gax.retrying.BasicRetryingFuture.(BasicRetryingFuture.java:77)
      at com.google.api.gax.retrying.CallbackChainRetryingFuture.(CallbackChainRetryingFuture.java:62)
      at com.google.api.gax.retrying.ScheduledRetryingExecutor.createFuture(ScheduledRetryingExecutor.java:86)
      at com.google.api.gax.grpc.RetryingCallable.futureCall(RetryingCallable.java:57)
      at com.google.api.gax.grpc.RetryingCallable.futureCall(RetryingCallable.java:42)
      at com.google.api.gax.grpc.AuthCallable.futureCall(AuthCallable.java:57)
      at com.google.api.gax.grpc.UnaryCallable.futureCall(UnaryCallable.java:282)
      at com.google.api.gax.grpc.UnaryCallable.futureCall(UnaryCallable.java:293)
      at com.google.api.gax.grpc.UnaryCallable.call(UnaryCallable.java:321)
      at com.google.cloud.vision.v1.ImageAnnotatorClient.batchAnnotateImages(ImageAnnotatorClient.java:201)
      at com.google.cloud.vision.v1.ImageAnnotatorClient.batchAnnotateImages(ImageAnnotatorClient.java:177)
      at za.co.thumbtribe.core.googlevision.service.impl.GoogleVisionServiceImpl.processOCR(GoogleVisionServiceImpl.java:55)</p>
</blockquote>

<p>Here are my POM Dependencies : </p>

<pre><code>&lt;dependencies&gt;
&lt;!-- Spring --&gt;
&lt;dependency&gt;
  &lt;groupId&gt;org.springframework&lt;/groupId&gt;
  &lt;artifactId&gt;spring-web&lt;/artifactId&gt;
  &lt;version&gt;4.2.5.RELEASE&lt;/version&gt;
  &lt;scope&gt;compile&lt;/scope&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;
    &lt;artifactId&gt;google-cloud-vision&lt;/artifactId&gt;
    &lt;version&gt;0.20.3-beta&lt;/version&gt;
    &lt;exclusions&gt;
    &lt;exclusion&gt;
      &lt;groupId&gt;com.google.auth&lt;/groupId&gt;
      &lt;artifactId&gt;google-auth-library-oauth2-http&lt;/artifactId&gt;
    &lt;/exclusion&gt;
    &lt;exclusion&gt;
      &lt;groupId&gt;com.google.auth&lt;/groupId&gt;
      &lt;artifactId&gt;google-auth-library-credentials&lt;/artifactId&gt;
    &lt;/exclusion&gt;
  &lt;exclusion&gt;
    &lt;groupId&gt;com.google.guava&lt;/groupId&gt;
    &lt;artifactId&gt;*&lt;/artifactId&gt;
&lt;/exclusion&gt;
  &lt;/exclusions&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
  &lt;groupId&gt;com.google.auth&lt;/groupId&gt;
  &lt;artifactId&gt;google-auth-library-oauth2-http&lt;/artifactId&gt;
  &lt;version&gt;0.7.0&lt;/version&gt;
 &lt;/dependency&gt;
 &lt;dependency&gt;
  &lt;groupId&gt;com.google.auth&lt;/groupId&gt;
  &lt;artifactId&gt;google-auth-library-credentials&lt;/artifactId&gt;
  &lt;version&gt;0.7.0&lt;/version&gt;
&lt;/dependency&gt;

&lt;/dependencies&gt;
</code></pre>

<p>I have tried excluding guava and including multiple versions of the API.</p>

<p>The code shown is the sample code from the google vision client implementation.</p>

<p>any ideas ?</p>",,3,0,,2017-07-25 14:18:04.920 UTC,,2017-08-04 14:00:46.070 UTC,,,,,2902112,1,8,java|maven|spring-boot|google-vision,575
store the output of watson services to ibm database,40278271,store the output of watson services to ibm database,"<p>I need to store the output  of IBM Watson visual recognition service in ibm cloud or any other database, which can be accessed for further use.Can anyone please help me out? thank you.</p>",,2,0,,2016-10-27 07:05:47.177 UTC,,2016-10-28 08:20:02.950 UTC,,,,,7078861,1,-1,ibm-cloud|ibm-watson|visual-recognition,79
Filter on a dynamodb statement,51004854,Filter on a dynamodb statement,"<p>Its a small query I just want to get those records which have event name = 'newevent'</p>

<p>how to apply this filter as in documentaion all queries are of composite keys like this </p>

<pre><code>face = dynamodb.get_item(
                                TableName='athlete_collection',
                                Key={'RekognitionId': {'S': match['Face']['FaceId']}
                                    ,'EventName': {'S' : 'celeb'}
                                     }
                            )
</code></pre>

<p>but in my case its not composite key . </p>",,1,0,,2018-06-23 20:38:09.877 UTC,,2018-06-24 01:36:23.683 UTC,,,,,7958560,1,0,amazon-web-services|amazon-dynamodb,322
Does google-cloud-vision stores uploaded images ? what is privacy policy for that?,36540684,Does google-cloud-vision stores uploaded images ? what is privacy policy for that?,"<p>I want to implement google-cloud-vision API for OCR on my Project.
But due to compliance issues, I need to know does google-cloud-vision stores the uploaded image? if yes what is the privacy policy for that?</p>

<p>Does anyone have any information regarding this?</p>

<p>Thanks! </p>",,2,0,,2016-04-11 05:47:52.007 UTC,2,2017-10-26 06:15:28.900 UTC,2017-07-03 12:24:26.983 UTC,,81019,,2513029,1,5,google-cloud-platform|google-cloud-vision,1517
Where to put API Key in Google Cloud Vision for PHP,53239821,Where to put API Key in Google Cloud Vision for PHP,"<p>I want to use Google Cloud Vision API on a family photo. I activated the API on my GCP account, received an API Key but I don't know where I should insert it. Here's my code :</p>

<pre><code>&lt;?
require 'vendor/autoload.php';
use Google\Cloud\Vision\VisionClient;
$vision = new VisionClient();
$image = $vision-&gt;image(
    fopen('data/family_photo.jpg', 'r'),
    ['faces']
);
$annotation = $vision-&gt;annotate($image);
var_dump($annotation);
die();

?&gt;
</code></pre>

<p>I get the following error :  ""error"": { ""code"": 403, ""message"": ""The request is missing a valid API key."", ""status"": ""PERMISSION_DENIED"" } }.</p>

<p>Update: Thanks to the provided answer by Dan D., I added the following line: </p>

<pre><code>putenv(""GOOGLE_APPLICATION_CREDENTIALS=book.json"");
</code></pre>",53240102,1,0,,2018-11-10 14:14:38.663 UTC,,2018-11-10 15:19:23.790 UTC,2018-11-10 15:19:23.790 UTC,,1007015,,1007015,1,2,php|api|google-cloud-platform|google-cloud-vision,305
IBM Watson Visual Recognition in Java,37306516,IBM Watson Visual Recognition in Java,"<p>I want to use IBM Watson Visual Recognition for my android app and want to call APIs in JAVA but i don't find any example or any reference to the list of methods in JAVA to use this service. You can see the JAVA examples are missing <a href=""https://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/visual-recognition/api/v2/?java#introduction"" rel=""nofollow"">here</a>. Please help me to find few suitable examples or any reference to these methods. Please also tell me what is bluemix platform and is it necessary to use it in order to use IBM Watson Visual Recognition? Thanks in Advance!    </p>",37308428,4,0,,2016-05-18 17:31:51.723 UTC,1,2017-01-18 05:12:18.900 UTC,2016-05-24 01:01:02.700 UTC,,3198917,,3387921,1,1,ibm-cloud|ibm-watson|visual-recognition,1586
Accessing the subscription/num_oustanding_messages metric in Google PubSub from Python,41211840,Accessing the subscription/num_oustanding_messages metric in Google PubSub from Python,"<p>Is it possible to access the <code>subscription/num_outstanding_messages</code> metric listed <a href=""https://cloud.google.com/monitoring/api/metrics#gcp-pubsub"" rel=""nofollow noreferrer"">https://cloud.google.com/monitoring/api/metrics#gcp-pubsub</a> from google-cloud-python? I've used similar code to successfully access the <code>num_undelivered_messages</code> metric, but iterating over the results of the following query (which succeeds) always yields an empty list.</p>

<pre><code>Python 2.7.6 (default, Oct 26 2016, 20:30:19) 
[GCC 4.8.4] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; from google.cloud import monitoring
&gt;&gt;&gt; client = monitoring.Client()
&gt;&gt;&gt; q = client.query(
...     ""pubsub.googleapis.com/subscription/num_undelivered_messages"",
...     minutes=90
... )
&gt;&gt;&gt; len(list(q)) &gt; 0
True
&gt;&gt;&gt; q = client.query(
...     ""pubsub.googleapis.com/subscription/num_outstanding_messages"",
...     minutes=90
... )
&gt;&gt;&gt; len(list(q)) &gt; 0
False
</code></pre>

<p>Relevant google-cloud-python library versions:</p>

<pre><code>~:pip freeze | grep google
gapic-google-logging-v2==0.10.1
gapic-google-pubsub-v1==0.10.1
google-cloud==0.21.0
google-cloud-bigquery==0.21.0
google-cloud-bigtable==0.21.0
google-cloud-core==0.21.0
google-cloud-datastore==0.21.0
google-cloud-dns==0.21.0
google-cloud-error-reporting==0.21.0
google-cloud-happybase==0.20.0
google-cloud-language==0.21.0
google-cloud-logging==0.21.0
google-cloud-monitoring==0.21.0
google-cloud-pubsub==0.21.0
google-cloud-resource-manager==0.21.0
google-cloud-runtimeconfig==0.21.0
google-cloud-storage==0.21.0
google-cloud-translate==0.21.0
google-cloud-vision==0.21.0
google-gax==0.14.1
googleapis-common-protos==1.5.0
grpc-google-iam-v1==0.10.1
grpc-google-logging-v2==0.10.1
grpc-google-pubsub-v1==0.10.1
</code></pre>",41228077,1,0,,2016-12-18 19:05:05.383 UTC,,2016-12-19 17:36:44.177 UTC,2016-12-19 14:13:49.860 UTC,,1477614,,1477614,1,3,google-cloud-platform|google-cloud-pubsub|google-cloud-python|google-cloud-monitoring,516
"What unit is (X,Y) coordinate in Microsoft Azure Text recognition bounding box response?",53963357,"What unit is (X,Y) coordinate in Microsoft Azure Text recognition bounding box response?","<p>What unit is (X,Y) coordinate in Microsoft Azure Text recognition bounding box response?</p>

<p>Ex.: </p>

<pre><code>{
  ""status"": ""Succeeded"",
  ""succeeded"": true,
  ""failed"": false,
  ""finished"": true,
  ""recognitionResult"": {
    ""lines"": [
      {
        ""boundingBox"": [
          67,
          204,
          668,
          210,
          667,
          272,
          66,
          267
        ],
        ""text"": ""Our greatest glory is not"",
        ...
</code></pre>

<p>The json response shows the four coordinates of the bounding boxes in a clockwise disposition. However, I haven't found the unit. I assume that it is pixels, but it's not written anywhere...</p>

<p>The API is available here:</p>

<p><a href=""https://westus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/587f2cf1154055056008f201"" rel=""nofollow noreferrer"">https://westus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/587f2cf1154055056008f201</a></p>",53964581,1,0,,2018-12-28 19:21:35.097 UTC,,2018-12-31 00:13:18.927 UTC,2018-12-31 00:13:18.927 UTC,,8161670,,3837560,1,1,azure|microsoft-cognitive|azure-cognitive-services|face-api,2010
AWS Rekognition Service: Improve facial recognition accuracy,51186135,AWS Rekognition Service: Improve facial recognition accuracy,"<p><br/></p>

<p>I'm finding the right way to use AWS Rekognition service.</p>

<p>My problem is <strong>How to verify a person image on multi collections</strong>, I'm reading <a href=""https://aws.amazon.com/blogs/machine-learning/build-your-own-face-recognition-service-using-amazon-rekognition/"" rel=""nofollow noreferrer"">Build Your Own Face Recognition Service Using Amazon Rekognition | AWS Machine Learning Blog</a> from Amazon but cannot find the implementation document for it. My point is <strong>Face verification</strong> title.</p>

<p><strong>Update 1</strong>:</p>

<p>My target is: Using AWS Rekognition to get person's info by their face.</p>

<p>My problem is: How to make AWS Rekognition improves its accuracy when recognizing a face.</p>

<p>What I tried:</p>

<ol>
<li>Upload multi captured portraits of a person with same <em>ExternalImageID</em> but I'm not sure it works or not.</li>
<li>Finding a way to create <strong>Collection</strong> for each person, then upload person's portraits to their <strong>Collection</strong> but I don't how to search a face through multiple <strong>Collections</strong>.</li>
<li>I'm trying use S3 for storage people's images then using Lambda function to do something that I've not got yet.</li>
</ol>

<p><strong>Update 2</strong>:</p>

<ol>
<li><strong>What is your input material:</strong> Input materials are some people's portrait photo with ExternalImageID is their name (eg: my portrait photo will have ExternalImageID is ""Long"").</li>
<li><strong>What are you trying to do:</strong> I'm trying to get ExternalImageID when I send a portrait photo of a registered person. (eg: with my other portrait photo, AWS has to response ExternalImageID is ""Long"").</li>
<li><strong>Do you have it working, but it is not recognizing some people?</strong> Yes, it's work but sometimes it cannot recognize exactly people.</li>
<li><strong>Please tell us your use-case / scenario and what you are trying to accomplish:</strong>

<ul>
<li>Create an AWS Rekognition collection with sample name (eg facetest).</li>
<li>Register some people with their name is ExternalImageID.</li>
<li>Submit an image to AWS Rekognition API to get ExternalImageID - his name.</li>
</ul></li>
</ol>",51256870,1,4,,2018-07-05 08:00:55.873 UTC,,2018-07-10 03:33:56.667 UTC,2018-07-10 03:28:31.460 UTC,,174777,,5325133,1,0,amazon-web-services|amazon-rekognition,376
JSON parsing issue when using filepath as image url,42146912,JSON parsing issue when using filepath as image url,"<p>I am having an issue getting my image to upload to the microsoft face api.</p>

<p>I have a function that posts to the server, which implements another function that turns a user selected image into a base64 encoded stream.</p>

<pre><code>        public async Task getImageID(){
            //filedialogs, etc...

            HttpResponseMessage response;
            string responseBodyAsText;
            byte[] byteData = Encoding.UTF8.GetBytes(""{ \""url\"":\""""+baseEncodeImage(getPhoto.FileName)+"" \""}"");

            using (var content = new ByteArrayContent(byteData)){
                    content.Headers.ContentType = new MediaTypeHeaderValue(""application/json"");
                    response = await client.PostAsync(uri, content);
                    responseBodyAsText = await response.Content.ReadAsStringAsync();

                //debug prints
                Console.Write(responseBodyAsText+""\n""+ getPhoto.FileName+""\n""+byteData);
            }
        }

       public string baseEncodeImage(string filePath){
            //This function will take the filepath selected from the filedialog
            //and turn it into a base64 encoded stream to be used by the face api
            using (Image image = Image.FromFile(filePath))
            {
                using (MemoryStream m = new MemoryStream())
                {
                    image.Save(m, image.RawFormat);
                    byte[] imageBytes = m.ToArray();

                    // Convert byte[] to Base64 String
                    string base64String = Convert.ToBase64String(imageBytes);
                    return base64String;
                }
            }


        }
</code></pre>

<p>It posts to the server, and returns the following in the command line:
<a href=""https://i.stack.imgur.com/BoNaM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BoNaM.png"" alt=""Error""></a></p>

<p>What do I need to manipulate so that it works with the base64 encoding? It was posting with an image url off the internet prior to the modifications.</p>",42153114,1,9,,2017-02-09 21:09:36.617 UTC,,2017-02-10 06:47:31.520 UTC,2017-02-09 22:11:32.427 UTC,,809357,,7531695,1,1,c#|.net|json,95
AWS Sagemaker Multiple Object Detection in Image Recognition / Classification,51183169,AWS Sagemaker Multiple Object Detection in Image Recognition / Classification,"<p>Does anyone know if any SageMaker built-in algorithm supports multiple object detection in image recognition? I am thinking something like multi-label image training and detection / inference. </p>

<p>Thus, can we: </p>

<p><strong>a) train using multi-label images</strong> </p>

<p>and/or </p>

<p><strong>b) infer multiple objects from images (sort of like AWS Rekognition but with custom labels and training / transfer learning).</strong></p>

<p>Also, I know that the doc for SageMaker Image Classification Algorithm says ""takes an image as input and classifies it into <strong>one</strong> of multiple output categories"". </p>

<p>Any recommendations are also welcome.</p>",51317676,2,0,,2018-07-05 03:54:14.690 UTC,,2018-08-30 09:14:29.950 UTC,2018-08-30 09:14:29.950 UTC,,472495,,674305,1,0,amazon-web-services|machine-learning|computer-vision|amazon-rekognition|amazon-sagemaker,905
text is recognized when i click the picture very closely. it is not even recognizing the text on a business card,52877687,text is recognized when i click the picture very closely. it is not even recognizing the text on a business card,"<p>I am using google vision API to get the text that is written in a business card but when I click the picture it only recognizes text that is written in a bigger text or I need to take the picture very closely. then it only recognizes. But I need to focus on a phone no or name or address I couldn't get the whole text that is written on a card together.</p>

<p>And instead if I set the picture in an imageView manually that is not captured but in a good resolution then it recognizes perfect. Is this issue is related to resolution? If anyone having a perfect code for this can you send it?</p>

<p>here is my java code</p>

<pre><code>public class MainActivity extends AppCompatActivity {


    ImageView imgPic;
    TextView tvText;
    Button btnClick, btnCapture;
    private int REQUEST_IMAGE_CAPTURE = 1;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);

        imgPic = findViewById(R.id.img_pic);
        tvText = findViewById(R.id.tv_text);
        btnClick = findViewById(R.id.btn_click);
        btnCapture = findViewById(R.id.btn_capture);

        btnCapture.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {

                Intent takePictureIntent = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);
                if (takePictureIntent.resolveActivity(getPackageManager()) != null) {
                    startActivityForResult(takePictureIntent, REQUEST_IMAGE_CAPTURE);
                }


            }
        });

@Override
    protected void onActivityResult(int requestCode, int resultCode, Intent data) {
        super.onActivityResult(requestCode, resultCode, data);
        if (requestCode == REQUEST_IMAGE_CAPTURE &amp;&amp; resultCode == RESULT_OK) {
            Bundle extras = data.getExtras();
            Bitmap imageBitmap = (Bitmap) extras.get(""data"");
            imgPic.setImageBitmap(imageBitmap);


            TextRecognizer textRecognizer = new TextRecognizer.Builder(getApplicationContext()).build();
            if (!textRecognizer.isOperational()) {
                Toast.makeText(MainActivity.this, ""could not get the text"", Toast.LENGTH_SHORT).show();
            } else {

                Frame frame = new Frame.Builder().setBitmap(imageBitmap).build();
                SparseArray&lt;TextBlock&gt; items = textRecognizer.detect(frame);
                int size = items.size();
                Log.e(""size"", String.valueOf(size));
                //  Toast.makeText(MainActivity.this, size, Toast.LENGTH_SHORT).show();
                StringBuilder sb = new StringBuilder();
                for (int i = 0; i &lt; items.size(); i++) {
                    TextBlock myItem = items.valueAt(i);
                    Log.e(""hello"", myItem.getValue());
                    sb.append(myItem.getValue());
                    sb.append(""\n"");


                }

                tvText.setText(sb.toString());


            }


        }

    }


}
</code></pre>

<p>Here is my xml</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;ScrollView android:layout_height=""match_parent""
    android:layout_width=""match_parent""
    xmlns:android=""http://schemas.android.com/apk/res/android""&gt;--&gt;
&lt;LinearLayout
    xmlns:android=""http://schemas.android.com/apk/res/android""
    xmlns:tools=""http://schemas.android.com/tools""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""
    android:orientation=""vertical""
    tools:context="".MainActivity""&gt;
    &lt;Button
        android:id=""@+id/btn_capture""
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:layout_marginTop=""20dp""
        android:text=""capture""
        /&gt;


    &lt;ImageView
        android:id=""@+id/img_pic""
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""/&gt;

    &lt;TextView
        android:textColor=""@color/colorAccent""
        android:id=""@+id/tv_text""
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""

        android:textSize=""25sp""
       /&gt;
&lt;/LinearLayout&gt;

&lt;/ScrollView&gt;
</code></pre>",,1,0,,2018-10-18 15:36:56.507 UTC,1,2018-10-18 17:12:05.193 UTC,2018-10-18 17:12:05.193 UTC,,2649012,,9633929,1,1,java|android|google-vision,34
Where do I find documentation for older versions of google-cloud-python?,51866993,Where do I find documentation for older versions of google-cloud-python?,"<p>We've got a pretty extensive BI system built on <code>python 2.7</code>/<code>google-cloud 0.20.0</code>. We periodically add products from the Google Cloud Platform, and need to access documentation for the version of the python module we're using. Here is the relevant <code>pip freeze</code> if it helps:</p>

<pre><code>gapic-google-logging-v2==0.10.1
gapic-google-pubsub-v1==0.10.1
gax-google-logging-v2==0.8.1
gax-google-pubsub-v1==0.8.1
gcloud==0.18.1
google-api-python-client==1.5.1
google-cloud==0.20.0
google-cloud-bigquery==0.20.0
google-cloud-bigtable==0.20.0
google-cloud-core==0.20.0
google-cloud-datastore==0.20.1
google-cloud-dns==0.20.0
google-cloud-error-reporting==0.20.0
google-cloud-language==0.20.0
google-cloud-logging==0.20.0
google-cloud-monitoring==0.20.0
google-cloud-pubsub==0.20.0
google-cloud-resource-manager==0.20.0
google-cloud-storage==0.20.0
google-cloud-translate==0.20.0
google-cloud-vision==0.20.0
google-gax==0.14.1
googleads==4.7.0
googleapis-common-protos==1.5.0
grpc-google-iam-v1==0.10.1
grpc-google-logging-v2==0.10.1
grpc-google-pubsub-v1==0.10.1
</code></pre>",51867729,1,0,,2018-08-15 21:55:16.807 UTC,,2018-08-15 23:26:08.730 UTC,,,,,3347351,1,0,python|google-cloud-platform,28
How to use the Google Vision API for text detection from base64 encoded image?,43094048,How to use the Google Vision API for text detection from base64 encoded image?,"<p>I am having a base64 encoded image.</p>

<pre><code>imageData = 'data:image/png;base64,iVBORw0rrfwfwHReger32QRQWr...'
</code></pre>

<p>How I should proceed for text detection with google cloud vision python library?</p>

<p>My Code looks like :</p>

<pre><code>from google.cloud import vision
client = vision.Client()
imageData = 'data:image/png;base64,iVBORw0rrfwfwHReger32QRQWr...'
image = client.image(content=imageData)
texts = image.detect_text()
print texts[0].description
</code></pre>",,1,0,,2017-03-29 12:50:59.577 UTC,,2018-04-22 00:07:15.917 UTC,,,,,6714203,1,1,python|google-vision,3109
StatusCode.RESOURCE_EXHAUSTED before reaching quota limit + confusing dashboard,50580902,StatusCode.RESOURCE_EXHAUSTED before reaching quota limit + confusing dashboard,"<p>I'm using Google Vision API for text &amp; logo detection. When trying to run 300 annotation requests, each with up to 6 images, I'm getting this error (python library):</p>

<pre><code>grpc._channel._Rendezvous: &lt;_Rendezvous of RPC that terminated with (StatusCode.RESOURCE_EXHAUSTED, Resource has been exhausted (e.g. check quota).)&gt;
</code></pre>

<p>I'm making up to 8 concurrent requests, whole process takes about 65 seconds.</p>

<p>According to <a href=""https://cloud.google.com/vision/quotas"" rel=""nofollow noreferrer"">Quotas and Limits</a> I should be able to:</p>

<ul>
<li>make 600 requests per minute</li>
<li>send up to 16 images per request</li>
</ul>

<p>There's also a limit for image size &amp; JSON request object size, but with images like <a href=""https://cdn.filestackcontent.com/PPECA8dS3MLu6D2ijeiw"" rel=""nofollow noreferrer"">this one</a> (under 50KB), that should not be a problem (right?).</p>

<p>I could ask for a quota increase, but since I'm not able to get to the default 600 req/min, I would have to make a guess (or is my quota math incorrect?)</p>

<p>Looking at Google Cloud Vision API dashboard confuses me even more, here are <strong>results for the same minute</strong>, just after a page refresh:
<a href=""https://i.stack.imgur.com/IzjGA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IzjGA.png"" alt=""enter image description here""></a></p>

<p>Did anyone have a similar issue? I would like to reach 300req/min threshold (at least).</p>",,0,2,,2018-05-29 09:29:01.223 UTC,,2018-07-20 06:35:33.083 UTC,2018-07-20 06:35:33.083 UTC,,9980065,,5010671,1,1,google-cloud-platform|google-cloud-vision,187
Watson Visual Recognition and Python,50702342,Watson Visual Recognition and Python,"<p>I was trying to create a code using python that uses Watson Visual Recognition
<br>
I was wondering if you can send the image URL instead of a local image path to classify.</p>

<p><br></p>

<pre><code>visual_recognition = VisualRecognitionV3(
version='2016-05-20',
api_key='###########################',
url='https://gateway.watsonplatform.net/visual-recognition/api')

classes = visual_recognition.classify(url='https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Cat_November_2010-1a.jpg/449px-Cat_November_2010-1a.jpg')        
print(json.dumps(classes, indent=2)
</code></pre>",,1,0,,2018-06-05 14:14:29.530 UTC,,2019-02-13 13:07:02.287 UTC,2018-06-05 14:22:26.393 UTC,,5819877,,9202785,1,0,python|watson,46
ASP .NET Core 2.1-preview2 HttpClient deadlock,49831568,ASP .NET Core 2.1-preview2 HttpClient deadlock,"<p>I have ASP.NET Core 2.1 application hosted on Azure web app. I am sending photos base64 string over WebSockets and then by HttpClient to Azure Face API.</p>

<p>After some 150-250 requests HttpClient stops responding and I can't use HttpClient class in any part of my application.</p>

<p>In my localhost it works properly and I never get this problem.</p>

<pre><code>public class FaceApiHttpClient
{
    private HttpClient _client;

    public FaceApiHttpClient(HttpClient client)
    {
        _client = client;
    }

    public async Task&lt;string&gt; GetStringAsync(byte[] byteData,string uri)
    { 
        using (ByteArrayContent content = new ByteArrayContent(byteData))
        {
            content.Headers.ContentType = new MediaTypeHeaderValue(""application/octet-stream"");

            HttpResponseMessage response = await _client.PostAsync(uri, content).ConfigureAwait(false);

            return await response.Content.ReadAsStringAsync().ConfigureAwait(false);
        }

    }
}
</code></pre>

<p><strong>DI:</strong></p>

<pre><code>         services.AddHttpClient&lt;FaceApiHttpClient&gt;(
            client =&gt; {
                client.BaseAddress = new Uri(""xxx"");
                client.DefaultRequestHeaders.Add(""Ocp-Apim-Subscription-Key"", ""xxx"");
            });
</code></pre>

<p>The method from FaceApiClient is invoke in a Scoped Service:</p>

<pre><code>public interface IFaceAPIService
{
    Task&lt;DataServiceResult&lt;List&lt;Face&gt;&gt;&gt; GetFacesDataFromImage(byte[] byteArray);
}

public class FaceAPIService: ServiceBase, IFaceAPIService
{
    private readonly IServerLogger _serverLogger;
    private FaceApiHttpClient _httpClient;
    //Consts
    public const string _APIKey = ""xxx"";
    public const string _BaseURL = ""xxx"";

    public FaceAPIService(IServerLogger serverLogger, FaceApiHttpClient client)
    {
        _serverLogger = serverLogger;
        _httpClient = client;          
    }

    public async Task&lt;DataServiceResult&lt;List&lt;Face&gt;&gt;&gt; GetFacesDataFromImage(byte[] byteData)
    {
        try
        {
            // Request parameters. A third optional parameter is ""details"".
            string requestParameters = ""returnFaceId=true&amp;returnFaceLandmarks=false&amp;returnFaceAttributes=age,gender,headPose,smile,facialHair,glasses,emotion,hair,makeup,occlusion,accessories,blur,exposure,noise"";

            // Assemble the URI for the REST API Call.
            string uri = _BaseURL + ""/detect"" + ""?"" + requestParameters;
            var result = await _httpClient.GetStringAsync(byteData, uri).ConfigureAwait(false);
            List&lt;Face&gt; faces = JsonConvert.DeserializeObject&lt;List&lt;Face&gt;&gt;(result);
            return Success(faces);

        }
        catch (Exception ex)
        {
            _serverLogger.LogExceptionFromService(ex);
            return DataServiceResult.ErrorResult&lt;List&lt;Face&gt;&gt;(ex.Message);
        }
    }
}
</code></pre>

<p>a) on localhost enviroment it works. I run 11 simulators with many request per seconds and it never broke (10 hours of simulators, over 20k requests).</p>

<p>b) HttpClient stops working in any part of application not only in one class.</p>

<p>How to fix this?</p>",50350583,2,11,,2018-04-14 12:42:46.863 UTC,1,2018-05-15 12:45:36.737 UTC,2018-04-14 14:02:14.850 UTC,,2141621,,1945043,1,4,c#|asp.net-core,615
Searching for image Firebase using ML Kit,55298114,Searching for image Firebase using ML Kit,<p>I have a set of a hundred or so images (eventually this will be a few thousand). From my app I want to be able to take a picture and upload it to Firebase and search wether the picture contains one of the images from the set and if so which one. Does ML Kit provide a suitable way to do this? I also saw that there is now a Google Cloud Vision API but this might be overkill? Is there already some open source projects on something similar?</p>,,1,1,,2019-03-22 10:56:00.200 UTC,,2019-05-09 10:39:58.370 UTC,,,,,6158471,1,0,firebase|google-cloud-vision|firebase-mlkit,24
Conflicts with Google PubSub and Google Cloud Vision in same project,40025350,Conflicts with Google PubSub and Google Cloud Vision in same project,"<p>I am building a component in Java / Maven that pulls a message off a Google PubSub Subscription, extracts the Google Cloud Storage image location from the message and calls Google Cloud Vision on the image. I have been able to get PubSub functioning in isolation and the Cloud Vision component functioning in isolation. However, when I try to run them together, I get the following error:  </p>

<pre><code>java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at org.codehaus.mojo.exec.ExecJavaMojo$1.run(ExecJavaMojo.java:294)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NoSuchMethodError: com.google.common.util.concurrent.Futures.catching(Lcom/google/common/util/concurrent/ListenableFuture;Ljava/lang/Class;Lcom/google/common/base/Function;)Lcom/google/common/util/concurrent/ListenableFuture;
    at com.google.cloud.pubsub.spi.DefaultPubSubRpc.translate(DefaultPubSubRpc.java:168)
    at com.google.cloud.pubsub.spi.DefaultPubSubRpc.pull(DefaultPubSubRpc.java:251)
    at com.google.cloud.pubsub.PubSubImpl.pullAsync(PubSubImpl.java:491)
    at com.google.cloud.pubsub.PubSubImpl.pull(PubSubImpl.java:481)
    at xyz.wingman.face_image.GoogleVision.main(GoogleVision.java:68)
    ... 6 more
</code></pre>

<p>This is the second project this has happened on; the first was a similar conflict between PubSub and Firebase. Based on my research, it appears to be a transitive dependency conflict in Guava versions, but I am stuck on how to structure <code>pom.xml</code> to avoid this conflict (if that is indeed the cause):  </p>

<pre><code>&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
  xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

  &lt;groupId&gt;com.skroot.image_face&lt;/groupId&gt;
  &lt;artifactId&gt;gcloud-test&lt;/artifactId&gt;
  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
  &lt;packaging&gt;jar&lt;/packaging&gt;

  &lt;name&gt;gcloud-test&lt;/name&gt;
  &lt;url&gt;http://maven.apache.org&lt;/url&gt;

  &lt;properties&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
  &lt;/properties&gt;

  &lt;build&gt;
    &lt;plugins&gt;
      &lt;plugin&gt;
        &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;
        &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt;
        &lt;version&gt;1.5.0&lt;/version&gt;
        &lt;configuration&gt;
          &lt;mainClass&gt;com.skroot.image_face.GoogleVision&lt;/mainClass&gt;
        &lt;/configuration&gt;
      &lt;/plugin&gt;
      &lt;plugin&gt;
        &lt;!-- Build an executable JAR --&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
        &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;
        &lt;version&gt;3.0.2&lt;/version&gt;
        &lt;configuration&gt;
          &lt;archive&gt;
            &lt;manifest&gt;
              &lt;addClasspath&gt;true&lt;/addClasspath&gt;
              &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt;
              &lt;mainClass&gt;com.skroot.image_face.GoogleVision&lt;/mainClass&gt;
            &lt;/manifest&gt;
          &lt;/archive&gt;
        &lt;/configuration&gt;
      &lt;/plugin&gt;
    &lt;/plugins&gt;
  &lt;/build&gt;

  &lt;dependencies&gt;
    &lt;!-- https://mvnrepository.com/artifact/com.google.cloud.dataflow/google-cloud-dataflow-java-sdk-all --&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;junit&lt;/groupId&gt;
      &lt;artifactId&gt;junit&lt;/artifactId&gt;
      &lt;version&gt;4.12&lt;/version&gt;
      &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;com.google.protobuf&lt;/groupId&gt;
      &lt;artifactId&gt;protobuf-java-util&lt;/artifactId&gt;
      &lt;version&gt;3.0.2&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;com.google.cloud.dataflow&lt;/groupId&gt;
      &lt;artifactId&gt;google-cloud-dataflow-java-sdk-all&lt;/artifactId&gt;
      &lt;version&gt;1.8.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.hamcrest&lt;/groupId&gt;
      &lt;artifactId&gt;hamcrest-all&lt;/artifactId&gt;
      &lt;version&gt;1.3&lt;/version&gt;
      &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;com.skroot&lt;/groupId&gt;
      &lt;artifactId&gt;image_face&lt;/artifactId&gt;
      &lt;version&gt;0.1.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;
      &lt;artifactId&gt;google-cloud&lt;/artifactId&gt;
      &lt;version&gt;0.3.0&lt;/version&gt;
      &lt;exclusions&gt;
        &lt;exclusion&gt;
          &lt;groupId&gt;io.netty&lt;/groupId&gt;
          &lt;artifactId&gt;netty-codec-http2&lt;/artifactId&gt;
        &lt;/exclusion&gt;
        &lt;exclusion&gt;
          &lt;groupId&gt;io.grpc&lt;/groupId&gt;
          &lt;artifactId&gt;grpc-core&lt;/artifactId&gt;
        &lt;/exclusion&gt;
      &lt;/exclusions&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;
      &lt;artifactId&gt;google-cloud-pubsub&lt;/artifactId&gt;
      &lt;version&gt;0.3.0&lt;/version&gt;
      &lt;exclusions&gt;
        &lt;exclusion&gt;
          &lt;groupId&gt;io.netty&lt;/groupId&gt;
          &lt;artifactId&gt;netty-codec-http2&lt;/artifactId&gt;
        &lt;/exclusion&gt;
        &lt;exclusion&gt;
          &lt;groupId&gt;io.grpc&lt;/groupId&gt;
          &lt;artifactId&gt;grpc-core&lt;/artifactId&gt;
        &lt;/exclusion&gt;
      &lt;/exclusions&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;com.google.api-client&lt;/groupId&gt;
      &lt;artifactId&gt;google-api-client&lt;/artifactId&gt;
      &lt;version&gt;1.22.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;com.google.apis&lt;/groupId&gt;
      &lt;artifactId&gt;google-api-services-vision&lt;/artifactId&gt;
      &lt;version&gt;v1-rev25-1.22.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;com.google.protobuf&lt;/groupId&gt;
      &lt;artifactId&gt;protobuf-java&lt;/artifactId&gt;
      &lt;version&gt;3.0.2&lt;/version&gt;
    &lt;/dependency&gt;
  &lt;/dependencies&gt;
&lt;/project&gt;
</code></pre>",,1,0,,2016-10-13 15:37:30.083 UTC,,2016-10-20 18:01:09.993 UTC,,,,,2051586,1,0,java|maven|google-cloud-platform|google-cloud-pubsub,392
Vision API error out stating 'com.google.api.gax.grpc.ApiException: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED',44240464,Vision API error out stating 'com.google.api.gax.grpc.ApiException: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED',"<p>I executed the Vision API for text extract from an image, on running the sample code it is errorring out with he below error stack.</p>

<blockquote>
  <p>May 28, 2017 10:46:48 AM io.grpc.internal.ManagedChannelImpl 
  INFO: [ManagedChannelImpl@543788f3] Created with target vision.googleapis.com:443
  com.google.api.gax.grpc.ApiException: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED
  End
      at com.google.api.gax.grpc.ExceptionTransformingCallable$ExceptionTransformingFuture.onFailure(ExceptionTransformingCallable.java:109)
      at com.google.api.gax.core.ApiFutures$1.onFailure(ApiFutures.java:52)
      at com.google.common.util.concurrent.Futures$6.run(Futures.java:1764)
      at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:456)
      at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:817)
      at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:753)
      at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:634)
      at io.grpc.stub.ClientCalls$GrpcFuture.setException(ClientCalls.java:466)
      at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:442)
      at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:481)
      at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:398)
      at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:513)
      at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:52)
      at io.grpc.internal.SerializingExecutor$TaskRunner.run(SerializingExecutor.java:154)
      at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
      at java.util.concurrent.FutureTask.run(FutureTask.java:266)
      at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
      at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
      at java.lang.Thread.run(Thread.java:745)
  Caused by: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED
      at io.grpc.Status.asRuntimeException(Status.java:545)
      ... 13 more</p>
</blockquote>

<p>I run the code from Eclipse in my local system.</p>

<p>I tried the below items as found in some forums;
1) Degraded all the netty* jars from 4.1.6 to 4.1.3
2) Degraded google-cloud-vision-0.10.0-beta.jar to google-cloud-vision-0.9.4-beta.jar
3) Adding the pom.xml
4) Adding GOOGLE_APPLICATION_CREDENTIALS in windows environment variable - pointed to the JSON file downloaded for the Service Account</p>",,1,2,,2017-05-29 10:40:06.357 UTC,,2017-08-09 12:54:43.337 UTC,2017-05-29 14:55:04.750 UTC,,3163306,,8070628,1,0,grpc|grpc-java|vision-api,240
AWS Rekognition detect label Invalid image encoding error,51959646,AWS Rekognition detect label Invalid image encoding error,"<p>I am using boto3 to make calls to recognition's detect label method which takes an image (in form of base64-encoded bytes) as an input. However I keep getting InvalidImageFormatException and I don't see why. I have read the documentation and looked at some examples but I really can't figure out why I am receiving this error.</p>

<p>Below is my code and what I've tried so far</p>

<pre><code>self.rekog_client = boto3.client('rekognition', 'us-east-1')
with open('abc100.jpg', ""rb"") as cf:
    base64_image=base64.b64encode(cf.read()).decode(""ascii"")
    #also tried this) ==&gt; base64_image=base64.b64encode(cf.read())
resp = self.rekog_client.detect_labels(Image={'Bytes': base64_image})
</code></pre>

<p>Output/Exception: </p>

<pre><code>botocore.errorfactory.InvalidImageFormatException: An error occurred(InvalidImageFormatException) when calling the DetectLabels operation: Invalid image encoding
</code></pre>",,1,0,,2018-08-22 03:56:46.520 UTC,,2018-08-23 14:05:00.990 UTC,2018-08-23 14:05:00.990 UTC,,7044618,,7044618,1,0,python|boto3|amazon-rekognition,322
How to fix AWSIoTPythonSdk.exception.AWSIoTException.subscribeTimeoutException,54060251,How to fix AWSIoTPythonSdk.exception.AWSIoTException.subscribeTimeoutException,"<p>I am trying to run  a script to connect to a iot topic on aws which has a custome message callback but the connection wont take place and throws the error mentioned in the question </p>

<p>This is for raspberry pi that uses aws iot for subscribing to a topic and receiving the custom message i have checked the endpoint its correct i have given only the partial code below </p>

<p><img src=""https://i.stack.imgur.com/VuCu3.jpg"" alt=""this is the error ""></p>

<pre><code># Custom MQTT message callback
def photoVerificationCallback(client, userdata, message):
print(""Received a new message: "")
data = json.loads(message.payload)
try:
    similarity = data[1][0]['Similarity']
    print(""Received similarity: "" + str(similarity))
    if(similarity &gt;= 90):
        print(""Access allowed, opening doors."")
        print(""Thank you!"")
except:
    pass
print(""Finished processing event."")

def checkRFIDNumber(rfidnumber):
return rfidnumber == '0004098554'

# Connect and subscribe to AWS IoT
myAWSIoTMQTTClient.connect()
myAWSIoTMQTTClient.subscribe(""rekognition/result"", 1, 
photoVerificationCallback)
time.sleep(2)


# Publish to the same topic in a loop forever
while True:
print(""waiting.."")
scan = waitForRFIDScan()
print(scan)
if(checkRFIDNumber(scan)):
    print(""RFID correct, taking photo..."")
    uploadToS3(scan)
else:
    print(""Bad RFID - Access Denied"")
</code></pre>",54981024,1,3,,2019-01-06 09:32:21.610 UTC,,2019-03-04 10:14:38.413 UTC,2019-02-02 15:03:52.897 UTC,,504554,,10599727,1,0,python-3.x|amazon-web-services|mqtt|aws-iot,204
ModuleNotFoundError using Google Cloud Speech,48514721,ModuleNotFoundError using Google Cloud Speech,"<p><strong>Edited:</strong></p>

<p>I installed <strong>Google Cloud Speech</strong> by using the followning command:</p>

<pre><code>pip install --upgrade google-cloud-speech
</code></pre>

<p>when I run <strong>Python Program</strong> to convert <strong>Audio</strong> into <strong>Text</strong> using <strong>Python3</strong>, I get an error:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/prateek/Google Drive/projects/linuxAI/src/linuxAI.py"", line 2, in &lt;module&gt;
    from google.cloud import speech
  File ""/usr/lib/python3.6/site-packages/google/cloud/speech/__init__.py"", line 22, in &lt;module&gt;
    from google.cloud.speech.client import Client
  File ""/usr/lib/python3.6/site-packages/google/cloud/speech/client.py"", line 22, in &lt;module&gt;
    from google.cloud.client import Client as BaseClient
ModuleNotFoundError: No module named 'google.cloud.client'
</code></pre>

<p>But If I run the same program using <strong>Python(Python2),</strong> I get the correct output.
I want to run the program using <strong>Python3</strong>. I am using <strong>OpenSuse Tumblebeed.</strong></p>

<p>So, I tried the following commands to install <strong>google-cloud</strong>:</p>

<pre><code>sudo pip install --upgrade google-cloud
sudo pip3 install --upgrade google-cloud
sudo pip3.6 install --upgrade google-cloud
</code></pre>

<p>But I am getting the following error by all these commands:</p>

<pre><code>Requirement already satisfied: google-cloud in /usr/lib/python3.6/si
te-packages/google_cloud-0.32.1.dev1-py3.6.egg
Requirement already satisfied: google-api-core&lt;0.2.0dev,&gt;=0.1.2 in /
usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-bigquery&lt;0.29dev,&gt;=0.28.
0 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-bigquery-datatransfer&lt;0.
2dev,&gt;=0.1.0 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-bigtable&lt;0.29dev,&gt;=0.28.
1 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-container&lt;0.2dev,&gt;=0.1.0
 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-core&lt;0.29dev,&gt;=0.28.0 in
 /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-datastore&lt;1.5dev,&gt;=1.4.0
 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-dns&lt;0.29dev,&gt;=0.28.0 in 
/usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-error-reporting&lt;0.29dev,
&gt;=0.28.0 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-firestore&lt;0.29dev,&gt;=0.28
.0 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-language&lt;1.1dev,&gt;=1.0.0 
in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-logging&lt;1.5dev,&gt;=1.4.0 i
n /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-monitoring&lt;0.29dev,&gt;=0.2
8.0 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-pubsub&lt;0.31dev,&gt;=0.30.0 
in /usr/lib/python3.6/site-packages/google_cloud_pubsub-0.30.1-py3.6
.egg (from google-cloud)
Requirement already satisfied: google-cloud-resource-manager&lt;0.29dev
,&gt;=0.28.0 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-runtimeconfig&lt;0.29dev,&gt;=
0.28.0 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-spanner&lt;0.30dev,&gt;=0.29.0
 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-speech&lt;0.31dev,&gt;=0.30.0 
in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-storage&lt;1.7dev,&gt;=1.6.0 i
n /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-trace&lt;0.18dev,&gt;=0.17.0 i
n /usr/lib/python3.6/site-packages/google_cloud_trace-0.17.0-py3.6.e
gg (from google-cloud)
Requirement already satisfied: google-cloud-translate&lt;1.4dev,&gt;=1.3.0
 in /usr/lib/python3.6/site-packages/google_cloud_translate-1.3.0-py
3.6.egg (from google-cloud)
Requirement already satisfied: google-cloud-videointelligence&lt;1.1dev
,&gt;=1.0.0 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-vision&lt;0.30dev,&gt;=0.29.0 
in /usr/lib/python3.6/site-packages/google_cloud_vision-0.29.0-py3.6
.egg (from google-cloud)
Requirement already satisfied: google-auth&lt;2.0.0dev,&gt;=0.4.0 in /usr/
lib/python3.6/site-packages (from google-api-core&lt;0.2.0dev,&gt;=0.1.2-&gt;
google-cloud)
Requirement already satisfied: pytz in /usr/lib/python3.6/site-packa
ges (from google-api-core&lt;0.2.0dev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: googleapis-common-protos&lt;2.0dev,&gt;=1.5
.3 in /usr/lib/python3.6/site-packages (from google-api-core&lt;0.2.0de
v,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: requests&lt;3.0.0dev,&gt;=2.18.0 in /usr/li
b/python3.6/site-packages (from google-api-core&lt;0.2.0dev,&gt;=0.1.2-&gt;go
ogle-cloud)
Requirement already satisfied: protobuf&gt;=3.0.0 in /usr/lib64/python3
.6/site-packages (from google-api-core&lt;0.2.0dev,&gt;=0.1.2-&gt;google-clou
d)
Requirement already satisfied: six&gt;=1.10.0 in /usr/lib/python3.6/sit
e-packages (from google-api-core&lt;0.2.0dev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: setuptools&gt;=34.0.0 in /usr/lib/python
3.6/site-packages (from google-api-core&lt;0.2.0dev,&gt;=0.1.2-&gt;google-clo
ud)
Requirement already satisfied: google-resumable-media&gt;=0.2.1 in /usr
/lib/python3.6/site-packages (from google-cloud-bigquery&lt;0.29dev,&gt;=0
.28.0-&gt;google-cloud)
Requirement already satisfied: google-gax&lt;0.16dev,&gt;=0.15.7 in /usr/l
ib/python3.6/site-packages (from google-cloud-bigtable&lt;0.29dev,&gt;=0.2
8.1-&gt;google-cloud)
Requirement already satisfied: gapic-google-cloud-datastore-v1&lt;0.16d
ev,&gt;=0.15.0 in /usr/lib/python3.6/site-packages (from google-cloud-d
atastore&lt;1.5dev,&gt;=1.4.0-&gt;google-cloud)
Requirement already satisfied: gapic-google-cloud-error-reporting-v1
beta1&lt;0.16dev,&gt;=0.15.0 in /usr/lib/python3.6/site-packages (from goo
gle-cloud-error-reporting&lt;0.29dev,&gt;=0.28.0-&gt;google-cloud)
Requirement already satisfied: gapic-google-cloud-logging-v2&lt;0.92dev
,&gt;=0.91.0 in /usr/lib/python3.6/site-packages (from google-cloud-log
ging&lt;1.5dev,&gt;=1.4.0-&gt;google-cloud)
Requirement already satisfied: grpc-google-iam-v1&lt;0.12dev,&gt;=0.11.1 i
n /usr/lib/python3.6/site-packages (from google-cloud-pubsub&lt;0.31dev
,&gt;=0.30.0-&gt;google-cloud)
Collecting psutil&lt;6.0dev,&gt;=5.2.2 (from google-cloud-pubsub&lt;0.31dev,&gt;
=0.30.0-&gt;google-cloud)
  Using cached psutil-5.4.3.tar.gz
Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/lib/pyt
hon3.6/site-packages (from google-auth&lt;2.0.0dev,&gt;=0.4.0-&gt;google-api-
core&lt;0.2.0dev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: cachetools&gt;=2.0.0 in /usr/lib/python3
.6/site-packages (from google-auth&lt;2.0.0dev,&gt;=0.4.0-&gt;google-api-core
&lt;0.2.0dev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: rsa&gt;=3.1.4 in /usr/lib/python3.6/site
-packages (from google-auth&lt;2.0.0dev,&gt;=0.4.0-&gt;google-api-core&lt;0.2.0d
ev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: urllib3&lt;1.23,&gt;=1.21.1 in /usr/lib/pyt
hon3.6/site-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-co
re&lt;0.2.0dev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/lib/python
3.6/site-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;
0.2.0dev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: idna&lt;2.7,&gt;=2.5 in /usr/lib/python3.6/
site-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;0.2.
0dev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/lib/pyt
hon3.6/site-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-co
re&lt;0.2.0dev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: dill&lt;0.3dev,&gt;=0.2.5 in /usr/lib/pytho
n3.6/site-packages (from google-gax&lt;0.16dev,&gt;=0.15.7-&gt;google-cloud-b
igtable&lt;0.29dev,&gt;=0.28.1-&gt;google-cloud)
Requirement already satisfied: future&lt;0.17dev,&gt;=0.16.0 in /usr/lib/p
ython3.6/site-packages (from google-gax&lt;0.16dev,&gt;=0.15.7-&gt;google-clo
ud-bigtable&lt;0.29dev,&gt;=0.28.1-&gt;google-cloud)
Requirement already satisfied: ply==3.8 in /usr/lib/python3.6/site-p
ackages (from google-gax&lt;0.16dev,&gt;=0.15.7-&gt;google-cloud-bigtable&lt;0.2
9dev,&gt;=0.28.1-&gt;google-cloud)
Requirement already satisfied: grpcio&lt;2.0dev,&gt;=1.0.2 in /usr/lib64/p
ython3.6/site-packages (from google-gax&lt;0.16dev,&gt;=0.15.7-&gt;google-clo
ud-bigtable&lt;0.29dev,&gt;=0.28.1-&gt;google-cloud)
Requirement already satisfied: oauth2client&lt;4.0dev,&gt;=2.0.0 in /usr/l
ib/python3.6/site-packages (from gapic-google-cloud-datastore-v1&lt;0.1
6dev,&gt;=0.15.0-&gt;google-cloud-datastore&lt;1.5dev,&gt;=1.4.0-&gt;google-cloud)
Requirement already satisfied: proto-google-cloud-datastore-v1[grpc]
&lt;0.91dev,&gt;=0.90.3 in /usr/lib/python3.6/site-packages (from gapic-go
ogle-cloud-datastore-v1&lt;0.16dev,&gt;=0.15.0-&gt;google-cloud-datastore&lt;1.5
dev,&gt;=1.4.0-&gt;google-cloud)
Requirement already satisfied: proto-google-cloud-error-reporting-v1
beta1[grpc]&lt;0.16dev,&gt;=0.15.3 in /usr/lib/python3.6/site-packages (fr
om gapic-google-cloud-error-reporting-v1beta1&lt;0.16dev,&gt;=0.15.0-&gt;goog
le-cloud-error-reporting&lt;0.29dev,&gt;=0.28.0-&gt;google-cloud)
Requirement already satisfied: proto-google-cloud-logging-v2[grpc]&lt;0
.92dev,&gt;=0.91.3 in /usr/lib/python3.6/site-packages (from gapic-goog
le-cloud-logging-v2&lt;0.92dev,&gt;=0.91.0-&gt;google-cloud-logging&lt;1.5dev,&gt;=
1.4.0-&gt;google-cloud)
Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.1 in /usr/lib/pyth
on3.6/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;2.0.0de
v,&gt;=0.4.0-&gt;google-api-core&lt;0.2.0dev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: httplib2&gt;=0.9.1 in /usr/lib/python3.6
/site-packages (from oauth2client&lt;4.0dev,&gt;=2.0.0-&gt;gapic-google-cloud
-datastore-v1&lt;0.16dev,&gt;=0.15.0-&gt;google-cloud-datastore&lt;1.5dev,&gt;=1.4.
0-&gt;google-cloud)
Installing collected packages: psutil
  Running setup.py install for psutil ... error
    Complete output from command /usr/bin/python3 -u -c ""import setu
ptools, tokenize;__file__='/tmp/pip-build-1mt8_94q/psutil/setup.py';
f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\
r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install
 --record /tmp/pip-co20my3o-record/install-record.txt --single-versi
on-externally-managed --compile:
    running install
    running build
    running build_py
    creating build
    creating build/lib.linux-x86_64-3.6
    creating build/lib.linux-x86_64-3.6/psutil
    copying psutil/_exceptions.py -&gt; build/lib.linux-x86_64-3.6/psut
il
    copying psutil/_psposix.py -&gt; build/lib.linux-x86_64-3.6/psutil
    copying psutil/_psbsd.py -&gt; build/lib.linux-x86_64-3.6/psutil
    copying psutil/_psaix.py -&gt; build/lib.linux-x86_64-3.6/psutil
    copying psutil/_pssunos.py -&gt; build/lib.linux-x86_64-3.6/psutil
    copying psutil/_pslinux.py -&gt; build/lib.linux-x86_64-3.6/psutil
    copying psutil/_common.py -&gt; build/lib.linux-x86_64-3.6/psutil
    copying psutil/__init__.py -&gt; build/lib.linux-x86_64-3.6/psutil
    copying psutil/_psosx.py -&gt; build/lib.linux-x86_64-3.6/psutil
    copying psutil/_compat.py -&gt; build/lib.linux-x86_64-3.6/psutil
    copying psutil/_pswindows.py -&gt; build/lib.linux-x86_64-3.6/psuti
l
    creating build/lib.linux-x86_64-3.6/psutil/tests
    copying psutil/tests/__main__.py -&gt; build/lib.linux-x86_64-3.6/p
sutil/tests
    copying psutil/tests/test_bsd.py -&gt; build/lib.linux-x86_64-3.6/p
sutil/tests
    copying psutil/tests/test_contracts.py -&gt; build/lib.linux-x86_64
-3.6/psutil/tests
    copying psutil/tests/test_aix.py -&gt; build/lib.linux-x86_64-3.6/p
sutil/tests
    copying psutil/tests/test_memory_leaks.py -&gt; build/lib.linux-x86
_64-3.6/psutil/tests
    copying psutil/tests/test_connections.py -&gt; build/lib.linux-x86_
64-3.6/psutil/tests
    copying psutil/tests/test_system.py -&gt; build/lib.linux-x86_64-3.
6/psutil/tests
    copying psutil/tests/test_linux.py -&gt; build/lib.linux-x86_64-3.6
/psutil/tests
    copying psutil/tests/test_misc.py -&gt; build/lib.linux-x86_64-3.6/
psutil/tests
    copying psutil/tests/__init__.py -&gt; build/lib.linux-x86_64-3.6/p
sutil/tests
    copying psutil/tests/test_osx.py -&gt; build/lib.linux-x86_64-3.6/p
sutil/tests
    copying psutil/tests/test_process.py -&gt; build/lib.linux-x86_64-3
.6/psutil/tests
    copying psutil/tests/test_windows.py -&gt; build/lib.linux-x86_64-3
.6/psutil/tests
    copying psutil/tests/test_posix.py -&gt; build/lib.linux-x86_64-3.6
/psutil/tests
    copying psutil/tests/test_sunos.py -&gt; build/lib.linux-x86_64-3.6
/psutil/tests
    copying psutil/tests/test_unicode.py -&gt; build/lib.linux-x86_64-3
.6/psutil/tests
    running build_ext
    building 'psutil._psutil_linux' extension
    creating build/temp.linux-x86_64-3.6
    creating build/temp.linux-x86_64-3.6/psutil
    gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -fmessag
e-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fsta
ck-protector-strong -funwind-tables -fasynchronous-unwind-tables -g 
-DOPENSSL_LOAD_CONF -fmessage-length=0 -grecord-gcc-switches -O2 -Wa
ll -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fas
ynchronous-unwind-tables -g -fmessage-length=0 -grecord-gcc-switches
 -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tab
les -fasynchronous-unwind-tables -g -fPIC -DPSUTIL_POSIX=1 -DPSUTIL_
VERSION=543 -DPSUTIL_LINUX=1 -I/usr/include/python3.6m -c psutil/_ps
util_common.c -o build/temp.linux-x86_64-3.6/psutil/_psutil_common.o
    psutil/_psutil_common.c:9:10: fatal error: Python.h: No such fil
e or directory
     #include &lt;Python.h&gt;
              ^~~~~~~~~~
    compilation terminated.
    error: command 'gcc' failed with exit status 1

    ----------------------------------------
Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file_
_='/tmp/pip-build-1mt8_94q/psutil/setup.py';f=getattr(tokenize, 'ope
n', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();ex
ec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-co20m
y3o-record/install-record.txt --single-version-externally-managed --
compile"" failed with error code 1 in /tmp/pip-build-1mt8_94q/psutil/
</code></pre>

<p>I think <strong>GCC</strong> is trying to find <strong>python.h</strong> so the process is exiting with error code 1.</p>",,1,2,,2018-01-30 05:51:41.293 UTC,,2018-01-31 05:54:35.723 UTC,2018-01-30 15:24:51.147 UTC,,9280781,,9280781,1,-2,python|pip|google-cloud-platform|google-cloud-speech,363
Microsoft Custom Vision - Deleting a iteration doesn't delete images associated with it,52276444,Microsoft Custom Vision - Deleting a iteration doesn't delete images associated with it,"<p>On the Microsoft Custom Vision documentation there is this Note: ""...When you delete an iteration, you end up deleting any images that are uniquely associated with it.""</p>

<p>But when I use the Python <code>trainer.delete_iteration(project_id, iteration.id)</code> my images that are uniquely associated with the last trained iteration are not deleted.</p>

<p>Do I need to do something else or this is not working?</p>",,1,0,,2018-09-11 12:52:59.093 UTC,,2018-09-11 21:32:43.463 UTC,2018-09-11 21:32:43.463 UTC,,186013,,9327981,1,0,microsoft-cognitive|azure-cognitive-services|api-cognitive-services,100
How do I create a Flash/torch button for google vision api ($vision) in Jasonette?,54482365,How do I create a Flash/torch button for google vision api ($vision) in Jasonette?,"<p>I'm trying to create a flash/torch button when I'm using the provided $vision api inside of my Jasonette app.</p>

<p>First I have to create the actual flash button in Android Studio and then link it to a Jasonette Action. And I don't know how to do that.</p>

<p>I've tried to use the response of this question here:
<a href=""https://stackoverflow.com/questions/35811411/accessing-autofocus-flash-with-google-vision-barcode-reader"">Accessing AutoFocus/Flash with Google Vision BarCode Reader</a>
But I'm not sure as to how I'd insert it into my existing code, which seems to be wildly different than the one they're using in that example.</p>

<pre><code>void openCamera(Activity context, SurfaceHolder holder, final int side) {
    try {
        if (cameraSource != null) {
            cameraSource.stop();
        }

        cameraSource = new CameraSource
                .Builder(context, detector)
                .setFacing(side)
                .setRequestedFps(20.0f) // this makes it brighter for some reason, idk man
                .setAutoFocusEnabled(true)
              //  I imagine I have to create a function and then insert it here.
                .build();
        cameraSource.start(holder);

        ((JasonViewActivity)context).simple_trigger(""$vision.ready"", new JSONObject(), context);

    } catch (Exception e) {
        Log.d(""Warning"", e.getStackTrace()[0].getMethodName() + "" : "" + e.toString());
    }
}    
</code></pre>

<p>I also posted this on the Jasonelle(jasonette continuation) github issue page, found here:
<a href=""https://github.com/jasonelle/jasonelle/issues/27"" rel=""nofollow noreferrer"">https://github.com/jasonelle/jasonelle/issues/27</a></p>",,0,0,,2019-02-01 15:22:57.323 UTC,,2019-02-01 15:22:57.323 UTC,,,,,10941529,1,0,android-studio|google-vision|jasonette,62
Select records in BigQuery that have no record,47558371,Select records in BigQuery that have no record,"<p>I have the results of a Google Vision API call in BigQuery in a table with a schema that looks like:</p>

<pre><code>image STRING NULLABLE
...
labelAnnotations    RECORD  REPEATED    
labelAnnotations.description    STRING  NULLABLE    
...
</code></pre>

<p>I am able to get all images that have one or more labels with a query like:</p>

<pre><code>  SELECT image,
      count(labelAnnotations.description) as n_labels
  FROM datasetid.tableid,
  UNNEST(labelAnnotations) as labelAnnotations
  GROUP BY 1
</code></pre>

<p>How do I get the <code>image</code> value when there is no labelAnnotations record for a particular image? ie. the API returned an empty labelAnnotations record, or no record at all.</p>

<p>I'm hoping this is obvious, but attempts to use <code>WHERE labelAnnotations IS NULL</code> failed.</p>",47560006,1,0,,2017-11-29 17:24:42.400 UTC,,2017-11-29 20:44:55.253 UTC,2017-11-29 20:44:55.253 UTC,,5780341,,5780341,1,1,google-bigquery,537
similarity search API of Watson Visual Recognition related to bulk volume of images,44362421,similarity search API of Watson Visual Recognition related to bulk volume of images,"<p>We have a customer requirement to search similar images in a collection using Watson Visual Recognition. The documentation mentions that each collection can contain 1 million images. Thus, I have the following questions:</p>

<p>a) What is the maximum size of the image?</p>

<p>b) Each image upload takes up to 1 second and the standard plan has a limit of 25000 images per day. So, can only 25k images added to the collection/day? </p>

<p>c) The customer has about 2 million images. How can we upload the images faster?</p>

<p>d) Is there a separate plan available for bulk volumes?</p>",44396440,2,1,,2017-06-05 05:46:53.123 UTC,,2017-06-06 17:38:44.597 UTC,2017-06-06 00:16:46.203 UTC,,3198917,,6238354,1,0,ibm-cloud|ibm-watson|visual-recognition,181
IBM Watson visual recognition c#,44994201,IBM Watson visual recognition c#,"<p>I'm trying to develop an ASP.Net app ( software app or the web) in visual studio 2013 OR 2015 that uses IBM Watson Visual Recognition service.</p>

<p>I have seen the examples for QA service but it is outdated with the Watson credentials and functionality.
the example: (it requires username and password as credentials, which are not supplied anymore when creating a service):</p>

<p><a href=""http://www.nechai.net/2016/07/05/invoking-the-web-api-of-ibm-watsons-speech-to-text-service-from-net/"" rel=""nofollow noreferrer"">http://www.nechai.net/2016/07/05/invoking-the-web-api-of-ibm-watsons-speech-to-text-service-from-net/</a></p>

<p>This example that I have found looks updated to today's credentials ( the API Key instead of username and pass) but I can't import, open or use the sub projects inside it, visual studio does not know how to recognize it)
""<a href=""https://github.com/watson-developer-cloud/visual-recognition-aspnet"" rel=""nofollow noreferrer"">https://github.com/watson-developer-cloud/visual-recognition-aspnet</a>""</p>

<p>the two inner projects that the project rely on are VisualRecognition and WatsonServices projects in the main project.
They have a project file with the xproj extension file, which visual studio 2013 AND 2015 seems to not recognize so I can't try it or reuse its code in my test app.</p>

<p>the above example project is too complicated to just grab the code and try it (after failing to import and making it work on VS 2013)</p>

<p>Is there a very simple example on how to connect to a watson service using this type of credentials? :</p>

<p>""credentials"": {
""url"": ""<a href=""https://gateway-a.watsonplatform.net/visual-recognition/api"" rel=""nofollow noreferrer"">https://gateway-a.watsonplatform.net/visual-recognition/api</a>"",
""note"": ""It may take up to 5 minutes for this key to become active"",
""api_key"": ""********************************************************""
}</p>

<p>I have also tried to install the Watson services SDK by Nuget and by downloading the source and opening it in VS (for visual recognition specifically and the whole services option as well) but with no luck as well.
When opening the source code in VS, it says ""incompatible"" with all the files.
""<a href=""https://github.com/watson-developer-cloud/dotnet-standard-sdk"" rel=""nofollow noreferrer"">https://github.com/watson-developer-cloud/dotnet-standard-sdk</a>""</p>

<p>When trying to install with Nuget I get Errors:
in VS2013:
Install-Package: Could not install package 'IBM.WatsonDeveloperCloud.VisualRecognition.v3 1.0.0'. You are trying to install this package into a project that targets '.NETFram
ework,Version=v4.6', but the package does not contain any assembly references or content files that are compatible with that framework. For more information, contact the package author.
At line:1 char:1
+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException
+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommand</p>

<p>in VS 2015:
Install-Package : An error occurred while retrieving package metadata for 'Newtonsoft.Json.10.0.3' from source 'd:\Users*****\Documents\Visual Studio 2015\Projects\FaceDetection\packages'.
At line:1 char:1
+ Install-Package IBM.WatsonDeveloperCloud.VisualRecognition.v3
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ CategoryInfo : NotSpecified: (:) [Install-Package], Exception
+ FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PackageManagement.PowerShellCmdlets.InstallPackageCommand</p>

<p>There is no example on how to use or install the SDKs except for using Nuget so I'm lost here.</p>",,1,0,,2017-07-09 07:46:35.063 UTC,,2017-08-24 18:15:32.257 UTC,2017-08-24 18:15:32.257 UTC,,1033581,,8278397,1,-3,c#|ibm-watson,795
How to disable DEBUG log,49819964,How to disable DEBUG log,"<p>So, I'm making a java app using the Google Cloud Vision API and the method is returning many DEBUG logs to my console. I would like to disable them, but I don't know how. I'm getting this output <a href=""https://pastebin.com/gVVJprhV"" rel=""nofollow noreferrer"">https://pastebin.com/gVVJprhV</a></p>

<p>This is my code </p>

<pre><code>public String googleapi(String image) throws IOException, Exception {

        List&lt;AnnotateImageRequest&gt; requests = new ArrayList&lt;&gt;();

        ByteString imgBytes = ByteString.readFrom(new FileInputStream(image));

        Image img = Image.newBuilder().setContent(imgBytes).build();
        Feature feat = Feature.newBuilder().setType(Type.TEXT_DETECTION).build();
        AnnotateImageRequest request =
                AnnotateImageRequest.newBuilder().addFeatures(feat).setImage(img).build();
        requests.add(request);

        String question = null;

        try (ImageAnnotatorClient client = ImageAnnotatorClient.create()) {
            BatchAnnotateImagesResponse response = client.batchAnnotateImages(requests);
            List&lt;AnnotateImageResponse&gt; responses = response.getResponsesList();

            for (AnnotateImageResponse res : responses) {
                if (res.hasError()) {
                    //System.out.printf(""Error: %s\n"", res.getError().getMessage());
                    System.out.println(""Se ta bom, n mexas"");
                    return null;
                }

                EntityAnnotation annotation = res.getTextAnnotations(0);
                question = annotation.getDescription();
                question = question.replaceAll(""\n"", "" "");
            }

        }
        return question;
    }
</code></pre>

<p>I don't know why it's throwing out all this debug, but I would like to disable it. Thanks in advance.</p>",,1,2,,2018-04-13 15:00:44.733 UTC,,2018-05-04 20:40:29.390 UTC,,,,,7558274,1,1,java|google-cloud-platform|vision-api,336
Seeking IBM Watson credentials fix,50733961,Seeking IBM Watson credentials fix,"<p>I am following instructions from a github page documentation. And I am expected to provide in my API key, which I believe was auto-generated when I first signed up for IBM Watson - Visual Recognition.</p>

<p><code>curl -X POST -u ""apikey:{INSERT-YOUR-IAM-APIKEY-HERE}""</code></p>

<p>Actually, I am posting a few zip files into IBM-Watson visual recognition and when I just do that I get following error -</p>

<p><code>413 Request Entity Too Large</code></p>

<p>As per the github doc, I am expected to be given a classifier ID. But I get request too large error.</p>

<p>So I did the obvious and tried to post one zip file in my curl command that's when I learnt I don't have my credentials set properly.. can you please help?</p>

<p><code>{""code"":401, ""error"": ""Unauthorized""}</code> , I get this error when I try posting one zip file instead of posting a few, as said before.</p>",,1,0,,2018-06-07 06:04:37.870 UTC,,2018-06-07 13:13:58.257 UTC,2018-06-07 13:03:42.957 UTC,,1167890,,9906977,1,0,api|curl|api-key|visual-recognition,145
"In Android, How to draw a rect on a QR based on the points fetched by the detector?",54213952,"In Android, How to draw a rect on a QR based on the points fetched by the detector?","<p>I am trying to make a QR code scanner using google vision api in Android. I am successful in detecting the QR code and fetching the content, i am using <strong>Detector</strong> class to detect the QR. After scanning, I stop the camera preview and  want to draw a box around the scanned QR. For this i am getting the Scanned QR points(X,Y co-ordinates) from the detector class.But these points are not matching the QR image in the camera source preview. <a href=""https://i.stack.imgur.com/7nas5.jpg"" rel=""nofollow noreferrer"">Here is an screenshot of scanned QR</a></p>

<p>Below is the method which will be called when a QR is detected and in this method i am fetching the QR codes then stopping the camera and calling the draw rectangle method with QR position to draw a box in place of QR code </p>

<pre><code> @Override
 public void receiveDetections(Detector.Detections&lt;Barcode&gt; detections) 
 {
     final SparseArray&lt;Barcode&gt; qrCodes = detections.getDetectedItems();
     final Detector.Detections&lt;Barcode&gt; frame =   detections;

            if (qrCodes.size() != 0 )
            {
                scannerDetector.release();
                Handler handler = new Handler(Looper.getMainLooper());
                handler.post(new Runnable()
                {
                    @Override
                    public void run() 
                    {
                        cameraSource.stop();//Stopping the camera after a QR is detected

                        for (int i = 0; i &lt; qrCodes.size(); i++)
                        {
                            final int key = qrCodes.keyAt(i);

                            // this gives the rect co-ordinates of the detected QR
                            Rect rect = qrCodes.get(key).getBoundingBox();

                            drawRectangle(rect);                            

                            break;
                        }
                    }

                });
             }
 } 

 private void drawRectangle(final Rect rect)
 {
   runOnUiThread(new Runnable()
   {
       @Override
       public void run()
       {
          // detected frame is a constraint layout and view returned from Draw Rectangle class will be  added as its subview
           detectedFrame.addView(new DrawRectangle(QRCodeScannerActivity.this, rect)); 
       }
   });
 }
</code></pre>

<p>Here i am drawing a Box with the rect co-ordinates that i got from the detector.</p>

<pre><code>public class DrawRectangle extends View
{ 
Rect rect;
public DrawRectangle(Context context, Rect rect )
{
    super(context);
    this.rect = rect;
}

@Override
protected void onDraw(Canvas canvas)
 {
    super.onDraw(canvas);

    Paint paint = new Paint();

    //Rectangle
    paint.setColor(Color.BLUE);
    paint.setStrokeWidth(5);
    paint.setStyle(Paint.Style.STROKE);

    try
    {

       canvas.drawRect(rect,paint);
    }
    catch (Exception e)
    {

    }
  }
}
</code></pre>",,0,0,,2019-01-16 09:29:45.553 UTC,,2019-01-16 11:39:07.780 UTC,2019-01-16 11:39:07.780 UTC,,8599695,,8599695,1,0,java|android|qr-code|barcode-scanner|google-vision,58
whether Amazon Rekognition or Azure Cognitive Face can use for face recognition based attendance system,51711390,whether Amazon Rekognition or Azure Cognitive Face can use for face recognition based attendance system,<p>I want to avoid all the spoofing and other violence related to face recognition security application. which provider would you prefer to use from Amazon Rekognition or Azure Cognitive Face?</p>,,0,0,,2018-08-06 15:47:53.780 UTC,1,2018-08-06 15:47:53.780 UTC,,,,,2398490,1,1,azure|amazon,51
Google Cloud AutoML predict service returned 'Internal error encountered',55323321,Google Cloud AutoML predict service returned 'Internal error encountered',"<p>I trained a model on Google cloud vision AutoML service and whenever I try to predict an image from the console it returned 'Internal error encountered'. this is also happening from the API. it returns this json</p>

<pre><code>{
    ""error"": {
        ""code"": 500,
        ""message"": ""Internal error encountered."",
        ""status"": ""INTERNAL""
    }
}
</code></pre>

<p>The model has been training for 24 hours</p>

<p>it should return the image predicated classes as trained by the model</p>",,1,1,,2019-03-24 11:29:44.827 UTC,,2019-04-09 05:50:56.587 UTC,,,,,2943431,1,0,google-cloud-platform|google-cloud-vision|automl|google-cloud-automl,81
Is it good practice to make images gray-scaled before sending them to Google Cloud Vision API for OCR?,47478044,Is it good practice to make images gray-scaled before sending them to Google Cloud Vision API for OCR?,"<p>I'm using Google Cloud Vision API to perform OCR on my documents. In <a href=""https://cloud.google.com/vision/docs/supported-files"" rel=""nofollow noreferrer"">optimization</a> page, there is no information about color-space of images, but for OCR processing, color depth usually doesn't matter. So instead of making images smaller than the original one, we can limit the color of documents for example in gray-scaled format and send the processed image to be OCRed.</p>

<p>My question is:</p>

<p>1- Does OCR of Vision API care about colors for example in RBG format?</p>

<p>2- If so, how much the accuracy will be effected by making images gray-scaled? is it worth it?</p>

<p>3- If not, why isn't it documented in the optimization page?</p>",,0,3,,2017-11-24 17:45:37.747 UTC,1,2017-11-24 17:45:37.747 UTC,,,,,5717111,1,1,image-processing|google-cloud-platform|ocr|google-cloud-vision,90
"What is the ""annotate"" method in Google Vision API?",41254591,"What is the ""annotate"" method in Google Vision API?","<p>I am reading the Google Vision API documentation:
(<a href=""https://cloud.google.com/vision/docs/requests-and-responses"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/requests-and-responses</a>)
It says something like the follwing:</p>

<pre><code>Currently, the Vision API consists of one collection (images) which supports one HTTP Request method (annotate):
POST https://vision.googleapis.com/v1/images:annotate
</code></pre>

<p>My question is what does it mean by the method ""annotate""? Also, how do I read this syntax with the colon "":""? Is this just a notation that Google uses or some kind of industry standard where you use the colon and calling the stuff after a ""method""?</p>

<p>I am a financial Java developer but noob to Web/HTTP technology (I have read some basic of GET/POST but that did not seem to help me with this question). If it seems to you that I am totally lacking in some fundamentals, is there any pointer for me to read up some related books/website/tutorial/documentation that can help me understand this better? Any help is appreciated!</p>",,0,0,,2016-12-21 03:24:25.843 UTC,1,2016-12-21 03:24:25.843 UTC,,,,,1141665,1,2,google-api|http-post,244
TypeError: vision.labelDetection is not a function,47109354,TypeError: vision.labelDetection is not a function,"<p>I'm trying to get Google Cloud Vision API working within NodeJS using official Google documentation and keep running into the following error. I checked multiple times, have correctly installed @google-cloud/vision using npm, everything is up to date.</p>

<blockquote>
  <p>TypeError: vision.labelDetection is not a function
      at /home/pi/functions/server.js:34:12
      at Layer.handle [as handle_request] (/home/pi/functions/node_modules/express/lib/router/layer.js:95:5)
      at next (/home/pi/functions/node_modules/express/lib/router/route.js:137:13)
      at Route.dispatch (/home/pi/functions/node_modules/express/lib/router/route.js:112:3)
      at Layer.handle [as handle_request] (/home/pi/functions/node_modules/express/lib/router/layer.js:95:5)
      at /home/pi/functions/node_modules/express/lib/router/index.js:281:22
      at Function.process_params (/home/pi/functions/node_modules/express/lib/router/index.js:335:12)
      at next (/home/pi/functions/node_modules/express/lib/router/index.js:275:10)
      at SendStream.error (/home/pi/functions/node_modules/serve-static/index.js:121:7)
      at emitOne (events.js:115:13)</p>
</blockquote>

<p>I have been trying to get this to work for hours upon hours, and arrived at a dead end. Have tried everything I could come up with but it keeps telling me the function doesn't exist.</p>",,0,3,,2017-11-04 09:37:55.080 UTC,,2017-11-04 10:39:58.953 UTC,2017-11-04 10:39:58.953 UTC,,7735875,,7735875,1,0,javascript|node.js|express|google-cloud-platform|google-cloud-vision,97
AWS face rekognition Bot on Raspberry Pi,49470158,AWS face rekognition Bot on Raspberry Pi,"<p>I am following a tutorial (<a href=""https://github.com/just4give/raspi-dexter-lambda"" rel=""nofollow noreferrer"">https://github.com/just4give/raspi-dexter-lambda</a>) that utilizes AWS rekognition on a raspberry pi to make a robot. I am having trouble figuring out how to create logs to see where the code is hanging up.</p>

<p>Here is the lambda function I need to debug:</p>

<pre><code>'use strict';
const request = require('request');
const AWS = require('aws-sdk');
AWS.config.update({region:'us-east-1'});
const rekognition = new AWS.Rekognition();
const moment = require('moment-timezone');
const sns = new AWS.SNS();
const s3 = new AWS.S3();

var config = {
  ""awsRegion"":""us-east-1"",
  ""s3Bucket"":""raspi118528"",
  ""awsFaceCollection"":""raspifacecollection""
}
module.exports.bot = (event, context, callback) =&gt; {
  try {
        // By default, treat the user request as coming from the America/New_York time zone.

        var intentName= event.currentIntent.name;
        console.log('intentName',intentName);

        switch (intentName) {
          case 'HelloIntent':
                  request(
                    {
                      url : process.env.URL_CAPTURE,
                      method:'POST',
                      headers : {
                      ""Authorization"" : ""Basic "" + new Buffer( ""raspi:secret"").toString(""base64"")
                    }
                    },
                    function (error, response, body) {

                        if(error){
                          callback(error);
                        }else{
                          body = JSON.parse(body);
                          console.log('body',body);
                          console.log('now',new Date());


                          if(body.status ==='matched'){
                            var hour = parseInt(moment().tz('America/New_York').format('h'));
                            console.log('hour',hour);
                            var content="""";

                            if(hour &gt;= 5 &amp;&amp; hour&lt;12){
                              content =""Good morning, ""+ body.message+"" ! Have a nice day!"";
                            }else if(hour &gt;=12 &amp;&amp; hour &lt;18){
                              content =""Good afternoon, ""+ body.message+"" ! Have a nice rest of the day!"";
                            }else if(hour &gt;=18 &amp;&amp; hour &lt; 21){
                              content =""Good evening, ""+ body.message+"" ! Talk to you soon!"";
                            }else{
                              content =""Hi, ""+ body.message+"" ! It's been a long day! Good Night!"";
                            }

                            const url = s3.getSignedUrl('getObject', {
                                Bucket: config.s3Bucket,
                                Key: body.key,
                                Expires: 300
                            })

                            //console.log(url)

                            sns.publish({
                              Message: 'I just saw '+ body.message+' . See the image I captured. Link will expire in 5 minutes. ' + url,
                              TopicArn: 'arn:aws:sns:us-east-1:027378352884:raspiFaceTextMessage'
                            }, function (err, data) {
                              if(err){
                                console.log(""error"", err);
                              }else{
                                console.log('success',data);
                              }

                            });

                            callback(null,{
                                sessionAttributes: {key: body.key},
                                dialogAction: {
                                    type: 'Close',
                                    fulfillmentState:'Fulfilled',
                                    message:{
                                        contentType: 'PlainText',
                                        content: content
                                    }

                                },
                            })
                          }else if(body.status ==='error'){
                            callback(null,{
                                sessionAttributes: {key: body.key},
                                dialogAction: {
                                    type: 'Close',
                                    fulfillmentState:'Fulfilled',
                                    message:{
                                        contentType: 'PlainText',
                                        content: body.message
                                    }

                                },
                            })
                          }else if(body.status==='unmatched'){
                            callback(null,{
                                sessionAttributes: {key: body.key},
                                dialogAction: {
                                    type: 'ElicitIntent',
                                    //fulfillmentState:'Fulfilled',
                                    message:{
                                        contentType: 'PlainText',
                                        content: body.message
                                    }

                                },
                            })
                          }


                        }
                    }
                  );

              break;

          case 'NameIntent':
                    var name = event.currentIntent.slots.name;
                    var key = event.sessionAttributes.key;
                    var params = {
                        ""CollectionId"": config.awsFaceCollection,
                        ""DetectionAttributes"": [ ""ALL"" ],
                        ""ExternalImageId"": name,
                        ""Image"": {
                          ""S3Object"": {
                             ""Bucket"": config.s3Bucket,
                             ""Name"": key
                          }
                        }
                      }

                    rekognition.indexFaces(params, function(err, data) {
                        if (err) {
                          console.log(err, err.stack);
                        }else{
                          //console.log(data);

                          const url = s3.getSignedUrl('getObject', {
                              Bucket: config.s3Bucket,
                              Key: key,
                              Expires: 300
                          })

                          //console.log(url)

                          sns.publish({
                            Message: 'I stored face of '+ name+' . See the image I loaded in my brain. Link will expire in 5 minutes. ' + url,
                            TopicArn: 'arn:aws:sns:us-east-1:027378352884:raspiFaceTextMessage'
                          }, function (err, data) {
                            if(err){
                              console.log(""error"", err);
                            }else{
                              console.log('success',data);
                            }

                          });

                          callback(null,{
                              sessionAttributes: {key: key},
                              dialogAction: {
                                  type: 'Close',
                                  fulfillmentState:'Fulfilled',
                                  message:{
                                      contentType: 'PlainText',
                                      content: ""I stored your picture ""+name+"" . Have a nice day!""
                                  }

                              },
                          })
                        }

                    });

              break;

              case 'ClearAllFacesIntent':
                var params = {
                    ""CollectionId"": config.awsFaceCollection,
                    ""MaxResults"": 20
                }

                rekognition.listFaces(params, function(err, data) {
                    if (err) console.log(err, err.stack); // an error occurred
                      else     {
                      console.log(data);
                      var faceIds=[];
                      data.Faces.forEach(function(face){
                        faceIds.push(face.FaceId);
                      })

                    if(faceIds.length&gt;0){
                      var params = {
                      CollectionId: config.awsFaceCollection,
                      FaceIds: faceIds
                      };
                      rekognition.deleteFaces(params, function(err, data) {

                        if (err) {
                          callback(null,{
                              sessionAttributes: {key: key},
                              dialogAction: {
                                  type: 'Close',
                                  fulfillmentState:'Fulfilled',
                                  message:{
                                      contentType: 'PlainText',
                                      content: ""Something went wrong! I could not delete faces!""
                                  }

                              },
                          })
                        }
                        else   {
                          callback(null,{
                              sessionAttributes: {key: key},
                              dialogAction: {
                                  type: 'Close',
                                  fulfillmentState:'Fulfilled',
                                  message:{
                                      contentType: 'PlainText',
                                      content: ""I have deleted all the faces from memory!""
                                  }

                              },
                          })
                        }

                      });
                    }

                    }

                 });

                    break;

              case 'CloseIntent':
                    callback(null,{
                        sessionAttributes: {},
                        dialogAction: {
                            type: 'Close',
                            fulfillmentState:'Fulfilled',
                            message:{
                                contentType: 'PlainText',
                                content: ""Good bye!""
                            }

                        },
                    })

              break;
          default:

        }




    } catch (err) {
        callback(err);
    }
};
</code></pre>

<p>This lambda is called upon by the bot in AWS. Whenever I use the NameIntent to say ""My name is David"" the server just asks me to repeat the statement.</p>

<p>Here is the logs in AWS console:
<a href=""https://gyazo.com/ce778072ff659f727d0ccf227b466a3b"" rel=""nofollow noreferrer"">https://gyazo.com/ce778072ff659f727d0ccf227b466a3b</a></p>

<p>Please and Thank You for any help,
David</p>",,1,0,,2018-03-24 21:31:32.893 UTC,,2018-03-26 09:31:50.160 UTC,,,,,9493647,1,-1,javascript|amazon-web-services|aws-lambda|robot,114
AWS Android SDK Rekognition 2.4.4 detectFaces fails with null object reference,45160721,AWS Android SDK Rekognition 2.4.4 detectFaces fails with null object reference,"<p>I am trying to do a simple face detect call using AWS Android SDK Reckognition 2.4.4.  Can someone point what is going wrong?</p>

<p>I am getting the following error </p>

<pre><code>FATAL EXCEPTION: main
Process: com.indus.myfirstapp, PID: 8887
java.lang.RuntimeException: Failure delivering result ResultInfo{who=null, request=1, result=-1, data=Intent { act=inline-data (has extras) }} to activity {com.indus.myfirstapp/com.indus.myfirstapp.MainActivity}: android.os.NetworkOnMainThreadException
at android.app.ActivityThread.deliverResults(ActivityThread.java:3929)
at android.app.ActivityThread.handleSendResult(ActivityThread.java:3972)
at android.app.ActivityThread.-wrap16(ActivityThread.java)
at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1537)
at android.os.Handler.dispatchMessage(Handler.java:111)
at android.os.Looper.loop(Looper.java:207)
at android.app.ActivityThread.main(ActivityThread.java:5728)
at java.lang.reflect.Method.invoke(Native Method)
at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:789)
at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:679)
Caused by: android.os.NetworkOnMainThreadException
at android.os.StrictMode$AndroidBlockGuardPolicy.onNetwork(StrictMode.java:1288)
at java.net.InetAddress.lookupHostByName(InetAddress.java:432)
at java.net.InetAddress.getAllByNameImpl(InetAddress.java:253)
at java.net.InetAddress.getAllByName(InetAddress.java:215)
at com.android.okhttp.internal.Network$1.resolveInetAddresses(Network.java:29)
at com.android.okhttp.internal.http.RouteSelector.resetNextInetSocketAddress(RouteSelector.java:188)
at com.android.okhttp.internal.http.RouteSelector.nextProxy(RouteSelector.java:157)
at com.android.okhttp.internal.http.RouteSelector.next(RouteSelector.java:100)
at com.android.okhttp.internal.http.HttpEngine.createNextConnection(HttpEngine.java:368)
at com.android.okhttp.internal.http.HttpEngine.nextConnection(HttpEngine.java:351)
at com.android.okhttp.internal.http.HttpEngine.connect(HttpEngine.java:341)
at com.android.okhttp.internal.http.HttpEngine.sendRequest(HttpEngine.java:259)
at com.android.okhttp.internal.huc.HttpURLConnectionImpl.execute(HttpURLConnectionImpl.java:454)
at com.android.okhttp.internal.huc.HttpURLConnectionImpl.connect(HttpURLConnectionImpl.java:114)
at com.android.okhttp.internal.huc.HttpURLConnectionImpl.getOutputStream(HttpURLConnectionImpl.java:245)
at com.android.okhttp.internal.huc.DelegatingHttpsURLConnection.getOutputStream(DelegatingHttpsURLConnection.java:218)
at com.android.okhttp.internal.huc.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java)
at com.amazonaws.http.UrlHttpClient.writeContentToConnection(UrlHttpClient.java:162)
at com.amazonaws.http.UrlHttpClient.execute(UrlHttpClient.java:75)
at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:371)
at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:212)
at com.amazonaws.services.rekognition.AmazonRekognitionClient.invoke(AmazonRekognitionClient.java:1229)
at com.amazonaws.services.rekognition.AmazonRekognitionClient.detectFaces(AmazonRekognitionClient.java:628)
at com.indus.myfirstapp.MainActivity.onActivityResult(MainActivity.java:95)
at android.app.Activity.dispatchActivityResult(Activity.java:6500)
at android.app.ActivityThread.deliverResults(ActivityThread.java:3925)
at android.app.ActivityThread.handleSendResult(ActivityThread.java:3972) 
at android.app.ActivityThread.-wrap16(ActivityThread.java) 
at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1537) 
at android.os.Handler.dispatchMessage(Handler.java:111) 
at android.os.Looper.loop(Looper.java:207) 
at android.app.ActivityThread.main(ActivityThread.java:5728) 
at java.lang.reflect.Method.invoke(Native Method) 
at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:789) 
at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:679) 
</code></pre>

<p>Here is the code</p>

<pre><code>package com.indus.myfirstapp;

import android.content.Intent;
import android.graphics.Bitmap;
import android.os.AsyncTask;
import android.os.Bundle;
import android.os.Environment;
import android.provider.MediaStore;
import android.support.v7.app.AppCompatActivity;
import android.util.Log;
import android.view.View;
import android.widget.ImageView;

import com.amazonaws.auth.CognitoCachingCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.rekognition.AmazonRekognition;
import com.amazonaws.services.rekognition.AmazonRekognitionClient;
import com.amazonaws.auth.AWSCredentialsProvider;
import com.amazonaws.services.rekognition.model.Attribute;
import com.amazonaws.services.rekognition.model.DetectFacesRequest;
import com.amazonaws.services.rekognition.model.DetectFacesResult;
import com.amazonaws.services.rekognition.model.Image;

import java.io.ByteArrayOutputStream;
import java.io.File;
import java.nio.ByteBuffer;




public class MainActivity extends AppCompatActivity {
    //public static final String EXTRA_MESSAGE = ""com.indus.myfirstapp.MESSAGE"";
    static final int REQUEST_IMAGE_CAPTURE = 1;
    static AmazonRekognition client = null;
    Image searchImage;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);

        //call
        new CredRetriever().execute();
    }

    class CredRetriever extends AsyncTask&lt;Void, Void, Void&gt; {
        @Override
        protected Void doInBackground(Void... params) {
            AWSCredentialsProvider credentialsProvider = new CognitoCachingCredentialsProvider(
                    getApplicationContext(),
                    ""us-west-2:xxxxx"", // Identity pool ID
                    Regions.US_WEST_2 // Region
            );
            Log.i(""TEST"",credentialsProvider.getCredentials().toString());
            client = new AmazonRekognitionClient(credentialsProvider);
            return null;
        }
    }

    public void startCamera(View view) {
        Intent takePictureIntent = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);
        if (takePictureIntent.resolveActivity(getPackageManager()) != null) {
            startActivityForResult(takePictureIntent, REQUEST_IMAGE_CAPTURE);
        }
    }

    public void onActivityResult(int requestCode, int resultCode, Intent data) {
        if (requestCode == REQUEST_IMAGE_CAPTURE &amp;&amp; resultCode == RESULT_OK) {
            Bundle extras = data.getExtras();
            Bitmap imageBitmap = (Bitmap) extras.get(""data"");

            ByteArrayOutputStream stream = new ByteArrayOutputStream();
            imageBitmap.compress(Bitmap.CompressFormat.JPEG,100,stream);

            ByteBuffer imageBytes = ByteBuffer.wrap(stream.toByteArray());
            searchImage = new Image();
            searchImage.withBytes(imageBytes);

            ImageView imageView = (ImageView) findViewById(R.id.imageView);
            imageView.setImageBitmap(imageBitmap);

            DetectFacesRequest request = new DetectFacesRequest()
                    .withAttributes(Attribute.ALL.toString())
                    .withImage(searchImage);
            DetectFacesResult result = client.detectFaces(request);
            result.getFaceDetails();


        }
    }

}
</code></pre>",45202874,1,0,,2017-07-18 07:56:37.473 UTC,,2018-10-29 14:52:31.900 UTC,2018-10-29 14:52:31.900 UTC,,4826457,,8323074,1,1,android|amazon-web-services|amazon-rekognition,435
How amazon Rekognition handles data privacy?,51817443,How amazon Rekognition handles data privacy?,"<p>My question is a little bit general, we want to build a solution based on amazon rekognition. But we want to make sure that amazon don't keep our data after the process is completed for example. When i use the detect_text function in boto3 like this.</p>

<pre><code>response = client.detect_text(Image={'Bytes': images_bytes})
</code></pre>

<p>After i get the response, what happen to the images_bytes that has been uploaded to amazon for processing? Is it automatically destroyed or amazon keeps it locally?</p>",,1,0,,2018-08-13 07:42:13.700 UTC,,2018-08-14 06:59:20.857 UTC,,,,,4841787,1,0,amazon-web-services|amazon-rekognition,45
API for Google translate app like feature for website,44025456,API for Google translate app like feature for website,"<p>Is there any API for Google translate app like feature for website in which user upload an image and start selecting image textual area and he/she get the text? </p>

<p>I found google vision API but its not just what i want. I want that image should be showing on website and user is selecting textual part of the image with mouse and user is getting the same text in some variable so that he/she can print the text wherever required.</p>",,0,1,,2017-05-17 12:53:29.503 UTC,,2017-05-19 17:20:22.390 UTC,2017-05-19 17:20:22.390 UTC,,502381,,4119808,1,1,javascript|google-api|ocr|image-recognition,65
How to change CameraSource size like setPreviewSize and PictureSize,47656251,How to change CameraSource size like setPreviewSize and PictureSize,"<p>This is pertaining the google vision API. The CameraPreview works well for full Screen. But the image is distorted when previewing the camera in a box.</p>

<p>This is the code I have used. My problem is that the preview fills up the box but the preview is stretched and I am not able to adjust the size of the cameraPreview. The variable CameraPreview is of type SurfaceView. I use a CameraSource to start the preview. Is there any way to change the size of the CameraPreview(SurfaceView) and make the preview crop fit in a box?</p>

<pre><code>  cameraPreview.getHolder().addCallback(new SurfaceHolder.Callback() {
            protected List&lt;Camera.Size&gt; mPreviewSizeList;
            protected List&lt;Camera.Size&gt; mPictureSizeList;
            protected Camera.Size mPreviewSize;
            protected Camera.Size mPictureSize;
            protected boolean mSurfaceChanged = false;
            private int mSurfaceChangedCallDepth = 0;
            private int mCenterPosX = -1;
            private int mCenterPosY = 0;
            @Override
            public void surfaceCreated(SurfaceHolder surfaceHolder) {
                if (ActivityCompat.checkSelfPermission(getApplicationContext(), Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {
                    ActivityCompat.requestPermissions(ClientIdScannerActivity.this,
                            new String[]{Manifest.permission.CAMERA}, RequestCameraPermissionId);
                    Toast.makeText(ClientIdScannerActivity.this, ""The App requires this permission"", Toast.LENGTH_SHORT).show();
                    return;
                }
                try {
                    cameraSource.start(cameraPreview.getHolder());
                } catch (Exception ex) {
                    ex.printStackTrace();
                }
            }
            @Override
            public void surfaceChanged(SurfaceHolder surfaceHolder, int i, int i1, int i2) {
                mSurfaceChangedCallDepth++;

             /*   cameraSource.stop();

                Camera.Parameters cameraParams = mCamera.getParameters();

                if (!mSurfaceChanged) {
                    Camera.Size previewSize = determinePreviewSize(cameraWidth, cameraHight);
                    Camera.Size pictureSize = determinePictureSize(previewSize);
                    mPreviewSize = previewSize;
                    mPictureSize = pictureSize;
                    mSurfaceChanged = adjustSurfaceLayoutSize(previewSize,i1, i2);

                    if (mSurfaceChanged &amp;&amp; (mSurfaceChangedCallDepth &lt;= 1)) {
                        return;
                    }
                }

                cameraParams.setRotation(270);
                if (UtilFunctions.isTablet()) {
                } else {
                    mCamera.setDisplayOrientation(Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.N ? 270 : 90);
                }*/

//            cameraParams.setRotation(270);
//            mCamera.setDisplayOrientation(UtilFunctions.isTablet() ? 90 : 270);

              /*  cameraParams.set(""orientation"", ""portrait"");
                //  cameraParams.setFocusMode(Camera.Parameters.FOCUS_MODE_CONTINUOUS_PICTURE);
                cameraParams.setPreviewSize(mPreviewSize.width, mPreviewSize.height);
                cameraParams.setPictureSize(mPictureSize.width, mPictureSize.height);
                if (cameraParams.getSupportedFocusModes().contains(
                        Camera.Parameters.FOCUS_MODE_CONTINUOUS_PICTURE)) {
                    cameraParams.setFocusMode(Camera.Parameters.FOCUS_MODE_CONTINUOUS_PICTURE);
                }*/

                mSurfaceChanged = false;

                try {
                    if (ActivityCompat.checkSelfPermission(ClientIdScannerActivity.this, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {
                        // TODO: Consider calling
                        //    ActivityCompat#requestPermissions
                        // here to request the missing permissions, and then overriding
                        //   public void onRequestPermissionsResult(int requestCode, String[] permissions,
                        //                                          int[] grantResults)
                        // to handle the case where the user grants the permission. See the documentation
                        // for ActivityCompat#requestPermissions for more details.
                        Toast.makeText(ClientIdScannerActivity.this, ""App requires Camera permission to scan ID"", Toast.LENGTH_SHORT).show();
                        return;
                    }
                    cameraSource.start(cameraPreview.getHolder());
                } catch (Exception e) {
                    // Remove failed size

                    // Reconfigure
                    if (mPreviewSizeList.size() &gt; 0) { // prevent infinite loop
                        surfaceChanged(null, 0, i1, i2);
                    } else {
//                Utils.showToast(CameraNewActivity.this, ""Can't start preview"", Toast.LENGTH_LONG);
                    }
                }

                mSurfaceChangedCallDepth--;
            }

            protected Camera.Size determinePreviewSize(int reqWidth, int reqHeight) {

                int reqPreviewWidth = 640;//reqHeight; // requested width in terms of camera hardware
                int reqPreviewHeight = 480;// reqWidth; // requested height in terms of camera hardware
                Camera.Size retSize = null;
                for (Camera.Size size : mPreviewSizeList) {
                    if (size.width == reqPreviewWidth &amp;&amp; size.height == reqPreviewHeight) {
                        retSize = size;
                    }
                }
                //  retSize = mPreviewSizeList.get(mPreviewSizeList.size()-1);
                return retSize;
            }

            protected Camera.Size determinePictureSize(Camera.Size previewSize) {
                Camera.Size retSize = null;
                for (Camera.Size size : mPictureSizeList) {
                    if (size.equals(previewSize)) {
                        return size;
                    }
                }

                // if the preview size is not supported as a picture size
                float reqRatio = ((float) previewSize.width) / previewSize.height;
                float curRatio, deltaRatio;
                float deltaRatioMin = Float.MAX_VALUE;
                for (Camera.Size size : mPictureSizeList) {
                    curRatio = ((float) size.width) / size.height;
                    deltaRatio = Math.abs(reqRatio - curRatio);
                    if (deltaRatio &lt; deltaRatioMin) {
                        deltaRatioMin = deltaRatio;
                        retSize = size;
                    }
                }

                return retSize;
            }

            protected boolean adjustSurfaceLayoutSize(Camera.Size previewSize,
                                                      int availableWidth, int availableHeight) {

                float tmpLayoutHeight = previewSize.width;
                float tmpLayoutWidth = previewSize.height;

                float factH, factW, fact;
                factH = availableHeight / tmpLayoutHeight;
                factW = availableWidth / tmpLayoutWidth;

//        if (factH &lt; factW) {
//            fact = factH;
//        } else {
                fact = factW;
                //  }

                FrameLayout.LayoutParams layoutParams =(FrameLayout.LayoutParams) cameraPreview.getLayoutParams();
                int layoutHeight = (int) (tmpLayoutHeight * fact);
                int layoutWidth = (int) (tmpLayoutWidth * fact);

                boolean layoutChanged;
                if ((layoutWidth != cameraPreview.getWidth()) || (layoutHeight != cameraPreview.getHeight())) {
                    layoutParams.height = layoutHeight;
                    layoutParams.width = layoutWidth;
                    if (mCenterPosX &gt;= 0) {
                        layoutParams.topMargin = mCenterPosY - (layoutHeight / 2);
                        layoutParams.leftMargin = mCenterPosX - (layoutWidth / 2);
                    }
                    cameraPreview.setLayoutParams(layoutParams); // this will trigger another surfaceChanged invocation.
                    layoutChanged = true;
                } else {
                    layoutChanged = false;
                }

                return layoutChanged;
            }

            @Override
            public void surfaceDestroyed(SurfaceHolder surfaceHolder) {
                cameraSource.stop();
            }
        });
</code></pre>",,1,1,,2017-12-05 14:41:56.010 UTC,,2017-12-06 16:16:35.710 UTC,,,,,8898301,1,0,android|api|camera|android-camera|vision,549
Microsoft Face Api response time?,40472437,Microsoft Face Api response time?,"<p>I am using microsoft face api from my client side code using java script/Jquery.
Here is the code. I am capturing the image using camera and then convert that image to a blob and send that to the api. I am getting the results. But this api takes around 4-6 seconds to get the results. Is this usual or there could be some performance improvement?</p>

<p>Thank you!</p>

<pre><code>var params = {
    // Request parameters
    ""returnFaceId"": ""true"",
    ""returnFaceLandmarks"": ""false"",
    ""returnFaceAttributes"": ""age,gender,glasses"",
};

jQuery.ajax({
    url: ""https://api.projectoxford.ai/face/v1.0/detect?"" + $.param(params),
    beforeSend: function(xhrObj){
        // Request headers
        xhrObj.setRequestHeader(""Content-Type"",""application/octet-stream"");
        xhrObj.setRequestHeader(""Ocp-Apim-Subscription-Key"",""Mykey"");
    },
    type: ""POST"",
    // Request body
    data: data,
    processData: false,

})
.done(function(data) {
    console.timeEnd(""callMicrosoftService"");

    parseData(data);

})
.fail(function() {
    alert(""error"");
});
</code></pre>",,2,4,,2016-11-07 18:51:31.607 UTC,1,2018-03-17 04:59:02.930 UTC,,,,,6417293,1,1,javascript|jquery|face-detection,265
Tensorflow checkpoints are not correctly saved when using gcloud compute unit instead of local,51481469,Tensorflow checkpoints are not correctly saved when using gcloud compute unit instead of local,"<p>When I train locally using google cloud buckets as data source and destination with:</p>

<pre><code>gcloud ml-engine local train --module-name trainer.task_v2s --package-path trainer/
</code></pre>

<p>I get normal results and checkpoints are getting saved properly in 20 seps since my dataset is 400 examples and I use 20 as batchsize: 400/20 = 20 steps = 1 Epoch. These files get saved in my model dir in the bucket</p>

<ul>
<li><p>model.ckpt-0.data-00000-of-00001</p></li>
<li><p>model.ckpt-0.index</p></li>
<li><p>model.ckpt-0.meta</p></li>
<li><p>model.ckpt-20.data-00000-of-00001</p></li>
<li><p>model.ckpt-20.index</p></li>
<li><p>model.ckpt-20.meta</p></li>
</ul>

<p>Furthermore my local GPU is properly engaged:</p>

<pre><code>+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1018      G   /usr/lib/xorg/Xorg                           212MiB |
|    0      1889      G   compiz                                        69MiB |
|    0      5484      C   ...rtualenvs/my_project/bin/python  2577MiB         |
+-----------------------------------------------------------------------------+
</code></pre>

<p>When I now try to use a gcloud compute unit:</p>

<pre><code>gcloud ml-engine jobs submit training my_job_name \
--module-name trainer.task_v2s --package-path trainer/ \
--staging-bucket gs://my-bucket --region europe-west1 \
--scale-tier BASIC_GPU --runtime-version 1.8 --python-version 3.5
</code></pre>

<p>It takes around the same time to save a checkpoint, but it is getting saved in 1 step increment, though the data sources have not changed. The loss is also decreasing way slower, as it would when only one example would be trained. This is how the files look:</p>

<ul>
<li>model.ckpt-0.data-00000-of-00001</li>
<li>model.ckpt-0.index</li>
<li>model.ckpt-0.meta</li>
<li>model.ckpt-1.data-00000-of-00001</li>
<li>model.ckpt-1.index</li>
<li>model.ckpt-1.meta</li>
</ul>

<p>The GPU is also not getting engaged at all:</p>

<pre><code>+-----------------------------------------------------------------------------+  
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |      
+-----------------------------------------------------------------------------+  
</code></pre>

<p>I'm using a custom estimator with no configured clusterspec, as I assume you only need that for distributed camputing and my run_config looks like this:</p>

<blockquote>
  <p>Using config: {'_master': '', '_num_ps_replicas': 0, '_session_config': None, '_task_id': 0, '_model_dir': 'gs://my_bucket/model_dir', '_save_checkpoints_steps': None, '_tf_random_seed': None, '_task_type': 'master', '_keep_checkpoint_max': 5, '_evaluation_master': '', '_device_fn': None, '_save_checkpoints_secs': 600, '_save_summary_steps': 100, '_cluster_spec': , '_log_step_count_steps': 100, '_is_chief': True, '_global_id_in_cluster': 0, '_num_worker_replicas': 1, '_service': None, '_keep_checkpoint_every_n_hours': 10000, '_train_distribute': None} </p>
</blockquote>

<p>From the logs I can also see the TF_CONFIG environment variable:</p>

<blockquote>
  <p>{'environment': 'cloud', 'cluster': {'master': ['127.0.0.1:2222']}, 'job': {'python_version': '3.5', 'run_on_raw_vm': True, 'package_uris': ['gs://my-bucket/my-project10/27cb2041a4ae5a14c18d6e7f8622d9c20789e3294079ad58ab5211d8e09a2669/MyProject-0.9.tar.gz'], 'runtime_version': '1.8', 'python_module': 'trainer.task_v2s', 'scale_tier': 'BASIC_GPU', 'region': 'europe-west1'}, 'task': {'cloud': 'qc6f9ce45ab3ea3e9-ml', 'type': 'master', 'index': 0}}</p>
</blockquote>

<p>My guess is that I need to configure something I haven't but I have no idea what. I also do get some warnings at the beginning, but I don't think they have something to do with this:</p>

<blockquote>
  <p>google-cloud-vision 0.29.0 has requirement requests&lt;3.0dev,>=2.18.4, but you'll have requests 2.13.0 which is incompatible.</p>
</blockquote>",51482368,1,0,,2018-07-23 14:39:35.767 UTC,,2018-07-24 09:47:14.657 UTC,2018-07-23 14:57:38.963 UTC,,2368505,,2368505,1,1,python-3.x|tensorflow|gcloud|google-cloud-ml|tensorflow-estimator,139
Google Vision API: ModuleNotFoundError: module not found 'google.oauth2',55037756,Google Vision API: ModuleNotFoundError: module not found 'google.oauth2',"<p>I am a making a very simple API call to the Google Vision API, but all the time it's giving me error that 'google.oauth2' module not found. I've pip installed all the dependcies. To check this, I've imported google.oauth2 module in command line Python and It's working there. Please help me with this.</p>",55037862,1,0,,2019-03-07 06:56:15.873 UTC,,2019-03-07 07:03:02.627 UTC,,,,,9913535,1,0,python|google-cloud-platform|google-vision,26
How to post a image correctly to alchemy node.js server?,38325741,How to post a image correctly to alchemy node.js server?,"<p>I try to implement a post method on the node.js server for alchemy.
The problem is how to handle the give image provided be a mobile app.</p>

<p>I get the Image as a part of a json.
The json extract the image and convert it to a binary. (hopefully right)
Then prepare the post method, with the need alchemy parameter.
Doing the post and examine the result.</p>

<p>There is a <strong>'cannot-analyze:downstream-issue'</strong> problem.</p>

<pre><code>2016-07-12T00:57:29.185+0200
[App/0]
OUT
'x-alchemyapi-params': 'sentiment=0&amp;knowledgeGraph=0&amp;detectedLanguage=unknown&amp;submitLanguage=detect',
2016-07-12T00:57:29.186+0200
[App/0]
OUT
""NOTICE"": ""THIS API FUNCTIONALITY IS DEPRECATED AND HAS BEEN MIGRATED TO WATSON VISUAL RECOGNITION. THIS API WILL BE DISABLED ON MAY 19, 2017."",
2016-07-12T00:57:29.186+0200
[App/0]
OUT
""usage"": ""By accessing AlchemyAPI or using information generated by AlchemyAPI, you are agreeing to be bound by the AlchemyAPI Terms of Use: http://www.alchemyapi.com/company/terms.html"",
2016-07-12T00:57:29.185+0200
[App/0]
OUT
'access-control-allow-origin': '*' }
2016-07-12T00:57:29.186+0200
[App/0]
OUT
}
2016-07-12T00:57:29.185+0200
[App/0]
OUT
'x-alchemyapi-error-msg': 'cannot-analyze:downstream-issue',
</code></pre>

<p>Here is the source code to the server method with the documentation information I found:</p>

<pre><code>// Documentation: http://www.ibm.com/watson/developercloud/doc/visual-recognition/tutorials.shtml#classify
// curl -X POST -F ""images_file=@prez.jpg"" ""https://gateway-a.watsonplatform.net/visual-recognition/api/v3/detect_faces?api_key={api-key}&amp;version=2016-05-20""
// Other Documentation: https://www.npmjs.com/package/form-data
// http://stackoverflow.com/questions/6926016/nodejs-saving-a-base64-encoded-image-to-disk
// https://github.com/expressjs/body-parser#limit
// https://www.npmjs.com/package/multer#limits
app.post('/myInformation', function(req, res){
  var theImage = 'unassigned';
  var result = 'unassigned';

  if (req.method == 'POST') {
      console.log(""[200] "" + req.method + "" to "" + req.url);
      var fullBody = '';
      req.on('data', function(chunk) {
        // append the current chunk of data to the fullBody variable
        fullBody += chunk.toString();
      });
      console.log('---&gt; fullBody : ',fullBody);
  }

  if(req.body.body.image) {
    theImage = req.body.body.image;
    console.log('---&gt; Type : ', req.body.body.type);

    // Create Base64 Object
    var Base64={_keyStr:"" XXXXXXXXX rn t}}

    var rawData = theImage;
    var data = rawData.split("","").pop();
    var decodedString = Base64.decode(data);

    var https = require('http'); // Changed to http
    var theHost = 'gateway-a.watsonplatform.net';
    var thePort = 80;
    var theMethode = 'POST';
    var api_key = 'XXXXXXXXXXXX';
    var thePath = '/calls/image/ImageGetRankedImageKeywords?apikey='+api_key+'&amp;outputMode=json&amp;imagePostMode=raw';

    var postheaders = {
      'Content-Type'  : 'application/x-www-form-urlencoded',
      'Content-Length': Buffer.byteLength(decodedString)
    };

    // the post options
    var optionspost = {
      host : theHost,
      port : thePort,
      path : thePath,
      method : theMethode,
      headers : postheaders
    };

    console.info('---&gt; Options prepared:');
    console.info(optionspost);
    console.info('---&gt; Do the POST call');

    // do the POST call using https or http
    var reqPost = https.request(optionspost, function(res) {
        console.log(""---&gt; statusCode: "", res.statusCode);
        // uncomment it for header details
        console.log(""---&gt; headers: "", res.headers);

        res.on('data', function(d) {
            console.info('---&gt; POST result:\n');
            process.stdout.write(d);
            console.info('\n\n---&gt; POST completed');
        });
    });

    // write the image Push Data
    reqPost.write(decodedString);
    reqPost.end();
    reqPost.on('error', function(e) {
      console.error(e);
    });

    console.log(""---&gt; Keywords for Images"");
  };
  res.end(""OK"");
});
</code></pre>",38957427,2,0,,2016-07-12 10:05:22.197 UTC,,2016-08-16 13:59:04.137 UTC,,,,,4342353,1,0,javascript|json|node.js|post|ibm-watson,164
Google vision APIs : How to detect the image is painting image or real picture?,44041039,Google vision APIs : How to detect the image is painting image or real picture?,"<p>I have an issue with detecting image whether it is painting image or real picture taken. I have checked Google Vision REST-APIs documentation, it seems that it does not mention for that. </p>

<p>Appreciate if you can share algorithm how to detect it.<a href=""https://i.stack.imgur.com/JUbsj.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JUbsj.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/Rd10L.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rd10L.jpg"" alt=""enter image description here""></a></p>",,1,0,,2017-05-18 07:15:15.860 UTC,,2017-05-18 16:01:53.087 UTC,2017-05-18 16:01:53.087 UTC,,5231007,,1976804,1,0,java|google-cloud-vision,196
P5.js send request to Azure Custom Vision,54813301,P5.js send request to Azure Custom Vision,"<p>So, </p>

<p>I take an image from a canvas using p5js and i want to send it to the Azure Custom Vision Service(the code bellow).</p>

<p>Is p5 image even the same as the normal js image(like when you take a capture from a video) ?</p>

<p>My problem is when i send a form as a json like : </p>

<pre><code>let c = get(0,0,250,250);
var http = new XMLHttpRequest();
http.open('POST', prediction_URL, true);
http.onreadystatechange = function() {
    if(http.readyState == 4 &amp;&amp; http.status == 200) {
        console.log(http.responseText);
    }
} 
var objurl = window.URL.createObjectURL(new Blob([c]));
http.send({
 url: prediction_URL,
 encoding: null,
 json: false,
headers: {
  'Content-Type': 'application/octet-stream',
  'Prediction-Key': 'xxxxxxxxxxxxx'
},
body: objurl
});
</code></pre>

<p>But it gaves me ""401 Acces Denied"".</p>

<p>Is it even possible to send an image created using p5 via http request?</p>",54819094,1,6,,2019-02-21 17:51:52.300 UTC,,2019-02-22 02:13:31.033 UTC,,,,,10849251,1,1,javascript|azure|computer-vision|p5.js,28
Recognize vertical text using google cloud vision,55334563,Recognize vertical text using google cloud vision,"<p>I'm trying to recognize vertical text using google cloud vision. Image example:
<a href=""https://i.stack.imgur.com/3wwYp.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3wwYp.jpg"" alt=""A B C D E""></a> </p>

<p>I use Try This API on <a href=""https://cloud.google.com/vision/docs/ocr"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/ocr</a> to test the engine.</p>

<p>Request body:</p>

<pre><code>{
  ""requests"": [
    {
      ""features"": [
        {
          ""type"": ""DOCUMENT_TEXT_DETECTION""
        }
      ],
      ""image"": {
        ""source"": {
          ""imageUri"": ""https://i.stack.imgur.com/3wwYp.jpg""
        }
      }
    }
  ]
}
</code></pre>

<p>The result is</p>

<pre><code>{
  ""responses"": [
    {}
  ]
}
</code></pre>

<p>Am I missing something? Thank you.</p>",,2,0,,2019-03-25 09:18:25.930 UTC,1,2019-05-28 14:12:29.143 UTC,,,,,887290,1,2,ocr|google-cloud-vision,67
Python multiprocessing with different constant for each thread,47182750,Python multiprocessing with different constant for each thread,"<p>I want to create a pool with a function calling the boto3 api and using a different bucket name for each thread:</p>

<p>my function is:</p>

<pre class=""lang-py prettyprint-override""><code>def face_reko(source_data, target_data):

        bucket = s3.Bucket(bucket_name)
    for key in bucket.objects.all():
        key.delete()

    s3_client.put_object(Bucket=bucket_name, Key=target_img, Body=target_data)
    s3_client.put_object(Bucket=bucket_name, Key=source_img, Body=source_data)

    response = reko.compare_faces(
        SourceImage={
            'S3Object': {
                'Bucket': bucket_name,
                'Name' : source_img
            }
        },
        TargetImage={
            'S3Object' : {
                'Bucket' : bucket_name,
                'Name' : target_img
            }
        }
    )
    if len(response['FaceMatches']) &gt; 0:
        return True
    else:
        return False
</code></pre>

<p>So basically it deletes all in the bucket, upload 2 new image then use the Rekognition api to compare the 2 images. Since I can't create the same image twice in the same bucket, i'd like to create a bucket for each thread then pass a constant to the function for the bucket name instead of the <code>bucket_name</code> const.</p>",47352319,1,1,,2017-11-08 14:52:40.053 UTC,,2017-11-17 13:59:15.067 UTC,,,,,8496077,1,0,python|multithreading|amazon-s3|boto3|amazon-rekognition,239
Google Api default credential,43735666,Google Api default credential,"<p>I am trying to make one simple application in Xamrin android using
<a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">Google Vision API</a>.</p>

<p>What I did is, </p>

<p>Installed </p>

<ol>
<li>Google cloud vision v1, </li>
<li>Google.Apis.Auth.OAuth2;</li>
<li>Newtonsoft.Json; </li>
</ol>

<p>in my xamarin.android project from NuGet manager. </p>

<p>I created </p>

<ol>
<li>API Key, </li>
<li>Service Account,</li>
<li>OAuth 2.0 client IDs </li>
</ol>

<p>from google cloud console. </p>

<p>I created GOOGLE_APPLICATION_CREDENTIALS (Environmenta variable) 
and linked those Json file (both service account and
OAuth 2.0 client IDs json) tried both.</p>

<p>Then I just copied code from google vision API documentaion.</p>

<pre><code>// Load an image from a local file.
var image = Image.FromFile(filePath);
var client = ImageAnnotatorClient.Create();
var response = client.DetectLogos(image);
foreach (var annotation in response)
{
if (annotation.Description != null)
    textView.Text = (annotation.Description);
}
</code></pre>

<p>everytime i try to compile the program it throws exception </p>

<blockquote>
  <blockquote>
    <p>System.InvalidOperationException: The Application Default 
    Credentials are not available. They are available if 
    running in Google Compute Engine. Otherwise, 
    the environment variable GOOGLE_APPLICATION_CREDENTIALS 
    must be defined pointing to a file defining the credentials. 
    See <a href=""https://developers.google.com/accounts/docs/"" rel=""nofollow noreferrer"">https://developers.google.com/accounts/docs/</a>
    application-default-credentials for more information.</p>
  </blockquote>
</blockquote>

<p>I need help to set default credentials, I tried many links from google
documentation but no luck.
has anyone got any idea how to handle this exception.</p>",,0,11,,2017-05-02 10:30:20.297 UTC,,2017-08-10 04:59:37.853 UTC,2017-05-02 22:37:50.980 UTC,,7120682,,7120682,1,0,oauth|google-api|xamarin.android|google-cloud-vision,563
"Emotion API, FATAL EXCEPTION: AsyncTask #1 An error occurred while executing doInBackground()",47415721,"Emotion API, FATAL EXCEPTION: AsyncTask #1 An error occurred while executing doInBackground()","<p>I want to learn Microsoft Emotion API on android.</p>

<p>So, I try to run the Android SDK example.
(<a href=""https://github.com/Microsoft/Cognitive-emotion-android"" rel=""nofollow noreferrer"">https://github.com/Microsoft/Cognitive-emotion-android</a>)</p>

<p>But, when I select a photo, there is crash and exit on result scene.</p>

<p>this is log.</p>

<pre><code>11-21 14:23:39.643 6245-6290/com.emotion.emotionapi E/AndroidRuntime: FATAL EXCEPTION: AsyncTask #1
Process: com.emotion.emotionapi, PID: 6245
java.lang.RuntimeException: An error occurred while executing doInBackground()
at android.os.AsyncTask$3.done(AsyncTask.java:309)
at java.util.concurrent.FutureTask.finishCompletion(FutureTask.java:354)
at java.util.concurrent.FutureTask.setException(FutureTask.java:223)
at java.util.concurrent.FutureTask.run(FutureTask.java:242)
at android.os.AsyncTask$SerialExecutor$1.run(AsyncTask.java:234)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1113)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:588)
at java.lang.Thread.run(Thread.java:818)
Caused by: java.lang.NoClassDefFoundError: Failed resolution of: Lorg/apache/commons/io/IOUtils;
at com.microsoft.projectoxford.emotion.EmotionServiceRestClient.recognizeImage(EmotionServiceRestClient.java:105)
at com.microsoft.projectoxford.emotionsample.RecognizeActivity.processWithFaceRectangles(RecognizeActivity.java:224)
at com.microsoft.projectoxford.emotionsample.RecognizeActivity.access$000(RecognizeActivity.java:67)
at com.microsoft.projectoxford.emotionsample.RecognizeActivity$doRequest.doInBackground(RecognizeActivity.java:257)
at com.microsoft.projectoxford.emotionsample.RecognizeActivity$doRequest.doInBackground(RecognizeActivity.java:236)
at android.os.AsyncTask$2.call(AsyncTask.java:295)
at java.util.concurrent.FutureTask.run(FutureTask.java:237)
at android.os.AsyncTask$SerialExecutor$1.run(AsyncTask.java:234) 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1113) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:588) 
at java.lang.Thread.run(Thread.java:818) 
Caused by: java.lang.ClassNotFoundException: Didnt find class ""org.apache.commons.io.IOUtils"" on path: DexPathList[[zip file ""/data/app/com.emotion.emotionapi-1/base.apk"", zip file ""/data/app/com.emotion.emotionapi-1/split_lib_dependencies_apk.apk"", zip file ""/data/app/com.emotion.emotionapi-1/split_lib_slice_0_apk.apk"", zip file ""/data/app/com.emotion.emotionapi-1/split_lib_slice_1_apk.apk"", zip file ""/data/app/com.emotion.emotionapi-1/split_lib_slice_2_apk.apk"", zip file ""/data/app/com.emotion.emotionapi-1/split_lib_slice_3_apk.apk"", zip file ""/data/app/com.emotion.emotionapi-1/split_lib_slice_4_apk.apk"", zip file ""/data/app/com.emotion.emotionapi-1/split_lib_slice_5_apk.apk"", zip file ""/data/app/com.emotion.emotionapi-1/split_lib_slice_6_apk.apk"", zip file ""/data/app/com.emotion.emotionapi-1/split_lib_slice_7_apk.apk"", zip file ""/data/app/com.emotion.emotionapi-1/split_lib_slice_8_apk.apk"", zip file ""/data/app/com.emotion.emotionapi-1/split_lib_slice_9_apk.apk""],nativeLibraryDirectories=[/data/app/com.emotion.emotionapi-1/lib/x86_64, /vendor/lib64, /system/lib64]]
at dalvik.system.BaseDexClassLoader.findClass(BaseDexClassLoader.java:56)
at java.lang.ClassLoader.loadClass(ClassLoader.java:511)
at java.lang.ClassLoader.loadClass(ClassLoader.java:469)
at com.microsoft.projectoxford.emotion.EmotionServiceRestClient.recognizeImage(EmotionServiceRestClient.java:105) 
at com.microsoft.projectoxford.emotionsample.RecognizeActivity.processWithFaceRectangles(RecognizeActivity.java:224) 
at com.microsoft.projectoxford.emotionsample.RecognizeActivity.access$000(RecognizeActivity.java:67) 
at com.microsoft.projectoxford.emotionsample.RecognizeActivity$doRequest.doInBackground(RecognizeActivity.java:257) 
at com.microsoft.projectoxford.emotionsample.RecognizeActivity$doRequest.doInBackground(RecognizeActivity.java:236) 
at android.os.AsyncTask$2.call(AsyncTask.java:295) 
at java.util.concurrent.FutureTask.run(FutureTask.java:237) 
at android.os.AsyncTask$SerialExecutor$1.run(AsyncTask.java:234) 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1113) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:588) 
at java.lang.Thread.run(Thread.java:818) 
Suppressed: java.lang.ClassNotFoundException: org.apache.commons.io.IOUtils
at java.lang.Class.classForName(Native Method)
at java.lang.BootClassLoader.findClass(ClassLoader.java:781)
at java.lang.BootClassLoader.loadClass(ClassLoader.java:841)
at java.lang.ClassLoader.loadClass(ClassLoader.java:504)
... 12 more
Caused by: java.lang.NoClassDefFoundError: Class not found using the boot class loader; no stack trace available
</code></pre>

<p>And this is line 224</p>

<pre><code>result = this.client.recognizeImage(inputStream, faceRectangles);
</code></pre>

<p>How can I fix it?</p>",,0,3,,2017-11-21 14:36:54.827 UTC,,2017-11-21 15:13:45.837 UTC,2017-11-21 15:13:45.837 UTC,,2649012,,8980191,1,0,android,40
"google cloud vision api, how to read text and structure it",50780962,"google cloud vision api, how to read text and structure it","<p>I'm using google cloud vision api python to scan document to read the text from it. Document is an invoice which has customer details and tables. Document to text data conversion works perfect. However the data is not sorted. I'm not able to find a way how to sort the data because I need to extract few values from it. And the data which I want to extract is located sometimes in different position which is making me difficult to extract.</p>

<blockquote>
  <p><a href=""https://cloud.google.com/vision/docs/fulltext-annotations"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/fulltext-annotations</a></p>
</blockquote>

<p>Here is my python code:</p>

<pre><code>import io
import os
from google.cloud import vision
from google.cloud.vision import types
import glob


def scan_img(image_file):
    with io.open(image_file, 'rb') as image_file:
        content = image_file.read()

    image = types.Image(content=content)

    response = client.document_text_detection(image=image)
    document = response.full_text_annotation
    img_out_array = document.text.split(""\n"")
    invoice_no_raw = """"
    invoice_date_raw = """"
    net_total_idx = """"
    customer_name_index = """"

    for index, line in enumerate(img_out_array):
        if ""Invoice No"" in line:
            invoice_no_raw = line
        if ""Customer Name"" in line:
            index += 6
            customer_name_index = index
        if ""Date :"" in line:
            invoice_date_raw = line
        if ""Our Bank details"" in line:
            index -= 1
            net_total_idx = index

    net_total_sales_raw = img_out_array[net_total_idx]
    customer_name_raw = img_out_array[customer_name_index]
    print(""Raw data:: "", invoice_no_raw, invoice_date_raw, customer_name_raw, img_out_array[net_total_idx])

    invoice_no = invoice_no_raw.split("":"")[1]
    invoice_date = invoice_date_raw.split("":"")[1]
    customer_name = customer_name_raw.replace("".."", """")
    net_total_sales = net_total_sales_raw.split("" "")[-1]

    return [invoice_no, invoice_date, customer_name, net_total_sales]


if __name__ == '__main__':
    os.environ[""GOOGLE_APPLICATION_CREDENTIALS""] = 
    ""path/to/imgtotext.json""
    client = vision.ImageAnnotatorClient()
    images = glob.glob(""/path/Documents/invoices/*.jpg"")
    for image in images:
        print(""scanning the image:::::"" + image)
        invoice_no, invoice_date, customer_name, net_total_sales = 
        scan_img(image)
        print(""Formatted data:: "", invoice_no, invoice_date, 
        customer_name, net_total_sales)
</code></pre>

<p>document 1 output:</p>

<pre><code>Customer Name
Address
**x customer**
area name
streetname
Customer LPO
</code></pre>

<p>document 2 output :</p>

<pre><code>Customer LPO
**y customer**
area name
streetname
LPO Date
Payment Terms
Customer Name
Address
Delivery Location
</code></pre>

<p>Please advice, I want to read the x and y customer and this location is changing from document to document and I have several documents. How to structure it and read the data. </p>

<blockquote>
  <p><code>There are other several fields which I'm able successfully read it.</code></p>
</blockquote>

<p>Thanks in advance.</p>",52761763,1,0,,2018-06-10 05:55:13.147 UTC,,2018-10-11 13:48:58.880 UTC,2018-08-01 08:28:40.680 UTC,,9920156,,9920156,1,0,python-2.7|ocr|google-cloud-vision,512
Object of type 'RepeatedCompositeContainer' is not JSON serializable,50860448,Object of type 'RepeatedCompositeContainer' is not JSON serializable,"<p>Using Google Client Library interacting with the vision library.</p>

<p>I have a function to detect labels from an image.</p>

<p><strong>GoogleVision.py</strong></p>

<pre><code>import os

from google.cloud import vision
from google.cloud.vision import types
from google.protobuf.json_format import MessageToJson


class GoogleVision():

    def detectLabels(self, uri):

        client = vision.ImageAnnotatorClient()
        image = types.Image()
        image.source.image_uri = uri

        response = client.label_detection(image=image)
        labels = response.label_annotations

        return labels
</code></pre>

<p>I have an api to call this function.</p>

<pre><code>from flask_restful import Resource
from flask import request
from flask import json
from util.GoogleVision import GoogleVision

import os


class Vision(Resource):

    def get(self):

        return {""message"": ""API Working""}

    def post(self):

        googleVision = GoogleVision()

        req = request.get_json()

        url = req['url']

        result = googleVision.detectLabels(url)

        return result
</code></pre>

<p>However, it does not return the result and errors with the following</p>

<blockquote>
  <p>TypeError: Object of type 'RepeatedCompositeContainer' is not JSON
  serializable</p>
</blockquote>",,0,6,,2018-06-14 14:57:41.480 UTC,,2018-06-14 15:10:32.420 UTC,2018-06-14 15:10:32.420 UTC,,9492765,,9492765,1,1,python|google-cloud-platform,912
ListCollections is not listing correct collections in Android App - AWS Rekognition,53117918,ListCollections is not listing correct collections in Android App - AWS Rekognition,"<p>Ok so I have been stuck here for about more than a week now and I know its some dumb mistake. Just can't figure it out. I am working on a project that is available of two platforms, Android &amp; iOS. Its sort of a facial recognition app. 
When I try to create/access collections from iOS application and python script, they both access the same collection from my AWS account.</p>

<p>But when I try to access from Android application, it creates/accesses it's own collections. The collections created by Android app are neither accessible anywhere other than this Android app nor this android app can access collections created by iOS app or python script.</p>

<p>I have tried listing collections on all these three platforms. iOS and Python list exact same collections while the collections listed by Android app are the ones created by an android app only.</p>

<p>Here is the code I am using to list collections on android:</p>

<pre><code>        mCredentialsProvider = new CognitoCachingCredentialsProvider(
                mContext.getApplicationContext(),
                ""us-east-2:4xbx0x6x-9xbx-xax7-x9xf-x5x0xexfx1xb"", // Identity pool ID
                Regions.US_EAST_2 // Region
        );

        mAmazonRekognitionClient = new AmazonRekognitionClient(mCredentialsProvider);

        ListCollectionsResult listCollectionsResult = mAmazonRekognitionClient.listCollections(new ListCollectionsRequest().withMaxResults(20));
        Log.i(TAG, listCollectionsResult.getCollectionIds().toString());
</code></pre>

<p>This is the log result:</p>

<blockquote>
  <p>[i_facesbxyxuxqxbxvxlxwx6x7xex5xmxfx, i_facestxnxaxoxoxqxaxwx4xtxuxwxoxrx, root_faces_data]</p>
</blockquote>

<p>This is the python code I using to list collections:</p>

<pre><code>import boto3
client = boto3.client('rekognition')
response = client.list_collections()
print(response['CollectionIds'])
</code></pre>

<p>This is the result:</p>

<blockquote>
  <p>['i_facesbxyxuxqxbxvxlxwx6x7xex5xmxfx', 'root_faces_data']</p>
</blockquote>

<p>That's it. Nothing else. Just this code. You can see that one is showing 3 collections while other is showing two. 
I am using the exact same region and identity pool ID in iOS app and it's listing same collections as in python.
The reason I think iOS app is fine because the collections listed by both iOS and python are same. 
Is there anything I need to change? Is there any additional setup I need to do to make it work?
Please let me know. Thanks.</p>",53128777,1,0,,2018-11-02 11:39:32.190 UTC,,2018-11-03 05:48:48.877 UTC,,,,,4395264,1,0,android|amazon-web-services|aws-sdk|amazon-cognito|amazon-rekognition,55
Uploading file to Google Cloud storage locally using NodeJS,40808590,Uploading file to Google Cloud storage locally using NodeJS,"<p>I'm trying to upload an image to Google Cloud Storage using the simple code locally on my machine with my service account:</p>

<pre><code>const storage = require('@google-cloud/storage');
const fs = require('fs');
const gcs = storage({
    projectId: 'ID',
    keyFilename: 'KEYNAME'
});
var bucket = gcs.bucket('BUCKET NAME');

bucket.upload('hiking-image.jpg', function(err, file) {
    if (err) throw new Error(err);
});

if (err) throw new Error(err);
         ^
</code></pre>

<p>However, I get the error message below. Is the Google Cloud API only supposed to work when deployed on App Engine or am I doing something wrong here? I was able to get the Google Vision API to work locally using the same Service Account.</p>

<pre><code>Error: ApiError: Forbidden
    at /Users/user/google-cloud/upload/upload.js:10:20
    at Pumpify.&lt;anonymous&gt; (/Users/user/node_modules/@google-cloud/storage/src/bucket.js:1218:9)
    at emitOne (events.js:101:20)
    at Pumpify.emit (events.js:188:7)
    at Pumpify.Duplexify._destroy (/Users/user/node_modules/duplexify/index.js:184:15)
    at /Users/user/node_modules/duplexify/index.js:175:10
    at _combinedTickCallback (internal/process/next_tick.js:67:7)
    at process._tickCallback (internal/process/next_tick.js:98:9)
</code></pre>",44087817,2,3,,2016-11-25 15:52:00.010 UTC,1,2017-05-20 15:41:48.673 UTC,,,,,3642173,1,1,node.js|google-cloud-storage,845
Google Cloud Vision OCR misses single numbers and symbols,41488436,Google Cloud Vision OCR misses single numbers and symbols,"<p>I am using the Google Cloud Vision API to detect text in receipts. In some cases not all text on the receipt is detected. Mainly short numbers, symbols and words are not detected. </p>

<p>An example of this problem can be found <a href=""https://i.stack.imgur.com/Gfzzw.jpg"" rel=""nofollow noreferrer"">here</a>, which is a Dutch receipt which was processed with the ""Try the API"" interface. As seen in the image, not all text is detected.</p>

<p>The image is according to the best practices guidelines as set in the documentation.</p>

<p>Is there a way to improve the image or to configure the API so that all text and symbols are detected? Any hints or help are much appreciated.</p>",,0,0,,2017-01-05 15:14:32.580 UTC,1,2017-05-02 11:39:39.790 UTC,,,,,2644025,1,2,google-cloud-platform|ocr|google-vision,356
"AWS Rekognition, how to rename a collection?",46287956,"AWS Rekognition, how to rename a collection?","<p>I have created a collection in was CLI like so:</p>

<pre><code>aws rekognition create-collection --collection-id ""collectionName""
</code></pre>

<p>I would like to rename that collection to another string. I can't find how to do that. Any suggestion?</p>",46305023,1,0,,2017-09-18 20:47:22 UTC,,2017-09-19 16:02:53.403 UTC,,,,,728246,1,0,amazon-s3|amazon-rekognition,112
What is Google API Discovery?,36289389,What is Google API Discovery?,"<p>I am having trouble understanding the concept of “API discovery” as used in Google products/services.  Here’s some Python code that uses the said discovery service to access Google Cloud Vision:</p>

<pre><code>from googleapiclient.discovery import build
from oauth2client.client import GoogleCredentials
… 
API_DISCOVERY_FILE = 'https://vision.googleapis.com/$discovery/rest?version=v1'
hlh = httplib2.Http()
credentials = GoogleCredentials.get_application_default().create_scoped(
    ['https://www.googleapis.com/auth/cloud-platform'])
credentials.authorize(hlh)
service = build(serviceName='vision', version='v1', http=hlh, discoveryServiceUrl=API_DISCOVERY_FILE)
service_request = service.images().annotate(body={ &lt;more JSON code here&gt; })
</code></pre>

<p>Here’s another bit of Python code that also accesses Google Cloud Vision, but does not use API discovery <em>and works just fine</em>:</p>

<pre><code>import requests
…
ENDPOINT_URL = 'https://vision.googleapis.com/v1/images:annotate'
response = requests.post(ENDPOINT_URL,
    data=make_image_data(image_filenames),
    params={'key': api_key},
    headers={'Content-Type': 'application/json'})
</code></pre>

<p>What I can’t wrap my head around is this question: You need to know the details of the API that you are going to be calling so that you can tailor the call; this is obvious.  So, how would API discovery help you at the time of the call, <em>after you have already prepared the code for calling that API</em>? </p>

<p>PS: I did look at the following resources prior to posting this question:
<br>
<a href=""https://developers.google.com/discovery/v1/getting_started"" rel=""nofollow noreferrer"">https://developers.google.com/discovery/v1/getting_started</a>
<br>
<a href=""https://developers.google.com/discovery/v1/using"" rel=""nofollow noreferrer"">https://developers.google.com/discovery/v1/using</a>
<br>
I did see <a href=""https://stackoverflow.com/questions/33846841/what-is-discovery-based-in-rest-api"">this</a> answered question but would appreciate additional insight.</p>",36354914,1,5,,2016-03-29 16:02:05.133 UTC,1,2016-04-01 11:13:54.580 UTC,2017-05-23 12:15:32.610 UTC,,-1,,360840,1,1,python|rest|api-discovery,1160
Android OCR result matching with database,52843616,Android OCR result matching with database,"<p>So I am trying to use a OCR to translate text that I record with my phone's camera to a string, I am currently using Google vision OCR for android and have implemented the OCR correctly, the problem is that sometimes the result is not as good as expected thats why a solution I think might work is matching the result given by the OCR with my database. For example if my camera reads ""How you?"" then I would find in my database a entry that is similar ""How are you?"" and would display this instead. So the real problem is that the OCR is constantly reading from the camera, so that means that I would need to make an HTTP request to a server and query the database for a similar match every second or two and wait for the response, that could be very bad execution if there are many users overloading the server. One solution that I thought was downloading the list of all strings in the database and make the matching locally, but what if the data changes after that in the database? What would be a good approach to this?</p>

<p>I'm using this to read text from supermarket products such as name and description, so what I thought was match the products name and then query my database for all complementary information. Its important to note that this is going to be used by visually impaired people so reading bar codes is not a good choice right now. </p>",52860495,1,0,,2018-10-16 20:30:07.620 UTC,,2018-10-17 21:54:23.213 UTC,2018-10-17 21:42:22.097 UTC,,5245639,,5245639,1,0,android|ocr|scalability|image-recognition|google-vision,75
How can I calculate the accuracy for my Custom Vision Model?,52335053,How can I calculate the accuracy for my Custom Vision Model?,"<p>On the Microsoft Custom Vision Portal I only have the Precision and Recall results, how can I generate the  True positive (TP), False positive (FP), True negative (TN)  and False negative (FN) from the same images that the service used to corss validate itself to then get the accurancy?</p>

<p>tks</p>",,0,0,,2018-09-14 15:28:31.660 UTC,,2018-09-14 15:28:31.660 UTC,,,,,9327981,1,1,microsoft-cognitive|azure-cognitive-services|api-cognitive-services,57
Google Cloud Vision API. Golang . How to get API JSON,53259815,Google Cloud Vision API. Golang . How to get API JSON,"<p>I use Google Cloud Vision API with the Go SDK.
In some cases I don't want to use Golang structures to read API results, I just want to get full JSON response of an API call. For example,</p>

<pre><code>// detectDocumentText gets the full document text from the Vision API for an
// image at the given file path.
func detectDocumentTextURI(w io.Writer, file string) error {
        ctx := context.Background()

        client, err := vision.NewImageAnnotatorClient(ctx)
        if err != nil {
                return err
        }

        image := vision.NewImageFromURI(file)
        annotation, err := client.DetectDocumentText(ctx, image, nil)
        if err != nil {
                return err
        }

        if annotation == nil {
                fmt.Fprintln(w, ""No text found."")
        } else {
                fmt.Fprintf(w, ""API Response %s"", ...JSON...)
        }

        return nil
}
</code></pre>

<p>How can I get that JSON from annotation structure? Is it possible?</p>",,1,0,,2018-11-12 10:03:33.550 UTC,,2018-11-13 15:05:56.093 UTC,2018-11-12 14:30:21.863 UTC,,9609198,,629960,1,1,go|google-cloud-vision,129
Can Google Cloud Vision return more language of label?,56285264,Can Google Cloud Vision return more language of label?,"<p>I tried sample of Google Vision API (PHP)</p>

<pre><code># includes the autoloader for libraries installed with composer
require __DIR__ . '/vendor/autoload.php';

# imports the Google Cloud client library
use Google\Cloud\Vision\V1\ImageAnnotatorClient;

# instantiates a client
$imageAnnotator = new ImageAnnotatorClient();

# the name of the image file to annotate
$fileName = 'test/data/wakeupcat.jpg';

# prepare the image to be annotated
$image = file_get_contents($fileName);

# performs label detection on the image file
$response = $imageAnnotator-&gt;labelDetection($image);
$labels = $response-&gt;getLabelAnnotations();

if ($labels) {
    echo(""Labels:"" . PHP_EOL);
    foreach ($labels as $label) {
        echo($label-&gt;getDescription() . PHP_EOL);
    }
} else {
    echo('No label found' . PHP_EOL);
}
</code></pre>

<p>I can get label of objects in image, its awesome, but label is English language. Can I config API return other language or multi language?</p>

<p>P/S: Sorry for my bad English :(</p>",,0,0,,2019-05-24 02:55:28.353 UTC,,2019-05-24 02:55:28.353 UTC,,,,,7266930,1,1,php|google-api|google-cloud-vision,8
How to convert 34 facial landmark points to 68 points?,44606352,How to convert 34 facial landmark points to 68 points?,"<p>By using Google Cloud Vision it detects face and returns only 34 facial landmark points.<br>
Is there any way to generate / derive 68 facial landmark points from the existing 34 landmark points like generating with <a href=""http://dlib.net"" rel=""nofollow noreferrer"">dlib</a>?</p>",44755926,1,0,,2017-06-17 15:38:07.990 UTC,1,2017-06-26 08:45:54.253 UTC,,,,,1445874,1,1,face-detection|dlib|google-cloud-vision,345
how to crop only face instead of whole body in oval form using Google Vision Face detection API in android,55391510,how to crop only face instead of whole body in oval form using Google Vision Face detection API in android,"<pre><code>**I want to make app which take photo only face but i get whole image instead of face when i save into my storage file . so how to crop  face and save into storage with the help google vision face detection API.**
</code></pre>

<p>so how to use frame in my code to get face list as well as how can i convert into bitmap and save into my storage.
I want to make app which take photo only face but i get whole image instead of face when i save into my storage file . so how to crop  face and save into storage with the help google vision face detection API.**
so how to use frame in my code to get face list as well as how can i convert into bitmap and save into my storage</p>

<pre><code>main.xml
--------

&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;

&lt;FrameLayout
    xmlns:android=""http://schemas.android.com/apk/res/android""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""&gt;
&lt;LinearLayout
    xmlns:android=""http://schemas.android.com/apk/res/android""
    android:id=""@+id/topLayout""
    android:orientation=""vertical""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""
    android:keepScreenOn=""true""&gt;


  &lt;com.google.android.gms.samples.vision.face.facetracker.ui.camera.CameraSourcePreview
      android:id=""@+id/preview""
      android:layout_width=""match_parent""
      android:layout_height=""match_parent""&gt;

    &lt;com.google.android.gms.samples.vision.face.facetracker.ui.camera.GraphicOverlay
        android:id=""@+id/faceOverlay""
        android:layout_width=""match_parent""
        android:layout_height=""match_parent"" /&gt;



  &lt;/com.google.android.gms.samples.vision.face.facetracker.ui.camera.CameraSourcePreview&gt;


&lt;/LinearLayout&gt;
  &lt;android.support.v7.widget.AppCompatButton
      android:id=""@+id/take_pic_btn""
      android:layout_gravity=""bottom""
      android:gravity=""center""
      android:background=""@color/green""
      android:layout_margin=""10dp""
      android:text=""Take Image""
      android:textStyle=""bold""
      android:textSize=""20sp""
      android:textAllCaps=""false""
      android:layout_width=""match_parent""
      android:layout_height=""wrap_content"" /&gt;

&lt;/FrameLayout&gt;



FaceTrackerActivity .java
-------------------------


public final class FaceTrackerActivity extends AppCompatActivity {
    private static final String TAG = ""FaceTracker"";

    private CameraSource mCameraSource = null;

    private CameraSourcePreview mPreview;
    private GraphicOverlay mGraphicOverlay;

    private static final int RC_HANDLE_GMS = 9001;
    // permission request codes need to be &lt; 256
    private static final int RC_HANDLE_CAMERA_PERM = 2;
    private Button takePicButton;
    FaceDetector detector;


    @Override
    public void onCreate(Bundle icicle) {
        super.onCreate(icicle);
        setContentView(R.layout.main);

        mPreview = (CameraSourcePreview) findViewById(R.id.preview);
        mGraphicOverlay = (GraphicOverlay) findViewById(R.id.faceOverlay);
        takePicButton=(Button)findViewById(R.id.take_pic_btn);

        int rc = ActivityCompat.checkSelfPermission(this, Manifest.permission.CAMERA);
        int gc = ActivityCompat.checkSelfPermission(this, Manifest.permission.WRITE_EXTERNAL_STORAGE);
        if (rc == PackageManager.PERMISSION_GRANTED &amp;&amp; gc == PackageManager.PERMISSION_GRANTED) {
            createCameraSource();
        } else {
            requestCameraPermission();
        }
        takePicButton.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                Toast.makeText(getApplicationContext(),""dilip"",Toast.LENGTH_LONG).show();
                captureImage();


            }
        });
    }


    private void requestCameraPermission() {
        Log.w(TAG, ""Camera permission is not granted. Requesting permission"");

        final String[] permissions = new String[]{Manifest.permission.CAMERA,Manifest.permission.WRITE_EXTERNAL_STORAGE};

        if (!ActivityCompat.shouldShowRequestPermissionRationale(this,
                Manifest.permission.CAMERA) &amp;&amp; !ActivityCompat.shouldShowRequestPermissionRationale(this,
                Manifest.permission.WRITE_EXTERNAL_STORAGE) ) {
            ActivityCompat.requestPermissions(this, permissions, RC_HANDLE_CAMERA_PERM);
            return;
        }

        final Activity thisActivity = this;

        View.OnClickListener listener = new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                ActivityCompat.requestPermissions(thisActivity, permissions,
                        RC_HANDLE_CAMERA_PERM);
            }
        };

        Snackbar.make(mGraphicOverlay, R.string.permission_camera_rationale,
                Snackbar.LENGTH_INDEFINITE)
                .setAction(R.string.ok, listener)
                .show();
    }


    private void createCameraSource() {

        Context context = getApplicationContext();
       /* FaceDetector detector = new FaceDetector.Builder(context)
                .setClassificationType(FaceDetector.ALL_CLASSIFICATIONS)
                .build();

        detector.setProcessor(
                new MultiProcessor.Builder&lt;&gt;(new GraphicFaceTrackerFactory())
                        .build());*/

        detector= new FaceDetector.Builder(context)
                .setClassificationType(FaceDetector.ALL_CLASSIFICATIONS)
                .build();
        MyFaceDetector myFaceDetector = new MyFaceDetector(detector);

        detector.setProcessor(
                new MultiProcessor.Builder&lt;&gt;(new GraphicFaceTrackerFactory())
                        .build());

        mCameraSource = new CameraSource.Builder(context, myFaceDetector)
                .build();

        if (!detector.isOperational()) {
           }

        mCameraSource = new CameraSource.Builder(context, detector)
                .setRequestedPreviewSize(640, 480)
                .setFacing(CameraSource.CAMERA_FACING_FRONT)
                .setRequestedFps(10.0f)
                .build();
    }

    /**
     * Restarts the camera.
     */
    @Override
    protected void onResume() {
        super.onResume();

        startCameraSource();
    }

    /**
     * Stops the camera.
     */
    @Override
    protected void onPause() {
        super.onPause();
        mPreview.stop();
    }


    @Override
    protected void onDestroy() {
        super.onDestroy();
        if (mCameraSource != null) {
            mCameraSource.release();
        }
    }

    /**
     * Callback for the result from requesting permissions. This method
     * is invoked for every call on {@link #requestPermissions(String[], int)}.
     * &lt;p&gt;
     * &lt;strong&gt;Note:&lt;/strong&gt; It is possible that the permissions request interaction
     * with the user is interrupted. In this case you will receive empty permissions
     * and results arrays which should be treated as a cancellation.
     * &lt;/p&gt;
     *
     * @param requestCode  The request code passed in {@link #requestPermissions(String[], int)}.
     * @param permissions  The requested permissions. Never null.
     * @param grantResults The grant results for the corresponding permissions
     *                     which is either {@link PackageManager#PERMISSION_GRANTED}
     *                     or {@link PackageManager#PERMISSION_DENIED}. Never null.
     * @see #requestPermissions(String[], int)
     */
    @Override
    public void onRequestPermissionsResult(int requestCode, String[] permissions, int[] grantResults) {
        if (requestCode != RC_HANDLE_CAMERA_PERM) {
            Log.d(TAG, ""Got unexpected permission result: "" + requestCode);
            super.onRequestPermissionsResult(requestCode, permissions, grantResults);
            return;
        }

        if (grantResults.length != 0 &amp;&amp; grantResults[0] == PackageManager.PERMISSION_GRANTED &amp;&amp;grantResults[1] == PackageManager.PERMISSION_GRANTED) {
            Log.d(TAG, ""Camera permission granted - initialize the camera source"");
            // we have permission, so create the camerasource
            createCameraSource();
            return;
        }

        Log.e(TAG, ""Permission not granted: results len = "" + grantResults.length +
                "" Result code = "" + (grantResults.length &gt; 0 ? grantResults[0] : ""(empty)""));

        DialogInterface.OnClickListener listener = new DialogInterface.OnClickListener() {
            public void onClick(DialogInterface dialog, int id) {
                finish();
            }
        };

        AlertDialog.Builder builder = new AlertDialog.Builder(this);
        builder.setTitle(""Face Tracker sample"")
                .setMessage(R.string.no_camera_permission)
                .setPositiveButton(R.string.ok, listener)
                .show();
    }

    //==============================================================================================
    // Camera Source Preview
    //==============================================================================================

    /**
     * Starts or restarts the camera source, if it exists.  If the camera source doesn't exist yet
     * (e.g., because onResume was called before the camera source was created), this will be called
     * again when the camera source is created.
     */
    private void startCameraSource() {

        // check that the device has play services available.
        int code = GoogleApiAvailability.getInstance().isGooglePlayServicesAvailable(
                getApplicationContext());
        if (code != ConnectionResult.SUCCESS) {
            Dialog dlg =
                    GoogleApiAvailability.getInstance().getErrorDialog(this, code, RC_HANDLE_GMS);
            dlg.show();
        }

        if (mCameraSource != null) {
            try {
                mPreview.start(mCameraSource, mGraphicOverlay);
            } catch (IOException e) {
                Log.e(TAG, ""Unable to start camera source."", e);
                mCameraSource.release();
                mCameraSource = null;
            }
        }
    }

    //==============================================================================================
    // Graphic Face Tracker
    //==============================================================================================

    /**
     * Factory for creating a face tracker to be associated with a new face.  The multiprocessor
     * uses this factory to create face trackers as needed -- one for each individual.
     */
    private class GraphicFaceTrackerFactory implements MultiProcessor.Factory&lt;Face&gt; {
        @Override
        public Tracker&lt;Face&gt; create(Face face) {
            return new GraphicFaceTracker(mGraphicOverlay);
        }
    }

    /**
     * Face tracker for each detected individual. This maintains a face graphic within the app's
     * associated face overlay.
     */
    private class GraphicFaceTracker extends Tracker&lt;Face&gt; {
        private GraphicOverlay mOverlay;
        private FaceGraphic mFaceGraphic;

        GraphicFaceTracker(GraphicOverlay overlay) {
            mOverlay = overlay;
            mFaceGraphic = new FaceGraphic(overlay);
        }

        /**
         * Start tracking the detected face instance within the face overlay.
         */
        @Override
        public void onNewItem(int faceId, Face item) {
            mFaceGraphic.setId(faceId);
        }

    **`strong text`**
        @Override
        public void onUpdate(FaceDetector.Detections&lt;Face&gt; detectionResults, Face face) {
            mOverlay.add(mFaceGraphic);
            mFaceGraphic.updateFace(face);
        }

        /**
         * Hide the graphic when the corresponding face was not detected.  This can happen for
         * intermediate frames temporarily (e.g., if the face was momentarily blocked from
         * view).
         */
        @Override
        public void onMissing(FaceDetector.Detections&lt;Face&gt; detectionResults) {
            mOverlay.remove(mFaceGraphic);
        }

        /**
         * Called when the face is assumed to be gone for good. Remove the graphic annotation from
         * the overlay.
         */
        @Override
        public void onDone() {
            mOverlay.remove(mFaceGraphic);
        }
    }


    private void captureImage() {
        mPreview.setDrawingCacheEnabled(true);
       final Bitmap drawingCache = mPreview.getDrawingCache();



        mCameraSource.takePicture(null, new CameraSource.PictureCallback() {
            @Override
            public void onPictureTaken(byte[] bytes) {
                int orientation = Exif.getOrientation(bytes);


                Bitmap temp = BitmapFactory.decodeByteArray(bytes, 0, bytes.length);
                Bitmap picture = rotateImage(temp,orientation);
                Bitmap overlay = Bitmap.createBitmap(mGraphicOverlay.getWidth(),mGraphicOverlay.getHeight(),picture.getConfig());
                Canvas canvas = new Canvas(overlay);

                Matrix matrix = new Matrix();

                matrix.setScale((float)overlay.getWidth()/(float)picture.getWidth(),(float)overlay.getHeight()/(float)picture.getHeight());

                // mirror by inverting scale and translating
                matrix.preScale(-1, 1);
                matrix.postTranslate(canvas.getWidth(), 0);

                Paint paint = new Paint();
                canvas.drawBitmap(picture,matrix,paint);
                canvas.drawBitmap(drawingCache,0,0,paint);

                try {
                    String mainpath = getExternalStorageDirectory() + separator + ""MaskIt"" + separator + ""images"" + separator;
                    File basePath = new File(mainpath);
                    if (!basePath.exists())
                        Log.d(""CAPTURE_BASE_PATH"", basePath.mkdirs() ? ""Success"": ""Failed"");
                    String path = mainpath + ""photo_"" + getPhotoTime() + "".jpg"";
                    File captureFile = new File(path);
                    captureFile.createNewFile();
                    if (!captureFile.exists())
                        Log.d(""CAPTURE_FILE_PATH"", captureFile.createNewFile() ? ""Success"": ""Failed"");
                    FileOutputStream stream = new FileOutputStream(captureFile);
                    overlay.compress(Bitmap.CompressFormat.PNG, 100, stream);
                    stream.flush();
                    stream.close();
                    picture.recycle();
                    drawingCache.recycle();
                    mPreview.setDrawingCacheEnabled(false);
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }

            private String getPhotoTime() {
                DateFormat dateFormatter = new SimpleDateFormat(""yyyyMMdd hhmmss"");
                dateFormatter.setLenient(false);
                Date today = new Date();
                String s = dateFormatter.format(today);
                return  s;
            }
        });
    }




    private Bitmap rotateImage(Bitmap bm, int i) {
        Matrix matrix = new Matrix();
        switch (i) {
            case ExifInterface.ORIENTATION_NORMAL:
                return bm;
            case ExifInterface.ORIENTATION_FLIP_HORIZONTAL:
                matrix.setScale(-1, 1);
                break;
            case ExifInterface.ORIENTATION_ROTATE_180:
                matrix.setRotate(180);
                break;
            case ExifInterface.ORIENTATION_FLIP_VERTICAL:
                matrix.setRotate(180);
                matrix.postScale(-1, 1);
                break;
            case ExifInterface.ORIENTATION_TRANSPOSE:
                matrix.setRotate(90);
                matrix.postScale(-1, 1);
                break;
            case ExifInterface.ORIENTATION_ROTATE_90:
                matrix.setRotate(90);
                break;
            case ExifInterface.ORIENTATION_TRANSVERSE:
                matrix.setRotate(-90);
                matrix.postScale(-1, 1);
                break;
            case ExifInterface.ORIENTATION_ROTATE_270:
                matrix.setRotate(-90);
                break;
            default:
                return bm;
        }
        try {
            Bitmap bmRotated = Bitmap.createBitmap(bm, 0, 0, bm.getWidth(), bm.getHeight(), matrix, true);
            bm.recycle();
            return bmRotated;
        } catch (OutOfMemoryError e) {
            e.printStackTrace();
            return null;
        }
    }





}







CameraSourcePreview.java
------------------------


package com.google.android.gms.samples.vision.face.facetracker.ui.camera;

import android.content.Context;
import android.content.res.Configuration;
import android.util.AttributeSet;
import android.util.Log;
import android.view.SurfaceHolder;
import android.view.SurfaceView;
import android.view.ViewGroup;

import com.google.android.gms.common.images.Size;
import com.google.android.gms.vision.CameraSource;

import java.io.IOException;

public class CameraSourcePreview extends ViewGroup {
    private static final String TAG = ""CameraSourcePreview"";

    private Context mContext;
    private SurfaceView mSurfaceView;
    private boolean mStartRequested;
    private boolean mSurfaceAvailable;
    private CameraSource mCameraSource;

    private GraphicOverlay mOverlay;

    public CameraSourcePreview(Context context, AttributeSet attrs) {
        super(context, attrs);
        mContext = context;
        mStartRequested = false;
        mSurfaceAvailable = false;

        mSurfaceView = new SurfaceView(context);
        mSurfaceView.getHolder().addCallback(new SurfaceCallback());
        addView(mSurfaceView);
    }

    public void start(CameraSource cameraSource) throws IOException {
        if (cameraSource == null) {
            stop();
        }

        mCameraSource = cameraSource;

        if (mCameraSource != null) {
            mStartRequested = true;
            startIfReady();
        }
    }

    public void start(CameraSource cameraSource, GraphicOverlay overlay) throws IOException {
        mOverlay = overlay;
        start(cameraSource);
    }

    public void stop() {
        if (mCameraSource != null) {
            mCameraSource.stop();
        }
    }

    public void release() {
        if (mCameraSource != null) {
            mCameraSource.release();
            mCameraSource = null;
        }
    }

    private void startIfReady() throws IOException {
        if (mStartRequested &amp;&amp; mSurfaceAvailable) {
            mCameraSource.start(mSurfaceView.getHolder());
            if (mOverlay != null) {
                Size size = mCameraSource.getPreviewSize();
                int min = Math.min(size.getWidth(), size.getHeight());
                int max = Math.max(size.getWidth(), size.getHeight());
                if (isPortraitMode()) {
                    // Swap width and height sizes when in portrait, since it will be rotated by
                    // 90 degrees
                    mOverlay.setCameraInfo(min, max, mCameraSource.getCameraFacing());
                } else {
                    mOverlay.setCameraInfo(max, min, mCameraSource.getCameraFacing());
                }
                mOverlay.clear();
            }
            mStartRequested = false;
        }
    }

    private class SurfaceCallback implements SurfaceHolder.Callback {
        @Override
        public void surfaceCreated(SurfaceHolder surface) {
            mSurfaceAvailable = true;
            try {
                startIfReady();
            } catch (IOException e) {
                Log.e(TAG, ""Could not start camera source."", e);
            }
        }

        @Override
        public void surfaceDestroyed(SurfaceHolder surface) {
            mSurfaceAvailable = false;
        }

        @Override
        public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {
        }
    }

    @Override
    protected void onLayout(boolean changed, int left, int top, int right, int bottom) {
        int previewWidth = 320;
        int previewHeight = 240;
        if (mCameraSource != null) {
            Size size = mCameraSource.getPreviewSize();
            if (size != null) {
                previewWidth = size.getWidth();
                previewHeight = size.getHeight();
            }
        }

        // Swap width and height sizes when in portrait, since it will be rotated 90 degrees
        if (isPortraitMode()) {
            int tmp = previewWidth;
            previewWidth = previewHeight;
            previewHeight = tmp;
        }

        final int viewWidth = right - left;
        final int viewHeight = bottom - top;

        int childWidth;
        int childHeight;
        int childXOffset = 0;
        int childYOffset = 0;
        float widthRatio = (float) viewWidth / (float) previewWidth;
        float heightRatio = (float) viewHeight / (float) previewHeight;

        // To fill the view with the camera preview, while also preserving the correct aspect ratio,
        // it is usually necessary to slightly oversize the child and to crop off portions along one
        // of the dimensions.  We scale up based on the dimension requiring the most correction, and
        // compute a crop offset for the other dimension.
        if (widthRatio &gt; heightRatio) {
            childWidth = viewWidth;
            childHeight = (int) ((float) previewHeight * widthRatio);
            childYOffset = (childHeight - viewHeight) / 2;
        } else {
            childWidth = (int) ((float) previewWidth * heightRatio);
            childHeight = viewHeight;
            childXOffset = (childWidth - viewWidth) / 2;
        }

        for (int i = 0; i &lt; getChildCount(); ++i) {
            // One dimension will be cropped.  We shift child over or up by this offset and adjust
            // the size to maintain the proper aspect ratio.
            getChildAt(i).layout(
                    -1 * childXOffset, -1 * childYOffset,
                    childWidth - childXOffset, childHeight - childYOffset);
        }

        try {
            startIfReady();
        } catch (IOException e) {
            Log.e(TAG, ""Could not start camera source."", e);
        }
    }


    private boolean isPortraitMode() {
        int orientation = mContext.getResources().getConfiguration().orientation;
        if (orientation == Configuration.ORIENTATION_LANDSCAPE) {
            return false;
        }
        if (orientation == Configuration.ORIENTATION_PORTRAIT) {
            return true;
        }

        Log.d(TAG, ""isPortraitMode returning false by default"");
        return false;
    }
}


GraphicOverlay.java
-------------------

package com.google.android.gms.samples.vision.face.facetracker.ui.camera;

import android.content.Context;
import android.graphics.Canvas;
import android.util.AttributeSet;
import android.view.View;

import com.google.android.gms.vision.CameraSource;

import java.util.HashSet;
import java.util.Set;


public class GraphicOverlay extends View {
    private final Object mLock = new Object();
    private int mPreviewWidth;
    private float mWidthScaleFactor = 1.0f;
    private int mPreviewHeight;
    private float mHeightScaleFactor = 1.0f;
    private int mFacing = CameraSource.CAMERA_FACING_BACK;
    private Set&lt;Graphic&gt; mGraphics = new HashSet&lt;&gt;();

    public static abstract class Graphic {
        private GraphicOverlay mOverlay;

        public Graphic(GraphicOverlay overlay) {
            mOverlay = overlay;
        }


        public abstract void draw(Canvas canvas);


        public float scaleX(float horizontal) {
            return horizontal * mOverlay.mWidthScaleFactor;
        }

        public float scaleY(float vertical) {
            return vertical * mOverlay.mHeightScaleFactor;
        }


        public float translateX(float x) {
            if (mOverlay.mFacing == CameraSource.CAMERA_FACING_FRONT) {
                return mOverlay.getWidth() - scaleX(x);
            } else {
                return scaleX(x);
            }
        }


        public float translateY(float y) {
            return scaleY(y);
        }

        public void postInvalidate() {
            mOverlay.postInvalidate();
        }
    }

    public GraphicOverlay(Context context, AttributeSet attrs) {
        super(context, attrs);
    }


    public void clear() {
        synchronized (mLock) {
            mGraphics.clear();
        }
        postInvalidate();
    }

    public void add(Graphic graphic) {
        synchronized (mLock) {
            mGraphics.add(graphic);
        }
        postInvalidate();
    }

    public void remove(Graphic graphic) {
        synchronized (mLock) {
            mGraphics.remove(graphic);
        }
        postInvalidate();
    }


    public void setCameraInfo(int previewWidth, int previewHeight, int facing) {
        synchronized (mLock) {
            mPreviewWidth = previewWidth;
            mPreviewHeight = previewHeight;
            mFacing = facing;
        }
        postInvalidate();
    }

    /**
     * Draws the overlay with its associated graphic objects.
     */
    @Override
    protected void onDraw(Canvas canvas) {
        super.onDraw(canvas);

        synchronized (mLock) {
            if ((mPreviewWidth != 0) &amp;&amp; (mPreviewHeight != 0)) {
                mWidthScaleFactor = (float) canvas.getWidth() / (float)mPreviewWidth;
                mHeightScaleFactor = (float) canvas.getHeight() / (float) mPreviewHeight;
            }

            for (Graphic graphic : mGraphics) {
                graphic.draw(canvas);
            }
        }
    }
}
</code></pre>",,1,0,,2019-03-28 06:37:14.693 UTC,1,2019-03-28 12:35:57.953 UTC,,,,,10233861,1,1,java|android|crop|face-detection|google-vision,42
Unable to authorize Google Vision API,54082512,Unable to authorize Google Vision API,"<p>I'm not able to call the Google Vision API due to authorization issues. The exception tells me to set the environment variable GOOGLE_APPLICATION_CREDENTIALS.</p>

<p>Google explains that you have to set an environment variable as such:
<a href=""https://cloud.google.com/docs/authentication/production"" rel=""nofollow noreferrer"">https://cloud.google.com/docs/authentication/production</a></p>

<p>I generated my credentials (in a .json file) and I have already set my system environment variable manually to: <code>GOOGLE_APPLICATION_CREDENTIALS=C:\Users\[username]\Downloads\[FILE_NAME].json</code></p>

<p>Previously, I had a similar approach working.</p>

<p>Does anybody have ideas of things I could try to make this work?</p>",54089458,1,3,,2019-01-07 22:02:17.400 UTC,,2019-01-08 10:48:00.583 UTC,2019-01-08 10:48:00.583 UTC,,3396821,,6791783,1,0,google-cloud-platform,33
How can i configure my custom s3 putevent that triggers AWS lambda?,54587451,How can i configure my custom s3 putevent that triggers AWS lambda?,"<p>It would be very much help full if someone might walk me through this, i Have tried going through event structure but was not that helpful !</p>

<p><a href=""https://docs.aws.amazon.com/AmazonS3/latest/dev/notification-content-structure"" rel=""nofollow noreferrer"">Aws lambda Event structure</a></p>

<p>I am still not abel to understand Many of u attended the question its grate full, but yet im not able to understand you, I'am completly new to aws.</p>

<p>I went through the event structure, it says you will find the configuration id over here 
 ""configurationId"":""ID found in the bucket notification configuration""
but i could not</p>

<p>and I'am still clue less about </p>

<blockquote>
  <p>""x-amz-request-id"":""Amazon S3 generated request ID"",</p>
  
  <p>""x-amz-id-2"":""Amazon S3 host that processed the request""</p>
</blockquote>

<pre><code>from __future__ import print_function

import boto3
from decimal import Decimal
import json
import urllib

print('Loading function')

rekognition = boto3.client('rekognition')
iot = boto3.client('iot-data')


# --------------- Helper Functions to call Rekognition APIs ------------------

def compare_faces(bucket, key, key_target, threshold=90):
    response = rekognition.compare_faces(
        SourceImage={
            ""S3Object"": {
                ""Bucket"": 'dacsup',
                ""Name"": 'obama.jpg',
            }
        },
        TargetImage={
            ""S3Object"": {
                ""Bucket"": 'targetts',
                ""Name"": 'obama2.jpg',
            }
        },
        SimilarityThreshold=threshold,
    )
    return response['SourceImageFace'], response['FaceMatches']

# --------------- Main handler ------------------


def lambda_handler(event, context):
    print(""Received event: "" + json.dumps(event, indent=2))
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = urllib.unquote_plus(event['Records'][0]['s3']['object']['key'].encode('utf8'))
    key_target = ""targetts/"" + key
    try:
        response = compare_faces(bucket, key, key_target)
        print(response)
        mypayload = json.dumps(response)
        iotResponse = iot.publish(
            topic=""rekognition/result"",
            qos=1,
            payload=mypayload)
        print(iotResponse)
        return iotResponse
        print(response)
        return response
    except Exception as e:
        print(e)
        print(""Error processing object {} from bucket {}. "".format(key, bucket) +
              ""Make sure your object and bucket exist and your bucket is in the same region as this function."")
        raise e
</code></pre>

<p>-----------------------event------------------</p>

<pre><code>{
  ""Records"": [
    {
      ""eventVersion"": ""2.1"",
      ""eventTime"": ""2019-02-08T11:49:26.471Z"",
      ""requestParameters"": {
        ""sourceIPAddress"": ""My ip""
      },
      ""s3"": {
        ""configurationId"": ""----------------"",
        ""object"": {
          ""eTag"": ""99b7ce351fec8c0e7b30fd194a8c81b3"",
          ""sequencer"": ""-----------"",
          ""key"": ""obama.jpg"",
          ""size"": 5908
        },
        ""bucket"": {
          ""arn"": ""  arn:aws:s3:::bucketname"",
          ""name"": "" dacsup "",
          ""ownerIdentity"": {
            ""principalId"": ""Mypprincipalid""
          }
        },
        ""s3SchemaVersion"": ""1.0""
      },
      ""responseElements"": {
        ""x-amz-id-2"": ""-------------"",
        ""x-amz-request-id"": ""-----------""
      },
      ""awsRegion"": "" us-east-2"",
      ""eventName"": ""ObjectCreated:Put"",
      ""userIdentity"": {
        ""principalId"": ""myrincipalid""
      },
      ""eventSource"": ""aws:s3""
    }
  ]
}
</code></pre>

<p>error----------------------------xxxxxxxxx------------------------</p>

<pre><code>START RequestId: 24820efa-d454-4ae4-9c49-e2eedd1c96ce Version: $LATEST
Received event: {
  ""Records"": [
    {
      ""eventVersion"": ""2.1"", 
      ""eventTime"": ""2019-03-04T11:49:26.471Z"", 
      ""requestParameters"": {
        ""sourceIPAddress"": ""My ip""
      }, 
      ""s3"": {
        ""configurationId"": """", 
        ""object"": {
          ""eTag"": ""99b7ce351fec8c0e7b30fd194a8c81b3 "", 
          ""key"": ""obama.jpg"", 
          ""sequencer"": ""0A1B2C3D4E5F678901 "", 
          ""size"": 5908
        }, 
        ""bucket"": {
          ""ownerIdentity"": {
            ""principalId"": ""My pid""
          }, 
          ""name"": "" dacsup "", 
          ""arn"": ""  arn:aws:s3:::dacsup ""
        }, 
        ""s3SchemaVersion"": ""1.0""
      }, 
      ""responseElements"": {
        ""x-amz-id-2"": """", 
        ""x-amz-request-id"": """"
      }, 
      ""awsRegion"": "" us-east-2"", 
      ""eventName"": ""ObjectCreated:Put"", 
      ""userIdentity"": {
        ""principalId"": ""pid""
      }, 
      ""eventSource"": ""aws:s3""
    }
  ]
}
An error occurred (InvalidS3ObjectException) when calling the CompareFaces operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.
Error processing object obama.jpg from bucket . Make sure your object and bucket exist and your bucket is in the same region as this function.
An error occurred (InvalidS3ObjectException) when calling the CompareFaces operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.: InvalidS3ObjectException
Traceback (most recent call last):
  File ""/var/task/lambda_function.py"", line 59, in lambda_handler
    raise e
InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the CompareFaces operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.

END RequestId: 24820efa-d454-4ae4-9c49-e2eedd1c96ce
REPORT RequestId: 24820efa-d454-4ae4-9c49-e2eedd1c96ce  Duration: 372.53 ms Billed Duration: 400 ms     Memory Size: 128 MB Max Memory Used: 67 MB  
</code></pre>",,1,14,,2019-02-08 07:01:59.430 UTC,,2019-03-04 10:26:02.783 UTC,2019-03-04 10:26:02.783 UTC,,10599727,,10599727,1,0,amazon-web-services|amazon-s3|aws-lambda|aws-iot,94
App with integrated Google Cloud Vision API crashes with null object reference,54612783,App with integrated Google Cloud Vision API crashes with null object reference,"<p>I am currently working on an Android app where I want to integrate the Google Cloud Vision API and do Facial Recognition on images. </p>

<p>The code I implemented so far to make this work: </p>

<pre><code>import android.app.Activity;
import android.content.Context;
import android.content.Intent;
import android.content.pm.ActivityInfo;
import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.graphics.Matrix;
import android.media.ExifInterface;
import android.net.Uri;
import android.os.AsyncTask;
import android.os.Bundle;
import android.os.Environment;
import android.provider.MediaStore;
import android.support.v4.content.FileProvider;
import android.support.v7.app.AppCompatActivity;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.Toast;

import com.google.api.client.extensions.android.json.AndroidJsonFactory;
import com.google.api.client.http.javanet.NetHttpTransport;
import com.google.api.services.vision.v1.Vision;
import com.google.api.services.vision.v1.VisionRequestInitializer;
import com.google.api.services.vision.v1.model.AnnotateImageRequest;
import com.google.api.services.vision.v1.model.BatchAnnotateImagesRequest;
import com.google.api.services.vision.v1.model.BatchAnnotateImagesResponse;
import com.google.api.services.vision.v1.model.FaceAnnotation;
import com.google.api.services.vision.v1.model.Feature;
import com.google.api.services.vision.v1.model.Image;

import org.apache.commons.io.IOUtils;

import java.io.ByteArrayOutputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.Arrays;
import java.util.List;

import static org.apache.commons.io.IOUtils.toByteArray;

public class HomeActivity extends AppCompatActivity {

    /* Variables */
    MarshMallowPermission mmp = new MarshMallowPermission(this);


    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.home);
        super.setRequestedOrientation(ActivityInfo.SCREEN_ORIENTATION_PORTRAIT);

        Vision.Builder visionBuilder = new Vision.Builder(
                new NetHttpTransport(),
                new AndroidJsonFactory(),
                null);

        visionBuilder.setVisionRequestInitializer(
                new VisionRequestInitializer(""AIzaSyCnPwvnEQakkUXpkFaj2TcwJs_E3DPqjm0""));
        final Vision vision = visionBuilder.build();

        // Create new thread
        AsyncTask.execute(new Runnable() {
            @Override
            public void run() {
                // Convert photo to byte array
                InputStream inputStream =
                        getResources().openRawResource(R.raw.apollo9);
                byte[] photoData = new byte[0];
                try {
                    photoData = toByteArray(inputStream);
                } catch (IOException e) {
                    e.printStackTrace();
                }
                try {
                    inputStream.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }

                Image inputImage = new Image();
                inputImage.encodeContent(photoData);
                Feature desiredFeature = new Feature();
                desiredFeature.setType(""FACE_DETECTION"");
                AnnotateImageRequest request = new AnnotateImageRequest();
                request.setImage(inputImage);
                request.setFeatures(Arrays.asList(desiredFeature));
                BatchAnnotateImagesRequest batchRequest =
                        new BatchAnnotateImagesRequest();

                batchRequest.setRequests(Arrays.asList(request));
                BatchAnnotateImagesResponse batchResponse =
                        null;
                try {
                    batchResponse = vision.images().annotate(batchRequest).execute();
                } catch (IOException e) {
                    e.printStackTrace();
                }
                List&lt;FaceAnnotation&gt; faces = batchResponse.getResponses()
                        .get(0).getFaceAnnotations();

                                    // Count faces
                                    int numberOfFaces = faces.size();

                    // Get joy likelihood for each face
                                    String likelihoods = """";
                                    for(int i=0; i&lt;numberOfFaces; i++) {
                                        likelihoods += ""\n It is "" +
                                                faces.get(i).getJoyLikelihood() +
                                                "" that face "" + i + "" is happy"";
                                    }

                    // Concatenate everything
                                    final String message =
                                            ""This photo has "" + numberOfFaces + "" faces"" + likelihoods;

                    // Display toast on UI thread
                                    runOnUiThread(new Runnable() {
                                        @Override
                                        public void run() {
                                            Toast.makeText(getApplicationContext(),
                                                    message, Toast.LENGTH_LONG).show();
                                        }
                                    });
            }
        });



        // Check for Permission for Storage
        if (!mmp.checkPermissionForReadExternalStorage()) {
            mmp.requestPermissionForReadExternalStorage();
        }

        // MARK: - CAMERA BUTTON ------------------------------------
        Button camButt = findViewById(R.id.cameraButt);
        camButt.setOnClickListener(new View.OnClickListener() {
          @Override
          public void onClick(View view) {
              if (!mmp.checkPermissionForCamera()) {
                  mmp.requestPermissionForCamera();
              } else { openCamera(); }
        }});


        // MARK: - GALLERY BUTTON ------------------------------------
        Button galleryButt = findViewById(R.id.galleryButt);
        galleryButt.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                if (!mmp.checkPermissionForReadExternalStorage()) {
                    mmp.requestPermissionForReadExternalStorage();
                } else { openGallery(); }
        }});





    }// end onCreate()




    // IMAGE HANDLING METHODS ------------------------------------------------------------------------
    int CAMERA = 0;
    int GALLERY = 1;
    Uri imageURI;
    File file;


    // OPEN CAMERA
    public void openCamera() {
        Intent intent= new Intent(MediaStore.ACTION_IMAGE_CAPTURE);
        file = new File(Environment.getExternalStorageDirectory(), ""image.jpg"");
        imageURI = FileProvider.getUriForFile(getApplicationContext(), getPackageName() + "".provider"", file);
        intent.putExtra(MediaStore.EXTRA_OUTPUT, imageURI);
        startActivityForResult(intent, CAMERA);
    }


    // OPEN GALLERY
    public void openGallery() {
        Intent intent = new Intent();
        intent.setType(""image/*"");
        intent.setAction(Intent.ACTION_GET_CONTENT);
        startActivityForResult(Intent.createChooser(intent, ""Select Image""), GALLERY);
    }



    // IMAGE PICKED DELEGATE -----------------------------------
    @Override
    public void onActivityResult(int requestCode, int resultCode, Intent data) {
        super.onActivityResult(requestCode, resultCode, data);

        if (resultCode == Activity.RESULT_OK) {
            Bitmap bm = null;

            // Image from Camera
            if (requestCode == CAMERA) {

                try {
                    File f = file;
                    ExifInterface exif = new ExifInterface(f.getPath());
                    int orientation = exif.getAttributeInt(ExifInterface.TAG_ORIENTATION, ExifInterface.ORIENTATION_NORMAL);

                    int angle = 0;
                    if (orientation == ExifInterface.ORIENTATION_ROTATE_90) { angle = 90; }
                    else if (orientation == ExifInterface.ORIENTATION_ROTATE_180) { angle = 180; }
                    else if (orientation == ExifInterface.ORIENTATION_ROTATE_270) { angle = 270; }
                    Log.i(""log-"", ""ORIENTATION: "" + orientation);

                    Matrix mat = new Matrix();
                    mat.postRotate(angle);

                    Bitmap bmp = BitmapFactory.decodeStream(new FileInputStream(f), null, null);
                    bm = Bitmap.createBitmap(bmp, 0, 0, bmp.getWidth(), bmp.getHeight(), mat, true);

                    // Get FINAL IMAGE URI
                    Configs.finalImageUri = getImageUri(HomeActivity.this, bm);

                }
                catch (IOException | OutOfMemoryError e) { Log.i(""log-"", e.getMessage()); }


                // Image from Gallery
            } else if (requestCode == GALLERY) {
                try {
                    bm = MediaStore.Images.Media.getBitmap(getApplicationContext().getContentResolver(), data.getData());

                    // Get FINAL IMAGE URI
                    Configs.finalImageUri = getImageUri(HomeActivity.this, bm);

                } catch (IOException e) { e.printStackTrace(); }
            }

            Log.i(""log-"", ""FINAL IMAGE URI: "" + Configs.finalImageUri);
            startActivity(new Intent(HomeActivity.this, ImageEditor.class));
        }

    }
    //---------------------------------------------------------------------------------------------




    // Method to get URI of a stored image
    public Uri getImageUri(Context inContext, Bitmap inImage) {
        ByteArrayOutputStream bytes = new ByteArrayOutputStream();
        inImage.compress(Bitmap.CompressFormat.JPEG, 100, bytes);
        String path = MediaStore.Images.Media.insertImage(inContext.getContentResolver(), inImage, ""image"", null);
        return Uri.parse(path);
    }



}// @end
</code></pre>

<p>The Layout XML File: </p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;RelativeLayout
    xmlns:android=""http://schemas.android.com/apk/res/android""
    xmlns:app=""http://schemas.android.com/apk/res-auto""
    xmlns:tools=""http://schemas.android.com/tools""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""
    tools:context="".HomeActivity""&gt;


    &lt;TextView
        android:id=""@+id/textView2""
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:layout_alignParentTop=""true""
        android:layout_centerHorizontal=""true""
        android:layout_marginTop=""50dp""
        android:text=""TEXTIFEYE""
        android:textSize=""30sp"" /&gt;

    &lt;ImageView
        android:id=""@+id/imageView""
        android:layout_width=""100dp""
        android:layout_height=""100dp""
        android:layout_below=""@+id/textView2""
        android:layout_centerHorizontal=""true""
        android:layout_marginTop=""10dp""
        android:scaleType=""centerCrop""
        app:srcCompat=""@drawable/logo"" /&gt;

    &lt;Button
        android:id=""@+id/cameraButt""
        android:layout_width=""44dp""
        android:layout_height=""44dp""
        android:layout_below=""@+id/imageView""
        android:layout_marginTop=""50dp""
        android:layout_toStartOf=""@+id/imageView""
        android:background=""@drawable/camera_butt"" /&gt;

    &lt;TextView
        android:id=""@+id/textView3""
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:layout_alignEnd=""@+id/cameraButt""
        android:layout_alignStart=""@+id/cameraButt""
        android:layout_below=""@+id/cameraButt""
        android:text=""CAMERA""
        android:textAlignment=""center""
        android:textSize=""10sp"" /&gt;

    &lt;Button
        android:id=""@+id/galleryButt""
        android:layout_width=""44dp""
        android:layout_height=""44dp""
        android:layout_alignTop=""@+id/cameraButt""
        android:layout_toEndOf=""@+id/imageView""
        android:background=""@drawable/gallery_butt"" /&gt;

    &lt;TextView
        android:id=""@+id/textView4""
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:layout_alignEnd=""@+id/galleryButt""
        android:layout_alignStart=""@+id/galleryButt""
        android:layout_below=""@+id/galleryButt""
        android:text=""GALLERY""
        android:textSize=""10sp"" /&gt;
&lt;/RelativeLayout&gt;
</code></pre>

<p>The Log message: </p>

<pre><code>java.lang.NullPointerException: Attempt to invoke virtual method 'java.util.List com.google.api.services.vision.v1.model.BatchAnnotateImagesResponse.getResponses()' on a null object reference
    at gmbh.webagenten.textifeye.HomeActivity$1.run(HomeActivity.java:113)
    at android.os.AsyncTask$SerialExecutor$1.run(AsyncTask.java:245)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
    at java.lang.Thread.run(Thread.java:764)
</code></pre>

<p>I basically don´t know why the app crashes there with a null object reference and would appreciate any hints and feedback, thanks!</p>",54626451,1,0,,2019-02-10 02:22:53.893 UTC,,2019-02-11 08:25:44.277 UTC,,,,,1120165,1,1,android|google-vision,50
Aws Rekognition Text detect,55302297,Aws Rekognition Text detect,"<p>I have been trying to use the Rekognition API to detect text in an image.</p>

<p>I have enabled full access for the Rekognition API (IAM), and am configuring the credentials and region in config of my app.</p>

<p>Here is my code: </p>

<pre><code>client = Aws::Rekognition::Client.new
resp = client.detect_text({
  image: {
    s3_object: {
      bucket: bucket_name,
      name: ""uploads/path/#{image_files.first}"",
    },
  },
})
</code></pre>

<p>I have tested the API out with other methods such as 'detect_labels' and this returns data as expected, so the issue is not to do with the API not being enabled. </p>

<p>My error is 'undefined method `detect_text' for Aws::Rekognition::Client>', which suggests the request isn't even getting to the body.</p>

<p>The gem I am using is 'aws-sdk-rekognition', '~> 1.0.0.rc2', which as mentioned works for detect_labels but not detect_text.</p>

<p>I am not sure what the issue might be, here are the docs for the method <a href=""https://docs.aws.amazon.com/sdkforruby/api/Aws/Rekognition/Client.html#detect_text-instance_method"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sdkforruby/api/Aws/Rekognition/Client.html#detect_text-instance_method</a>.</p>",55308987,1,0,,2019-03-22 14:52:24.767 UTC,,2019-03-22 23:19:06.887 UTC,,,,,10662470,1,0,ruby-on-rails|ruby|amazon-web-services|amazon-rekognition,56
Issue with Azure Computer Vision API,55376350,Issue with Azure Computer Vision API,"<p>I am facing an issue with Azure Computer Vision API. If I send a request with contentType = application/json and image URL in JSON request body things work fine but on sending a binary image(base 64 encoded) with contentType = application/octet-stream it gives me ImageFormatInvalid in the resp</p>

<pre><code>[
    {
        ""key"": ""Ocp-Apim-Subscription-Key"", 
        ""value"": ""6f7ff175139e4e1d8b943c6170fe5b8e"",
        ""description"": """"
    },
    {
        ""key"": ""Content-Type"",
        ""value"": ""multipart/form-data"",
        ""description"": """"
    }
]
</code></pre>

<p>Content-Type: multipart/form-data and asking input as binary image data</p>",,1,1,,2019-03-27 11:41:48.753 UTC,,2019-03-27 14:30:35.577 UTC,2019-03-27 14:30:35.577 UTC,,1752496,,11225763,1,0,azure|azure-cognitive-services,34
"How do I fix ""startActivity() in ContextCompat cannot be applied to?""",56155219,"How do I fix ""startActivity() in ContextCompat cannot be applied to?""","<p>I am using Google Vision OCR to grab the email from a business card (the OCR Graphic activity) and send it to the the To destination in the SendEmail activity. My log shows that the email text is detected. </p>

<p>I tried to set the intent to send it to the next activity, but I am getting two errors, ""cannot resolve constructor Intent"" on my new intent, and start activity cannot be applied to.</p>

<p>This is the OcrGraphic activity</p>

<pre class=""lang-java prettyprint-override""><code>List&lt;Line&gt; lines = (List&lt;Line&gt;) text.getComponents();
        for(Line elements : lines) {
            float left = translateX(elements.getBoundingBox().left);
            float bottom = translateY(elements.getBoundingBox().bottom);
            if (elements != null &amp;&amp; elements.getValue() != null) {


                if (elements.getValue().matches(""^[_A-Za-z0-9-\\\\+]+(\\\\.[_A-Za-z0-9-]+)*@\""\n"" +
                        ""\t\t+ \""[A-Za-z0-9-]+(\\\\.[A-Za-z0-9]+)*(\\\\.[A-Za-z]{2,})$"") || elements.getValue().contains(""@"")) {
                    Log.e(""elementsemail"", elements.getValue());
                    String email;
                    email = elements.getValue();
                    cEmail = email;
                    Intent sendIntent = new Intent(this, SendEmail.class);
                    sendIntent.putExtra(Intent.EXTRA_EMAIL, cEmail);
                    startActivity(sendIntent);


                }
</code></pre>

<p>this is my Send Email activity</p>

<pre class=""lang-java prettyprint-override""><code>private void sendMail(){

        Intent getIntent = getIntent();
        String recipientList = getIntent.getStringExtra(OcrGraphic.cEmail);;
        String[] recipients = recipientList.split("","");
        String subject = mEditTextSubject.getText().toString();
        String message = mEditTextMessage.getText().toString();

        Intent intent = new Intent(Intent.ACTION_SEND);
        intent.putExtra(Intent.EXTRA_EMAIL, recipients);
        intent.putExtra(Intent.EXTRA_SUBJECT, subject);
        intent.putExtra(Intent.EXTRA_TEXT, message);
        intent.setType(""message/rfc822"");
        startActivity(Intent.createChooser(intent, ""Choose an email client""));
    }
</code></pre>

<p>I want to send the email address to the SendEmail activity. I am new to java and android, any help is welcomed.</p>",,2,1,,2019-05-15 18:01:46.717 UTC,,2019-05-15 21:02:31.533 UTC,,,,,8351390,1,0,java|android|android-intent|google-cloud-vision,24
Missing description in Web Detection feature of Google Cloud Vision API?,54295015,Missing description in Web Detection feature of Google Cloud Vision API?,"<p>I'm using the <code>[Web Detection][1]</code> feature of Google Cloud Vision API. However, for some images, the JSON response I receive don't have description parameters for some entities. On looking further, I found that description is missing for the entities whose id start with ""/t/"" and description is present for most of the entities whose id starts with ""/m/"". Can anyone suggest how should I go about this? Is this a bug or is this supposed to behave like this only? Also, is there any way where I can get some more details on the entities id and their syntax?</p>

<p>Here is the sample web detection JSON output with entity id starting with ""/t"" &amp; ""/m"" having no description.</p>

<pre><code>{
    ""webEntities"": [
      {
        ""entityId"": ""/m/013_1c"",
        ""score"": 0.608,
        ""description"": ""Statue""
      },
      {
        ""entityId"": ""/t/21mxcct4492j5"",
        ""score"": 0.6404
      },
      {
        ""entityId"": ""/m/0jg24"",
        ""score"": 0.5815,
        ""description"": ""Image""
      },
      {
        ""entityId"": ""/g/11b77b4nf8"",
        ""score"": 0.4837,
        ""description"": ""2018""
      },
      {
        ""entityId"": ""/t/24mypdx4svpvn"",
        ""score"": 0.3909
      },
      {
        ""entityId"": ""/m/0svqtrf"",
        ""score"": 0.3664
      },
      {
        ""entityId"": ""/t/2cvnxcsw4b8sf"",
        ""score"": 0.3552
      },
]
</code></pre>",,0,1,,2019-01-21 17:26:44.650 UTC,,2019-01-22 06:29:58.980 UTC,2019-01-22 06:29:58.980 UTC,,814502,,10946095,1,3,image-processing|google-cloud-platform|google-cloud-vision|google-vision,66
Watson Visual Recognition: How to find location of classified content,40493285,Watson Visual Recognition: How to find location of classified content,"<p>Is there a way to get Watson Visual Recognition to return the location of the classified content (car, tree, etc.) when using image classification? This capability exists in the face recognition service and would be invaluable in general image classification. </p>

<p>The current <a href=""http://www.ibm.com/watson/developercloud/visual-recognition/api/v3/#classify_an_image"" rel=""nofollow noreferrer"">documentation for image classification</a> has no information on this topic.  </p>",40498772,1,2,,2016-11-08 17:43:33.977 UTC,,2016-11-09 00:44:38.077 UTC,,,,,2144140,1,1,node.js|location|visual-recognition|watson,179
How to retrieve data from Firebase database by passing a variable to reference.child()?,54656996,How to retrieve data from Firebase database by passing a variable to reference.child()?,"<p>I have created an app that scans QR codes using Google Vision API and pass the QR value to Firebase real-time database.The retrieved data is displayed inside TextViews. However values are not retrieved from Firebase if a variable is passed as a child when referencing the database. How to set a variable as a child when referencing the database?</p>

<p><strong>Note: There is no problem with the database because data gets retrieved when passing a string as child</strong></p>

<p>This app uses google Vision API to scan QR codes.All dependencies are updated to latest versions and no exceptions are shown as well.</p>

<p><strong>NOT WORKING(When variable QRCODE is passed as a child) class name=AnimalInfo</strong></p>

<pre><code>@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.activity_animal_information);
    Toolbar toolbar = (Toolbar) findViewById(R.id.toolbar);
    setSupportActionBar(toolbar);

    animalClass = (TextView) findViewById(R.id.aniClass);
    animalFamily = (TextView) findViewById(R.id.aniFamily);
    animalOrder = (TextView) findViewById(R.id.aniOrder);

    Intent intent = getIntent();
    QRCODE = intent.getStringExtra(QrScanner.QR_CODE);

    DB = FirebaseDatabase.getInstance();
    ref = DB.getReference().child(""Animals"").child(**QRCODE**);
    ref.addValueEventListener(new ValueEventListener() {
        @Override
        public void onDataChange(@NonNull DataSnapshot dataSnapshot) {

            String DBAnimalClass = dataSnapshot.child(""class"").getValue().toString();
            String DBAnimalFamily = dataSnapshot.child(""family"").getValue().toString();
            String DBAnimalOrder = dataSnapshot.child(""order"").getValue().toString();

            animalClass.setText(DBAnimalClass);
            animalFamily.setText(DBAnimalFamily);
            animalOrder.setText(DBAnimalOrder);

        }

        @Override
        public void onCancelled(@NonNull DatabaseError databaseError) {

        }
    });
}
</code></pre>

<p><strong>WORKING(When String is passed as a child)  class name=AnimalInfo</strong></p>

<pre><code>@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.activity_animal_information);
    Toolbar toolbar = (Toolbar) findViewById(R.id.toolbar);
    setSupportActionBar(toolbar);

    animalClass = (TextView) findViewById(R.id.aniClass);
    animalFamily = (TextView) findViewById(R.id.aniFamily);
    animalOrder = (TextView) findViewById(R.id.aniOrder);

    Intent intent = getIntent();
    QRCODE = intent.getStringExtra(QrScanner.QR_CODE);

    DB = FirebaseDatabase.getInstance();
    ref = DB.getReference().child(""Animals"").child(**""00000001""**);
    ref.addValueEventListener(new ValueEventListener() {
        @Override
        public void onDataChange(@NonNull DataSnapshot dataSnapshot) {

            String DBAnimalClass = dataSnapshot.child(""class"").getValue().toString();
            String DBAnimalFamily = dataSnapshot.child(""family"").getValue().toString();
            String DBAnimalOrder = dataSnapshot.child(""order"").getValue().toString();

            animalClass.setText(DBAnimalClass);
            animalFamily.setText(DBAnimalFamily);
            animalOrder.setText(DBAnimalOrder);

        }

        @Override
        public void onCancelled(@NonNull DatabaseError databaseError) {

        }
    });
}
</code></pre>

<p><strong>QRScanner class: gets QR code and pass it to AnimalInfo class using intent</strong></p>

<pre><code>@Override
        public void receiveDetections(Detector.Detections&lt;Barcode&gt; detections) {

            final SparseArray&lt;Barcode&gt; qrCodes = detections.getDetectedItems();

            if(qrCodes.size()&gt;0)
            {
                qrResult.post(new Runnable() {
                    @Override
                    public void run() {

                        qrResult.setText(qrCodes.valueAt(0).displayValue);
                        Intent intent = new Intent(QrScanner.this,AnimalInformation.class);
                        intent.putExtra(QR_CODE,qrCodes.valueAt(0).displayValue);
                        startActivity(intent);
                    }
                });
            }
        }
</code></pre>

<p>I want to pass QR code value as the reference for database so it gives data according rather than hard coding the reference.</p>

<p><strong>Another Problem: AnimalInfo layout gets created several times when its loaded upon starting the intent in QRScanner.java and I have to press the back button several times to go to Home page.</strong></p>",,0,6,,2019-02-12 19:02:19.810 UTC,,2019-02-12 20:03:53.367 UTC,,,,,6737536,1,0,java|android|firebase|firebase-realtime-database|google-vision,46
Meteor can't run gcloud on server side,37742610,Meteor can't run gcloud on server side,"<p>I'm building a small application that allows you to upload files, store them in the cloud and analyze them with Google Cloud Vision API.</p>

<p>I got the uploading and storing working now, I use firebase for that, but when I try to run gcloud I run into some issues.</p>

<p>In the main.js file in server folder I run:</p>

<pre><code>import gcloud from 'gcloud';
console.log('gcloud', gcloud);
</code></pre>

<p>But that causes an error in the terminal: </p>

<pre><code>=&gt; Started proxy.
=&gt; Started MongoDB.

Unable to resolve some modules:

  ""memcpy"" in /C/Users/Zino/Documents/Meteor Projects/find-it/node_modules/bytebuffer/dist/ByteBufferNB.js
(os.windows.x86_32)

If you notice problems related to these missing modules, consider running:

  meteor npm install --save memcpy
</code></pre>

<p>My site does not load so I run:</p>

<pre><code>meteor npm install --save memcpy
</code></pre>

<p>But then I get an error I'n can't find anything about in the internet:
<a href=""http://zino.hofmann.amsterdam/npm-debug.log"" rel=""nofollow"">node error log</a></p>

<p>I've been been trying to solve this issue for 2 days now, without luck. Any suggestions?</p>",46341221,2,10,,2016-06-10 07:41:00.517 UTC,1,2017-09-21 10:13:18.463 UTC,,,,,6121420,1,2,meteor|reactjs|npm|memcpy|gcloud-node,261
How to Reduce the size of camera source in mobile vision API text detection,39439437,How to Reduce the size of camera source in mobile vision API text detection,<p>As google vision api text detection previews full screen to scan text. I want a small rectangle for text detection and display the recognized text below in a textbox</p>,,1,0,,2016-09-11 18:42:53.237 UTC,,2016-09-12 20:25:23.467 UTC,,,,,6819970,1,6,android|text|detection|android-vision,1474
How can we find an exhaustive list (or graph) of all logos which are effectively recognized using Google Vision logo detection feature?,45484524,How can we find an exhaustive list (or graph) of all logos which are effectively recognized using Google Vision logo detection feature?,"<p>With my work team we are wondering which logos are referenced in the list (or graph) used by the Google Vision API.<br>
Apparently, there are a lot of very famous logos which are not recognized at all in pictures.</p>

<p>For instance, on the following picture, only 5 results are returned by the Google Vision API (and ""Google"" logo does not belong to those results). Obviously, the max result parameter is already set to 40.</p>

<p><a href=""https://i.stack.imgur.com/Ss24Q.png"" rel=""nofollow noreferrer"">Famous logo to detect</a></p>

<p>But, here, the question is not really why the LOGO_DETECTION feature does not work well but more : ""How can we have the garantee that the logo exists in the Google Vision's database (or graph) and that it could be recognized by the API at more than 0%?""</p>

<p>On the other hand, the logo detection feature is not free so, how can we paid without the garantee that the logo we are interested in belongs to the Google logos list (or graph)? Is there a way to check if the logo can be recognized or if there is any location where we can see all the logos referenced?</p>",,1,0,,2017-08-03 12:41:48.053 UTC,,2017-08-04 05:20:07.623 UTC,2017-08-04 05:20:07.623 UTC,,322020,,8411506,1,1,google-api|google-cloud-platform|google-cloud-vision,44
Unable to obtain response from google cloud vision api,40087567,Unable to obtain response from google cloud vision api,"<p>I am trying to test out google cloud vision api by following <a href=""https://cloud.google.com/vision/docs/requests-and-responses"" rel=""nofollow"">Google's tutorial</a> on using cloud vision api.</p>

<p><strong>Step 1:</strong> Generating JSON Requests by typing the following command in the terminal</p>

<pre><code>$ python path/to/generate_json.py -i path/to/cloudVisionInputFile -o path/to/request.json
</code></pre>

<p>The above command generates request.json file.</p>

<p><strong>Step 2:</strong> Using Curl to Send Generated Requests</p>

<pre><code>$ curl -v -k -s -H ""Content-Type: application/json"" https://vision.googleapis.com/v1/images:annotate?key=AIzaSyD7Pm-ebpjas62ihvp9v1gAhTk --data-binary @/path/to/request.json &gt; result.json
</code></pre>

<p><strong>Output in Terminal (following step 2)</strong>
Notice that the output in the terminal (see below) shows <code>Content-Length: 0</code> and <code>[data not shown]</code>.</p>

<p><strong>Can someone please advise why the content length is zero ? and also why I am unable to obtain the JSON response from google cloud vision api ?</strong></p>

<p><strong><em>The below is the out put in Terminal</em></strong></p>

<pre><code>* Hostname was NOT found in DNS cache
*   Trying 216.58.347.74...
* Connected to vision.googleapis.com (216.58.347.74) port 443 (#0)
* successfully set certificate verify locations:
*   CAfile: /opt/local/share/curl/curl-ca-bundle.crt
  CApath: none
* SSLv3, TLS handshake, Client hello (1):
} [data not shown]
* SSLv3, TLS handshake, Server hello (2):
{ [data not shown]
* SSLv3, TLS handshake, CERT (11):
{ [data not shown]
* SSLv3, TLS handshake, Server key exchange (12):
{ [data not shown]
* SSLv3, TLS handshake, Server finished (14):
{ [data not shown]
* SSLv3, TLS handshake, Client key exchange (16):
} [data not shown]
* SSLv3, TLS change cipher, Client hello (1):
} [data not shown]
* SSLv3, TLS handshake, Finished (20):
} [data not shown]
* SSLv3, TLS change cipher, Client hello (1):
{ [data not shown]
* SSLv3, TLS handshake, Finished (20):
{ [data not shown]
* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256
* Server certificate:
*    subject: C=US; ST=California; L=Mountain View; O=Google Inc; CN=*.googleapis.com
*    start date: 2016-10-06 12:44:36 GMT
*    expire date: 2016-12-29 12:28:00 GMT
*    issuer: C=US; O=Google Inc; CN=Google Internet Authority G2
*    SSL certificate verify ok.
&gt; POST /v1/images:annotate?key=AIzaSyD7Pm-ebpjas62ihvp9v1gAhTk HTTP/1.1
&gt; User-Agent: curl/7.37.1
&gt; Host: vision.googleapis.com
&gt; Accept: */*
&gt; Content-Type: application/json
&gt; Content-Length: 0
&gt; 
&lt; HTTP/1.1 200 OK
&lt; Content-Type: application/json; charset=UTF-8
&lt; Vary: X-Origin
&lt; Vary: Referer
&lt; Date: Mon, 17 Oct 2016 13:02:56 GMT
* Server ESF is not blacklisted
&lt; Server: ESF
&lt; Cache-Control: private
&lt; X-XSS-Protection: 1; mode=block
&lt; X-Frame-Options: SAMEORIGIN
&lt; X-Content-Type-Options: nosniff
&lt; Alt-Svc: quic="":443""; ma=2592000; v=""36,35,34,33,32""
&lt; Accept-Ranges: none
&lt; Vary: Origin,Accept-Encoding
&lt; Transfer-Encoding: chunked
&lt; 
{ [data not shown]
* Connection #0 to host vision.googleapis.com left intact
</code></pre>

<p>Below is the JSON request generated in request.json file</p>

<pre><code>{
    ""requests"": [{
        ""image"": {
            ""content"": ""/9j/4AAQSkZJRgABAQAA...""
        },
        ""features"": [{
            ""type"": ""TYPE_UNSPECIFIED"",
            ""maxResults"": 10
        }, {
            ""type"": ""FACE_DETECTION"",
            ""maxResults"": 10
        }, {
            ""type"": ""LANDMARK_DETECTION"",
            ""maxResults"": 10
        }, {
            ""type"": ""LOGO_DETECTION"",
            ""maxResults"": 10
        }, {
            ""type"": ""LABEL_DETECTION"",
            ""maxResults"": 10
        }, {
            ""type"": ""TEXT_DETECTION"",
            ""maxResults"": 10
        }, {
            ""type"": ""SAFE_SEARCH_DETECTION"",
            ""maxResults"": 10
        }]
    }, {
        ""image"": {
            ""content"": ""/9j/4AAQSkZJRgABAQAAAQABAAD...""
        },
        ""features"": [{
            ""type"": ""TYPE_UNSPECIFIED"",
            ""maxResults"": 10
        }, {
            ""type"": ""FACE_DETECTION"",
            ""maxResults"": 10
        }, {
            ""type"": ""LANDMARK_DETECTION"",
            ""maxResults"": 10
        }, {
            ""type"": ""LOGO_DETECTION"",
            ""maxResults"": 10
        }, {
            ""type"": ""LABEL_DETECTION"",
            ""maxResults"": 10
        }, {
            ""type"": ""TEXT_DETECTION"",
            ""maxResults"": 10
        }, {
            ""type"": ""SAFE_SEARCH_DETECTION"",
            ""maxResults"": 10
        }]
    }]
}
</code></pre>

<p>Below is the <a href=""https://raw.githubusercontent.com/GoogleCloudPlatform/cloud-vision/master/python/utils/generatejson.py"" rel=""nofollow"">Code</a> in <code>generate_json.py</code></p>

<pre><code>import argparse
import base64
import json
import sys



def main(cloudVisionInputFile, request):
    """"""Translates the input file into a json output file.

    Args:
        input_file: a file object, containing lines of input to convert.
        output_filename: the name of the file to output the json to.
    """"""
    # Collect all requests into an array - one per line in the input file
    request_list = []
    for line in input_file:
        # The first value of a line is the image. The rest are features.
        image_filename, features = line.lstrip().split(' ', 1)

        # First, get the image data
        with open(image_filename, 'rb') as image_file:
            content_json_obj = {
                'content': base64.b64encode(image_file.read()).decode('UTF-8')
            }

        # Then parse out all the features we want to compute on this image
        feature_json_obj = []
        for word in features.split(' '):
            feature, max_results = word.split(':', 1)
            feature_json_obj.append({
                'type': get_detection_type(feature),
                'maxResults': int(max_results),
            })

        # Now add it to the request
        request_list.append({
            'features': feature_json_obj,
            'image': content_json_obj,
        })

    # Write the object to a file, as json
    # with open(output_filename, 'w') as output_file:
    with open(request, 'w') as output_file:
        json.dump({'requests': request_list}, output_file)


DETECTION_TYPES = [
    'TYPE_UNSPECIFIED',
    'FACE_DETECTION',
    'LANDMARK_DETECTION',
    'LOGO_DETECTION',
    'LABEL_DETECTION',
    'TEXT_DETECTION',
    'SAFE_SEARCH_DETECTION',
]


def get_detection_type(detect_num):
    """"""Return the Vision API symbol corresponding to the given number.""""""
    detect_num = int(detect_num)
    if 0 &lt; detect_num &lt; len(DETECTION_TYPES):
        return DETECTION_TYPES[detect_num]
    else:
        return DETECTION_TYPES[0]
# [END generate_json]

FILE_FORMAT_DESCRIPTION = '''
Each line in the input file must be of the form:

    file_path feature:max_results feature:max_results ....

where 'file_path' is the path to the image file you'd like
to annotate, 'feature' is a number from 1 to %s,
corresponding to the feature to detect, and max_results is a
number specifying the maximum number of those features to
detect.

The valid values - and their corresponding meanings - for
'feature' are:

    %s
'''.strip() % (
    len(DETECTION_TYPES) - 1,
    # The numbered list of detection types
    '\n    '.join(
        # Don't present the 0th detection type ('UNSPECIFIED') as an option.
        '%s: %s' % (i + 1, detection_type)
        for i, detection_type in enumerate(DETECTION_TYPES[1:])))


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        '-i', dest='input_file', required=True,
        help='The input file to convert to json.\n' + FILE_FORMAT_DESCRIPTION)
    parser.add_argument(
        '-o', dest='output_file', required=True,
        help='The name of the json file to output to.')
    args = parser.parse_args()
    try:
        with open(args.input_file, 'r') as input_file:
            main(input_file, args.output_file)
    except ValueError as e:
        sys.exit('Invalid input file format.\n' + FILE_FORMAT_DESCRIPTION)
</code></pre>

<p>The below is the text inside cloudVisionInputFile</p>

<pre><code>/Users/pravishanthmadepally/documents/machineLearning/googleCloudVisionAPI/images/img1.jpeg 0:10 1:10 2:10 3:10 4:10 5:10 6:10
/Users/pravishanthmadepally/documents/machineLearning/googleCloudVisionAPI/images/img2.jpeg 0:10 1:10 2:10 3:10 4:10 5:10 6:10
</code></pre>",,2,0,,2016-10-17 13:22:16.767 UTC,,2017-07-03 12:07:03.173 UTC,,,,,2534233,1,0,google-cloud-platform|google-cloud-vision,771
Microsoft Computer Vision API with android volley response code 400,49099718,Microsoft Computer Vision API with android volley response code 400,"<p>I'm having some trouble getting JSON response from Microsoft custom vision API (Optical Character Recognition API) when using Android Volley request.</p>

<p>I have used this approach with other API's without any problems, but for this API I cant get it to work.</p>

<pre><code>    String URL = ""https://westcentralus.api.cognitive.microsoft.com/vision/v1.0/ocr"";
    final ProgressDialog pDialog = new ProgressDialog(this);
    pDialog.setMessage(""Getting License plate..."");
    pDialog.setCancelable(false);
    pDialog.show();
    try {
        RequestQueue requestQueue = Volley.newRequestQueue(this);
        JSONObject jsonBody = new JSONObject();
        jsonBody.put(""url"", ""https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/Atomist_quote_from_Democritus.png/338px-Atomist_quote_from_Democritus.png"");

        final String requestBody = jsonBody.toString();

        StringRequest stringRequest = new StringRequest(Request.Method.POST, URL, new Response.Listener&lt;String&gt;() {
            @Override
            public void onResponse(String response) {
                pDialog.hide();
                Log.i(""VOLLEY"", response);
            }
        }, new Response.ErrorListener() {
            @Override
            public void onErrorResponse(VolleyError error) {
                Log.e(""VOLLEY"", error.toString());
            }
        }) {
            @Override
            public byte[] getBody() throws AuthFailureError {
                try {
                    return requestBody == null ? null : requestBody.getBytes(""utf-8"");
                } catch (UnsupportedEncodingException uee) {
                    VolleyLog.wtf(""Unsupported Encoding while trying to get the bytes of %s using %s"", requestBody, ""utf-8"");
                    return null;
                }
            }

            @Override
            public Map&lt;String, String&gt; getHeaders() throws AuthFailureError {
                HashMap&lt;String, String&gt; headers = new HashMap&lt;&gt;();
                headers.put(""Content-Type"", ""application/json"");
                headers.put(""Ocp-Apim-Subscription-Key"", ""123124123123123123213"");
                return headers;
            }
        };
        requestQueue.add(stringRequest);
    } catch (JSONException e) {
        e.printStackTrace();
    }
</code></pre>

<p>I'm getting this response back:</p>

<pre><code>E/Volley: [2163] BasicNetwork.performRequest: Unexpected response code 400 for https://westcentralus.api.cognitive.microsoft.com/vision/v1.0/ocr
</code></pre>

<p>When using postman I'm not getting any errors. </p>

<p><a href=""https://i.stack.imgur.com/P7O7O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P7O7O.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/DjENN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DjENN.png"" alt=""enter image description here""></a></p>

<p>So hope you can see what I'm doing wrong.
Let me know if you want me to elaborate on anything.</p>

<p>Thanks!</p>",49103730,2,1,,2018-03-04 19:51:51.983 UTC,,2018-03-18 22:49:37.103 UTC,2018-03-04 20:12:25.737 UTC,,5309273,,5309273,1,-1,android|api|android-volley|microsoft-cognitive|http-status-code-400,196
Check if image contains text,53203939,Check if image contains text,"<p>I want to detect and extroact texts in naturel image if it contains ,like google vision do.i found a library on githup that detect  regions of image to find texts and after detection it does ocr. I want it to be faster and before text detection and extraction,I want to check if an image contains text or not.
I know I can run OCR on it but I want it to be faster than that. If it contains text then it should OCR, if not it should discard the image. Any ideas?</p>",,0,7,,2018-11-08 08:30:34.247 UTC,,2018-11-08 08:30:34.247 UTC,,,,,2163087,1,0,image|text|contains,214
Google Cloud vision demo behaving differently from actual API?,44362759,Google Cloud vision demo behaving differently from actual API?,"<p>The same image leads to different text detection results in the google cloud vision API demo versus the actual API. In the demo, the accuracy is much higher. More importantly, the newline behavior is more correct in the demo; blocks of text are treated as together, whereas in the API I'm using with the free trial, the ordering of the text is treated as strictly ""top to bottom"" with no regard for horizontal proximity. Am I doing something wrong, or is this a bug? </p>",44375562,2,0,,2017-06-05 06:15:16.870 UTC,1,2017-07-12 18:16:41.390 UTC,,,,,1165643,1,0,google-cloud-platform|google-cloud-vision,241
does image orientation matter in google vision OCR on the mobile,53980744,does image orientation matter in google vision OCR on the mobile,<p>i am trying to use the google OCR sample code to analyze text documents but it does not recognize any text on it; if i load the same document on the google vision cloud api it recognizes all the text on the document. My concern is - the mobile client side OCR detection is not able to account for image rotation whereas the cloud api can. Is this indeed the case.</p>,,0,1,,2018-12-30 19:28:53.103 UTC,,2018-12-30 19:28:53.103 UTC,,,,,10849976,1,0,android,66
Microsoft Computer Vision Handwriting API not sending 'Content-Location' Header,51027161,Microsoft Computer Vision Handwriting API not sending 'Content-Location' Header,"<p>I am trying to use Microsoft Computer Vision to make a handwriting analysis of an image, but I'm not getting a 202 response, and I'm not able to find the ""Content-Location"" header on my responde, I try it on PHP, Javascript, Python and Curl, using Microsoft Examples, but I couldn't find the header in any of my tests.</p>

<pre><code>require_once 'HTTP/Request2.php';

$request = new Http_Request2('https://brazilsouth.api.cognitive.microsoft.com/vision/v1.0/analyze?recognizeText');
$url = $request-&gt;getUrl();

$subscriptionKey = &lt;&gt;;

$headers = array(
    'Content-Type' =&gt; 'application/json',
    'Ocp-Apim-Subscription-Key' =&gt; $subscriptionKey,
);

$request-&gt;setHeader($headers);

$parameters = array(
    'handwriting' =&gt; 'true',
);

$url-&gt;setQueryVariables($parameters);

$request-&gt;setMethod(HTTP_Request2::METHOD_POST);

$body = array('url' =&gt; 'http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database/images/processed-strokes.png');

$request-&gt;setBody(json_encode($body));

$func = function($value) {
    return $value;
};

try{
    $response = $request-&gt;send();

    echo json_encode(json_decode($response-&gt;getBody()), JSON_PRETTY_PRINT);

}
catch (HttpException $ex){
    echo $ex;
}
</code></pre>

<p>That's my response:</p>

<pre><code>    {
    ""categories"": [
        {
            ""name"": ""others_"",
            ""score"": 0.00390625
        },
        {
            ""name"": ""text_"",
            ""score"": 0.5625
        },
        {
            ""name"": ""text_sign"",
            ""score"": 0.16796875
        }
    ],
    ""requestId"": ""eaa0613b-f14d-4889-9488-487afabbab17"",
    ""metadata"": {
        ""height"": 205,
        ""width"": 543,
        ""format"": ""Png""
    }
}
</code></pre>

<p>I also try to dump the response, but I also wasn't able to find the header.</p>",,0,0,,2018-06-25 15:35:29.667 UTC,,2018-10-23 05:05:27.890 UTC,2018-10-23 05:05:27.890 UTC,,1033581,,8445669,1,0,php|azure|computer-vision|ocr|httpresponse,39
Can Google's Vision API accept a URL from any external server and return a response on that image?,43631861,Can Google's Vision API accept a URL from any external server and return a response on that image?,"<p>I have been able to run Google's Vision API successfully on locally stored images. However, whenever I run my script on an image stored on an external server. I get an error.</p>

<pre><code>import io
import os
from google.cloud import vision

vision_client = vision.Client()
file_name = ""https://static.pexels.com/photos/36753/flower-purple-lical-blosso.jpg""

with io.open(file_name, 'rb') as image_file:
    content = image_file.read()
    image = vision_client.image(content=content, )

    labels = image.detect_labels()
    for label in labels:
        print(label.description)
</code></pre>

<p>The error says</p>

<pre><code>  Traceback (most recent call last):
  File ""visionex.py"", line 8, in &lt;module&gt;
    with io.open(file_name, 'rb') as image_file:
  IOError: [Errno 2] No such file or directory: 
 'https://static.pexels.com/photos/36753/flower-purple-lical-blosso.jpg'
</code></pre>",43632132,1,2,,2017-04-26 10:40:07.897 UTC,,2017-04-26 11:00:22.600 UTC,,,,,6497582,1,0,python|python-2.7|gcloud|vision,186
Set the request body while calling the API with C++Rest SDK,54432180,Set the request body while calling the API with C++Rest SDK,"<p>I want to call the Face API of Microsoft Computer Vision to post a picture with the C++Rest SDK. I have succeed with GET method but I don't know what to do with POST method. I have figure it out that the problem is in ""request.set_body"" method. I want to use it in two ways, one is posting a picture from my computer, another is posting a picture from a link of the website. If anyone knows about this problem, please help me. Thank you.</p>

<p>Here is the link of Face API: 
<a href=""https://westcentralus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236"" rel=""nofollow noreferrer"">https://westcentralus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236</a>
And here is the code. In this code, I try to post a picture from a website:</p>

<pre><code>{
    http_client client(U(""https://westcentralus.api.cognitive.microsoft.com/face/v1.0/detect""));

    http_request request(methods::POST);
    request.headers().set_content_type(L""application/json"");

    uri_builder builder;
    // Append the query parameters: [?returnFaceId][&amp;returnFaceLandmarks]
    builder.append_query(U(""returnFaceId""), U(""true""));
    builder.append_query(U(""returnFaceLandmarks""), U(""false""));
    builder.append_query(U(""subscription-key""), U(""*********************""));

    web::json::value requestParameters;
    requestParameters[U(""bar"")] = web::json::value::object(U(""https://cdn.explus.vn/media.phunutoday.vn/files/upload_images/2016/02/02/my-tam-cam-ngan-mo-5-phunutoday_vn.jpg""));

    utility::stringstream_t paramStream;
    requestParameters.serialize(paramStream);

    request.set_body(paramStream.str());
    request.set_request_uri(builder.to_uri());

    auto path_query_fragment = builder.to_string();

    // Make an HTTP GET request and asynchronously process the response
    return client.request(request).then([](http_response response)
</code></pre>",,0,0,,2019-01-30 01:37:20.860 UTC,,2019-01-30 05:16:23.777 UTC,2019-01-30 05:16:23.777 UTC,,10965549,,10965549,1,0,c++|c++11|cpprest-sdk,47
How to add an item in a collection in amazon s3?,46304883,How to add an item in a collection in amazon s3?,"<p>I have  this code  for create a data collection:</p>

<pre><code>import boto3

def createCollection():
        cliente = boto3.client('rekognition')
        respuesta = cliente.create_collection (CollectionId = 'Fotos')
        print (respuesta)

createCollection()
</code></pre>

<p>But my question is how to add an item in this collection.
Could you please help me. 
thanks.</p>",46305370,1,0,,2017-09-19 15:55:17.507 UTC,,2017-09-19 17:07:33.753 UTC,2017-09-19 17:07:33.753 UTC,,6502500,,7163809,1,0,python|amazon-web-services|amazon-s3|amazon|boto3,40
"Cloud Vision: What gets billed, the call or the successful completion of the call?",51934306,"Cloud Vision: What gets billed, the call or the successful completion of the call?","<p>We are just thinking whether to validate some input for google vision (OCR).
We can either throw everything at google vision and check the result, or we validate client side and hope to minimize BadRequest responses.</p>

<p>For us it depends on whether google vision would charge for a BadRequest, like image too large or of the wrong type.</p>

<p>I can't find it the documentation. Does anyone know?</p>",,0,0,,2018-08-20 15:46:48.980 UTC,,2018-09-03 07:59:26.523 UTC,,,,,1105889,1,3,google-vision,34
botocore.errorfactory.InvalidS3ObjectException,50763441,botocore.errorfactory.InvalidS3ObjectException,"<p>I have aws recognition code written in Python, and it run's by Node API, which works fine on Windows system but when I'm deploying it on Linux I'm facing this issue:- <code>botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectText operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.</code></p>

<p>I have given both AmazonRekognitionFullAccess and AmazonS3ReadOnlyAccess access role to I'm user. Still I don't know how to get things going.</p>

<p>Python code:-</p>

<pre><code>bucket = 'image-test'
def image_to_dict(fileName, bucket):
    client = boto3.client('rekognition', 'us-east-2')
    response = client.detect_text(Image = { 'S3Object': { 'Bucket': bucket, 
    'Name': fileName } })
    return response
</code></pre>

<p>Node Code used to run Python script:-</p>

<pre><code>var options = {
            mode: 'text',
            pythonPath:""/usr/bin/python2.7""
            pythonOptions: ['-u'],
            scriptPath: ""/home/ubuntu/test"",
            args: [imageURl]
        };
        PythonShell.run('script.py', options, function (err, results) {
            if (err)
                throw err;

        console.log(""Data is: ""+results)
</code></pre>

<p>I have Python version 2.7 installed on my Ubuntu, pip version 10.0.1.</p>",50796824,1,4,,2018-06-08 15:01:42.483 UTC,,2018-06-11 11:36:58.527 UTC,2018-06-08 15:04:09.110 UTC,,6296561,,8784559,1,0,python|node.js|amazon-web-services|amazon-s3,249
Unable to get ocr text with google vision api,50716375,Unable to get ocr text with google vision api,"<p>I use google vision api to detect the ocr.</p>

<p>However, I got no ocr text. </p>

<p>On the other hand, I uploaded the image to <code>https://cloud.google.com/vision/</code> and got <code>DBR Weddings\n</code>.</p>

<p>Is it normal?</p>

<p>How should I correct my code?</p>

<p>Thank you very much.</p>

<p>Here is my code:</p>

<pre><code>import requests
from PIL import Image
from io import BytesIO
import cv2
import base64
import ast

url = r'https://drive.google.com/uc?id=1pjofUsy5V4v7DSVsRERw6S-Zo36hO1bZ'
feature_json_obj = [{'type': 'TEXT_DETECTION', 'maxResults': 100}]
img = Image.open(BytesIO(requests.get(url).content))
arr = cv2.cvtColor(np.asarray(img.convert('RGB'), dtype=np.uint8), cv2.COLOR_RGB2BGR)

content_json_obj = {'content': base64.b64encode(a_numpy.tostring()).decode('UTF-8')}
request_list = {'requests': [{'features': feature_json_obj, 'image': content_json_obj, 'imageContext': {'languageHints': ""zh-TW""}}]}
req = requests.post(url='https://vision.googleapis.com/v1/images:annotate?key=my_key', data = str.encode(str(request_list)), headers={'Content-Type': 'application/json'}).text
receive_data = req.encode(sys.stdin.encoding, ""replace"").decode(sys.stdin.encoding)
data = ast.literal_eval(receive_data)['responses'][0]['fullTextAnnotation']['text']#get google-vision result
</code></pre>",,0,2,,2018-06-06 09:05:26.793 UTC,,2018-06-06 09:17:12.413 UTC,2018-06-06 09:17:12.413 UTC,,9087866,,9087866,1,0,python|google-api|ocr|google-vision,80
Microsoft emotion api video version with javascript,39009764,Microsoft emotion api video version with javascript,"<p>I'm trying to use the Microsoft Emotion API.</p>

<p>I can use the image version without any issues but when I try to use the video version I get an empty response.</p>

<p>It seems that I can successfully connect with the API because when I give it a wrong file type it returns the proper error code.</p>

<p>Here is my code. Would appreciate any help! </p>

<pre><code>&lt;script type=""text/javascript""&gt;
    //apiKey: Replace this with your own Project Oxford Emotion API key, please do not use my key. I inlcude it here so you can get up and running quickly but you can get your own key for free at https://www.projectoxford.ai/emotion 
    var apiKey = ""mykey"";

    //apiUrl: The base URL for the API. Find out what this is for other APIs via the API documentation
    var apiUrl = ""https://api.projectoxford.ai/emotion/v1.0/recognizeinvideo"";

    $('#btn').click(function () {
        CallAPI(""https://media.kairos.com/emodemo/videos/demo1.mp4"", apiUrl, apiKey);
    });

    function CallAPI(fileUrl, apiUrl, apiKey)
    {
        $.ajax({
            url: apiUrl,
            beforeSend: function (xhrObj) {
                xhrObj.setRequestHeader(""Content-Type"", ""application/json"");
                xhrObj.setRequestHeader(""Ocp-Apim-Subscription-Key"", apiKey);
            },
            type: ""POST"",
            data: '{""url"": ""' +fileUrl +'""}'
        })
            .done(function (response) {
                console.log(response);
                //ProcessResult(response);
            })
            .fail(function (error) {
                $(""#response"").text(error.getAllResponseHeaders());
            });
    }

    function ProcessResult(response)
    {
        var data = JSON.stringify(response);
        $(""#response"").text(data);
    }
&lt;/script&gt;
</code></pre>

<p></p>

<p></p>",,0,2,,2016-08-18 03:58:17.450 UTC,,2016-08-18 04:42:53.193 UTC,2016-08-18 04:42:53.193 UTC,,5273921,,6728948,1,0,javascript|face-detection|face-recognition,290
java.lang.arrayindexoutofboundsexception jsoup,54451771,java.lang.arrayindexoutofboundsexception jsoup,"<p>im trying to pull all images from a website and 
analyze each one using aws image recognition api, it works for some websites however some websites  return an error saying <code>500 server error java.lang.arrayindexoutofboundsexception index:281 size 281</code>  </p>

<p>bascily im scrapping images using jsoup and then creating an object to store name and image url for each image, after that i call the api and check each image in arraylist. for some reason it only works for some websites.</p>

<p>can someone please explain what im doing wrong and how to prevent this error  ?</p>

<pre><code>@WebServlet(name = ""HelloAppEngine"", urlPatterns = {
    ""/hello""
})
public class HelloAppEngine extends HttpServlet {

    static ArrayList &lt; ResponseData &gt; testImages = new ArrayList &lt; &gt; ();
    static AmazonRekognition rekognitionClient = AmazonRekognitionClientBuilder.defaultClient();

    public static void getimages() throws MalformedURLException, IOException {

        System.out.println(""getImages called"" + testImages);
        int index = 0;
        for (ResponseData data: testImages) {

            System.err.println(""open stream for:"" + data.getUrl());
            ByteBuffer imageBytes = null;
            try (InputStream inputStream = new URL(data.getUrl()).openStream()) {
                System.out.println(inputStream);
                imageBytes = ByteBuffer.wrap(IOUtils.toByteArray(inputStream));

                System.out.println(imageBytes);

            } catch (IOException e1) {
                System.err.println(e1.getMessage());
            }


            //
            DetectLabelsRequest request = new DetectLabelsRequest().withImage(new Image().withBytes(imageBytes)); //.withMaxLabels(10).withMinConfidence(77F);


            try {

                DetectLabelsResult result = rekognitionClient.detectLabels(request);
                List &lt; Label &gt; labels = result.getLabels();
                //System.out.println(labels);
                //System.out.println(""Detected labels for "" + photo+""""+labels);
                for (Label label: labels) {
                    //loop through all labels of object 
                    //create new responsedata object for each image
                       //where im getting error  
                     if (testImages.get(index) != null) {
                    ResponseData d = testImages.get(index);
                    d.setName(label.getName());
                    testImages.set(index, d);
                    //increment for making new image url and name
                    index++;


                    System.out.println(label.getName() + "": "" + label.getConfidence().toString());
                }
                }
                //
            } catch (AmazonRekognitionException e) {
                System.err.println(e.getMessage());
            }

        }
    }

    private static final long serialVersionUID = 1 L;

    protected static final Gson GSON = new GsonBuilder().create();

    // This is just a test array

    ArrayList &lt; String &gt; list = new ArrayList &lt; String &gt; ();

    @Override

    protected final void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {

        resp.setContentType(""text/json"");
        String servlet = req.getServletPath();
        System.setProperty(""http.proxyHost"", ""192.168.5.1"");
        System.setProperty(""http.proxyPort"", ""1080"");
        log(""servlet:"" + servlet);
        if (servlet.equalsIgnoreCase(""/main"")) {
            log(""if body start"");

            String urlString = java.net.URLDecoder.decode(req.getParameter(""url""), ""UTF-8"");

            // Connect to website. This can be replaced with your file loading
            // implementation
            Document doc = Jsoup.connect(urlString).get();

            // Get all img tags
            Elements img = doc.getElementsByTag(""img"");
            Elements media = doc.select(""[src]"");
            int counter = 0;

            // Loop through img tags
            for (Element src: media) {
                if (src.tagName().equals(""img"")) {
                    counter++;
                       //create reposnsedata object for each image url
                    ResponseData data = new ResponseData();
                      //set object url to image url
                    data.setUrl(src.attr(""abs:src""));
                     //set data name from aws 
                    data.setName("" "");
                    testImages.add(data);
                    // getimages();
                }
                if (src.tagName().equals(""link[href~=.*\\.(ico|png)]"")) {
                    System.out.println(""image is logo"");
                }
                if (src.tagName().equals(""meta[itemprop=image]"")) {
                    System.out.println(""image is logosss"");
                }

            }
        }
        //log(""list"" + testImages);
        getimages();
        //

        // getimages();
        System.err.println(GSON.toJson(testImages));
        resp.getWriter().println(GSON.toJson(testImages));
    }

    @Override
    protected final void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {
        doPost(req, resp);
    }

}
</code></pre>",,1,4,,2019-01-31 00:43:49.297 UTC,,2019-01-31 10:10:49.683 UTC,2019-01-31 01:11:48.077 UTC,,6830361,,6830361,1,0,java|amazon-web-services,34
Error while accessing multi-dimensional associative array in PHP (AWS/ResulData),53794638,Error while accessing multi-dimensional associative array in PHP (AWS/ResulData),"<p>I've been trying to implement Celebrity Recognition via AWS Rekognition using PHP. I was able to get the ResultData using,</p>

<pre><code>$result = $client-&gt;recognizeCelebrities();
</code></pre>

<p>And I converted the $result to an array using,</p>

<pre><code>$postResult = (array) $result;
</code></pre>

<p>I tried to print the array $postResult using,</p>

<pre><code>echo '&lt;pre&gt;';
print_r($postResult);
echo '&lt;/pre&gt;';
</code></pre>

<p>and it printed something similar to,</p>

<pre><code>Array
(
 [Aws\Resultdata] =&gt; Array
 (
   [CelebrityFaces] =&gt; Array
   (
     [0] =&gt; Array
     (
      [Name] =&gt; Emily Blunt                            
     )
   )
  )
)
</code></pre>

<p>I wanted to print only the value 'Name'. So I used,</p>

<pre><code>echo $postResult['Aws\Resultdata']['CelebrityFaces'][0]['Name']; 
</code></pre>

<p>But it throws an error as, <strong>Undefined index: Aws\Resultdata</strong></p>

<p>I also tried using the foreach loop, but it results in the same error</p>

<pre><code>foreach ($postResult as $array) {
    echo $array['Name'];
}
</code></pre>

<p>Here is the output for $result,</p>

<pre><code>Aws\Result Object
 (
  [data:Aws\Result:private] =&gt; Array
  (
   [CelebrityFaces] =&gt; Array
   (
    [0] =&gt; Array
    (
     [Name] =&gt; Emily Blunt                         
    )                          
   )
  )
 )
</code></pre>

<p>I've just started using PHP a few days back, so I'm just a beginner. And also I tried searching for a specific answer but it always threw the same error.</p>

<p>Any help would be appreciated! </p>",53795207,1,2,,2018-12-15 15:34:25.207 UTC,1,2018-12-15 16:57:03.997 UTC,2018-12-15 16:57:03.997 UTC,,9162851,,9162851,1,1,php|amazon-web-services|multidimensional-array|amazon-rekognition,54
Google Vision Text Detection returns too much unnecesary data,54469189,Google Vision Text Detection returns too much unnecesary data,"<p>When using Google Vision to run text detection on a menu, the response from their API is way too large and returns way too much data that I don't need. I just want the text from the menu, not all the coordinates that come with the response. I can't find anything about narrowing down the response in any documentation i've read. Does someone know how to specify what fields get returned in the response?</p>

<p>Heres my request:</p>

<pre><code>POST: https://vision.googleapis.com/v1/images:annotate?key=&lt;MY_KEY&gt;

BODY:

{
  ""requests"": [
    {
      ""image"": {
        ""content"": ""...base64-encoded-image-content...""
      },
      ""features"": [
        {
          ""type"": ""TEXT_DETECTION""
        }
      ]
    }
  ]
}
</code></pre>",54469334,1,3,,2019-01-31 21:01:49.820 UTC,,2019-01-31 21:14:50.420 UTC,,,,,4569894,1,0,api|ocr|google-vision,28
google-cloud-vision API gives fewer results,45122248,google-cloud-vision API gives fewer results,"<p>I was using ruby client of Google Cloud Vision, to extract the vehicle information on Automobile Original Titles.</p>

<p>Observations:</p>

<p>When I used the client API, i was getting 171 words.</p>

<p>But, when I used the google's API demo here: <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/</a>, I got 459 words. It has much of the information I was looking for.</p>

<p>Can anyone please explain, how to get the most out of the API ? </p>",,1,1,,2017-07-15 19:58:40.267 UTC,,2017-07-15 22:43:44.970 UTC,2017-07-15 20:21:09.737 UTC,,3326213,,3326213,1,1,ruby|google-cloud-platform|google-cloud-vision,119
End to End Google Vision Safe Search Test,48445901,End to End Google Vision Safe Search Test,"<p>I am using the Google Vision API to flag adult images uploaded my application.</p>

<p>I would like to be able to perform an ""end-to-end"" test where I upload an image and test that it gets handled correctly when flagged.</p>

<p>Does anyone know how to do this without an actual pornographic picture? As silly as this sounds, I was thinking about drawing genitals and uploading that since Google says their API handles drawings.</p>",,1,0,,2018-01-25 15:00:45.890 UTC,1,2018-01-25 15:59:28.920 UTC,,,,,1669208,1,1,google-cloud-vision,111
Google vision api to detect a particular font,50756150,Google vision api to detect a particular font,"<p>I am trying to collect the SYS and DIA values from bp monitors ( Ex. <a href=""https://i.stack.imgur.com/WDrhH.jpg"" rel=""nofollow noreferrer"">omron bp monitor</a> ) using Google Vision API, but i am not able to get the calculator font digits in the response ( I just get the other texts ).
Is there a way to detect particular fonts using GCP Vision API?</p>",,1,0,,2018-06-08 08:06:47.787 UTC,,2018-07-04 13:48:30.110 UTC,,,,,9912745,1,0,google-api|google-cloud-platform|ocr|google-vision|vision,415
AWS Rekognition API call per/time limit,47324768,AWS Rekognition API call per/time limit,"<p>I have been searching for a while, but have not found any information on whether there are limits on calls to Amazon Rekognition service.
Does anyone know the numbers, or any source where I can look? </p>

<p>What I want to know is if they limit the number of calls allowed per minute or second. I'm looking for information on the the paid (not free) service tier.</p>",,1,0,,2017-11-16 08:22:38.200 UTC,,2017-11-22 10:43:58.763 UTC,2017-11-16 13:43:05.623 UTC,,289812,,4430490,1,0,api|amazon-web-services|amazon-rekognition,156
get images after comparisonApi and store data in database with matched names,50739245,get images after comparisonApi and store data in database with matched names,"<p>hi on aws I have two folders  1  is boss where all images of boss are and indexed using indexfacesApi  now I want to modify this code to use all images from folder 'Event' and store in new table . like  After camparision I got 3 pictures of  boss name myboss</p>

<p>so In new database entry will be </p>

<p>image1   myboss
image4   myboss</p>

<p>and for other bosses as well same case . ATM using this </p>

<pre><code>import boto3
import io
from PIL import Image

rekognition = boto3.client('rekognition', region_name='eu-west-1')
dynamodb = boto3.client('dynamodb', region_name='eu-west-1')

image = Image.open(""group1.jpeg"")
stream = io.BytesIO()
image.save(stream,format=""JPEG"")
image_binary = stream.getvalue()


response = rekognition.search_faces_by_image(
        CollectionId='family_collection',
        Image={'Bytes':image_binary}                                       
        )

for match in response['FaceMatches']:
    print (match['Face']['FaceId'],match['Face']['Confidence'])

    face = dynamodb.get_item(
        TableName='family_collection',  
        Key={'RekognitionId': {'S': match['Face']['FaceId']}}
        )

    if 'Item' in face:
        print (face['Item']['FullName']['S'])
    else:
        print ('no match found in person lookup')
</code></pre>",50739884,1,0,,2018-06-07 10:44:37.263 UTC,,2018-06-07 11:18:12.777 UTC,,,,,7958560,1,0,python|amazon-web-services|amazon-rekognition,48
Make api calls parallel,45139378,Make api calls parallel,"<p>I try to analyze multiple images with AWS Rekognition</p>

<p>In main function in a loop:</p>

<pre><code> with open(""Test/frame%d.jpg"" % count, ""rb"") as image:
    for face in RekognitionClass.detect_faces(image):
     =&gt;Process result and write in DB
</code></pre>

<p>Rekognition Class:</p>

<pre><code>def detect_faces(image, attributes=['ALL'], region=""eu-west-1""):
rekognition = boto3.client(""rekognition"", region)
response = rekognition.detect_faces(Image={'Bytes': image.read()}, Attributes=attributes)
return response['FaceDetails']
</code></pre>

<p>But one api call takes about 2 second at least, and I would like to make more parallel calls if possible</p>",45416247,1,0,,2017-07-17 08:30:42.020 UTC,,2018-06-13 05:12:02.967 UTC,,,,,6455300,1,1,boto3,492
Python resize image and send to google vision function,45176829,Python resize image and send to google vision function,"<p>Since google vision has some <a href=""https://cloud.google.com/vision/docs/supported-files"" rel=""nofollow noreferrer"">restrictions</a> on input image size, I want to first resize input image and then use the <code>detect_labels()</code> function. </p>

<p>Here's their <a href=""https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/vision/cloud-client/detect/detect.py"" rel=""nofollow noreferrer"">sample code</a></p>

<pre><code>def detect_labels(path):
""""""Detects labels in the file.""""""
vision_client = vision.Client()

with io.open(path, 'rb') as image_file:
    content = image_file.read()

image = vision_client.image(content=content)

labels = image.detect_labels()
print('Labels:')

for label in labels:
    print(label.description)
</code></pre>

<p>they use <code>io</code> to open the image file. I wonder in this way, how to resize the image <strong>in memory</strong> and then call <code>detect_labels()</code> ?</p>",45177243,2,2,,2017-07-18 20:51:33.730 UTC,1,2019-04-23 10:17:58.380 UTC,2017-07-19 16:47:59.077 UTC,,5231007,,7189851,1,2,python-2.7|computer-vision|google-cloud-vision,733
AWS Lambda Console - Upgrade boto3 version,53736963,AWS Lambda Console - Upgrade boto3 version,"<p>I am creating a DeepLens project to recognise people, when one of select group of people are scanned by the camera.</p>

<p>The project uses a lambda, which processes the images and triggers the '<em>rekognition</em>' aws api.</p>

<ul>
<li><p>When I trigger the API from my local machine - I get a good response</p></li>
<li><p>When I trigger the API from AWS console      - I get failed response</p></li>
</ul>

<p><strong>Problem</strong></p>

<p>After much digging, I found that the 'boto3' (AWS python library) is of version:</p>

<ul>
<li><p>1.9.62   - on my local machine</p></li>
<li><p>1.8.9    - on AWS console</p></li>
</ul>

<p><strong>Question</strong></p>

<p>Can I upgrade the 'boto3' library version on the AWS lambda console ?? If so, how ?</p>",,1,0,,2018-12-12 06:03:25.937 UTC,,2018-12-12 06:25:44.063 UTC,,,,,2555384,1,3,amazon-web-services|aws-lambda|boto3|amazon-rekognition,826
Chemical displayed formula recognizer is it possible to get location of charcter from google vision?,47510168,Chemical displayed formula recognizer is it possible to get location of charcter from google vision?,"<p>I want to create an app that you can take a photo of a chemical displayed formula and it will give you its name and formula. I want to use google vision, but I am not sure if google vision can recognize characters or can it only recognize words? Also,  is it easier to get the location of characters using google vision or tesseract?</p>

<p><strong>Edit</strong>:
I am currently using tess two, I am able to display the characters of the words, but unsure of how to get the location of each character. </p>",,0,2,,2017-11-27 11:42:25.867 UTC,1,2017-11-30 09:04:11.247 UTC,2017-11-30 09:04:11.247 UTC,,8533200,,8533200,1,0,android|tesseract|google-vision|tess-two,51
"Accurate, Efficient and Effective Java SDK/API for extracting text from images of any type",51935317,"Accurate, Efficient and Effective Java SDK/API for extracting text from images of any type","<p>Please I need an accurate and efficient sdk/api to extract text from images of any type. I have tried using amazon rekognition service, google vision sdk, tesseract OCR but am not getting the correct information from the image.</p>

<p>Please any help that could be rendered will be appreciated.</p>",,0,0,,2018-08-20 16:53:06.630 UTC,,2018-08-20 16:53:06.630 UTC,,,,,8070880,1,0,image|text|extract,11
Detect all color of Image in android,53844994,Detect all color of Image in android,"<p>I have to extract all color of image in Android without using ML (Google vision, IBM Visual Recognition).</p>

<p>I had check below option.</p>

<p><strong>1.<a href=""https://developer.android.com/training/material/palette-colors"" rel=""nofollow noreferrer"">Palette</a></strong>
The palette library attempts to extract the following six color profiles.</p>

<p><strong>2. Get color of a particular pixel</strong></p>

<pre><code>public static int getDominantColor(Bitmap bitmap) {
Bitmap newBitmap = Bitmap.createScaledBitmap(bitmap, 1, 1, true);
final int color = newBitmap.getPixel(0, 0);
newBitmap.recycle();
return color;
}
</code></pre>

<p>if I break bitmap in small size then find out color and save in list.
Then there is time taken and OutOfMemoryError.</p>

<p>Please suggest any library  to find color of images.</p>

<p><strong>Edit</strong></p>

<p>Pick from gallary</p>

<pre><code> InputStream stream = getContentResolver().openInputStream(
                        data.getData());
                bitmap = BitmapFactory.decodeStream(stream);
                stream.close()
</code></pre>

<p>Get Color code from Pixel</p>

<pre><code>HashMap&lt;Integer,Integer&gt; colorList=new HashMap&lt;&gt;();
private void  getAllColorInImages(Bitmap bitmap)
{
    colorList.clear();
    if(bitmap==null)
        return;
    int width=bitmap.getWidth();
    int height=bitmap.getHeight();
    for(int i=0;i&lt;width;i++)
    {
        for(int j=0;j&lt;height;j++)
        {
            final int color = bitmap.getPixel(i, j);
            colorList.put(color,color);

        }
    }
    System.out.println(""color list size ""+colorList.size());
    System.out.println(""color list Name ""+colorList);

}
</code></pre>",,0,12,,2018-12-19 05:17:48.823 UTC,1,2018-12-19 08:30:45.650 UTC,2018-12-19 08:30:45.650 UTC,,1482632,,1482632,1,2,android|image|image-processing,259
How do you use the Google Vision API to Scan a Driver's License Barcode From a Server?,46693204,How do you use the Google Vision API to Scan a Driver's License Barcode From a Server?,"<p>We have a project where we are scanning the front and back of a Driver's License for information. </p>

<p>We need the actual scanning to take place server-side and cannot do the actual scan of the driver's license client-side because of <em>reasons</em>. So we therefore need to take a picture, upload it to our server / storage, and have the server perform the image recognition operations. </p>

<p>Google Vision will parse the Strings on the front quite well and we have been successful with pulling the data that way. The problem arises when we move to the back and attempt to scan the PDF417 barcode for information. </p>

<p>Using this code:</p>

<pre><code>https://vision.googleapis.com/v1/images:annotate?key=my_api_key

{
  ""requests"":[
    {
      ""image"":{
        ""content"":""base_64_encoded_image_goes_here""
      },
      ""features"":[
        {
          ""type"":""DOCUMENT_TEXT_DETECTION"",
          ""maxResults"":1
        }

      ]
    }
  ]
}
</code></pre>

<p>This will successfully return the info we need from the front. With regards to the back and the subsequent PDF417 barcode, I cannot find any documentation or examples for performing this type of scan via the server. </p>

<p>There is plenty of information on client-side ways of doing this, IE:</p>

<p>1) <a href=""https://developers.google.com/android/reference/com/google/android/gms/vision/barcode/Barcode.DriverLicense"" rel=""nofollow noreferrer"">https://developers.google.com/android/reference/com/google/android/gms/vision/barcode/Barcode.DriverLicense</a></p>

<p>2) <a href=""https://developers.google.com/vision/android/multi-tracker-tutorial"" rel=""nofollow noreferrer"">https://developers.google.com/vision/android/multi-tracker-tutorial</a></p>

<p>But nothing for the server / web. We are able to send this photo any way that is needed (base64, Firebase Storage, etc). <strong>Does anyone have any ideas as to how this can be done server-side?</strong></p>",,0,0,,2017-10-11 16:24:54.667 UTC,,2017-10-11 16:24:54.667 UTC,,,,,2480714,1,1,barcode-scanner|google-vision|pdf417,987
Google Cloud Vision sample code error (utf-8),53737023,Google Cloud Vision sample code error (utf-8),"<p>After following <a href=""https://cloud.google.com/vision/docs/pdf#vision-pdf-detection-gcs-python"" rel=""nofollow noreferrer"">google cloud vision pdf text detector sample code</a> I get one issue that after weeks of searching I haven't solved.</p>

<pre><code>My code shows an error:
    File ""/app/.heroku/python/lib/python3.6/site-packages/google/protobuf/json_format.py"", line 397, in Parse
if not isinstance(text, six.text_type): text = text.decode('utf-8')

'utf-8' codec can't decode byte 0x80 in position 11: invalid start byte
</code></pre>

<p>At this point:</p>

<pre><code>response = json_format.Parse(
    json_string, vision.types.AnnotateFileResponse())
</code></pre>

<p>As advised on forums, I ensured the requests module is up to date. Is there anything else you can advise?</p>

<p>P.S I'm doing pdf to text OCR in python3. The google vision link shows my code exactly.</p>

<p>My requirements.txt (the relevant parts):</p>

<pre><code>google-cloud-storage==1.11.0
google-resumable-media==0.3.1
google-cloud-core==0.28.1
requests==2.21.0
google-api-core==1.6.0
google-auth==1.6.1
google-cloud-vision==0.35.0
googleapis-common-protos==1.5.5
</code></pre>",,0,5,,2018-12-12 06:09:37.420 UTC,,2018-12-12 06:53:41.343 UTC,2018-12-12 06:53:41.343 UTC,,8297514,,8297514,1,1,json|python-3.x|pdf|google-cloud-vision,158
Unhandled Exception with Google Cloud Vision API and Xamarin,53658135,Unhandled Exception with Google Cloud Vision API and Xamarin,"<p>I am working on Xamarin Forms to create a native application. I am new to Xamarin and as well as google cloud vision API. I am trying to create ImageAnnotatorClient object (client) by passing it channel as parameter to the Create() method. However, it doesn't create the client and gives this exception.</p>

<p>Exception:</p>

<p>Unhandled Exception:</p>

<p>System.TypeInitializationException: The type initializer for 'Microsoft.Extensions.PlatformAbstractions.PlatformServices' threw an exception.</p>

<p>Code: 
<a href=""https://i.stack.imgur.com/6XPXI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6XPXI.png"" alt=""enter image description here""></a></p>

<p>So, could be a problem of passing the JSON file incorrectly? I am not sure if I am doing it correctly. If it is not that could any please help me figure out how to resolve this exception?</p>

<p>Any help will be highly appreciated.</p>

<p>Thanks,
Ghalib</p>",,0,3,,2018-12-06 19:08:36.563 UTC,,2018-12-06 19:08:36.563 UTC,,,,,9343460,1,0,c#|xamarin|xamarin.forms|google-cloud-vision,78
How to generate an Barcode and convert it to Bitmap using new Google Vision API?,33447957,How to generate an Barcode and convert it to Bitmap using new Google Vision API?,"<p>How to generate an Barcode and convert it to Bitmap using new Google Vision API?</p>

<pre><code>Barcode barcode = new Barcode();
Barcode.Email email = new Barcode.Email();
email.address = ""my_email@gmail.com"";
email.subject = ""My Subject;
email.body = ""My body content."";
barcode.email = email;
</code></pre>

<p>//Implement conversion
    Bitmap barcodeImage = barcodeToBitmap(barcode);// I do know this part.</p>",33826995,1,7,,2015-10-31 04:09:28.747 UTC,2,2015-11-20 12:40:14.733 UTC,2015-10-31 11:07:13.393 UTC,,5509247,,5509247,1,6,android|google-play-services|android-version|google-vision,4918
Putting a 3D face filter by using the Vision API,50515317,Putting a 3D face filter by using the Vision API,"<p>The similar question has been asked here: <a href=""https://stackoverflow.com/questions/47413657/add-2d-or-3d-face-filters-like-msqrd-snapchat-using-google-vision-api-for-ios"">Add 2D or 3D Face Filters like MSQRD/SnapChat Using Google Vision API for iOS</a>. However, it has not been answered yet.</p>

<p>Essentially, my work assignment is to put a 3D face filter on a person's face while the phone's front-facing camera is being used.</p>

<p>Given that the Mobile Vision API/the GitHub Android Vision project provide a way to detect a human face and stick some drawable images on it, but what my users want is a 3D object (cat or dog face) like what Facebook, Instagram, Snapchat, etc. have done.</p>

<p>I am also looking at Unity/Vuforia, but I have no idea how to integrate a Unity project to our Android app. What I want is to use a button to turn on/select this feature.</p>

<p>Added at 8th June 2018
Based on my reading, I believe Vuforia isn't designed for making a Face Filter on Android, and it's not that difficult to use the Android API on Unity. But, I have no idea how to do it another way around such as clicking a button to call the Unity Facial Filter plugged-in(<a href=""https://www.xzimg.com/Products"" rel=""nofollow noreferrer"">like XZIMG</a>) function to use the Facial Mask / Filter feature with the camera.</p>",,0,6,,2018-05-24 17:47:25.223 UTC,1,2018-06-08 03:20:58.753 UTC,2018-06-08 03:20:58.753 UTC,,4590043,,4590043,1,4,android|face-detection|android-vision,1310
botocore.errorfactory.InvalidS3ObjectException:,53543793,botocore.errorfactory.InvalidS3ObjectException:,"<p>I am attempting to run multiple images through the AWS system using python code, just a basic for loop. When I run the code I am getting an error. I am able to run  one image but once I attempt to run multiple images I again get an error code.</p>

<pre><code>import boto3

if __name__ == ""__main__"":

bucket='fastlane'
photo=','.join(('test.png',
'test2.png',
'test3.png',
'test4.png',
'test5.png',
'test6.png',
'test7.png',
'test8.png',
'test9.png',
'test10.png',
'test11.png',
'test12.png',
'test13.png',
'test14.png',
'test15.png',
'test16.png',
'test17.png',
'test18.png'))




client=boto3.client('rekognition')


response=client.detect_text(Image={'S3Object': 
{'Bucket':bucket,'Name':photo}})


textDetections=response['TextDetections']
print (response)
print ('Matching faces')
for text in textDetections:
        print ('Detected text:' + text['DetectedText'])
        print ('Confidence: ' + ""{:.2f}"".format(text['Confidence']) + ""%"")
        print ('Id: {}'.format(text['Id']))
        if 'ParentId' in text:
            print ('Parent Id: {}'.format(text['ParentId']))
        print ('Type:' + text['Type'])
        print
</code></pre>

<p>Error code:
Traceback (most recent call last):
  File ""main.py"", line 37, in 
    response=client.detect_text(Image={'S3Object':{'Bucket':bucket,'Name':photo}})
  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 320, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File ""/home/Zeus/farcry/AWS/env/lib/python3.5/site-packages/botocore/client.py"", line 624, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectText operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.</p>",53544935,1,0,,2018-11-29 16:44:27.353 UTC,,2018-11-29 17:53:59.917 UTC,,,,,10719571,1,0,python|amazon-web-services|amazon-s3|amazon-rekognition,79
Disable Text and detected frame drawing on Surface view in Google Vision Barcode scanning,53811267,Disable Text and detected frame drawing on Surface view in Google Vision Barcode scanning,"<p>I am using <code>GoogleVision</code> for Barcode Scanning in a app. I am following the sample 
<a href=""https://github.com/googlesamples/android-vision/"" rel=""nofollow noreferrer"">https://github.com/googlesamples/android-vision/</a>.</p>

<p>As per current Output QR COde is scanning and it draws content and boundary over Camera .
How can i disable this drawing content and boundary  over camera . See the green frame and Text. </p>

<p><a href=""https://i.stack.imgur.com/2q7eP.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2q7eP.jpg"" alt=""enter image description here""></a></p>

<p>I have checked the complete sample code and i do not find any code regarding drawing text (See green text in picture below) and boundary . If anyone faced this issue please provide a solution . 
I can not provide direct code for it cause i do not know which part is drawing text . Sample is already in the link .</p>",,0,0,,2018-12-17 08:21:54.493 UTC,,2018-12-18 04:03:29.597 UTC,2018-12-18 04:03:29.597 UTC,,7995966,,7995966,1,0,android|google-vision,28
Details about the detection part of AWS Rekognition,46151909,Details about the detection part of AWS Rekognition,"<p>I'm doing some investigations about AWS Rekognition. There are two issues I need to know but didn't get the answers. </p>

<p>1) how to get the category list of the object detection part. </p>

<p>2) how long does it take to process an image to get object labels in it without considering the data transmission time.</p>

<p>Is there anyone has any ideas?</p>",,1,0,,2017-09-11 09:06:27.063 UTC,,2017-09-11 10:07:49.640 UTC,2017-09-11 10:07:49.640 UTC,,174777,,7260341,1,1,amazon-web-services|amazon-rekognition,108
OCR doesn't work well on large images (with much text) - Google Cloud Vision API,40893623,OCR doesn't work well on large images (with much text) - Google Cloud Vision API,"<p>We noticed that Google Vision API doesn't work well if an image has a lot of text.
It returns 'strange' results.</p>

<p>Here is an exapmle:</p>

<p><a href=""https://www.dropbox.com/s/vhqxxwgj4stvfc9/screenwithproblem.jpg?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/vhqxxwgj4stvfc9/screenwithproblem.jpg?dl=0</a>   - Will return something like this:  <a href=""https://www.dropbox.com/s/r3gkn38rw36agvs/Screenshot%202016-11-30%2011.26.20.jpg?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/r3gkn38rw36agvs/Screenshot%202016-11-30%2011.26.20.jpg?dl=0</a></p>

<p>If we send just the part of that image, everything will be fine. It can be checked via demo page of API too (cloud.google.com/vision). </p>

<p>We tried on different images and get the same problem. </p>

<p>Can you advise us if we are doing something wrong or this is problem on Google's side?</p>

<p>Thank you in advanced!</p>",,1,3,,2016-11-30 16:39:26.947 UTC,,2018-10-31 09:07:56.390 UTC,,,,,7231684,1,0,ocr|google-cloud-vision,140
php - Reach empty JSON array,42004626,php - Reach empty JSON array,"<p>I use Microsoft Face API and I want to show data to final user, but how can I use foreach to atteint faceAttributes->age ?
There is an example of JSON file</p>

<pre><code>[
    {
        ""faceId"": ""c5c24a82-6845-4031-9d5d-978df9175426"",
        ""faceRectangle"": {
            ""width"": 78,
            ""height"": 78,
            ""left"": 394,
            ""top"": 54
        },
        ""faceAttributes"": {
            ""age"": 71.0,
            ""gender"": ""male"",
            ""smile"": 0.88,
            ""facialHair"": {
                ""mustache"": 0.8,
                ""beard"": 0.1,
                ""sideburns"": 0.02
                }
            },
            ""glasses"": ""sunglasses"",
            ""headPose"": {
                ""roll"": 2.1,
                ""yaw"": 3,
                ""pitch"": 0
            }
        }
    }
]
</code></pre>

<p>I tried this code but not working :</p>

<pre><code>&lt;?php

    $json = file_get_contents('file.json');
    $data =  json_decode($json);
    if (count($data-&gt;faceAttributes)) {
        // Cycle through the array
        foreach ($data-&gt;faceAttributes as $idx =&gt; $faceAttributes) {
            // Output a row
    echo $faceAttributes-&gt;age ;
    echo $faceAttributes-&gt;gender ;

?&gt;
</code></pre>

<p>Thanks !</p>",42004894,1,2,,2017-02-02 14:21:14.190 UTC,,2017-02-02 14:40:05.613 UTC,,,,,7418614,1,0,php|json|microsoft-cognitive,76
getting error System.InvalidOperationException: Error deserializing JSON credential data,52126752,getting error System.InvalidOperationException: Error deserializing JSON credential data,"<p>I am trying to call google cloud vision api from xamarin C# android application code.
I have set environment variable but still I was not able to call api.
So I decided to call it by passing credential json file but now I am getting error deserializing JSON credential data</p>

<p>here is my code </p>

<pre><code>string jsonPath = @""C:/Users/abcde/Documents/AndroidApp/My Project-d38b212eadaf.json"";

            var credential = GoogleCredential.FromJson(jsonPath);
            var channel = new Grpc.Core.Channel(ImageAnnotatorClient.DefaultEndpoint.ToString(), credential.ToChannelCredentials());
            var client = ImageAnnotatorClient.Create(channel);

            var image = Image.FromFile(@""C:/Users/abcde/Documents/AndroidApp/bg.jpg"");

            var response = client.DetectLabels(image);

            foreach (var annotation in response)
            {
                if (annotation.Description != null)
                    Console.WriteLine(annotation.Description);
            }
</code></pre>",,2,0,,2018-09-01 09:24:05.823 UTC,,2018-09-05 12:42:30.147 UTC,,,,,2851319,1,-1,c#|android|visual-studio|xamarin.forms|xamarin.android,95
CognitiveService Face - AddPersonFaceAsync requires GUID (instead of people's name) as second param?,43130920,CognitiveService Face - AddPersonFaceAsync requires GUID (instead of people's name) as second param?,"<p>I'm trying out Microsoft Cognitive Services Face API now, looking at here as reference: <a href=""https://www.microsoft.com/cognitive-services/en-us/face-api/documentation/face-api-how-to-topics/howtoidentifyfacesinimage"" rel=""nofollow noreferrer"">https://www.microsoft.com/cognitive-services/en-us/face-api/documentation/face-api-how-to-topics/howtoidentifyfacesinimage</a></p>

<p>Now, I don't understand why the second parameter for AddPersonFaceAsync is taking in GUID. My logic tells me that you would want to add the groupId of the person, and the name of the person (the same name that is used when calling CreatePersonAsync). But the function requires that I pass in a GUID?</p>

<p>What GUID do I use here? Do I just generate anything? How is that GUID is going to be associated with the person's name?</p>",43131313,1,0,,2017-03-31 01:24:35.840 UTC,,2017-03-31 02:19:27.100 UTC,,,,,2155584,1,0,.net|microsoft-cognitive,188
Watson Visual Recognition API C# Authorisation,51959287,Watson Visual Recognition API C# Authorisation,"<p>Trying to get Watson Visual Recognition working with C# but I am getting an unauthorised error when attempting to classify an image through the API. The credentials I'm using are the ""Auto-generated service credentials"".</p>

<p>The error that I am receiving is:
ServiceResponseException: The API query failed with status code Unauthorized: Unauthorized</p>

<p>Here is my code:</p>

<pre><code>class Program
{
    static void Main(string[] args)
    {
        string apiKey = ""xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"";
        string versionDate = ""2018-03-19"";
        string endpoint = ""https://gateway.watsonplatform.net/visual-recognition/api"";

        VisualRecognitionService visualRecognition = new VisualRecognitionService(apiKey, versionDate);
        visualRecognition.SetEndpoint(endpoint);

        // throws error here
        var result = visualRecognition.Classify(url: ""https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Collage_of_Nine_Dogs.jpg/1023px-Collage_of_Nine_Dogs.jpg""); 
    }
}
</code></pre>

<p>Also, let me know if I can provide anymore information that might help  </p>",51959822,1,3,,2018-08-22 02:58:25.377 UTC,,2018-08-22 04:27:30.740 UTC,2018-08-22 03:37:13.347 UTC,,10257346,,10257346,1,1,c#|ibm-watson|visual-recognition,170
Upload an image to Microsoft Computer Vision API through volley on android using octet-stream,49168443,Upload an image to Microsoft Computer Vision API through volley on android using octet-stream,"<p>I´ve been trying to make a request to Microsoft Computer Vision API using volley on android, but i want to upload the image from the phone and not just send an url. The reference from the API (<a href=""https://westcentralus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa"" rel=""nofollow noreferrer"">https://westcentralus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa</a>) says to put the Content-Type on application/octet-stream and in the body it just says ""[Binary image data]"".
I´ve tried sending the image as a byte array (byte[]) but i keep getting the response 400 (wich stands for InvalidImageFormat or Size).
It works fine if I use the url method, but I need to upload the image.</p>

<p><a href=""https://i.stack.imgur.com/TadGm.png"" rel=""nofollow noreferrer"">This is the only imformation that the documentation gives</a></p>

<p>This is the code I´ve been using:</p>

<pre><code>String URL = ""https://westcentralus.api.cognitive.microsoft.com/vision/v1.0/analyze?visualFeatures=Categories&amp;language=en"";
            StringRequest apiRequest = new StringRequest(Request.Method.POST, URL, new Response.Listener&lt;String&gt;() {
                @Override
                public void onResponse(String response) {
                    RespuestaApi.setText(""Respuesta: "" + response);
                }
            }, new Response.ErrorListener() {
                @Override
                public void onErrorResponse(VolleyError error) {

                    RespuestaApi.setText(""Error: "" + error.toString());
                }
            }){
                @Override
                public Map&lt;String, String&gt; getHeaders() throws AuthFailureError {
                    HashMap&lt;String, String&gt; headers = new HashMap&lt;&gt;();
                    headers.put(""Content-Type"", ""application/octet-stream"");
                    headers.put(""Ocp-Apim-Subscription-Key"", SubKey);
                    return headers;
                }
                @Override
                public byte[] getBody() throws AuthFailureError {
                    ByteArrayOutputStream baos = new ByteArrayOutputStream();
                    ImgTemp.compress(Bitmap.CompressFormat.JPEG, 50, baos);
                    byte[] imageBytes = baos.toByteArray();
                    return imageBytes;
                }

            };
            VolleySingleton.getInstancia(PruebaApi.this).agregarACola(apiRequest);
</code></pre>

<p>My Bitmap works fine by the way.
This is the error that the logcat gives me:</p>

<pre><code> E/Volley: [41310] BasicNetwork.performRequest: Unexpected response code 400 for https://westcentralus.api.cognitive.microsoft.com/vision/v1.0/analyze?visualFeatures=Categories&amp;language=en
</code></pre>

<p><a href=""https://i.stack.imgur.com/GJMbJ.png"" rel=""nofollow noreferrer"">This is the reference for the response</a></p>

<p>So, finally, what do I have to do to send the proper image format that the api requires?</p>

<p>Thank you in advance.</p>",,1,0,,2018-03-08 08:19:10.777 UTC,1,2018-07-16 18:15:02.530 UTC,,,,,9460890,1,0,android|computer-vision|byte|android-volley,169
Human body part detection in Android,39071341,Human body part detection in Android,"<p>Currently I am working on Google face detection <code>API</code> from this <code>API</code> I am able to detect human complete face (eye, nose and other parts) and also as per this concept I have developed one application, if you stand in front of the front face camera then it will detect your face and show some gestures.</p>

<pre><code>Context context = Applications.getAppContext();

FaceDetector detector = new FaceDetector.Builder(context)
            .setClassificationType(FaceDetector.ALL_CLASSIFICATIONS)
            .build();

detector.setProcessor(
         new MultiProcessor.Builder&lt;&gt;(new GraphicFaceTrackerFactory())
         .build());

if (!detector.isOperational()) {

}

mCameraSource = new CameraSource.Builder(context, detector)
            .setRequestedPreviewSize(640, 480)
            .setFacing(CameraSource.CAMERA_FACING_FRONT)
            .setRequestedFps(30.0f)
            .build();
</code></pre>

<p>Application video link which i have developed till now :-</p>

<p><a href=""https://www.dropbox.com/s/097todfu90ic12j/VirtualMirror.mp4?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/097todfu90ic12j/VirtualMirror.mp4?dl=0</a></p>

<p>Application play store link:-</p>

<p><a href=""https://play.google.com/store/apps/details?id=com.twins.virtualmirror"" rel=""nofollow"">https://play.google.com/store/apps/details?id=com.twins.virtualmirror</a></p>

<p>Is there any API available where we can recognize human other body parts (Chest, hand, legs and other parts of the body), because as per the Google vision <code>API</code> it's only able to detect face of the human not other parts.</p>",39071375,2,3,,2016-08-22 04:17:44.047 UTC,3,2016-08-22 13:06:27.163 UTC,2016-08-22 13:06:27.163 UTC,,1453704,,1453704,1,10,android|api|android-camera|face-detection,2396
Rekognition namespaces aren't resolved in Eclipse AWS toolkit,53709061,Rekognition namespaces aren't resolved in Eclipse AWS toolkit,"<p>In the Amazon Rekognition Developer Guide  there is a tutorial  ""Create the AWS Toolkit for Eclipse Lambda Project""  </p>

<p>It tells me to resolve namespace issues in Eclipse  by doing this : </p>

<pre><code> The Rekognition namespaces aren't resolved. To correct this:
• Pause your mouse over the underlined portion of the line import
com.amazonaws.services.rekognition.AmazonRekognition;.
• Choose Fix project set up... .
• Choose the latest version of the Amazon Rekognition archive.
• Choose OK to add the archive to the project 
</code></pre>

<p>However I am not given an option  to choose  the latest Amazon Rekognition archive.
<a href=""https://i.stack.imgur.com/6zC7f.png"" rel=""nofollow noreferrer"">No option to add archive</a></p>

<p>Is there another  way to load this archive  or  force the install of this archive? </p>",53757195,1,0,,2018-12-10 15:46:29.523 UTC,,2018-12-13 07:47:24.190 UTC,,,,,2841952,1,0,eclipse|amazon-web-services|aws-java-sdk|aws-toolkit,43
Drawing marker on top of camera view with Google Vision API for Android,38049867,Drawing marker on top of camera view with Google Vision API for Android,"<p>How can I draw shapes or graphics on top of the camera view with Google Vision API on Android?</p>

<p>Most barcode scanners I have seen so far have something like a blinking horizontal line in the middle of the screen to make focusing the barcode easier. However looking at the API reference I haven't found that yet.</p>

<p>Does Google Vision API have an easy way to achieve this?</p>",,0,1,,2016-06-27 09:05:18.147 UTC,1,2016-06-27 09:05:18.147 UTC,,,,,3726133,1,3,android|barcode-scanner|google-vision,383
Google Cloud vision and Facebook Messenger incompatible?,50765893,Google Cloud vision and Facebook Messenger incompatible?,"<p>I epxerienced problems when I send pictures hosted on FB (after an upload) to Google Cloud Vision. The image is not accepted. Does someone knows if there is a new incompatibilty. I'm pretty sure it was working 6 months ago.
Any ideas? or people that experiences similar problems ?
Regards</p>",,0,1,,2018-06-08 17:40:35.443 UTC,,2018-06-08 17:40:35.443 UTC,,,,,1444653,1,0,image-recognition|facebook-messenger-bot|google-cloud-vision|facebook-chatbot,53
getting many error in angular 2 for deploying google cloud vision ocr,48145425,getting many error in angular 2 for deploying google cloud vision ocr,"<pre><code> import { Injectable } from '@angular/core';

@Injectable()
export class ManualService {

  constructor() {
    // Imports the Google Cloud client library.

    const Storage = require('@google-cloud/storage');
 // Instantiates a client. If you don't specify credentials when constructing
 // the client, the client library will look for credentials in the
 // environment.
 const storage = Storage();

 // Makes an authenticated API request.
 storage
   .getBuckets()
   .then((results) =&gt; {
     const buckets = results[0];

     console.log('Buckets:');
     buckets.forEach((bucket) =&gt; {
       console.log(bucket.name);
     });
   })
   .catch((err) =&gt; {
     console.error('ERROR:', err);
   });
   }


}
</code></pre>

<p>I am Deploying Google cloud vision Ocr in My angular2 webapp.but i am getting many of the errors when i add this code in my webapp code.please help me to sort out this.</p>

<p>When I run this code ,it gives me this output:
<a href=""https://i.stack.imgur.com/Oq50A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Oq50A.png"" alt=""error""></a></p>",,1,2,,2018-01-08 06:42:33.427 UTC,1,2018-01-08 07:16:33.507 UTC,2018-01-08 07:16:33.507 UTC,,3196845,,8769212,1,-1,javascript|node.js|angular|typescript|google-cloud-platform,80
google cloud vision 403 permission denied error(other questions didn't work),50190527,google cloud vision 403 permission denied error(other questions didn't work),"<p>I am trying to perform OCR on pdf documents using google cloud vision API, i uploaded a pdf document into a cloud bucket and downloaded the oauth key file and added it in the script as below. But when i run the file, i get the permissiondenined: 403 error, can anyone please give me instructions on how to fix it, i did extensive google search and did not yield any results, i am surely missing something here.</p>

<pre><code>    #authenitcation file 
</code></pre>

<p>os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]=""mykeylocation/key1.json""</p>

<pre><code>    #method being called
    operation = client.async_batch_annotate_files(requests=[async_request])

    #error message
    PermissionDenied: 403 Error opening file: gs://myocrbucket-vamsi/2017 Form 3W-2.pdf.
</code></pre>

<p>I have checked the older stack overflow questions and the links provided in answers are not active anymore.</p>

<p>Thanks in advance for your help.</p>",,1,1,,2018-05-05 14:29:33.207 UTC,,2018-10-15 19:49:31.943 UTC,2018-06-07 18:35:16.597 UTC,,4092569,,5326115,1,0,python-3.x|google-cloud-platform|google-cloud-vision,242
Why google-cloud-python's vision API returns multiple annotations?,44816006,Why google-cloud-python's vision API returns multiple annotations?,"<p>I am working with Google cloud vision API with Python<br>
(<a href=""https://googlecloudplatform.github.io/google-cloud-python/stable/vision-usage.html"" rel=""nofollow noreferrer"">https://googlecloudplatform.github.io/google-cloud-python/stable/vision-usage.html</a>)</p>

<p>But I could not understand why the annotation result of a single image consists of <code>list</code> of <code>annotation</code>s.<br>
The <a href=""https://googlecloudplatform.github.io/google-cloud-python/stable/vision-usage.html#manual-detection"" rel=""nofollow noreferrer"">document</a> says:</p>

<pre><code>&gt;&gt;&gt; from google.cloud import vision
&gt;&gt;&gt; from google.cloud.vision.feature import Feature
&gt;&gt;&gt; from google.cloud.vision.feature import FeatureTypes
&gt;&gt;&gt; client = vision.Client()
&gt;&gt;&gt; image = client.image(source_uri='gs://my-test-bucket/image.jpg')
&gt;&gt;&gt; features = [Feature(FeatureTypes.FACE_DETECTION, 5),
...             Feature(FeatureTypes.LOGO_DETECTION, 3)]
&gt;&gt;&gt; annotations = image.detect(features)
&gt;&gt;&gt; len(annotations)
2
&gt;&gt;&gt; for face in annotations[0].faces:
...     print(face.joy)
Likelihood.VERY_LIKELY
Likelihood.VERY_LIKELY
Likelihood.VERY_LIKELY
&gt;&gt;&gt; for logo in annotations[0].logos:
...     print(logo.description)
'google'
'github'
</code></pre>

<p>Why <code>image.detect</code> returns multiple annotations for a single image?<br>
It seems unnecessary because detection results are contained in each attributes (<code>annotations[0].faces</code>, <code>annotations[0].logos</code>, etc.).</p>

<p>And when I try the api with my own image it returns the <code>annotations</code> of length 1.</p>

<p>So my question is:</p>

<ul>
<li>Why python's vision api client returns multiple annotations for a single image?</li>
<li>Do I need to parse every <code>annotation</code> in the list <code>annotations</code>?</li>
</ul>",,1,0,,2017-06-29 03:43:13.207 UTC,,2017-09-11 20:55:03.553 UTC,,,,,2500650,1,0,python|google-cloud-vision|google-cloud-python|vision-api,299
AWS full access policy for Application Load Balancer,51025861,AWS full access policy for Application Load Balancer,"<p>Right now, my AWS account has the following policies:</p>

<ul>
<li>AmazonEC2FullAccess </li>
<li>AmazonSQSFullAccess  </li>
<li>AmazonS3FullAccess</li>
<li>AmazonAPIGatewayInvokeFullAccess  </li>
<li>CloudWatchFullAccess</li>
<li>AmazonKinesisFullAccess  </li>
<li>AmazonRekognitionFullAccess</li>
<li>AmazonKinesisVideoStreamsFullAccess  </li>
<li>AmazonKinesisFirehoseFullAccess</li>
<li>AmazonSNSFullAccess</li>
</ul>

<p>In order to setup an ‘application load balancer’ with auto scaling group, target group, subnets in a VPC, what are the other policies that I would require?</p>",,1,0,,2018-06-25 14:27:53.727 UTC,,2018-06-25 20:53:43.873 UTC,,,,,8435450,1,0,amazon-web-services|amazon-ec2,30
AWS S3 putobject and after DetectFaces with Rekognition,43070014,AWS S3 putobject and after DetectFaces with Rekognition,"<p>Iam doing a project with nodejs and aws.</p>

<p>I am using WebRTC and taking photo.</p>

<p>After i am taking photos base64 data and posting nodejs and i am putting it my aws console and i am using it for detectfaces but its giving error.</p>

<p>But i am adding photo from my aws console manually detect faces not giving a error. </p>

<p>My codes here : 
MY WEBCAM JS : 
this is giving a base64 for me.</p>

<pre><code>&gt; drawImage(video, 0, 0, canvas.width, canvas.height);
&gt;      var data = canvas.toDataURL('image/jpeg');
&gt;       photo.setAttribute('src', data);
&gt;       console.log(data);
</code></pre>

<p>and i am trying post with POSTMAN CHROME EXTENSION to my nodejs i can put it well but i can't  using a detect faces.</p>

<p>My nodejs : </p>

<pre><code>app.post('/addPhoto', function (req, res) {
 var base64data=new Buffer(req.body.photo.replace(/^data:image\/\w+;base64,/, """"),'base64');
console.log(base64data);
  var params = {Bucket:  ""realeyeshomework"", Key:""111111111.jpg"", Body: base64data};
   s3.putObject(params, function(err, data) {
     if (err)
       console.log(err)
     else{
       console.log(""1"");

       rekognition.detectFaces( {
 Image: {
        S3Object: {
        Bucket: ""realeyeshomework"",
        Name:""111111111.jpg""
        }
      },
 Attributes: [
    ""ALL""
  ]
}, function(error, response) {
        if (error) console.log(error, error.stack); // an error occurred
        else  res.send(response);
    });
     }
   });
});
</code></pre>

<p>MY ERROR : 
<a href=""https://i.stack.imgur.com/62ZLS.png"" rel=""nofollow noreferrer"">CLICK HERE FOR ERROR</a></p>

<p>How can i do this please help me i could not fint anything.</p>

<p>Thanks for help.</p>",,2,1,,2017-03-28 12:47:02.283 UTC,,2018-01-12 15:03:21.610 UTC,,,,,6148669,1,2,javascript|node.js|amazon-web-services|amazon-rekognition,236
Azure custom vision object detection high latency,55086411,Azure custom vision object detection high latency,"<p>My Azure custom vision video object has high detection latency in the FO Tier. How can I minimize response time? Should I go for the S tier? </p>

<p>My plan used a custom vision object detection model which I trained on Azure custom vision portal to then use the prediction API in my Python script which sends a video frame by frame to an API. This has a lot of latency in response time. If I send a 1-minute video of 20FPS it takes 2+ hours to process it.</p>",,1,2,,2019-03-10 09:47:16.513 UTC,,2019-04-15 07:56:49.623 UTC,2019-04-15 07:56:49.623 UTC,,5580267,,11106718,1,0,python-3.x|microsoft-cognitive|microsoft-custom-vision,87
How to create function for an API function not belonging to any model class?,49114316,How to create function for an API function not belonging to any model class?,"<p>So I am using Django/DRF for my backend and I want the client (my web app) to be able to make an API call that looks up tags for an image using Google Cloud Vision API. Obviously, I don't want the client to have access to the API key, hence the server should perform the actual call to Google's API. What I'm struggling with currently is where to put this API function. Do I have to create a model for this (even though there is no db table) or is there some way around that? </p>",,0,7,,2018-03-05 15:50:27.917 UTC,,2018-03-05 15:50:27.917 UTC,,,,,1245418,1,0,django|django-rest-framework,24
Cut and join images in imagemagick on Ubuntu,48135978,Cut and join images in imagemagick on Ubuntu,"<p>I am a programmer with minimal understanding of <code>imagemagick</code>. I have a large set of <code>PNG</code> images from which I need to extract text through <code>Google Vision API</code>. Since this API is quite expensive I want to minimize the number of requests I make, so I want to join many images into single image which does not exceed 4MB size. </p>

<p>I have attached a sample image. <a href=""https://i.stack.imgur.com/qXAAS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qXAAS.png"" alt=""enter image description here""></a></p>

<ol>
<li>This image has 30 blocks of user data. Each block has a blank photograph section. I want to delete this blank part (entire section after text to vertical line).</li>
<li>Join resultants images from 30 such images. I want to join all user data images from 30-40 images into single image. So its going to be like 900 user data blocks in one image.</li>
</ol>

<p>I request some <code>imagemagick</code> experts to help me out.</p>",48137003,1,4,,2018-01-07 09:44:48.673 UTC,,2018-01-07 12:53:14.577 UTC,,,,,8852495,1,0,imagemagick,41
Is it possible to upload multiple images for AWS Rekognition without using s3 bucket?,52534025,Is it possible to upload multiple images for AWS Rekognition without using s3 bucket?,"<p>I am trying to implement AWS Rekognition. Be default API allows only one image.
I want to upload multiple image for face recognition. Is it possible?</p>",,1,0,,2018-09-27 09:43:34.583 UTC,,2018-10-07 16:17:02.953 UTC,,,,,6013019,1,0,amazon-web-services|amazon-rekognition,65
Retrieving personal information after the SearchFacesByImage is successful for the face match in amazon rekognition,52509581,Retrieving personal information after the SearchFacesByImage is successful for the face match in amazon rekognition,"<p>I am trying to create an app which makes use of <strong>Amazon Rekogition</strong> in AWS for identification of a person and retrieving the personal information for an internal storage system.</p>

<p>I wanted to know how to connect the Amazon Rekognition part and the information stored in the database. The face detection part will be done by Amazon Rekognition, but how will my app store and retrieve the personal information after detection of face? </p>

<p>Can anyone give me a sample code in Java for retrieving the information stored in <code>dynamodb</code> using <code>externalImageId</code>?</p>",,1,0,,2018-09-26 03:38:48.313 UTC,,2018-09-26 03:57:26.507 UTC,2018-09-26 03:51:22.840 UTC,,174777,,8319130,1,0,java|amazon-web-services|amazon-rekognition,35
Can't get BarcodeDetector to work on my Huawei Mate 9,55301066,Can't get BarcodeDetector to work on my Huawei Mate 9,"<p>I'm trying to use Google Vision's BarcodeDetector to work in my app. I followed the example from <a href=""https://www.journaldev.com/18198/qr-code-barcode-scanner-android"" rel=""nofollow noreferrer"">this website</a>, and it works fine on my Samsung Note 3 (an old model I keep for development purposes). I managed to get it to scan bar codes and QR codes fine.</p>

<p>But when I installed it into my other test model, a Huawei Mate 9, it never works. <code>detector.isOperational()</code> is always false, never true. Why is this, and how can I get around it?</p>

<p>I've done my share of homework and it tells me stuff like ensuring that my storage capacity is at least 10% and to ensure a good network connection. Both conditions are fulfilled, I simply can't get it to work.</p>",,0,0,,2019-03-22 13:49:48.893 UTC,,2019-03-22 13:49:48.893 UTC,,,,,1254149,1,0,api|barcode|huawei,10
microsoft emotion api multiple images PYTHON 2.7,47154016,microsoft emotion api multiple images PYTHON 2.7,"<p>I am currently using Microsoft Azure Emotion API to look at emotion of certain images. Although the sample code works (Python 2.7) , I want this to be more than one image. </p>

<p>I will have a directory (URL) that has 100 images in, labelled image1, image2, image3.</p>

<p>What I am looking for is a change of the code to give an average rating/score for the images that it has looped around.</p>

<p>The code I have is:</p>

<pre><code>import httplib, urllib, base64

headers = {
    'Content-Type': 'application/json',
    'Ocp-Apim-Subscription-Key': 'MY KEY HERE',
}

params = urllib.urlencode({
})

body = ""{ 'url': 'https://assets.mubi.com/images/notebook/post_images/22267/images-w1400.jpg?1474980339' }""

try:
    conn = httplib.HTTPSConnection('westus.api.cognitive.microsoft.com')
    conn.request(""POST"", ""/emotion/v1.0/recognize?%s"" % params, body, headers)
    response = conn.getresponse()
    data = response.read()
    print(data)
    conn.close()
except Exception as e:
    print(""[Errno {0}] {1}"".format(e.errno, e.strerror))
</code></pre>

<p>I am thinking a while loop:</p>

<pre><code>for x in range (0,100):
</code></pre>

<p>and change the URL: to the path with (x) 
But I can't get this to work.
Any help would be really appreciated.</p>

<p>Thanks, Nathan.</p>",47173482,1,1,,2017-11-07 09:16:11.753 UTC,,2017-11-08 07:03:10.677 UTC,,,,user8899080,,1,1,python|api|azure|emotion,80
Python - A socket operation was attempted to an unreachable network,49974375,Python - A socket operation was attempted to an unreachable network,"<p>I'm trying to install google's vision API on my system. 
I'm using the command </p>

<blockquote>
  <p>pip install --upgrade google-cloud-vision</p>
</blockquote>

<p>On executing this command the following error is shown:</p>

<blockquote>
<pre><code>Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x0000000005D77DA0&gt;: 
</code></pre>
  
  <p>Failed to establish a new connection: [WinError 10051] A socket operation was attempted to an unreachabl
  e network',)': /simple/google-cloud-vision/</p>
  
  <p>Could not find a version that satisfies the requirement google-cloud-vision (from versions: ). No matching distribution found for google-cloud-vision</p>
</blockquote>

<p>Please help. How can I solve the problem?</p>

<p><strong>Note:-
I had installed openCV through the .whl file and it installed just fine. But when I tried to install scikit-image through the same process, it shows the aforementioned error.</strong></p>",,1,0,,2018-04-23 06:04:42.847 UTC,,2018-04-24 09:19:09.023 UTC,2018-04-24 09:19:09.023 UTC,,9630186,,9630186,1,1,python|python-3.x|pip|vision,421
Formatting JSON Google Vision OCR Language Hints,44467350,Formatting JSON Google Vision OCR Language Hints,"<p>JSON formatting is a weakness of mine, and I am running a script that is submitting json requests to google vision API for OCR on images. The results are poor, so I think I may need to add Language Hints. Here is the basic json call:</p>

<pre><code>{
  ""requests"": [
    {
      ""image"": {
        ""source"": {
          ""gcsImageUri"": ""gs://YOUR_BUCKET_NAME/YOUR_FILE_NAME""
        }
      },
      ""features"": [
        {
          ""type"": ""TEXT_DETECTION""
        }
      ]
    }
  ]
}
</code></pre>

<p><a href=""https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate#imagecontext"" rel=""nofollow noreferrer"">Here is the page showing Language Hints.</a> How can i add it to the json code in a valid way. I keep getting syntax errors!!</p>",,1,0,,2017-06-09 22:11:12.843 UTC,1,2017-10-13 02:23:05.313 UTC,,,,,8139426,1,2,json|ocr,1432
Google vision api text detection,39712648,Google vision api text detection,"<p>I am using google vision api to recognise  text from image. The image in Japanese language. </p>

<p>But response is not in Japanese language it is in English. Can any body tell me how to change english to Japanese.</p>",39713140,2,0,,2016-09-26 21:24:23.693 UTC,,2018-02-20 18:10:47.540 UTC,,,,,6467306,1,-1,text|detection|vision,610
Use Google Vision to automate the extraction of data from an Identification document,55120982,Use Google Vision to automate the extraction of data from an Identification document,"<p>I would like to use Google Vision to automate the extraction of information from an id document supporting these formats:</p>

<p><strong>Format 1:</strong></p>

<p><a href=""https://i.stack.imgur.com/6EzjG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6EzjG.png"" alt=""enter image description here""></a></p>

<p>I should be capable of getting:</p>

<ul>
<li>First Name: CARMEN </li>
<li>Last Name: MUESTRA MUESTRA </li>
<li>Date of birth: 01/01/1980 </li>
<li>DNI: 12345678A</li>
</ul>

<p><strong>Format 2:</strong></p>

<p><a href=""https://i.stack.imgur.com/1uCEU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1uCEU.png"" alt=""enter image description here""></a></p>

<ul>
<li>First Name: NOMBRE </li>
<li>Last Name: APELLIDO1 APELLIDO2 </li>
<li>Date of birth: 01/05/1972 </li>
<li>DNI: 99999999-R</li>
</ul>

<p>Although the text recognition of the API is quite accurate, I'm having trouble making sense of the extracted the information.</p>

<p>The JSON response aggregates the text in different blocks in <strong>format 1</strong> for instante BLOCK 1 (ESPAÑA) BLOCK 2 (DOCUMENTO NACIONAL DE IDENTIDAD).</p>

<p>The problem is the blocks seem to be kind of arbitrary, sometimes it returns different blocks, o aggregates them differently.</p>

<p>1) What recommendations would you make to automate this process?</p>

<p>2) Can you show an example of the processing of the response in a similar scenario?</p>

<p>3) Is there a way to train the platform to aggregate the info according to what we want to extract?</p>",,0,0,,2019-03-12 12:00:26.357 UTC,,2019-03-12 12:00:26.357 UTC,,,,,1729795,1,0,google-cloud-platform|google-vision,13
Rails Active Storage + AWS Rekognition: How to get IO file,55099418,Rails Active Storage + AWS Rekognition: How to get IO file,"<p>I'm trying to integrate AWS Rekognition into my Rails app. After the user uploads his avatar via Active Storage, Rekognition should show some info about it.</p>

<pre><code>def update
  respond_to do |format|
    if @user.update(user_params)
      if @user.images.attached?
        Aws.config.update({
            region: 'us-west-2',
            credentials: Aws::Credentials.new('ACCESS_KEY', 'SECRET_KEY')
          })
          rekognition = Aws::Rekognition::Client.new(region: Aws.config[:region], credentials: Aws.config[:credentials])
          img = @user.images.first. # original was: File.read(ARGV.first)

          #detect faces  
          resp = rekognition.detect_faces({
            image: { bytes: img },
            attributes: [""ALL""], # What attributes to return
          })
          resp.face_details[0].emotions.each do |emo|
            puts emo.type + "" "" + emo.confidence.to_i.to_s #=&gt; Strings ""HAPPY"", ""SAD"", ""ANGRY""
          end
      end
   end
end
</code></pre>

<p>However, I get the error</p>

<p><code>expected params[:image][:bytes] to be a String or IO object, got value #&lt;ActiveStorage::Attachment id: 4, name: ""images"", record_type: ""User"", record_id: 47, blob_id: 9, created_at: """"&gt; (class: ActiveStorage::Attachment) instead.</code></p>

<p>How can I get the image file property into AWS Rekognition?</p>",,1,0,,2019-03-11 10:07:20.810 UTC,,2019-03-11 10:28:31.863 UTC,,,,,1835311,1,0,ruby-on-rails|amazon-rekognition,37
Microsoft Face Api key limit,42294083,Microsoft Face Api key limit,"<p>I am using Microsoft Face Api to verify two images.
I have created the api key as the picture given below.</p>

<p><a href=""https://i.stack.imgur.com/v7Akm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v7Akm.png"" alt=""enter image description here""></a></p>

<p>There are two api keys generated.
I am confused about these two keys , whether the quota limit of 30k per month is applicable for both the keys or individually for one key is 30k per month.
And one more is there any validity period of the key.</p>",,1,0,,2017-02-17 09:25:25.797 UTC,,2017-02-17 17:24:02.647 UTC,,,,,5038905,1,0,microsoft-cognitive|face-api,266
Google cloud vision api-authentication exception,49672973,Google cloud vision api-authentication exception,"<p>I am trying to use google cloud vision api text detection.</p>

<pre><code>using System;
using Google.Apis.Auth.OAuth2;
using Google.Cloud.Vision.V1;
using Google.Api.Gax.Grpc;
namespace blablabla
{
    class Program
    {
        static void Main(string[] args)
        {
            string filePath = @""D:\Manisha\Pictures\1.png"";

            var image = Image.FromFile(filePath);

            var client = ImageAnnotatorClient.Create();
            var response = client.DetectText(image);
            foreach (var annotation in response)
            {
                if (annotation.Description != null)
                    Console.WriteLine(annotation.Description);
            }
            Console.ReadLine();
        }
    }
}
</code></pre>

<p>I am getting the following error at<br>
""var client = ImageAnnotatorClient.Create();""</p>

<p>""The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See <a href=""https://developers.google.com/accounts/docs/application-default-credentials"" rel=""nofollow noreferrer"">https://developers.google.com/accounts/docs/application-default-credentials</a> for more information.""</p>

<p>I have set the GOOGLE_APPLICATION_CREDENTIALS to the json file path.
Where am i actually going wrong. Am i missing some important steps?</p>",,1,5,,2018-04-05 12:56:18.343 UTC,,2018-04-20 13:09:09.567 UTC,,,,,7376458,1,0,c#|authentication|ocr|google-cloud-vision,328
require: false in Gemfile not working with rake tasks,52285908,require: false in Gemfile not working with rake tasks,"<p>I have a Gemfile with something like this:</p>

<pre><code>gem 'google-cloud-vision', require: false
</code></pre>

<p>Yet, when I run a rake task like <code>bundle exec rake db:migrate</code>, the gem is still loaded (if I was to comment gem out in Gemfile, then I would get a cannot load such file error). I can even see warnings from the gem:</p>

<blockquote>
  <p>The Google Cloud API clients work best on supported versions of Ruby.
  Consider upgrading to Ruby 2.3 or later. See
  <a href=""https://www.ruby-lang.org/en/downloads/branches/"" rel=""nofollow noreferrer"">https://www.ruby-lang.org/en/downloads/branches/</a> for more info on the
  Ruby maintenance schedule. To suppress this message, set the
  GOOGLE_CLOUD_SUPPRESS_RUBY_WARNINGS environment variable.</p>
</blockquote>

<p>Isn't require: false not supposed to load the Gem? And if it does, then how can I prevent this gem from being loaded for rake tasks like db:migrate?</p>",,0,2,,2018-09-12 00:38:44.920 UTC,,2018-09-12 00:44:43.370 UTC,2018-09-12 00:44:43.370 UTC,,4501354,,4501354,1,0,ruby-on-rails|ruby,72
Getting image from ImageView instead of Drawable,55644244,Getting image from ImageView instead of Drawable,"<p><b>Please Note that I am a NEWBIE. Under Learning Process </b></p>

<p>Right now what this code is doing? It is reading an image from drawable and then getting the text from it using google vision.</p>

<pre><code> Bitmap bitmap = BitmapFactory.decodeResource(getApplicationContext().getResources(), R.drawable.changing);
        TextRecognizer textRecognizer = new TextRecognizer.Builder(getApplicationContext()).build();

        if(!textRecognizer.isOperational()){
            Toast.makeText(getApplicationContext(), """", Toast.LENGTH_SHORT).show();
        }
        else
        {
            Frame frame = new Frame.Builder().setBitmap(bitmap).build();

            SparseArray&lt;TextBlock&gt; item = textRecognizer.detect(frame); //Yeh Frame Detect Kr rha he

            StringBuilder sb = new StringBuilder();
            for(int i = 0 ; i&lt;item.size();i++){
                TextBlock myitem = item.valueAt(i);
                sb.append(myitem.getValue());
                sb.append(""\n"");
            }
</code></pre>

<p><b>NOTE:</b> R.drawable.changing is where ""changing"" is my image name. 
Now i want to replace the image with the image i uploaded on ImageView.</p>

<p>Any help would work.</p>",,3,3,,2019-04-12 03:51:10.070 UTC,,2019-04-12 06:02:42.787 UTC,2019-04-12 06:02:42.787 UTC,,2092392,,11349384,1,-1,java|android|android-imageview,71
Google Vision API hitting request limit even though I am sending way less no of requests,38804790,Google Vision API hitting request limit even though I am sending way less no of requests,"<p>The Google Vision API has a limit of 10 requests per second. I have put a time gap of 10 seconds between each request and even then the response I get for every request after the second or third request is as below. The first request always works just fine.</p>

<blockquote>
  <p>, https://vision.googleapis.com/v1/images:annotate?alt=json
  returned ""Insufficient tokens for quota group and limit
  DefaultGroupUSER-100s using the limit by ID ******@*******.</p>
</blockquote>

<p>What could be the reason that this is happening. Is there something the documentation that I am missing ?. The images I try to pass are in the range of 100-150 KB size only. </p>",41846744,1,0,,2016-08-06 13:28:58.053 UTC,,2017-01-25 08:34:04.893 UTC,2016-08-08 16:49:04.073 UTC,,5231007,,715620,1,2,google-api|vision|google-cloud-vision,747
Google vision ocr : vertical and horizontal lines text recognition,45455138,Google vision ocr : vertical and horizontal lines text recognition,"<p>we are using google vision ocr for gathering text from receipts.
In some cases the receipt have some text written in vertical  , like vat information and some other.</p>

<p>The question is that  google vision read efficiently only the text in the main orientation (horizontal by example) and discards all the text written in the same receipt in vertical orientation instead in horizontal.
Is there a parameter to set up for tell google vision to acquire also the text in vertical orientation?</p>

<p>I have put online an example with an image with text in two orientations .</p>

<p><a href=""https://drive.google.com/file/d/0B8kZz-q27lGGSUl5V3RjXzBLNnc/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/0B8kZz-q27lGGSUl5V3RjXzBLNnc/view?usp=sharing</a></p>

<p>Text recognized from g-vision : 
<em>Horizontal text line</em></p>

<p>Text I've expected to be recognized:
<em>Horizontal text line</em>
<em>Vertical text line</em></p>",,3,0,,2017-08-02 08:12:50.727 UTC,,2019-01-12 03:10:07.753 UTC,2017-08-02 11:40:32.060 UTC,,107625,,8404081,1,1,ocr|google-vision|text-recognition,1876
Google Vision - read URL from xls,52848759,Google Vision - read URL from xls,"<p>I would like to analyse pictures from Instagram with Google Vision. Up to now, I collected posts from Instagram by hashtag. So I have a CSV/XLS where the information per post is ""stored"". Title, coordinates and the links to the images.</p>

<p>Now I used the following python script to receive the annotations (labels) from pictures (I had to download the pictures first)</p>

<pre><code>import io
import os

from google.cloud import vision
from google.cloud.vision import types


os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]=""../mycredentials.json""

vision_client=vision.ImageAnnotatorClient()
file_name = ""horse.jpg""

with io.open(file_name,'rb') as image_file:
    content = image_file.read()

    image = types.Image(content=content)

    response = vision_client.label_detection(image=image)
    labels = response.label_annotations

for label in labels:
    print(label.description)
</code></pre>

<p>This script allows me to receive the the annotations by an individual, pre-downloaded script (here <code>horse.jpg</code>)</p>

<p>My question now: any idea of how to send the links for the images  of the post-collection (in my CSV/XLS) to google vision? In other words, not to download pictures first and analyse them via API one by one, but just using the Links and than receive the annotations automatically for all picture-links in the CSV?</p>",,0,2,,2018-10-17 06:44:25.680 UTC,,2018-10-17 07:01:05.073 UTC,2018-10-17 07:01:05.073 UTC,,4733879,,5976308,1,0,python|api|csv|hyperlink|google-vision,30
Android Surface View with Custom Size,51030561,Android Surface View with Custom Size,"<p>I am working on an OCR app with a Surface View and Google Vision API, which works fin; but the Surface View extends over the whole Display of the Device (Galaxy Grand Prime). It should only have half of the size or less, so that the recognised text can be displayed in a Text View beneath the Surface View. Although I googled a lot, I couldn´t solve this problem and would appreciate any support and advice, thanks! </p>

<p>The Main Activity: </p>

<pre><code>import android.Manifest;
import android.annotation.SuppressLint;
import android.content.Intent;
import android.content.pm.PackageManager;
import android.os.Bundle;
import android.support.annotation.NonNull;
import android.support.design.widget.FloatingActionButton;
import android.support.design.widget.Snackbar;
import android.support.v4.app.ActivityCompat;
import android.util.Log;
import android.util.SparseArray;
import android.view.SurfaceHolder;
import android.view.SurfaceView;
import android.view.View;
import android.support.design.widget.NavigationView;
import android.support.v4.view.GravityCompat;
import android.support.v4.widget.DrawerLayout;
import android.support.v7.app.ActionBarDrawerToggle;
import android.support.v7.app.AppCompatActivity;
import android.support.v7.widget.Toolbar;
import android.view.Menu;
import android.view.MenuItem;
import android.widget.Button;
import android.widget.TextView;

import com.google.android.gms.vision.CameraSource;
import com.google.android.gms.vision.Detector;
import com.google.android.gms.vision.text.TextBlock;
import com.google.android.gms.vision.text.TextRecognizer;

import java.io.IOException;

public class MainActivity extends AppCompatActivity
        implements NavigationView.OnNavigationItemSelectedListener {

    SurfaceView mCameraView;
    TextView mTextView;
    CameraSource mCameraSource;


    private static final String TAG = ""MainActivity"";
    private static final int requestPermissionID = 101;

    @SuppressLint(""WrongViewCast"")
    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);
        Toolbar toolbar = (Toolbar) findViewById(R.id.toolbar);
        mCameraView = findViewById(R.id.surfaceView);
        mTextView = findViewById(R.id.text_view);
        setSupportActionBar(toolbar);


        FloatingActionButton fab = (FloatingActionButton) findViewById(R.id.fab);
        fab.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                Snackbar.make(view, ""Replace with your own action"", Snackbar.LENGTH_LONG)
                        .setAction(""Action"", null).show();
            }
        });

        DrawerLayout drawer = (DrawerLayout) findViewById(R.id.drawer_layout);
        ActionBarDrawerToggle toggle = new ActionBarDrawerToggle(
                this, drawer, toolbar, R.string.navigation_drawer_open, R.string.navigation_drawer_close);
        drawer.addDrawerListener(toggle);
        toggle.syncState();

        NavigationView navigationView = (NavigationView) findViewById(R.id.nav_view);
        navigationView.setNavigationItemSelectedListener(this);
        startCameraSource();
    }

    @Override
    public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) {
        if (requestCode != requestPermissionID) {
            Log.d(TAG, ""Got unexpected permission result: "" + requestCode);
            super.onRequestPermissionsResult(requestCode, permissions, grantResults);
            return;
        }

        if (grantResults[0] == PackageManager.PERMISSION_GRANTED) {
            try {
                if (ActivityCompat.checkSelfPermission(this, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {
                    return;
                }
                mCameraSource.start(mCameraView.getHolder());
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }



    private void startCameraSource() {

        //Create the TextRecognizer
        final TextRecognizer textRecognizer = new TextRecognizer.Builder(getApplicationContext()).build();

        if (!textRecognizer.isOperational()) {
            Log.w(TAG, ""Detector dependencies not loaded yet"");
        } else {

            //Initialize camerasource to use high resolution and set Autofocus on.
            mCameraSource = new CameraSource.Builder(getApplicationContext(), textRecognizer)
                    .setFacing(CameraSource.CAMERA_FACING_BACK)
                    .setRequestedPreviewSize(1280, 1024)
                    .setAutoFocusEnabled(true)
                    .setRequestedFps(2.0f)
                    .build();

            /**
             * Add call back to SurfaceView and check if camera permission is granted.
             * If permission is granted we can start our cameraSource and pass it to surfaceView
             */
            mCameraView.getHolder().addCallback(new SurfaceHolder.Callback() {
                @Override
                public void surfaceCreated(SurfaceHolder holder) {
                    try {

                        if (ActivityCompat.checkSelfPermission(getApplicationContext(),
                                Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {

                            ActivityCompat.requestPermissions(MainActivity.this,
                                    new String[]{Manifest.permission.CAMERA},
                                    requestPermissionID);
                            return;
                        }
                        mCameraSource.start(mCameraView.getHolder());
                    } catch (IOException e) {
                        e.printStackTrace();
                    }
                }

                @Override
                public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {
                }

                @Override
                public void surfaceDestroyed(SurfaceHolder holder) {
                    mCameraSource.stop();
                }
            });

            //Set the TextRecognizer's Processor.
            textRecognizer.setProcessor(new Detector.Processor&lt;TextBlock&gt;() {
                @Override
                public void release() {
                }

                /**
                 * Detect all the text from camera using TextBlock and the values into a stringBuilder
                 * which will then be set to the textView.
                 * */
                @Override
                public void receiveDetections(Detector.Detections&lt;TextBlock&gt; detections) {
                    final SparseArray&lt;TextBlock&gt; items = detections.getDetectedItems();
                    if (items.size() != 0 ){

                        mTextView.post(new Runnable() {
                            @Override
                            public void run() {
                                StringBuilder stringBuilder = new StringBuilder();
                                for(int i=0;i&lt;items.size();i++){
                                    TextBlock item = items.valueAt(i);
                                    stringBuilder.append(item.getValue());
                                    stringBuilder.append(""\n"");
                                }
                                mTextView.setText(stringBuilder.toString());
                                        // Start NewActivity.class
                                        String recognizedText = mTextView.getText().toString();
                                        Intent i = new Intent(MainActivity.this, ListActivity.class);
                                        i.putExtra(""recognized Text"", recognizedText);


                            }
                        });
                    }
                }
            });
        }
    }


    @Override
    public void onBackPressed() {
        DrawerLayout drawer = (DrawerLayout) findViewById(R.id.drawer_layout);
        if (drawer.isDrawerOpen(GravityCompat.START)) {
            drawer.closeDrawer(GravityCompat.START);
        } else {
            super.onBackPressed();
        }
    }

    @Override
    public boolean onCreateOptionsMenu(Menu menu) {
        // Inflate the menu; this adds items to the action bar if it is present.
        getMenuInflater().inflate(R.menu.main, menu);
        return true;
    }

    @Override
    public boolean onOptionsItemSelected(MenuItem item) {
        // Handle action bar item clicks here. The action bar will
        // automatically handle clicks on the Home/Up button, so long
        // as you specify a parent activity in AndroidManifest.xml.
        int id = item.getItemId();

        //noinspection SimplifiableIfStatement
        if (id == R.id.action_settings) {
            return true;
        }

        return super.onOptionsItemSelected(item);
    }

    @SuppressWarnings(""StatementWithEmptyBody"")
    @Override
    public boolean onNavigationItemSelected(MenuItem item) {
        // Handle navigation view item clicks here.
        int id = item.getItemId();

        if (id == R.id.nav_camera) {
            String recognizedText = mTextView.getText().toString();
            Intent i = new Intent(MainActivity.this, ListActivity.class);
            i.putExtra(""recognized Text"", recognizedText);
        } else if (id == R.id.nav_gallery) {

        } else if (id == R.id.nav_slideshow) {

        } else if (id == R.id.nav_manage) {

        } else if (id == R.id.nav_share) {

        } else if (id == R.id.nav_send) {

        }

        DrawerLayout drawer = (DrawerLayout) findViewById(R.id.drawer_layout);
        drawer.closeDrawer(GravityCompat.START);
        return true;
    }
}
</code></pre>

<p>The activity_main.xml</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;android.support.v4.widget.DrawerLayout xmlns:android=""http://schemas.android.com/apk/res/android""
    xmlns:app=""http://schemas.android.com/apk/res-auto""
    xmlns:tools=""http://schemas.android.com/tools""
    android:id=""@+id/drawer_layout""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""
    android:fitsSystemWindows=""true""
    tools:openDrawer=""start""&gt;

    &lt;include
        layout=""@layout/app_bar_main""
        android:layout_width=""match_parent""
        android:layout_height=""match_parent"" /&gt;

    &lt;SurfaceView
        android:id=""@+id/surfaceView""
        android:layout_width=""10dp""
        android:layout_height=""10dp""

        android:layout_marginBottom=""20dp""
        android:layout_marginLeft=""20dp""
        android:layout_marginRight=""20dp""
        android:layout_weight=""2""/&gt;
    &lt;TextView
        android:id=""@+id/text_view""
        android:layout_width=""match_parent""
        android:layout_height=""0dp""
        android:layout_margin=""8dp""
        android:layout_weight=""1""
        android:gravity=""center""
        android:textStyle=""bold""
        android:text=""@string/txt_message""
        android:textColor=""@android:color/black""
        android:textSize=""20sp"" /&gt;


    &lt;android.support.design.widget.NavigationView
        android:id=""@+id/nav_view""
        android:layout_width=""wrap_content""
        android:layout_height=""match_parent""
        android:layout_gravity=""start""
        android:fitsSystemWindows=""true""
        app:headerLayout=""@layout/nav_header_main""
        app:menu=""@menu/activity_main_drawer"" /&gt;

&lt;/android.support.v4.widget.DrawerLayout&gt;
</code></pre>",,0,0,,2018-06-25 19:23:27.407 UTC,,2018-06-25 19:23:27.407 UTC,,,,,1120165,1,1,java|android|surfaceview,101
How can I use FaceDetectionAPI without google play Service in android AOSP?,54480590,How can I use FaceDetectionAPI without google play Service in android AOSP?,"<p>I am currently developing Face Detection Application in an Android marshmallow AOSP without Gapps. But I can't use Google Vision api without Google Play service. So, is there any way to use Google Vision api without Google Play Services? Or, I'll thank you if you let me know another good api!</p>",54480983,3,0,,2019-02-01 13:31:53.403 UTC,,2019-02-04 06:28:21.250 UTC,,,,,10765888,1,0,android|api|face-recognition|google-vision,52
Face Detection in Video using Google Cloud API,44629662,Face Detection in Video using Google Cloud API,"<p>I'm trying to do face detection in a video using Google Vision API. I'm using the following code:</p>

<pre><code>import argparse
import cv2
from google.cloud import vision
from PIL import Image, ImageDraw


def detect_face(face_file, max_results=4):
    """"""Uses the Vision API to detect faces in the given file.
    Args:
        face_file: A file-like object containing an image with faces.
    Returns:
        An array of Face objects with information about the picture.
    """"""
    content = face_file.read()
    # [START get_vision_service]
    image = vision.Client().image(content=content)
    # [END get_vision_service]

    return image.detect_faces()


def highlight_faces(frame, faces, output_filename):
    """"""Draws a polygon around the faces, then saves to output_filename.
    Args:
      image: a file containing the image with the faces.
      faces: a list of faces found in the file. This should be in the format
          returned by the Vision API.
      output_filename: the name of the image file to be created, where the
          faces have polygons drawn around them.
    """"""
    im = Image.open(frame)
    draw = ImageDraw.Draw(im)

    for face in faces:
        box = [(bound.x_coordinate, bound.y_coordinate)
               for bound in face.bounds.vertices]
        draw.line(box + [box[0]], width=5, fill='#00ff00')

    #im.save(output_filename)


def main(input_filename, max_results):

    video_capture = cv2.VideoCapture(input_filename)


    while True:
        # Capture frame-by-frame
        ret, frame = video_capture.read()
        faces = detect_face(frame, max_results)
        highlight_faces(frame, faces)
        cv2.imshow('Video', frame)
        if cv2.waitKey(1) &amp; 0xFF == ord('q'):
            break


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Detects faces in the given image.')
    parser.add_argument(
        'input_image', help='the image you\'d like to detect faces in.')
    parser.add_argument(
        '--max-results', dest='max_results', default=4,
        help='the max results of face detection.')
    args = parser.parse_args()

    main(args.input_image, args.max_results)
</code></pre>

<p>But I'm getting the error:</p>

<blockquote>
  <p>content = face_file.read() AttributeError: 'numpy.ndarray' object has
  no attribute 'read'</p>
</blockquote>

<p>The ""frames"" are getting read as numpy array. But don't know how to bypass them.</p>

<p>Can anyone please help me?</p>",,1,0,,2017-06-19 11:44:07.753 UTC,1,2017-06-21 23:15:28.537 UTC,2017-06-20 19:17:39.143 UTC,,794749,,697363,1,0,python|google-cloud-vision,426
"""Decoding error, image format unsupported"" in Microsoft Face API",36120746,"""Decoding error, image format unsupported"" in Microsoft Face API","<p>I'm trying to use Microsoft Face API. For that I have the following code that was given by Microsoft as a sample (at the end of this page <a href=""https://dev.projectoxford.ai/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236"" rel=""nofollow"">https://dev.projectoxford.ai/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236</a>):</p>

<pre><code>HttpClient httpclient = HttpClients.createDefault();

try {
    URIBuilder builder = new URIBuilder(""https://api.projectoxford.ai/face/v1.0/detect"");

    builder.setParameter(""returnFaceId"", ""false"");
    builder.setParameter(""returnFaceLandmarks"", ""false"");
    builder.setParameter(""returnFaceAttributes"", ""age,gender"");

    URI uri = builder.build();
    HttpPost request = new HttpPost(uri);
    request.setHeader(""Content-Type"", ""application/octet-stream"");
    request.setHeader(""Ocp-Apim-Subscription-Key"", ""..."");

    String body = Base64.encodeBase64String(img);

    StringEntity reqEntity = new StringEntity(body);
    request.setEntity(reqEntity);

    HttpResponse response = httpclient.execute(request);
    HttpEntity entity = response.getEntity();

    if (entity != null) {
            System.out.println(EntityUtils.toString(entity));
            return JsonParser.parse(EntityUtils.toString(entity));
    }
} catch (URISyntaxException | IOException | ParseException e) {
        System.out.println(e.getMessage());
}

return null;
</code></pre>

<p>but I get the following error:</p>

<pre><code>{""error"":{""code"":""InvalidImage"",""message"":""Decoding error, image format unsupported.""}}
</code></pre>

<p>The image that I am using for tests is this one:
<a href=""http://www.huntresearchgroup.org.uk/images/group/group_photo_2010.jpg"" rel=""nofollow"">http://www.huntresearchgroup.org.uk/images/group/group_photo_2010.jpg</a>
(found it on the internet in a quick search)</p>

<p>It respect all the requisits set by Microsoft, size and format... If I use it in the site it works <a href=""https://www.projectoxford.ai/demo/face#detection"" rel=""nofollow"">https://www.projectoxford.ai/demo/face#detection</a></p>

<p>The <code>String body</code> from the convertion of my array of bytes to a string in base64 is also ok, I test it in this website: <a href=""http://codebeautify.org/base64-to-image-converter"" rel=""nofollow"">http://codebeautify.org/base64-to-image-converter</a></p>

<p>The error message it's quite simple, but I fail to see where I am worng. Anyone might know whats the problem?</p>

<p><strong>UPDATE</strong></p>

<p>The variable <code>img</code>:</p>

<pre><code>img = Files.readAllBytes(Paths.get(imgPath));
</code></pre>",36139861,4,5,,2016-03-20 23:06:49.453 UTC,,2019-01-08 20:44:19.510 UTC,2016-03-21 01:10:55.607 UTC,,2581872,,4358010,1,1,java|api|face,1427
Importing translate from google.cloud,46061561,Importing translate from google.cloud,"<p>I am trying to do translate a document with google translate from the package google.cloud
I already did:</p>

<pre><code>pip install google.cloud 
</code></pre>

<p>and the result was:</p>

<blockquote>
  <p>Collecting google.cloud
    Using cached google_cloud-0.27.0-py2.py3-none-any.whl
  Requirement already satisfied: google-cloud-logging&lt;1.3dev,>=1.2.0 in c:\anaconda\lib\site-packages (from google.cloud)
  Requirement already satisfied: google-cloud-bigtable&lt;0.27dev,>=0.26.0 in c:\anaconda\lib\site-packages (from google.cloud)
  Requirement already satisfied: google-cloud-datastore&lt;1.3dev,>=1.2.0 in c:\anaconda\lib\site-packages (from google.cloud)
  Requirement already satisfied: google-cloud-error-reporting&lt;0.27dev,>=0.26.0 in c:\anaconda\lib\site-packages (from google.cloud)
  Requirement already satisfied: google-cloud-pubsub&lt;0.28dev,>=0.27.0 in c:\anaconda\lib\site-packages (from google.cloud)
  Requirement already satisfied: google-cloud-core&lt;0.27dev,>=0.26.0 in c:\anaconda\lib\site-packages (from google.cloud)
  Requirement already satisfied: google-cloud-storage&lt;1.4dev,>=1.3.0 in c:\anaconda\lib\site-packages (from google.cloud)
  Requirement already satisfied: google-cloud-resource-manager&lt;0.27dev,>=0.26.0 in c:\anaconda\lib\site-packages (from google.cloud)
  Requirement already satisfied: google-cloud-language&lt;0.28dev,>=0.27.0 in c:\anaconda\lib\site-packages (from google.cloud)
  Requirement already satisfied: google-cloud-translate&lt;1.2dev,>=1.1.0 in c:\anaconda\lib\site-packages (from google.cloud)
  Requirement already satisfied: google-cloud-speech&lt;0.29dev,>=0.28.0 in c:\anaconda\lib\site-packages (from google.cloud)
  Requirement already satisfied: google-cloud-videointelligence&lt;0.26dev,>=0.25.0 in c:\anaconda\lib\site-packages (from google.cloud)
  Requirement already satisfied: google-cloud-monitoring&lt;0.27dev,>=0.26.0 in c:\anaconda\lib\site-packages (from google.cloud)
  Requirement already satisfied: google-cloud-spanner&lt;0.27dev,>=0.26.0 in c:\anaconda\lib\site-packages (from google.cloud)
  Requirement already satisfied: google-cloud-runtimeconfig&lt;0.27dev,>=0.26.0 in c:\anaconda\lib\site-packages (from google.cloud)
  Requirement already satisfied: google-cloud-vision&lt;0.27dev,>=0.26.0 in c:\anaconda\lib\site-packages (from google.cloud)
  Requirement already satisfied: google-cloud-dns&lt;0.27dev,>=0.26.0 in c:\anaconda\lib\site-packages (from google.cloud)
  Requirement already satisfied: google-cloud-bigquery&lt;0.27dev,>=0.26.0 in c:\anaconda\lib\site-packages (from google.cloud)
  Requirement already satisfied: gapic-google-cloud-logging-v2&lt;0.92dev,>=0.91.0 in c:\anaconda\lib\site-packages (from google-cloud-logging&lt;1.3dev,>=1.2.0->google.cloud)
  Requirement already satisfied: grpcio&lt;2.0dev,>=1.2.0 in c:\anaconda\lib\site-packages (from google-cloud-logging&lt;1.3dev,>=1.2.0->google.cloud)
  Requirement already satisfied: google-gax&lt;0.16dev,>=0.15.7 in c:\anaconda\lib\site-packages (from google-cloud-bigtable&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: gapic-google-cloud-datastore-v1&lt;0.16dev,>=0.15.0 in c:\anaconda\lib\site-packages (from google-cloud-datastore&lt;1.3dev,>=1.2.0->google.cloud)
  Requirement already satisfied: gapic-google-cloud-error-reporting-v1beta1&lt;0.16dev,>=0.15.0 in c:\anaconda\lib\site-packages (from google-cloud-error-reporting&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: gapic-google-cloud-pubsub-v1&lt;0.16dev,>=0.15.0 in c:\anaconda\lib\site-packages (from google-cloud-pubsub&lt;0.28dev,>=0.27.0->google.cloud)
  Requirement already satisfied: protobuf>=3.0.0 in c:\anaconda\lib\site-packages (from google-cloud-core&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: tenacity&lt;5.0.0dev,>=4.0.0 in c:\anaconda\lib\site-packages (from google-cloud-core&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: requests&lt;3.0.0dev,>=2.4.0 in c:\anaconda\lib\site-packages (from google-cloud-core&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: google-auth&lt;2.0.0dev,>=0.4.0 in c:\anaconda\lib\site-packages (from google-cloud-core&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: googleapis-common-protos>=1.3.4 in c:\anaconda\lib\site-packages (from google-cloud-core&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: six in c:\anaconda\lib\site-packages (from google-cloud-core&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: google-resumable-media>=0.2.3 in c:\anaconda\lib\site-packages (from google-cloud-storage&lt;1.4dev,>=1.3.0->google.cloud)
  Requirement already satisfied: gapic-google-cloud-spanner-v1&lt;0.16dev,>=0.15.0 in c:\anaconda\lib\site-packages (from google-cloud-spanner&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: gapic-google-cloud-spanner-admin-database-v1&lt;0.16dev,>=0.15.0 in c:\anaconda\lib\site-packages (from google-cloud-spanner&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: gapic-google-cloud-spanner-admin-instance-v1&lt;0.16dev,>=0.15.0 in c:\anaconda\lib\site-packages (from google-cloud-spanner&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: proto-google-cloud-logging-v2[grpc]&lt;0.92dev,>=0.91.3 in c:\anaconda\lib\site-packages (from gapic-google-cloud-logging-v2&lt;0.92dev,>=0.91.0->google-cloud-logging&lt;1.3dev,>=1.2.0->google.cloud)
  Requirement already satisfied: oauth2client&lt;4.0dev,>=2.0.0 in c:\anaconda\lib\site-packages (from gapic-google-cloud-logging-v2&lt;0.92dev,>=0.91.0->google-cloud-logging&lt;1.3dev,>=1.2.0->google.cloud)
  Requirement already satisfied: dill&lt;0.3dev,>=0.2.5 in c:\anaconda\lib\site-packages (from google-gax&lt;0.16dev,>=0.15.7->google-cloud-bigtable&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: ply==3.8 in c:\anaconda\lib\site-packages (from google-gax&lt;0.16dev,>=0.15.7->google-cloud-bigtable&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: future&lt;0.17dev,>=0.16.0 in c:\anaconda\lib\site-packages (from google-gax&lt;0.16dev,>=0.15.7->google-cloud-bigtable&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: proto-google-cloud-datastore-v1[grpc]&lt;0.91dev,>=0.90.3 in c:\anaconda\lib\site-packages (from gapic-google-cloud-datastore-v1&lt;0.16dev,>=0.15.0->google-cloud-datastore&lt;1.3dev,>=1.2.0->google.cloud)
  Requirement already satisfied: proto-google-cloud-error-reporting-v1beta1[grpc]&lt;0.16dev,>=0.15.3 in c:\anaconda\lib\site-packages (from gapic-google-cloud-error-reporting-v1beta1&lt;0.16dev,>=0.15.0->google-cloud-error-reporting&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: proto-google-cloud-pubsub-v1[grpc]&lt;0.16dev,>=0.15.4 in c:\anaconda\lib\site-packages (from gapic-google-cloud-pubsub-v1&lt;0.16dev,>=0.15.0->google-cloud-pubsub&lt;0.28dev,>=0.27.0->google.cloud)
  Requirement already satisfied: grpc-google-iam-v1&lt;0.12dev,>=0.11.1 in c:\anaconda\lib\site-packages (from gapic-google-cloud-pubsub-v1&lt;0.16dev,>=0.15.0->google-cloud-pubsub&lt;0.28dev,>=0.27.0->google.cloud)
  Requirement already satisfied: setuptools in c:\anaconda\lib\site-packages\setuptools-23.0.0-py3.5.egg (from protobuf>=3.0.0->google-cloud-core&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: monotonic>=0.6 in c:\anaconda\lib\site-packages (from tenacity&lt;5.0.0dev,>=4.0.0->google-cloud-core&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: cachetools>=2.0.0 in c:\anaconda\lib\site-packages (from google-auth&lt;2.0.0dev,>=0.4.0->google-cloud-core&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: rsa>=3.1.4 in c:\anaconda\lib\site-packages (from google-auth&lt;2.0.0dev,>=0.4.0->google-cloud-core&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: pyasn1-modules>=0.0.5 in c:\anaconda\lib\site-packages (from google-auth&lt;2.0.0dev,>=0.4.0->google-cloud-core&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: pyasn1>=0.1.7 in c:\anaconda\lib\site-packages (from google-auth&lt;2.0.0dev,>=0.4.0->google-cloud-core&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: proto-google-cloud-spanner-v1[grpc]&lt;0.16dev,>=0.15.3 in c:\anaconda\lib\site-packages (from gapic-google-cloud-spanner-v1&lt;0.16dev,>=0.15.0->google-cloud-spanner&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: proto-google-cloud-spanner-admin-database-v1[grpc]&lt;0.16dev,>=0.15.3 in c:\anaconda\lib\site-packages (from gapic-google-cloud-spanner-admin-database-v1&lt;0.16dev,>=0.15.0->google-cloud-spanner&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: proto-google-cloud-spanner-admin-instance-v1[grpc]&lt;0.16dev,>=0.15.3 in c:\anaconda\lib\site-packages (from gapic-google-cloud-spanner-admin-instance-v1&lt;0.16dev,>=0.15.0->google-cloud-spanner&lt;0.27dev,>=0.26.0->google.cloud)
  Requirement already satisfied: httplib2>=0.9.1 in c:\anaconda\lib\site-packages (from oauth2client&lt;4.0dev,>=2.0.0->gapic-google-cloud-logging-v2&lt;0.92dev,>=0.91.0->google-cloud-logging&lt;1.3dev,>=1.2.0->google.cloud)
  Installing collected packages: google.cloud
  Successfully installed google.</p>
</blockquote>

<p>then I called the package in Spyder (Python 3.5):</p>

<pre><code>import google.cloud
from google.cloud import translate
</code></pre>

<p>I obtained this error:</p>

<blockquote>
  <p>Traceback (most recent call last):</p>
  
  <p>File """", line 2, in 
      from google.cloud import translate</p>
  
  <p>File ""C:\Anaconda\lib\site-packages\google\cloud\translate.py"", line 18, in 
      from google.cloud.translate_v2 import <strong>version</strong></p>
  
  <p>File ""C:\Anaconda\lib\site-packages\google\cloud\translate_v2__init__.py"", line 19, in 
      <strong>version</strong> = get_distribution('google-cloud-translate').version</p>
  
  <p>File ""C:\Anaconda\lib\site-packages\setuptools-23.0.0-py3.5.egg\pkg_resources__init__.py"", line 535, in get_distribution
      dist = get_provider(dist)</p>
  
  <p>File ""C:\Anaconda\lib\site-packages\setuptools-23.0.0-py3.5.egg\pkg_resources__init__.py"", line 415, in get_provider
      return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]</p>
  
  <p>File ""C:\Anaconda\lib\site-packages\setuptools-23.0.0-py3.5.egg\pkg_resources__init__.py"", line 943, in require
      needed = self.resolve(parse_requirements(requirements))</p>
  
  <p>File ""C:\Anaconda\lib\site-packages\setuptools-23.0.0-py3.5.egg\pkg_resources__init__.py"", line 834, in resolve
      raise VersionConflict(dist, req).with_context(dependent_req)</p>
  
  <p>ContextualVersionConflict: (pyasn1 0.1.9 (c:\anaconda\lib\site-packages), Requirement.parse('pyasn1==0.3.3'), {'pyasn1-modules'})</p>
</blockquote>",46227873,1,0,,2017-09-05 18:51:36.437 UTC,,2017-09-14 20:39:20.653 UTC,,,,,6087589,1,0,python|google-app-engine|cmd|google-translate,884
IBM Watson Visual Recognition - Access is denied due to invalid credentials,50703766,IBM Watson Visual Recognition - Access is denied due to invalid credentials,"<p>I'm trying to use IBM Watson Visual Recognition tool with nodejs (express).
I followed the instruction from the <a href=""https://www.ibm.com/watson/developercloud/visual-recognition/api/v3/node.html?node#authentication"" rel=""nofollow noreferrer"">guide</a>, but I can't connect with the tool.</p>

<pre><code>var fs = require('fs');
var VisualRecognitionV3 = require('watson-developer-cloud/visual-recognition/v3');

var visualRecognition = new VisualRecognitionV3({
    version: '2018-03-19',
    api_key: 'api key',
});

var images_file = fs.createReadStream('public/images/fruitbowl.jpg');

var classifier_ids = [""food""];

var params = {
    images_file: images_file,
    classifier_ids: classifier_ids
};

visualRecognition.classify(params, function(err, response) {
    if (err)
        console.log(err);
    else
        var resp = JSON.stringify(response, null, 2)
        console.log(JSON.stringify(response, null, 2))
});
</code></pre>

<p>When I run my nodejs app, I got this message</p>

<blockquote>
  <p>Error: Unauthorized: Access is denied due to invalid credentials.</p>
</blockquote>

<p>Does someone know the solution to this authentification problem?</p>",,1,2,,2018-06-05 15:25:50.877 UTC,,2018-06-05 16:01:45.037 UTC,,,,,5169711,1,2,javascript|node.js|express|ibm-watson|visual-recognition,485
IllegalArgumentException: Jetty ALPN/NPN has not been properly configured,44411988,IllegalArgumentException: Jetty ALPN/NPN has not been properly configured,"<p>I'm trying to use google vision in google api, however I have the following problem:</p>

<blockquote>
  <p>jun 07, 2017 8:50:00 AM io.grpc.internal.ChannelExecutor drain
  ADVERTÊNCIA: Runnable threw exception in ChannelExecutor
  java.lang.IllegalArgumentException: Jetty ALPN/NPN has not been
  properly configured.  at
  io.grpc.netty.GrpcSslContexts.selectApplicationProtocolConfig(GrpcSslContexts.java:174)
    at io.grpc.netty.GrpcSslContexts.configure(GrpcSslContexts.java:151)
    at io.grpc.netty.GrpcSslContexts.configure(GrpcSslContexts.java:139)
    at io.grpc.netty.GrpcSslContexts.forClient(GrpcSslContexts.java:109)
    at
  io.grpc.netty.NettyChannelBuilder.createProtocolNegotiatorByType(NettyChannelBuilder.java:335)
    at
  io.grpc.netty.NettyChannelBuilder.createProtocolNegotiator(NettyChannelBuilder.java:308)
    at
  io.grpc.netty.NettyChannelBuilder$NettyTransportFactory$DynamicNettyTransportParams.getProtocolNegotiator(NettyChannelBuilder.java:499)
    at
  io.grpc.netty.NettyChannelBuilder$NettyTransportFactory.newClientTransport(NettyChannelBuilder.java:448)
    at
  io.grpc.internal.CallCredentialsApplyingTransportFactory.newClientTransport(CallCredentialsApplyingTransportFactory.java:61)
    at
  io.grpc.internal.InternalSubchannel.startNewTransport(InternalSubchannel.java:209)
    at
  io.grpc.internal.InternalSubchannel.obtainActiveTransport(InternalSubchannel.java:186)
    at
  io.grpc.internal.ManagedChannelImpl$SubchannelImplImpl.obtainActiveTransport(ManagedChannelImpl.java:806)
    at
  io.grpc.internal.GrpcUtil.getTransportFromPickResult(GrpcUtil.java:568)
    at
  io.grpc.internal.DelayedClientTransport.reprocess(DelayedClientTransport.java:296)
    at
  io.grpc.internal.ManagedChannelImpl$LbHelperImpl$5.run(ManagedChannelImpl.java:724)
    at io.grpc.internal.ChannelExecutor.drain(ChannelExecutor.java:87)
    at
  io.grpc.internal.ManagedChannelImpl$LbHelperImpl.runSerialized(ManagedChannelImpl.java:715)
    at
  io.grpc.internal.ManagedChannelImpl$NameResolverListenerImpl.onUpdate(ManagedChannelImpl.java:752)
    at io.grpc.internal.DnsNameResolver$1.run(DnsNameResolver.java:174)
    at
  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at
  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)</p>
</blockquote>

<p>I'm trying to run the api's own sample code, below:</p>

<pre><code>public class QuickstartSample {
    public static void main(String... args) throws Exception {
        // Instantiates a client
        ImageAnnotatorClient vision = ImageAnnotatorClient.create();

        // The path to the image file to annotate
        String fileName = ""./resources/wakeupcat.jpg"";

        // Reads the image file into memory
        Path path = Paths.get(fileName);
        byte[] data = Files.readAllBytes(path);
        ByteString imgBytes = ByteString.copyFrom(data);

        // Builds the image annotation request
        List&lt;AnnotateImageRequest&gt; requests = new ArrayList&lt;&gt;();
        Image img = Image.newBuilder().setContent(imgBytes).build();
        Feature feat = Feature.newBuilder().setType(Type.LABEL_DETECTION).build();
        AnnotateImageRequest request = AnnotateImageRequest.newBuilder().addFeatures(feat).setImage(img).build();
        requests.add(request);

        // Performs label detection on the image file
        BatchAnnotateImagesResponse response = vision.batchAnnotateImages(requests);
        List&lt;AnnotateImageResponse&gt; responses = response.getResponsesList();

        for (AnnotateImageResponse res : responses) {
            if (res.hasError()) {
                System.out.printf(""Error: %s\n"", res.getError().getMessage());
                return;
            }

            for (EntityAnnotation annotation : res.getLabelAnnotationsList()) {
                annotation.getAllFields().forEach((k, v) -&gt; System.out.printf(""%s : %s\n"", k, v.toString()));
            }
        }
    }
}
</code></pre>

<p>I'm using the following dependencies:</p>

<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;com.google.apis&lt;/groupId&gt;
        &lt;artifactId&gt;google-api-services-vision&lt;/artifactId&gt;
        &lt;version&gt;v1-rev357-1.22.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;
        &lt;artifactId&gt;google-cloud-vision&lt;/artifactId&gt;
        &lt;version&gt;0.17.2-beta&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>Has anyone ever had this problem?</p>",,1,0,,2017-06-07 11:56:15.497 UTC,,2018-10-22 16:22:10 UTC,2017-06-08 04:27:38.760 UTC,,608639,,5123622,1,1,java|ssl|google-cloud-platform|google-vision,596
Image Processing: API to classify text based on font-type and size,46848590,Image Processing: API to classify text based on font-type and size,"<p>I am looking for an API which can take images as input and classify/identify the text in the images based on font-type and font-size. Now, the images are screenshots of screens in a mobile app, and hence represent the perfect fonts and are not distorted like handwritten text or images of printed documents.
I went through a few of the available API's like Google Vision API but could find a solution to it. </p>

<p>Any help will be appreciated. Thanks in advance.</p>",,1,2,,2017-10-20 11:57:12.610 UTC,1,2017-10-24 06:15:04.777 UTC,,,,,6575736,1,1,android|image-processing|fonts,80
AgeRange not in Rekognition response,42188322,AgeRange not in Rekognition response,"<p><a href=""https://boto3.readthedocs.io/en/latest/reference/services/rekognition.html#Rekognition.Client.delete_faces"" rel=""nofollow noreferrer"">The docs</a> mention AgeRange in the response of detect_faces.</p>

<p>But, using the Python SDK (boto3), I cannot see it in the response.</p>

<p>Am I missing something? Is the feature in the docs but not yet in production (it is a new feature from feb 10th)?</p>

<pre><code>AWS_ACCESS_KEY_ID = ""...""
AWS_SECRET_ACCESS_KEY = ""...""
os.environ['AWS_ACCESS_KEY_ID'] = AWS_ACCESS_KEY_ID
os.environ['AWS_SECRET_ACCESS_KEY'] = AWS_SECRET_ACCESS_KEY

client = boto3.client('rekognition')
reko = client.detect_faces(
    Image={'S3Object': {'Bucket': '...',
              'Name': 'user_uploads/....JPEG',}},
    Attributes=['ALL']
)

res['FaceDetails'][0].keys()
# outputs 
# dict_keys(['Landmarks', 'Eyeglasses', 'Quality', 'Confidence',
# 'Mustache', 'Emotions', 'Smile', 'BoundingBox', 'Beard', 'Gender',
# 'Pose', 'EyesOpen', 'Sunglasses', 'MouthOpen'])
</code></pre>",42190371,1,1,,2017-02-12 13:47:17.180 UTC,,2017-02-13 16:15:39.810 UTC,,,,,3218806,1,2,python|amazon-web-services|boto3|amazon-rekognition,153
Google Cloud Vision - Could not load the default credentials,42406824,Google Cloud Vision - Could not load the default credentials,"<p>I am using PHP library for using Google Cloud Vision. The docs tells about 2 ways of authentication - 1) API and 2) Service Account.</p>

<p><strong>How do I use API based auth with my VisionClient?</strong> There is no document on using it. Please let me know if I am wrong.</p>

<pre><code>    $vision = new VisionClient([
        'projectId' =&gt; 'ophoto'
    ]);

    $image = $vision-&gt;image($photoResource, 
             ['LABEL_DETECTION','SAFE_SEARCH_DETECTION','IMAGE_PROPERTIES']);

    $vision-&gt;annotate($image);
</code></pre>

<p>I get the below error message when running the above code.</p>

<pre><code>Message:    Could not load the default credentials. Browse to https://developers.google.com/accounts/docs/application-default-credentials for more information
File:   C:\Program Files\VertrigoServ\www\classes\vendor\google\cloud\src\RequestWrapper.php
Line:   219
Trace:  
#0 C:\Program Files\VertrigoServ\www\classes\vendor\google\cloud\src\RequestWrapper.php(185): Google\Cloud\RequestWrapper-&gt;convertToGoogleException(Object(DomainException))
#1 C:\Program Files\VertrigoServ\www\classes\vendor\google\cloud\src\RequestWrapper.php(167): Google\Cloud\RequestWrapper-&gt;fetchCredentials()
#2 C:\Program Files\VertrigoServ\www\classes\vendor\google\cloud\src\RequestWrapper.php(150): Google\Cloud\RequestWrapper-&gt;getToken()
#3 C:\Program Files\VertrigoServ\www\classes\vendor\google\cloud\src\RequestWrapper.php(131): Google\Cloud\RequestWrapper-&gt;signRequest(Object(GuzzleHttp\Psr7\Request))
#4 C:\Program Files\VertrigoServ\www\classes\vendor\google\cloud\src\RestTrait.php(80): Google\Cloud\RequestWrapper-&gt;send(Object(GuzzleHttp\Psr7\Request), Array)
#5 C:\Program Files\VertrigoServ\www\classes\vendor\google\cloud\src\Vision\Connection\Rest.php(57): Google\Cloud\Vision\Connection\Rest-&gt;send('images', 'annotate', Array)
#6 C:\Program Files\VertrigoServ\www\classes\vendor\google\cloud\src\Vision\VisionClient.php(265): Google\Cloud\Vision\Connection\Rest-&gt;annotate(Array)
#7 C:\Program Files\VertrigoServ\www\bol\service.php(60): Google\Cloud\Vision\VisionClient-&gt;annotateBatch(Array)
#8 C:\Program Files\VertrigoServ\www\init.php(34): PHOTOTAGS_BOL_Service-&gt;analyzeImages(Array)
#9 [internal function]: phototags_user_register(Object(OW_Event))
#10 C:\Program Files\VertrigoServ\www\oxwall\ow_core\event_manager.php(228): call_user_func('phototags_user_...', Object(OW_Event))
#11 C:\Program Files\VertrigoServ\www\oxwall\ow_plugins\photo\controllers\ajax_upload.php(263): OW_EventManager-&gt;trigger(Object(OW_Event))
#12 C:\Program Files\VertrigoServ\www\oxwall\ow_plugins\photo\controllers\ajax_upload.php(227): PHOTO_CTRL_AjaxUpload-&gt;onSubmitComplete('user', 1, Object(PHOTO_BOL_PhotoAlbum), Array)
#13 [internal function]: PHOTO_CTRL_AjaxUpload-&gt;ajaxSubmitPhotos(Array)
#14 C:\Program Files\VertrigoServ\www\oxwall\ow_core\request_handler.php(250): ReflectionMethod-&gt;invokeArgs(Object(PHOTO_CTRL_AjaxUpload), Array)
#15 C:\Program Files\VertrigoServ\www\oxwall\ow_core\request_handler.php(226): OW_RequestHandler-&gt;processControllerAction(Object(ReflectionMethod), Object(PHOTO_CTRL_AjaxUpload))
#16 C:\Program Files\VertrigoServ\www\oxwall\ow_core\application.php(346): OW_RequestHandler-&gt;dispatch()
#17 C:\Program Files\VertrigoServ\www\oxwall\index.php(76): OW_Application-&gt;handleRequest()
#18 {main}
Type:   Google\Cloud\Exception\ServiceException
</code></pre>",,2,0,,2017-02-23 04:05:17.580 UTC,,2017-03-14 20:31:48.537 UTC,2017-02-23 07:19:02.930 UTC,,2075839,,2075839,1,0,php|google-cloud-platform|google-cloud-vision,1195
How I can send image to GCP Vision API,46966332,How I can send image to GCP Vision API,"<p>Im going to use Google Cloud Vision API.</p>

<p>In tutorial it says that I need to send the image to their Google Cloud Storage and after that by using that link I need to make a request to API.
So the scheme looks like this:</p>

<p>Phone photo(Local Storage) <strong><em>--download--> GC Storage --get link--></em></strong> Send request with this link to GC Vision API --get JSON--> work with JSON</p>

<p>So the question is.
What for I need to storage image in cloud? Only for a link? Can I send the image direct to the Vision API without GC Storage?
So the scheme:</p>

<p><strong>Phone photo(Local Storage) --download-->to GC Vision API --get JSON--> work with JSON</strong></p>",,1,1,,2017-10-27 01:50:35.537 UTC,,2018-05-06 08:13:14.177 UTC,,,,,8442666,1,1,java|android|json|api|google-cloud-platform,337
php with python google vision api giving error when loading in browser,41440605,php with python google vision api giving error when loading in browser,"<p>I have a problem when running the php script.</p>

<p>I am detecting faces through google vision api. </p>

<p>when running it in the terminal: </p>

<pre><code>php name.php
</code></pre>

<p>I am seeing the output without having any problem.
But when I am running it from the web browser, I am having this error:</p>

<pre><code>Traceback (most recent call last):
  File ""test.py"", line 139, in &lt;module&gt;
    main(args.input_image, args.output, args.max_results)
  File ""test.py"", line 115, in main
    faces = detect_face(image, max_results)
  File ""test.py"", line 84, in detect_face
    response = request.execute()
  File ""/usr/local/lib/python2.7/dist-packages/oauth2client/_helpers.py"", line 133, in positional_wrapper
    return wrapped(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/googleapiclient/http.py"", line 838, in execute
    raise HttpError(resp, content, uri=self.uri)
googleapiclient.errors.HttpError: &lt;HttpError 403 when requesting https://vision.googleapis.com/v1/images:annotate?alt=json returned ""Request had insufficient authentication scopes.""&gt;
</code></pre>

<p>I tried to write in the terminal:</p>

<pre><code>export GOOGLE_APPLICATION_CREDENTIALS= file.json
</code></pre>

<p>and then reopen the webpage in the browser, I still have the same error.</p>

<p>Any idea?</p>",,0,7,,2017-01-03 09:48:21.047 UTC,,2017-01-04 17:10:12.643 UTC,2017-01-04 17:10:12.643 UTC,,5231007,,7316703,1,0,php|python|google-api|google-cloud-platform|google-cloud-vision,134
Multiple Validation Errors - AWS Rekognition,52825897,Multiple Validation Errors - AWS Rekognition,"<p>The code below is currently running in Lambda - NodeJS 6.10 with all the correct modules imported. </p>

<p>The expected output is a JobId </p>

<p>However I keep getting validation errors despite this being the syntax provided by AWS. If I remove both items causing the errors it runs successfully, however when calling subsequent Rekognition APIs on the returned JobId, it fails as the collection is never specified. </p>

<pre><code>var params = {
  CollectionId: 'myCollectionName', /* required */
  Video: { /* required */
    S3Object: {
      Bucket: 'myBucketName',
      Name: 'myVideoInS3'
    }
  },
  FaceMatchThreshold: 70.0,
  JobTag: 'myTag',
  NotificationChannel: {
    RoleArn: 'myNotificationRole', /* required */
    SNSTopicArn: 'myTopicARN' /* required */
  }
};

rekognition.startFaceSearch(params, function(err, data) {
  if (err) console.log(err, err.stack); // an error occurred
  else     console.log(data);           // successful response
});
</code></pre>

<p>The Error</p>

<pre><code>MultipleValidationErrors: There were 2 validation errors:
* UnexpectedParameter: Unexpected key 'CollectionId' found in params
* UnexpectedParameter: Unexpected key 'FaceMatchThreshold' found in params
at ParamValidator.validate (/var/task/node_modules/aws-sdk/lib/param_validator.js:40:28)
</code></pre>

<p>When I replicate this in Python it runs seamlessly so the error is not with the collection. Anybody got any ideas?</p>",,0,0,,2018-10-15 23:11:17.227 UTC,,2018-10-15 23:11:17.227 UTC,,,,,9147327,1,2,node.js|aws-lambda|amazon-rekognition,40
Atmostphere and From Menu Photos Tabs In Google Maps,50553795,Atmostphere and From Menu Photos Tabs In Google Maps,"<p>I was using Google Maps on my iPhone today and noticed that if you browse the photos there are two tabs at the top called ""FROM MENU"" and ""ATMOSPHERE"".  These don't seem to appear on the the desktop version or iOS Google Maps app but only for me on the iOS Chrome browser.</p>

<p>Is there a way to access these lists of photos? I don't see anything in the Places API.  The only way I could replicate this is by using Google Cloud Vision and parsing the tags of the images but it's an expensive service to subscribe to.</p>",,0,2,,2018-05-27 15:40:25.880 UTC,,2018-09-06 08:15:15.010 UTC,2018-09-06 08:15:15.010 UTC,,9036351,,2845311,1,0,google-places-api|google-places,28
Is there any way I can get a list of all possible responses from the google vision api?,37515812,Is there any way I can get a list of all possible responses from the google vision api?,<p>I am using the google cloud vision api to analyze pictures. Is there a list of all the possible responses for the labelAnnotations method? </p>,,2,0,,2016-05-29 23:09:53.290 UTC,,2017-07-04 21:31:10.240 UTC,,,,,5696891,1,3,google-cloud-vision,118
Bot hosted in Azure with Google Vision Api - gRPC issue,42914619,Bot hosted in Azure with Google Vision Api - gRPC issue,"<p>Trying to use Google Vision Api from Bot Framework app hosted on Azure. The code works just fine on local but I get this error when I try it on Azure. Can someone help?</p>

<pre><code>Exception while executing function: Functions.messages. mscorlib: Error: A dynamic link library (DLL) initialization routine failed.
\\?\D:\home\site\wwwroot\messages\node_modules\grpc\src\node\extension_binary\grpc_node.node
    at Error (native)
    at Object.Module._extensions..node (module.js:583:18)
    at Module.load (module.js:473:32)
    at tryModuleLoad (module.js:432:12)
    at Function.Module._load (module.js:424:3)
    at Module.require (module.js:483:17)
    at require (internal/module.js:20:19)
    at Object.&lt;anonymous&gt; (D:\home\site\wwwroot\messages\node_modules\grpc\src\node\src\grpc_extension.js:38:15)
    at Module._compile (module.js:556:32)
    at Object.Module._extensions..js (module.js:565:10).
</code></pre>

<p>Here is the package.json that I am using:</p>

<pre><code>  {
  ""name"": ""luisbot"",
  ""version"": ""1.0.0"",
  ""description"": """",
  ""main"": ""index.js"",
  ""dependencies"": {
    ""@google-cloud/vision"": ""^0.10.0"",
    ""botbuilder"": ""^3.7.0"",
    ""botbuilder-azure"": ""3.0.2"",
    ""botbuilder-location"": ""^1.0.4""
  },
  ""devDependencies"": {
    ""restify"": ""^4.3.0""
  },
  ""scripts"": {
    ""test"": ""echo \""Error: no test specified\"" &amp;&amp; exit 1""
  },
  ""author"": """",
  ""license"": ""ISC""
}
</code></pre>

<p>and the error throws while loading the vision api module -  at the line mention below</p>

<pre><code>var vision = require('@google-cloud/vision')
</code></pre>",43767464,1,3,,2017-03-20 21:52:03.180 UTC,,2017-05-08 20:49:51.063 UTC,2017-03-21 13:28:52.843 UTC,,90081,,5383380,1,1,node.js|azure|botframework|grpc|vision-api,275
cv2.imread alters the base64 string from images and generates different results between OSes,51257124,cv2.imread alters the base64 string from images and generates different results between OSes,"<p>I am using OpenCV in Python on MacOS and Linux Ubuntu systems. My OpenCV version is 3.4.1.15.</p>

<p>I have tried three different methods of generating base64 strings for images, on two OS systems respectively:</p>

<ol>
<li><p>Using plain Python:</p>

<pre><code>from base64 import b64encode
with open(""image_file.jpg"", ""rb"") as file:
    base64string = b64encode(file.read()).decode(""UTF-8"")
</code></pre></li>
<li><p>Using <code>cv2.imread</code>:</p>

<pre><code>import cv2
from base64 import b64encode
cvbase64string = b64encode(cv2.imencode('.jpg', cv2.imread(image_file.jpg))[1]).decode(""UTF-8"")
</code></pre></li>
<li><p>Using plain Python after using <code>cv2.imwrite</code> for what <code>cv2.imread</code> gets:</p>

<pre><code>import cv2
from base64 import b64encode
cv2.imwrite(""ioimage_file.jpg"", cv2.imread(""image_file.jpg""))
with open(""ioimage_file.jpg"", ""rb"") as file:
    iobase64string = b64encode(file.read()).decode(""UTF-8"")
</code></pre></li>
</ol>

<p>With my human eyes, I can't distinguish ""ioimage_file.jpg"" from ""image_file.jpg"". However, the base64 strings change.</p>

<ol>
<li>On the same OS (either MacOS or Linux), <code>base64string</code> != <code>cvbase64string</code> == <code>iobase64string</code>.</li>
<li>Across OSes, <code>MAC_base64string</code> == <code>LINUX_base64string</code>, but <code>MAC_cvbase64string</code> != <code>LINUX_cvbase64string</code> and <code>MAC_iobase64string</code> != <code>LINUX_iobase64string</code>.</li>
</ol>

<p>Why is that? Is there any way to solve it? Or is it a bug needed reporting?</p>

<p>This troubles me because I am developing OCR algorithm on different platforms, but different base64 strings yield different recognized characters from Google Vision API, which means I can't even reproduce my OCR results from one same image.</p>",,0,4,,2018-07-10 04:08:18.943 UTC,,2018-07-10 04:08:18.943 UTC,,,,,6666231,1,1,python|opencv|base64|cross-platform,182
How to compare faces in a Collection to faces in a Stored Video using AWS Rekognition?,54961012,How to compare faces in a Collection to faces in a Stored Video using AWS Rekognition?,"<p>I would appreciate some guidance on the following issue.</p>

<p><strong>Use Case:</strong> </p>

<ul>
<li>Create a collection with known faces.  </li>
<li>Search a stored video to
identify ""known"" faces &amp; draw a bounding box around them in the video
frame</li>
</ul>

<p><strong>Steps taken:</strong></p>

<ul>
<li>I'm able to create a collection and index faces I'm able to analyse
the stored video and get the results of PersonMatch and FaceMatch
using getFaceSearch() </li>
<li>I'm able to draw the bounding boxes around Persons found in the video, etc., however...</li>
</ul>

<p><strong>Issue:</strong></p>

<ul>
<li>The response of getFaceSearch() returns an array of FaceMatches. </li>
<li>However, when I access the FaceMatch the coordinates are of the face found in the source image that was indexed in the collection, not of the face matched in the video. </li>
<li>I've looked through the API documentation and have not been able to find any information on how to get the coordinates of a matched face in the video so I can draw a bounding box on the video frame. 
Here is the API document that I'm referring to. <a href=""https://docs.aws.amazon.com/rekognition/latest/dg/API_Face.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/rekognition/latest/dg/API_Face.html</a></li>
</ul>

<p>Thanks for your help on this issue!</p>",,0,0,,2019-03-02 17:16:35.187 UTC,,2019-03-02 17:16:35.187 UTC,,,,,11140957,1,0,amazon-rekognition,24
Google Sign-In IOS Swift 4 AppDelegate Error,50995907,Google Sign-In IOS Swift 4 AppDelegate Error,"<p>I am trying to implement Google sign-in along with the working Linkedin and Facebook sign-in's.</p>

<p>I tried following the documentation for integrating this. I installed the pod with cocoapods, edited my info.plist and now I am working on adding the AppDelegate methods. </p>

<p>The following snippet of code is giving me this error: </p>

<blockquote>
  <p>Cannot subscript a value of type '[String : AnyObject]' with an index of type 'UIApplicationOpenURLOptionsKey'</p>
</blockquote>

<p>Code:</p>

<pre><code>func application(application: UIApplication,
                 openURL url: NSURL, options: [String: AnyObject]) -&gt; Bool {

    return GIDSignIn.sharedInstance().handleURL(url as URL!,
                                                sourceApplication: options[UIApplicationOpenURLOptionsSourceApplicationKey] as? String,
                                                annotation: options[UIApplicationOpenURLOptionsAnnotationKey])
}
</code></pre>

<p>For one, the compiler doesn't seem to recognize what 'UIApplicationOpenURLOptionsSourceApplicationKey' is, this could be a hint to the solution.</p>

<p>The documentation shows exactly this so I pretty stuck on this one. Not sure how to make this delegate method work.</p>

<p><a href=""https://developers.google.com/identity/sign-in/ios/sign-in?ver=swift"" rel=""nofollow noreferrer"">https://developers.google.com/identity/sign-in/ios/sign-in?ver=swift</a></p>

<p>Here is my complete AppDelegate code:</p>

<pre><code>import UIKit
import CoreData
import AWSMobileClient
import AWSRekognition
import AWSDynamoDB
import FacebookCore
import FBSDKCoreKit
import FBSDKLoginKit
import AWSFacebookSignIn
import AWSGoogleSignIn
import GoogleSignIn


@UIApplicationMain
class AppDelegate: UIResponder, UIApplicationDelegate, GIDSignInDelegate {

    var window: UIWindow?


    func sign(_ signIn: GIDSignIn!, didSignInFor user: GIDGoogleUser!,
              withError error: Error!) {
        if let error = error {
            print(""\(error.localizedDescription)"")
        } else {
            print(user)
            // Perform any operations on signed in user here.
            let userId = user.userID                  // For client-side use only!
            let idToken = user.authentication.idToken // Safe to send to the server
            let fullName = user.profile.name
            let givenName = user.profile.givenName
            let familyName = user.profile.familyName
            let email = user.profile.email
            // ...
        }
    }

    func sign(_ signIn: GIDSignIn!, didDisconnectWith user: GIDGoogleUser!,
              withError error: Error!) {
        // Perform any operations when the user disconnects from app here.
        // ...
    }

    //LinkedIn oAuth 2
    func application(_ app: UIApplication, open url: URL, options: [UIApplicationOpenURLOptionsKey : Any] = [:]) -&gt; Bool {
        if LISDKCallbackHandler.shouldHandle(url){

            return LISDKCallbackHandler.application(app, open: url, sourceApplication: options[UIApplicationOpenURLOptionsKey.sourceApplication] as! String, annotation: nil)
        }
        return true
    }


    //google sign in
    func application(_ application: UIApplication,
                     didFinishLaunchingWithOptions launchOptions: [UIApplicationLaunchOptionsKey: Any]?) -&gt; Bool {
        // Initialize sign-in
        GIDSignIn.sharedInstance().clientID = ""REMOVED FOR THIS POST""
        GIDSignIn.sharedInstance().delegate = self

        return true
    }

    func application(application: UIApplication,
                     openURL url: NSURL, options: [String: AnyObject]) -&gt; Bool {

        return GIDSignIn.sharedInstance().handleURL(url as URL!,
                                                    sourceApplication: options[UIApplicationOpenURLOptionsSourceApplicationKey] as? String,
                                                    annotation: options[UIApplicationOpenURLOptionsAnnotationKey])
    }




    // Add a AWSMobileClient call in application:open url
    func application(_ application: UIApplication, open url: URL,
                     sourceApplication: String?, annotation: Any) -&gt; Bool {

        return AWSMobileClient.sharedInstance().interceptApplication(
            application, open: url,
            sourceApplication: sourceApplication,
            annotation: annotation)

    }

    func applicationWillResignActive(_ application: UIApplication) {
        // Sent when the application is about to move from active to inactive state. This can occur for certain types of temporary interruptions (such as an incoming phone call or SMS message) or when the user quits the application and it begins the transition to the background state.
        // Use this method to pause ongoing tasks, disable timers, and invalidate graphics rendering callbacks. Games should use this method to pause the game.
    }

    func applicationDidEnterBackground(_ application: UIApplication) {
        // Use this method to release shared resources, save user data, invalidate timers, and store enough application state information to restore your application to its current state in case it is terminated later.
        // If your application supports background execution, this method is called instead of applicationWillTerminate: when the user quits.
    }

    func applicationWillEnterForeground(_ application: UIApplication) {
        // Called as part of the transition from the background to the active state; here you can undo many of the changes made on entering the background.
    }

    func applicationDidBecomeActive(_ application: UIApplication) {
        // Restart any tasks that were paused (or not yet started) while the application was inactive. If the application was previously in the background, optionally refresh the user interface.
    }

    func applicationWillTerminate(_ application: UIApplication) {
        // Called when the application is about to terminate. Save data if appropriate. See also applicationDidEnterBackground:.
        // Saves changes in the application's managed object context before the application terminates.
        self.saveContext()
    }



    // MARK: - Core Data stack

    lazy var persistentContainer: NSPersistentContainer = {
        /*
         The persistent container for the application. This implementation
         creates and returns a container, having loaded the store for the
         application to it. This property is optional since there are legitimate
         error conditions that could cause the creation of the store to fail.
        */
        let container = NSPersistentContainer(name: ""Checkara"")
        container.loadPersistentStores(completionHandler: { (storeDescription, error) in
            if let error = error as NSError? {
                // Replace this implementation with code to handle the error appropriately.
                // fatalError() causes the application to generate a crash log and terminate. You should not use this function in a shipping application, although it may be useful during development.

                /*
                 Typical reasons for an error here include:
                 * The parent directory does not exist, cannot be created, or disallows writing.
                 * The persistent store is not accessible, due to permissions or data protection when the device is locked.
                 * The device is out of space.
                 * The store could not be migrated to the current model version.
                 Check the error message to determine what the actual problem was.
                 */
                fatalError(""Unresolved error \(error), \(error.userInfo)"")
            }
        })
        return container
    }()



    func saveContext () {
        let context = persistentContainer.viewContext
        if context.hasChanges {
            do {
                try context.save()
            } catch {
                // Replace this implementation with code to handle the error appropriately.
                // fatalError() causes the application to generate a crash log and terminate. You should not use this function in a shipping application, although it may be useful during development.
                let nserror = error as NSError
                fatalError(""Unresolved error \(nserror), \(nserror.userInfo)"")
            }
        }
    }

}
</code></pre>",50996085,1,0,,2018-06-22 21:39:18.003 UTC,,2018-06-22 22:07:34.103 UTC,,,,,6575837,1,0,ios|swift4|google-signin|googlesigninapi,463
dominant colors and aspect ratios detection in r with google vision,54406629,dominant colors and aspect ratios detection in r with google vision,"<p>Using google vision in R with (RoogleVision package), I am able to do ""Label_Detection"" , ""Text_Detection"" , ""LOGO_Detection"", ""LABEL_Detection"" all of them but unable to get the ""dominant color"" feature from Google vision API. Is there anyway that I can do that in R ?</p>

<p>Expample.. I am doing text detection for one creative </p>

<p>So my code is <code>getGoogleVisionResponse(file.choose(), feature = ""TEXT_DETECTION"")</code> this only gives me text on that creative. How do I get the color that are used on the creative below. I also want the dominant color detection feature which google vision has under there 'properties' tab.</p>

<p><a href=""https://i.stack.imgur.com/fjOeV.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fjOeV.jpg"" alt=""enter image description here""></a></p>",,0,1,,2019-01-28 16:46:12.453 UTC,,2019-01-28 21:00:52.720 UTC,2019-01-28 21:00:52.720 UTC,,9855745,,8233109,1,0,r|google-vision|color-detection,17
Error when trying to read AWS SNS message,53123056,Error when trying to read AWS SNS message,"<p>I need to return the message sent by Rekognition to SNS but I get this error in  CloudWatch:</p>

<blockquote>
  <p>'Records': KeyError Traceback (most recent call last): File
  ""/var/task/AnalyzeVideo/lambda_function.py"", line 34, in
  lambda_handler message = event[""Records""][0][""Sns""][""Message""]
  KeyError: 'Records'</p>
</blockquote>

<p>Code:</p>

<pre><code>def detect_labels(bucket, key):
    response = rekognition.start_label_detection(
        Video = {
            ""S3Object"": {
                ""Bucket"": BUCKET,
                ""Name"": KEY
            }
        },
        NotificationChannel = {
            ""SNSTopicArn"": TOPIC_ARN,
            ""RoleArn"": ROLE_ARN
        }
    )

    return response

def lambda_handler(event, context):
    reko_response = detect_labels(BUCKET, KEY)
    message = event[""Records""][0][""Sns""][""Message""]
    return message
</code></pre>

<p>And is this the correct way of implementing Rekognition stored video in AWS Lambda with python I didn't find any examples on it.</p>

<p>Update:</p>

<p>The steps my app needs to take are:</p>

<ol>
<li>In the frontend, the user triggers a lambda function with API gateway which 
sends a file to s3 </li>
<li>When the file arrives trigger the same lambda function to apply video 
recognition and send jobId to SNS </li>
<li>when SNS receives a message trigger the same lambda function to get the 
label data and return the data back to the user with API gateway</li>
</ol>",53127271,1,0,,2018-11-02 17:10:27.663 UTC,1,2018-11-03 10:13:52.970 UTC,2018-11-03 10:13:52.970 UTC,,5008598,,5008598,1,1,python|amazon-web-services|amazon-sns|amazon-rekognition,168
Read a symbol from an Image using Azure Services,56010701,Read a symbol from an Image using Azure Services,"<p>I need to read a mark like cylinder icon, tick within the image.</p>

<p>Currently I am using Azure Computer Vision to read an image which has handwritten text  on User Form. This form has a table where the user needs to enter values in each table cell -  all cells in the table are not mandatory. In each cell we have decided to include a symbol like cylinder,sphere ... to identify the cell using Azure Computer Service (while extracting data from the image -handwritten text on Form).</p>

<ol>
<li>What are the ways I can identify the cell and its value?</li>
<li>Will including a symbol like cylinder,tick,sphere ... help?</li>
<li>If so how can I do it?</li>
</ol>

<p>Please help</p>

<p>I am using Azure Computer Vision and it doesn't recognize the empty cell values of the table.</p>

<p>I am using <a href=""https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/quickstarts-sdk/csharp-hand-text-sdk"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/quickstarts-sdk/csharp-hand-text-sdk</a></p>

<p>I need to identify the cell data from each cell of the table in the image.</p>",,0,0,,2019-05-06 18:40:49.547 UTC,,2019-05-08 11:16:17.547 UTC,2019-05-08 11:16:17.547 UTC,,11457801,,11457801,1,0,c#|azure,24
Which AWS gems are really necessary for Paperclip?,47639286,Which AWS gems are really necessary for Paperclip?,"<p>Paperclip suggests using the aws-sdk gem for integrating with AWS S3, but the gem has a ridiculous amount of dependencies! <strong>Is there a better way to configure AWS gems for Paperclip?</strong></p>

<p>These are all the gems installed when you use aws-sdk:</p>

<pre><code>aws-partitions (1.44.0)
aws-sdk (3.0.1)
aws-sdk-acm (1.2.0)
aws-sdk-alexaforbusiness (1.0.0)
aws-sdk-apigateway (1.7.0)
aws-sdk-applicationautoscaling (1.6.0)
aws-sdk-applicationdiscoveryservice (1.0.0)
aws-sdk-appstream (1.2.0)
aws-sdk-appsync (1.0.0)
aws-sdk-athena (1.0.0)
aws-sdk-autoscaling (1.4.0)
aws-sdk-batch (1.3.0)
aws-sdk-budgets (1.2.0)
aws-sdk-cloud9 (1.0.0)
aws-sdk-clouddirectory (1.0.0)
aws-sdk-cloudformation (1.3.0)
aws-sdk-cloudfront (1.1.0)
aws-sdk-cloudhsm (1.3.0)
aws-sdk-cloudhsmv2 (1.1.0)
aws-sdk-cloudsearch (1.0.0)
aws-sdk-cloudsearchdomain (1.0.0)
aws-sdk-cloudtrail (1.0.0)
aws-sdk-cloudwatch (1.2.0)
aws-sdk-cloudwatchevents (1.1.0)
aws-sdk-cloudwatchlogs (1.2.0)
aws-sdk-codebuild (1.4.0)
aws-sdk-codecommit (1.2.0)
aws-sdk-codedeploy (1.1.0)
aws-sdk-codepipeline (1.1.0)
aws-sdk-codestar (1.1.0)
aws-sdk-cognitoidentity (1.0.0)
aws-sdk-cognitoidentityprovider (1.1.0)
aws-sdk-cognitosync (1.0.0)
aws-sdk-comprehend (1.0.0)
aws-sdk-configservice (1.4.0)
aws-sdk-core (3.11.0)
aws-sdk-costandusagereportservice (1.0.0)
aws-sdk-costexplorer (1.0.0)
aws-sdk-databasemigrationservice (1.3.0)
aws-sdk-datapipeline (1.0.0)
aws-sdk-dax (1.0.0)
aws-sdk-devicefarm (1.2.0)
aws-sdk-directconnect (1.1.0)
aws-sdk-directoryservice (1.0.0)
aws-sdk-dynamodb (1.3.0)
aws-sdk-dynamodbstreams (1.0.0)
aws-sdk-ec2 (1.21.0)
aws-sdk-ecr (1.2.0)
aws-sdk-ecs (1.5.0)
aws-sdk-efs (1.0.0)
aws-sdk-elasticache (1.3.0)
aws-sdk-elasticbeanstalk (1.2.0)
aws-sdk-elasticloadbalancing (1.1.0)
aws-sdk-elasticloadbalancingv2 (1.6.0)
aws-sdk-elasticsearchservice (1.2.0)
aws-sdk-elastictranscoder (1.0.0)
aws-sdk-emr (1.1.0)
aws-sdk-firehose (1.1.0)
aws-sdk-gamelift (1.1.0)
aws-sdk-glacier (1.5.0)
aws-sdk-glue (1.2.0)
aws-sdk-greengrass (1.2.0)
aws-sdk-guardduty (1.0.0)
aws-sdk-health (1.0.0)
aws-sdk-iam (1.3.0)
aws-sdk-importexport (1.0.0)
aws-sdk-inspector (1.1.0)
aws-sdk-iot (1.1.0)
aws-sdk-iotdataplane (1.0.0)
aws-sdk-iotjobsdataplane (1.0.0)
aws-sdk-kinesis (1.1.0)
aws-sdk-kinesisanalytics (1.1.0)
aws-sdk-kinesisvideo (1.0.0)
aws-sdk-kinesisvideoarchivedmedia (1.0.0)
aws-sdk-kinesisvideomedia (1.0.0)
aws-sdk-kms (1.3.0)
aws-sdk-lambda (1.2.0)
aws-sdk-lambdapreview (1.0.0)
aws-sdk-lex (1.2.0)
aws-sdk-lexmodelbuildingservice (1.2.0)
aws-sdk-lightsail (1.3.0)
aws-sdk-machinelearning (1.0.0)
aws-sdk-marketplacecommerceanalytics (1.0.0)
aws-sdk-marketplaceentitlementservice (1.0.0)
aws-sdk-marketplacemetering (1.0.0)
aws-sdk-mediaconvert (1.0.0)
aws-sdk-medialive (1.0.0)
aws-sdk-mediapackage (1.0.0)
aws-sdk-mediastore (1.0.0)
aws-sdk-mediastoredata (1.0.0)
aws-sdk-migrationhub (1.0.0)
aws-sdk-mobile (1.0.0)
aws-sdk-mq (1.0.0)
aws-sdk-mturk (1.1.0)
aws-sdk-opsworks (1.1.0)
aws-sdk-opsworkscm (1.2.0)
aws-sdk-organizations (1.7.0)
aws-sdk-pinpoint (1.2.0)
aws-sdk-polly (1.4.0)
aws-sdk-pricing (1.0.0)
aws-sdk-rds (1.8.0)
aws-sdk-redshift (1.1.0)
aws-sdk-rekognition (1.2.0)
aws-sdk-resourcegroups (1.0.0)
aws-sdk-resourcegroupstaggingapi (1.0.0)
aws-sdk-resources (3.8.0)
aws-sdk-route53 (1.5.0)
aws-sdk-route53domains (1.1.0)
aws-sdk-s3 (1.8.0)
aws-sdk-sagemaker (1.1.0)
aws-sdk-sagemakerruntime (1.0.0)
aws-sdk-serverlessapplicationrepository (1.0.0)
aws-sdk-servicecatalog (1.1.0)
aws-sdk-ses (1.4.0)
aws-sdk-shield (1.1.0)
aws-sdk-simpledb (1.0.0)
aws-sdk-sms (1.0.0)
aws-sdk-snowball (1.1.0)
aws-sdk-sns (1.1.0)
aws-sdk-sqs (1.3.0)
aws-sdk-ssm (1.5.0)
aws-sdk-states (1.2.0)
aws-sdk-storagegateway (1.2.0)
aws-sdk-support (1.0.0)
aws-sdk-swf (1.0.0)
aws-sdk-translate (1.0.0)
aws-sdk-waf (1.3.0)
aws-sdk-wafregional (1.3.0)
aws-sdk-workdocs (1.1.0)
aws-sdk-workspaces (1.0.0)
aws-sdk-xray (1.1.0)
aws-sigv2 (1.0.1)
aws-sigv4 (1.0.2)
</code></pre>",,0,0,,2017-12-04 17:59:53.007 UTC,,2017-12-04 18:16:27.190 UTC,,,,,431874,1,2,ruby-on-rails|amazon-s3|paperclip,379
Using cloud-vision API in Android Studio. The correct way?,53530035,Using cloud-vision API in Android Studio. The correct way?,"<p>I made a project in order to get all features of an image. It is: Labels and face features (This is my goal).</p>

<p>In the beginning, I use the api: ""com.google.apis:google-api-services-vision:v1-rev404-1.25.0"" with success when it detect labels; but it didnt work with faces recognition (Using getFaceAnnotations() function).</p>

<p>Then</p>

<p>I tried with the api: 'com.google.cloud:google-cloud-vision:1.53.0' (Because it has the funcion getFaceAnnotationsList) but it is impossible to me to  create the credentials correctly:</p>

<p>My code is:</p>

<pre><code> List&lt;AnnotateImageRequest&gt; requests = new ArrayList&lt;&gt;();

                    ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();
                    bitmap.compress(Bitmap.CompressFormat.JPEG, 90, byteArrayOutputStream);
                    byte[] imageBytes = byteArrayOutputStream.toByteArray();
                    ByteString imgBytes = ByteString.copyFrom(imageBytes);


                    Image img = Image.newBuilder().setContent(imgBytes).build();
                    Feature LabelFeat = Feature.newBuilder().setType(Type.LABEL_DETECTION).build();
                    Feature LabelFace = Feature.newBuilder().setType(Type.FACE_DETECTION).build();


                    AnnotateImageRequest requestLabels = AnnotateImageRequest.newBuilder().addFeatures(LabelFeat).setImage(img).build();
                    AnnotateImageRequest requestFaces = AnnotateImageRequest.newBuilder().addFeatures(LabelFace).setImage(img).build();

                    requests.add(requestLabels);
                    requests.add(requestFaces);


                     FileInputStream input = new FileInputStream(""/sdcard/E-Tourism-5c6f2693a1e9.json"");

                     Credentials myCredentials = ServiceAccountCredentials.fromStream(input);


                     ImageAnnotatorSettings imageAnnotatorSettings =
                             ImageAnnotatorSettings.newBuilder().setCredentialsProvider(FixedCredentialsProvider.create(myCredentials)).build();


                    try ( ImageAnnotatorClient client = ImageAnnotatorClient.create(imageAnnotatorSettings)) {
                     //   ImageAnnotatorClient client = ImageAnnotatorClient.create();

                        BatchAnnotateImagesResponse response = client.batchAnnotateImages(requests);
                        List&lt;AnnotateImageResponse&gt; responses = response.getResponsesList();
                        nombreAnalysesLancees++;
</code></pre>

<p>...It returns an exception in: ImageAnnotatorClient client = ImageAnnotatorClient.create(imageAnnotatorSettings.</p>

<p>I need help please. Which library should I use and if its the second one. What should I do?</p>

<p>Thank you</p>",,0,0,,2018-11-29 00:13:47.887 UTC,,2018-11-29 00:13:47.887 UTC,,,,,10691858,1,0,image|vision,70
How do I invoke AWS Rekognition from a Lambda within a VPC,46676980,How do I invoke AWS Rekognition from a Lambda within a VPC,"<p>I am working on a lambda function that needs to access <code>RDS</code>, <code>S3</code> and <code>Rekognition</code> services from AWS.</p>

<p>I gave <code>S3</code> and <code>Rekognition</code> permissions via the <code>AmazonS3FullAccess</code> and the <code>AmazonRekognitionFullAccess</code> policies respectively and it worked fine</p>

<p>The thing is that I could not access my <code>Aurora</code> instance inside <code>RDS</code> because it's inside a VPC</p>

<p>I changed my lambda network configurations so it would be able to access the VPC, and the <code>Aurora</code> connection worked as expected, but then the connection to <code>Rekognition</code> stopped working, whenever I invoke <code>detectLabels</code> for example it just hangs.</p>

<p>Am I missing some permission?</p>",46677340,1,4,,2017-10-10 22:27:55.863 UTC,,2017-10-10 23:10:02.630 UTC,,,,,1316000,1,3,amazon-web-services|aws-lambda|amazon-vpc|amazon-rekognition,403
Google vision API rest integrartion,41972937,Google vision API rest integrartion,"<p>I am using trail version of google vision API ,using the rest API i am trying to get the face_detection values from postman tool but i am facing an issue showed below.can anyone help me on this.</p>

<pre><code>""message"": ""Requests from referer chrome-extension://fhbjgbiflinjbdggehcddcbncdddomop are blocked."",
    ""status"": ""PERMISSION_DENIED"",
</code></pre>",,1,0,,2017-02-01 06:02:55.727 UTC,,2017-02-01 09:08:31.597 UTC,,,,,4501753,1,0,vision|google-cloud-vision,353
LibGDX: How to add Google Mobile Vision API to a LibGDX project?,38534147,LibGDX: How to add Google Mobile Vision API to a LibGDX project?,"<p>Maybe the answer is simple, however I could not find anything that could help me yet.
Basicaly, I want to add the Google Vision API to my project. I tried this by putting</p>

<pre><code>compile 'com.google.android.gms:play-services-vision:9.0.0+'
</code></pre>

<p>in <code>build.gradle</code> <code>dependencies</code> in the Android module, like in this <a href=""https://codelabs.developers.google.com/codelabs/mobile-vision-ocr/#3"" rel=""nofollow"">tutorial</a>. This did not work (maybe I should write it somewhere else? I can't figure it out). Now there are many inspections shown in this <code>build.gradle</code>. There is said that there components cannot be applied to <code>groovy.lang.Closure</code>.</p>

<p>I have installed the Google Repository. And I've completed that tutorial, which is not LibGDX, and everything works fine there.</p>

<p>So how to make it work with LibGDX?</p>",38546528,1,0,,2016-07-22 19:40:04.827 UTC,,2016-07-23 21:21:11.043 UTC,2016-07-23 10:14:02.810 UTC,,6019601,,6019601,1,1,gradle|libgdx|build.gradle|google-vision,111
Amazon Rekognition from India,42525482,Amazon Rekognition from India,"<p>I want to use Amazon Rekognition for some Detection project from India, each and every time I use to connect, either check for Region error pops up or S3 metadata issue?</p>

<p>botocore.errorfactory.InvalidS3ObjectException: An error occurred (InvalidS3ObjectException) when calling the DetectLabels operation: Unable to get image metadata from S3.  Check object key, region and/or access permissions.</p>

<p>thanks in advance.</p>",,1,2,,2017-03-01 07:17:40.313 UTC,,2017-03-01 11:29:03.110 UTC,2017-03-01 11:29:03.110 UTC,,6250659,,6250659,1,-5,amazon-web-services|amazon-rekognition,433
Reading from Vision API Text Detection and Filling Appropriate fields,37741299,Reading from Vision API Text Detection and Filling Appropriate fields,"<p>Have been trying to read data out of an Govt. Issued identity card and fill the fields of the form like following using google's Vision Api..</p>

<p><a href=""https://i.stack.imgur.com/x4YHT.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x4YHT.jpg"" alt=""Government ID""></a></p>

<p>I've successfully read the data from the vision API but now facing problems filling the form like following with appropriate data..</p>

<p><a href=""https://i.stack.imgur.com/bdRyq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bdRyq.png"" alt=""Form""></a> </p>

<p>How can i achieve this?</p>

<p><a href=""https://i.stack.imgur.com/RLTuQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RLTuQ.png"" alt=""What i Wish to get""></a></p>

<p>The response from Vision API:</p>

<pre><code>{
""responses"": [
    {
        ""textAnnotations"": [
            {
                ""locale"": ""en"",
                ""description"": ""amagas faATST\nINCOME TAX DEPARTMENT\nMAHENDRAKUMARRBAGUL\nRAMKRISHNA NATTHU BAGUL\n01/06/1981\n4Permanent Account Number\nANSAB4834E\nSignature\nGOVT OF INDIA\n"",
                ""boundingPoly"": {
                    ""vertices"": [
                        {
                            ""x"": 2,
                            ""y"": 64
                        },
                        {
                            ""x"": 4308,
                            ""y"": 64
                        },
                        {
                            ""x"": 4308,
                            ""y"": 2701
                        },
                        {
                            ""x"": 2,
                            ""y"": 2701
                        }
                    ]
                }
            },
            {
                ""description"": ""amagas"",
                ""boundingPoly"": {
                    ""vertices"": [
                        {
                            ""x"": 6,
                            ""y"": 64
                        },
                        {
                            ""x"": 774,
                            ""y"": 65
                        },
                        {
                            ""x"": 774,
                            ""y"": 374
                        },
                        {
                            ""x"": 6,
                            ""y"": 373
                        }
                    ]
                }
            },
</code></pre>

<p>Kindly Help</p>",37745316,2,0,,2016-06-10 06:27:32.430 UTC,1,2018-11-01 11:02:34.013 UTC,2016-06-10 07:37:21.677 UTC,,2232741,,2232741,1,1,javascript|html5|ocr|vision,935
Calling Google cloud Vision API's on numpy matrices,50415121,Calling Google cloud Vision API's on numpy matrices,"<p>I'm using the Google Text detection API for performing OCR on images. </p>

<p>I've found that my OCR results are much better when I perform some pre-processing on the images using opencv. </p>

<p>My question is - how can I call the Google cloud Vision API's on images I have in memory as Numpy arrays? The official Google docs only show the vision api accepting an image in disk as the input. </p>

<p>I want to avoid unnecessary disk writes.</p>",50554499,2,0,,2018-05-18 15:53:44.030 UTC,1,2018-05-30 09:21:47.640 UTC,2018-05-30 09:21:47.640 UTC,,4614444,,4614444,1,2,python|numpy|google-api|google-compute-engine|google-cloud-functions,426
Google vision throws a bunch of netty-related errors in jhipster project - Conscrypt class not found,54195144,Google vision throws a bunch of netty-related errors in jhipster project - Conscrypt class not found,"<p>I have set up a simple, monolitic jHipster project from the generator with oAuth support. After adding the spring cloud and google vision (following this tutorial: <a href=""https://spring.io/blog/2018/09/10/bootiful-gcp-use-spring-cloud-gcp-to-connect-to-other-gcp-services-7-8"" rel=""nofollow noreferrer"">https://spring.io/blog/2018/09/10/bootiful-gcp-use-spring-cloud-gcp-to-connect-to-other-gcp-services-7-8</a>), spring app/jHipster throws a bunch of errors at startup which begins with: </p>

<pre><code>2019-01-15 07:51:13.929 DEBUG 41492 --- [  restartedMain] p.a.s.media.security.jwt.TokenProvider   : Using a Base64-encoded JWT secret key
2019-01-15 07:51:18.216 DEBUG 41492 --- [  restartedMain] i.g.n.s.io.grpc.netty.GrpcSslContexts    : Conscrypt class not found. Not using Conscrypt

java.lang.ClassNotFoundException: org.conscrypt.Conscrypt
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at io.grpc.netty.shaded.io.grpc.netty.GrpcSslContexts.&lt;clinit&gt;(GrpcSslContexts.java:103)
    at io.grpc.netty.shaded.io.grpc.netty.NettyChannelBuilder.buildTransportFactory(NettyChannelBuilder.java:377)
    at io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:406)
    at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:246)
    at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:160)
    at com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:152)
    at com.google.api.gax.rpc.ClientContext.create(ClientContext.java:149)
    at com.google.cloud.vision.v1.stub.GrpcImageAnnotatorStub.create(GrpcImageAnnotatorStub.java:84)
    at com.google.cloud.vision.v1.stub.ImageAnnotatorStubSettings.createStub(ImageAnnotatorStubSettings.java:120)
    at com.google.cloud.vision.v1.ImageAnnotatorClient.&lt;init&gt;(ImageAnnotatorClient.java:136)
    at com.google.cloud.vision.v1.ImageAnnotatorClient.create(ImageAnnotatorClient.java:117)
    at pl.Test.server.media.TestMediaServerApp.imageAnnotatorClient(TestMediaServerApp.java:67)
    at pl.Test.server.media.TestMediaServerApp$$EnhancerBySpringCGLIB$$9e9835ab.CGLIB$imageAnnotatorClient$1(&lt;generated&gt;)
    at pl.Test.server.media.TestMediaServerApp$$EnhancerBySpringCGLIB$$9e9835ab$$FastClassBySpringCGLIB$$67791f80.invoke(&lt;generated&gt;)
    at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:228)
    at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:365)
    at pl.Test.server.media.TestMediaServerApp$$EnhancerBySpringCGLIB$$9e9835ab.imageAnnotatorClient(&lt;generated&gt;)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:154)
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:583)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1246)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1096)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:535)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495)
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:759)
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:867)
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:548)
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142)
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:386)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:307)
    at pl.Test.server.media.TestMediaServerApp.main(TestMediaServerApp.java:78)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49)

2019-01-15 07:51:18.229 DEBUG 41492 --- [  restartedMain] i.g.n.s.i.n.u.i.l.InternalLoggerFactory  : Using SLF4J as the default logging framework
</code></pre>

<p>As far as I can tell it is related to credentials and Bean</p>

<pre><code>    @Bean
    ImageAnnotatorClient imageAnnotatorClient(
        CredentialsProvider credentialsProvider) throws IOException {
        ImageAnnotatorSettings settings = ImageAnnotatorSettings
            .newBuilder()
            .setCredentialsProvider(credentialsProvider)
            .build();
        return ImageAnnotatorClient.create(settings);
    }
</code></pre>

<p>which is located in main application class.</p>

<p>Everything works fine and without any issues on clean Spring Boot project. I have spent about 10 hours straight trying to set up vision on jHipster 5 and I am at my wits' end. This is what I have added to the pom.xml, skipping the standard jHipster pom content:</p>

<pre><code>    &lt;repositories&gt;
        &lt;repository&gt;
            &lt;id&gt;spring-milestones&lt;/id&gt;
            &lt;name&gt;Spring Milestones&lt;/name&gt;
            &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt;
        &lt;/repository&gt;
        &lt;!-- jhipster-needle-maven-repository --&gt;
    &lt;/repositories&gt;

(...)

    &lt;properties&gt;
        &lt;!-- Build properties --&gt;
        (...)
        &lt;spring-cloud.version&gt;Greenwich.RC1&lt;/spring-cloud.version&gt;
        (...)
   &lt;/properties&gt;

(...)

    &lt;dependencyManagement&gt;
        (...)
            &lt;dependency&gt;
                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
                &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
                &lt;version&gt;${spring-cloud.version}&lt;/version&gt;
                &lt;type&gt;pom&lt;/type&gt;
                &lt;scope&gt;import&lt;/scope&gt;
            &lt;/dependency&gt;
            &lt;!-- jhipster-needle-maven-add-dependency-management --&gt;
        &lt;/dependencies&gt;
    &lt;/dependencyManagement&gt;

(...)

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-gcp-starter&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-gcp-starter-pubsub&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-gcp-starter-storage&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;google-cloud-vision&lt;/artifactId&gt;
        &lt;/dependency&gt;
(...)

&lt;resources&gt;
       &lt;resource&gt;
       &lt;directory&gt;src/main/resources/&lt;/directory&gt;
       &lt;filtering&gt;true&lt;/filtering&gt;
       &lt;includes&gt;
           &lt;include&gt;config/*.yml&lt;/include&gt;
           &lt;include&gt;config/key.json&lt;/include&gt;
           &lt;include&gt;config/*.properties&lt;/include&gt;
       &lt;/includes&gt;
       &lt;/resource&gt;
       &lt;resource&gt;
       &lt;directory&gt;src/main/resources/&lt;/directory&gt;
       &lt;filtering&gt;false&lt;/filtering&gt;
         &lt;excludes&gt;
           &lt;exclude&gt;config/*.yml&lt;/exclude&gt;
           &lt;exclude&gt;config/key.json&lt;/exclude&gt;
           &lt;exclude&gt;config/*.properties&lt;/exclude&gt;
         &lt;/excludes&gt;
       &lt;/resource&gt;
&lt;/resources&gt;
</code></pre>

<p>About the resource above, I have added application.properties to the project as I did not know how to add an external file with json key to the standard jhipster application.yaml.</p>

<p>application.properties looks like that:</p>

<pre><code>spring.cloud.config.enabled=false
spring.cloud.gcp.project-id=scantest
spring.cloud.gcp.credentials.location=classpath:config/key.json
</code></pre>

<p>Besides those errors, everything seems to work fine. Requesting to analyze an image throws another error:</p>

<pre><code>2019-01-15 08:25:51.974 DEBUG 33432 --- [  XNIO-2 task-4] io.grpc.Context                          : Storage override doesn't exist. Using default

java.lang.ClassNotFoundException: io.grpc.override.ContextStorageOverride
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at io.grpc.Context.createStorage(Context.java:137)
    at io.grpc.Context.storage(Context.java:129)
    at io.grpc.Context.current(Context.java:181)
    at io.grpc.Context$Key.get(Context.java:891)
    at io.grpc.internal.CensusTracingModule$TracingClientInterceptor.interceptCall(CensusTracingModule.java:384)
    at io.grpc.ClientInterceptors$InterceptorChannel.newCall(ClientInterceptors.java:156)
    at io.grpc.internal.CensusStatsModule$StatsClientInterceptor.interceptCall(CensusStatsModule.java:669)
    at io.grpc.ClientInterceptors$InterceptorChannel.newCall(ClientInterceptors.java:156)
    at com.google.api.gax.grpc.GrpcHeaderInterceptor.interceptCall(GrpcHeaderInterceptor.java:81)
    at io.grpc.ClientInterceptors$InterceptorChannel.newCall(ClientInterceptors.java:156)
    at com.google.api.gax.grpc.GrpcMetadataHandlerInterceptor.interceptCall(GrpcMetadataHandlerInterceptor.java:55)
    at io.grpc.ClientInterceptors$InterceptorChannel.newCall(ClientInterceptors.java:156)
    at io.grpc.internal.ManagedChannelImpl.newCall(ManagedChannelImpl.java:777)
    at io.grpc.internal.ForwardingManagedChannel.newCall(ForwardingManagedChannel.java:63)
    at com.google.api.gax.grpc.GrpcClientCalls.newCall(GrpcClientCalls.java:88)
    at com.google.api.gax.grpc.GrpcDirectCallable.futureCall(GrpcDirectCallable.java:58)
    at com.google.api.gax.grpc.GrpcExceptionCallable.futureCall(GrpcExceptionCallable.java:64)
    at com.google.api.gax.rpc.AttemptCallable.call(AttemptCallable.java:81)
    at com.google.api.gax.rpc.RetryingCallable.futureCall(RetryingCallable.java:63)
    at com.google.api.gax.rpc.RetryingCallable.futureCall(RetryingCallable.java:41)
    at com.google.api.gax.rpc.UnaryCallable$1.futureCall(UnaryCallable.java:126)
    at com.google.api.gax.rpc.UnaryCallable.futureCall(UnaryCallable.java:87)
    at com.google.api.gax.rpc.UnaryCallable.call(UnaryCallable.java:112)
    at com.google.cloud.vision.v1.ImageAnnotatorClient.batchAnnotateImages(ImageAnnotatorClient.java:210)
    at com.google.cloud.vision.v1.ImageAnnotatorClient.batchAnnotateImages(ImageAnnotatorClient.java:187)
    at pl.Test.server.media.web.rest.MobileAppResource.analyze(MobileAppResource.java:63)
    at pl.Test.server.media.web.rest.MobileAppResource$$FastClassBySpringCGLIB$$433cac50.invoke(&lt;generated&gt;)
    at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
    at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:746)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
    at org.springframework.aop.aspectj.MethodInvocationProceedingJoinPoint.proceed(MethodInvocationProceedingJoinPoint.java:88)
    at pl.Test.server.media.aop.logging.LoggingAspect.logAround(LoggingAspect.java:85)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethodWithGivenArgs(AbstractAspectJAdvice.java:644)
    at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethod(AbstractAspectJAdvice.java:633)
    at org.springframework.aop.aspectj.AspectJAroundAdvice.invoke(AspectJAroundAdvice.java:70)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
    at org.springframework.aop.aspectj.AspectJAfterThrowingAdvice.invoke(AspectJAfterThrowingAdvice.java:62)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
    at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
    at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:688)
    at pl.Test.server.media.web.rest.MobileAppResource$$EnhancerBySpringCGLIB$$e7430457.analyze(&lt;generated&gt;)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:209)
    at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:136)
    at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:102)
    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:891)
    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:797)
    at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
    at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:991)
    at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:925)
    at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:981)
    at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:884)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
    at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:858)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
    at io.undertow.servlet.handlers.ServletHandler.handleRequest(ServletHandler.java:74)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:129)
    at com.codahale.metrics.servlet.AbstractInstrumentedFilter.doFilter(AbstractInstrumentedFilter.java:111)
    at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:101)
    at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
    at org.springframework.boot.actuate.web.trace.servlet.HttpTraceFilter.doFilterInternal(HttpTraceFilter.java:90)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
    at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:320)
    at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
    at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
    at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:119)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
    at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
    at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
    at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
    at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
    at pl.Test.server.media.security.jwt.JWTFilter.doFilter(JWTFilter.java:38)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
    at org.springframework.web.filter.CorsFilter.doFilterInternal(CorsFilter.java:96)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
    at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
    at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:74)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
    at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
    at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
    at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:215)
    at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:178)
    at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:357)
    at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:270)
    at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
    at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
    at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
    at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:109)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
    at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
    at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
    at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
    at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.filterAndRecordMetrics(WebMvcMetricsFilter.java:117)
    at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:106)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
    at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
    at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:200)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
    at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
    at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
    at io.undertow.servlet.handlers.FilterHandler.handleRequest(FilterHandler.java:84)
    at io.undertow.servlet.handlers.security.ServletSecurityRoleHandler.handleRequest(ServletSecurityRoleHandler.java:62)
    at io.undertow.servlet.handlers.ServletChain$1.handleRequest(ServletChain.java:65)
    at io.undertow.servlet.handlers.ServletDispatchingHandler.handleRequest(ServletDispatchingHandler.java:36)
    at io.undertow.servlet.handlers.security.SSLInformationAssociationHandler.handleRequest(SSLInformationAssociationHandler.java:132)
    at io.undertow.servlet.handlers.security.ServletAuthenticationCallHandler.handleRequest(ServletAuthenticationCallHandler.java:57)
    at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
    at io.undertow.security.handlers.AbstractConfidentialityHandler.handleRequest(AbstractConfidentialityHandler.java:46)
    at io.undertow.servlet.handlers.security.ServletConfidentialityConstraintHandler.handleRequest(ServletConfidentialityConstraintHandler.java:64)
    at io.undertow.security.handlers.AuthenticationMechanismsHandler.handleRequest(AuthenticationMechanismsHandler.java:60)
    at io.undertow.servlet.handlers.security.CachedAuthenticatedSessionHandler.handleRequest(CachedAuthenticatedSessionHandler.java:77)
    at io.undertow.security.handlers.AbstractSecurityContextAssociationHandler.handleRequest(AbstractSecurityContextAssociationHandler.java:43)
    at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
    at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
    at io.undertow.servlet.handlers.SessionRestoringHandler.handleRequest(SessionRestoringHandler.java:119)
    at io.undertow.servlet.handlers.ServletInitialHandler.handleFirstRequest(ServletInitialHandler.java:292)
    at io.undertow.servlet.handlers.ServletInitialHandler.access$100(ServletInitialHandler.java:81)
    at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:138)
    at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:135)
    at io.undertow.servlet.core.ServletRequestContextThreadSetupAction$1.call(ServletRequestContextThreadSetupAction.java:48)
    at io.undertow.servlet.core.ContextClassLoaderSetupAction$1.call(ContextClassLoaderSetupAction.java:43)
    at io.undertow.servlet.handlers.ServletInitialHandler.dispatchRequest(ServletInitialHandler.java:272)
    at io.undertow.servlet.handlers.ServletInitialHandler.access$000(ServletInitialHandler.java:81)
    at io.undertow.servlet.handlers.ServletInitialHandler$1.handleRequest(ServletInitialHandler.java:104)
    at io.undertow.server.Connectors.executeRootHandler(Connectors.java:336)
    at io.undertow.server.HttpServerExchange$1.run(HttpServerExchange.java:830)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)

2019-01-15 08:25:52.005 DEBUG 33432 --- [  XNIO-2 task-4] io.grpc.internal.ClientCallImpl          : Call timeout set to '59944696501' ns, due to context deadline. Explicit call timeout was not set.
(...) 
</code></pre>

<p>but the results returned from google are correct. I am just worried that those errors will backfire at me in the later stage of the development or that it will expose some security issues. And it is pretty depressing to see those exceptions constantly.</p>

<p>I would be very grateful for even small hint about what I can do to resolve this issue.</p>",,0,0,,2019-01-15 08:31:01.253 UTC,0,2019-01-15 08:31:01.253 UTC,,,,,10915547,1,0,spring|google-cloud-platform|netty|jhipster,68
Why I'm getting exeption (file not found) when passing the location of the file testing but it does work with the same file using Postman?,55736217,Why I'm getting exeption (file not found) when passing the location of the file testing but it does work with the same file using Postman?,"<p>I'm using Postman to run my Java project which runs as expected pasing a directory file as an input and then getting an output file, but when I'm using Mockito with Unit Test to test the actual project Tika is throwing an exeption of ""File not found"". </p>

<p>I'm confused because I did even copy and paste the file's location from Postman to my test with no results and also tried to pass the same file harcoded inside of my class to test with no results as well. I also tried to change the logic of my code with no results. At the moment I haven't got the full path of the file as in Postman but it doesn't work with the full path either. I'm trying to test the following classes: </p>

<p>This is my RestHandler class which is called from Postman as well:</p>

<pre><code>public class RestHandler {
private final ApplicationConfig applicationConfig;
private final BasicFileHandlerService basicFileHandlerService;
private final TikaService tikaService;
private final AmazonRekognitionAsync amazonRekognitionAsync;


public void processFilesInFolder(String pickUpFolder) {
    //This will print out the file's path 
    System.out.println(""ListOfFiles!!!!! : "" + pickUpFolder);
    //Here is where I get the exeption
    List&lt;File&gt; allFile = basicFileHandlerService.getAllFile(pickUpFolder);
    for (int i = 0; i &lt; allFile.size() ; i++) {
        String st = String.valueOf(allFile.get(i)).replace(""%"", "" "");
        File file = new File(st);
        allFile.set(i, file);
    }

    allFile.forEach(file -&gt; {
        System.out.println(""This is the file: "" + file);
        //If I call Tika's method I do get the same exeption as well
        TikaResult extractText = tikaService.extractText(file);
        String fileNoPath = file.getName().substring(String.valueOf(file).indexOf(""\\"")-2);
</code></pre>

<p>This is my Tika service class:</p>

<pre><code>public class TikaService {

private final Parser parser;

public TikaResult extractText(File file) {
    //This doesn't get called because the file I'm passing exists
    if (!file.exists()) {
        System.out.println(""does not exist"");
    }

    String s = """";
    Metadata metadata = new Metadata();
    ParseContext parseContext = new ParseContext();


    try (ByteArrayOutputStream out = new ByteArrayOutputStream();) {
        ContentHandler contentHandler = new BodyContentHandler(out);

        parser.parse(new FileInputStream(file), contentHandler, metadata, parseContext);
        s = new String(out.toByteArray());

        boolean shouldThrowException = false;
    } catch (IOException | SAXException | TikaException e) {
        e.printStackTrace();
        System.out.println(e);
    }
</code></pre>

<p>This is my test:</p>

<pre><code>@InjectMocks
RestHandler restHandler;
TikaService tikaService;

@Test
public void contextLoads() {

    File fileForTikaService = new File(""src/test/java/TestFiles/Johnson_-_TestTXT.txt"");

    restHandler.processFilesInFolder(""src/test/java/TestFiles/Johnson_-_TestTXT.txt"");
    TikaResult extractText = tikaService.extractText(fileForTikaService);

    File outputTestFile = new File(""src/test/java/resources/resultFiles/Johnson_-_TestTXT.txt.csv"");
</code></pre>

<p>Thanks!</p>",,1,2,,2019-04-17 21:27:41.783 UTC,,2019-04-24 09:15:29.260 UTC,,,,,9539763,1,0,java|mockito|postman|apache-tika,36
Google Vision API: Buckets return nothing and error appears at ImageAnnotatorClient,54863800,Google Vision API: Buckets return nothing and error appears at ImageAnnotatorClient,"<p>I have this code for Google Vision API. I have Google credentials as a path and also as enviromental variable, but <code>Page&lt;Bucket&gt; buckets = storage.list();</code> return nothing and error appears at  <code>try (ImageAnnotatorClient client = ImageAnnotatorClient.create())</code></p>

<p>Here is code:</p>

<pre><code> public static void main(String... args) throws Exception {
        authExplicit(""D:\\bp-mihalova\\2\\apikey.json"");
        detectLabels(""D:\\bp-mihalova\\2\\Jellyfish.jpg"", System.out);
    }

    public static void detectLabels(String filePath, PrintStream out) throws Exception, IOException {
        List&lt;AnnotateImageRequest&gt; requests = new ArrayList&lt;&gt;();

        ByteString imgBytes = ByteString.readFrom(new FileInputStream(filePath));

        Image img = Image.newBuilder().setContent(imgBytes).build();
        Feature feat = Feature.newBuilder().setType(Type.LABEL_DETECTION).build();
        AnnotateImageRequest request =
                AnnotateImageRequest.newBuilder().addFeatures(feat).setImage(img).build();
        requests.add(request);

        try (ImageAnnotatorClient client = ImageAnnotatorClient.create()) {
            BatchAnnotateImagesResponse response = client.batchAnnotateImages(requests);
            List&lt;AnnotateImageResponse&gt; responses = response.getResponsesList();

            for (AnnotateImageResponse res : responses) {
                if (res.hasError()) {
                    out.printf(""Error: %s\n"", res.getError().getMessage());
                    return;
                }

                // For full list of available annotations, see http://g.co/cloud/vision/docs
                for (EntityAnnotation annotation : res.getLabelAnnotationsList()) {
                    annotation.getAllFields().forEach((k, v) -&gt; out.printf(""%s : %s\n"", k, v.toString()));
                }
            }
        }
    }

    static void authExplicit(String jsonPath) throws IOException {
        // You can specify a credential file by providing a path to GoogleCredentials.
        // Otherwise credentials are read from the GOOGLE_APPLICATION_CREDENTIALS environment variable.
        GoogleCredentials credentials = GoogleCredentials.fromStream(new FileInputStream(jsonPath))
                .createScoped(Lists.newArrayList(""https://www.googleapis.com/auth/cloud-platform""));
        Storage storage = StorageOptions.newBuilder().setCredentials(credentials).build().getService();

        System.out.println(""Buckets:"");
        Page&lt;Bucket&gt; buckets = storage.list();
        for (Bucket bucket : buckets.iterateAll()) {
            System.out.println(bucket.toString());
        }
    }
</code></pre>

<p>And here is console with empty <code>Buckets</code> and error:</p>

<pre><code>Buckets:
Exception in thread ""main"" java.io.IOException: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.
    at com.google.auth.oauth2.DefaultCredentialsProvider.getDefaultCredentials(DefaultCredentialsProvider.java:132)
    at com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:127)
    at com.google.auth.oauth2.GoogleCredentials.getApplicationDefault(GoogleCredentials.java:100)
    at com.google.api.gax.core.GoogleCredentialsProvider.getCredentials(GoogleCredentialsProvider.java:59)
    at com.google.api.gax.rpc.ClientContext.create(ClientContext.java:132)
    at com.google.cloud.vision.v1.stub.GrpcImageAnnotatorStub.create(GrpcImageAnnotatorStub.java:84)
    at com.google.cloud.vision.v1.stub.ImageAnnotatorStubSettings.createStub(ImageAnnotatorStubSettings.java:120)
    at com.google.cloud.vision.v1.ImageAnnotatorClient.&lt;init&gt;(ImageAnnotatorClient.java:136)
    at com.google.cloud.vision.v1.ImageAnnotatorClient.create(ImageAnnotatorClient.java:117)
    at com.google.cloud.vision.v1.ImageAnnotatorClient.create(ImageAnnotatorClient.java:108)
    at sk.Google.detectLabels(Google.java:40)
    at sk.Google.main(Google.java:26)

Process finished with exit code 1
</code></pre>",54874601,1,1,,2019-02-25 10:09:55.600 UTC,,2019-02-25 21:01:49.673 UTC,,,,,9123511,1,0,java|google-cloud-vision,58
What is the Ideal image dimensions for better OCR by google vision?,53117283,What is the Ideal image dimensions for better OCR by google vision?,"<p>I have been using google vision OCR for a while now. And I have observed that the OCR result varies with image dimension. Say for example an image with dimension 720 x 1280 gives a better result than 360 x 720. And it sometimes does worse the other way.</p>

<p>I have experienced the same with Microsoft's OCR API.</p>

<p>So is there an ideal image dimension that always gives a good OCR result? How does the image dimensions affect the OCR result?</p>",,3,0,,2018-11-02 10:58:29.843 UTC,,2018-11-17 02:33:41.053 UTC,,,,,8283737,1,0,image-processing|ocr|microsoft-cognitive|google-vision|image-preprocessing,170
Issue using AWS Comprehend iOS,51444352,Issue using AWS Comprehend iOS,"<p>I'm pretty sure I set up my IAM role appropriately (I literally attached the ComprehendFullAccess policy to the role) and the Cognito Pool was also setup appropriately (I know this because I'm also using Rekognition and it works with the IAM Role and Cognito ID Pool I created) and yet every time I try to send a request to AWS Comprehend I get the error </p>

<p><code>Error Domain=com.amazonaws.AWSServiceErrorDomain Code=6 ""(null)"" UserInfo={__type=AccessDeniedException, Message=User: arn:aws:sts::&lt;my sts&gt;:assumed-role/Cognito_&lt;my id pool name&gt;Unauth_Role/CognitoIdentityCredentials is not authorized to perform: comprehend:DetectEntities}</code></p>

<p>Any idea of what I can do in this situation? I tried creating a new Cognito Pool and creating a custom IAM Role that literally only allows <code>comprehend:DetectEntities</code> and it still doesn't work.</p>",51447914,1,0,,2018-07-20 14:05:38.650 UTC,,2018-07-20 17:48:25.727 UTC,,,,,10023275,1,1,ios|swift|amazon-web-services|aws-sdk-ios,53
Scan Identity Cards using Google Vision API,50700241,Scan Identity Cards using Google Vision API,"<p>It's possible to read Identity Cards information like name, address birthDate using Google Vision API?
In the documentation, I fount something but I don't know how to use it.</p>

<p><a href=""https://developers.google.com/android/reference/com/google/android/gms/vision/barcode/Barcode.DriverLicense"" rel=""nofollow noreferrer"">https://developers.google.com/android/reference/com/google/android/gms/vision/barcode/Barcode.DriverLicense</a></p>

<p>I checked also the google samples (<a href=""https://github.com/googlesamples/android-vision"" rel=""nofollow noreferrer"">https://github.com/googlesamples/android-vision</a>), but I didn't find anything related to Identity Cards scanning.</p>",,3,0,,2018-06-05 12:35:52.770 UTC,,2019-05-02 07:22:54.540 UTC,,,,,6818714,1,1,android-vision,674
How to use AWS Rekognition service to detect faces with Node.js,41824724,How to use AWS Rekognition service to detect faces with Node.js,"<p>I am trying to use Amazon Rekognition Service with Node.js,<br>
I uploaded a face image to S3 service in a bucket with a sample program and now I want to detect face with Node.js</p>

<p>The code is as below</p>

<pre><code>// Load the AWS SDK for Node.js
var AWS = require('aws-sdk');
// Load credentials and set region from JSON file
AWS.config.loadFromPath('./config.json');

var rekognition = new AWS.Rekognition({apiVersion: '2016-06-27'});
var s3 = new AWS.S3({apiVersion: '2006-03-01'});
</code></pre>

<p>/* This operation detects faces in an image stored in an AWS S3 bucket.     */</p>

<pre><code>var params = {
Image: {
   S3Object: {
   Bucket: ""rekognitionfortesting"", 
   Name: ""face1.jpeg""
  }
 },

};
rekognition.detectFaces(params, function(err, data) {
if (err) console.log(err, err.stack); // an error occurred
else     console.log(data);           // successful response

});
</code></pre>

<p>I coudn't get true data, this is the response I get:</p>

<p><a href=""https://i.stack.imgur.com/RLshF.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RLshF.jpg"" alt=""enter image description here""></a></p>

<p>[Object] is written. Normally it should give a response like below.</p>

<pre><code>data = {
    FaceDetails: [
       {
      BoundingBox: {
       Height: 0.18000000715255737, 
       Left: 0.5555555820465088, 
       Top: 0.33666667342185974, 
       Width: 0.23999999463558197
      }, 
      Confidence: 100, 
      Landmarks: [
         {
        Type: ""EYE_LEFT"", 
        X: 0.6394737362861633, 
        Y: 0.40819624066352844
       }, 
         {
        Type: ""EYE_RIGHT"", 
        X: 0.7266660928726196, 
        Y: 0.41039225459098816
       }, 
         {
        Type: ""NOSE_LEFT"", 
        X: 0.6912462115287781, 
        Y: 0.44240960478782654
       }, 
         {
        Type: ""MOUTH_DOWN"", 
        X: 0.6306198239326477, 
        Y: 0.46700039505958557
       }, 
         {
        Type: ""MOUTH_UP"", 
        X: 0.7215608954429626, 
        Y: 0.47114261984825134
       }
      ], 
      Pose: {
       Pitch: 4.050806522369385, 
       Roll: 0.9950747489929199, 
       Yaw: 13.693790435791016
      }, 
      Quality: {
       Brightness: 37.60169982910156, 
       Sharpness: 80
      }
     }
    ], 
    OrientationCorrection: ""ROTATE_0""
   }
   */
 });
</code></pre>

<p>How can I get the face data?</p>",,3,2,,2017-01-24 09:38:40.547 UTC,,2017-11-07 05:23:35.463 UTC,2017-01-24 10:26:19.650 UTC,,1535071,,7461947,1,2,node.js|amazon-web-services|detect|face|amazon-rekognition,4123
Watson Custom Visual Recognition in Java,47155510,Watson Custom Visual Recognition in Java,<p>I am trying to create a custom Watson Visual Recognition in java. I have already a classifier created using Curl. Currently I am using the default Watson Classifier. Are there any examples where Watson API is used for custom creation and training of classifiers in Java?</p>,47155907,1,0,,2017-11-07 10:25:19.730 UTC,,2017-11-07 10:43:16 UTC,,,,,5243291,1,0,java|ibm-cloud|watson|visual-recognition,46
Google Cloud Vision API shows null object reference,48037316,Google Cloud Vision API shows null object reference,"<p>I am trying to build an android application where I am using <code>google cloud vision API</code> for detecting faces. I am following <a href=""https://code.tutsplus.com/tutorials/how-to-use-the-google-cloud-vision-api-in-android-apps--cms-29009"" rel=""nofollow noreferrer"">this</a> tutorial. The problem is that, I am unable to produce the <code>toast message</code> that was supposed to be displayed on the screen after I click the button but the <code>logcat</code> shows:</p>

<blockquote>
  <p>Attempt to invoke virtual method 'com.google.api.services.vision.v1.Vision$Images com.google.api.services.vision.v1.Vision.images()' on a null object reference</p>
</blockquote>

<p>Here is the code : </p>

<p><strong>MainActivity.java</strong></p>

<pre><code>public class MainActivity extends AppCompatActivity {

private Button upload_button;
private ImageView image_capture;
final int CAMERA_PIC_REQUEST = 100;

private Vision vision;

@Override
protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.activity_main);

    upload_button = (Button) findViewById(R.id.uploadb);
    image_capture=(ImageView)findViewById(R.id.capturedImage);

    if (ContextCompat.checkSelfPermission(this, android.Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) {
        if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.M) {
            requestPermissions(new String[]{android.Manifest.permission.CAMERA},
                    5);
        }
    }

    Vision.Builder visionBuilder = new Vision.Builder(
            new NetHttpTransport(),
            new AndroidJsonFactory(),
            null);

    visionBuilder.setVisionRequestInitializer(
            new VisionRequestInitializer(""AIzaSyAueQjrrY_GiXh7kNGlbDLKWhYP-4q77vI""));
    Vision vision = visionBuilder.build();

    upload_button.setOnClickListener(new View.OnClickListener() {
        @Override
        public void onClick(View v) {
            /*Intent cameraIntent = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);
            startActivityForResult(cameraIntent, CAMERA_PIC_REQUEST);*/
            imageDectection();
        }
    });
}

private void imageDectection() {
    AsyncTask.execute(new Runnable() {
        @Override
        public void run() {
            try {
                InputStream inputStream = getResources().openRawResource(R.raw.crewf);
                byte[] photoData = IOUtils.toByteArray(inputStream);

                Image inputImage = new Image();
                inputImage.encodeContent(photoData);

                Feature desiredFeature = new Feature();
                desiredFeature.setType(""FACE_DETECTION"");

                AnnotateImageRequest request = new AnnotateImageRequest();
                request.setImage(inputImage);
                request.setFeatures(Arrays.asList(desiredFeature));

                BatchAnnotateImagesRequest batchRequest = new BatchAnnotateImagesRequest();
                batchRequest.setRequests(Arrays.asList(request));

                BatchAnnotateImagesResponse batchResponse =
                        vision.images().annotate(batchRequest).execute();

                List&lt;FaceAnnotation&gt; faces = batchResponse.getResponses()
                        .get(0).getFaceAnnotations();

                int numberOfFaces = faces.size();

                String likelihoods = """";
                for(int i=0; i&lt;numberOfFaces; i++) {
                    likelihoods += ""\n It is "" +
                            faces.get(i).getJoyLikelihood() +
                            "" that face "" + i + "" is happy"";
                }

                final String message =
                        ""This photo has "" + numberOfFaces + "" faces"" + likelihoods;

                runOnUiThread(new Runnable() {
                    @Override
                    public void run() {
                        Toast.makeText(getApplicationContext(),
                                message, Toast.LENGTH_LONG).show();
                    }
                });

            } catch(Exception e) {
                Log.d(""ERROR"", e.getMessage());
            }
        }
    });
}
}
</code></pre>

<p>The picture is stored in <code>/res/raw</code> location as <code>crewf.jpg</code>. I have tried setting the types as <code>LABEL_DETECTION</code>,<code>TEXT_DETECTION</code>,<code>LANDMARK_DETECTION</code> but none of it works.</p>

<p>Can anyone help me in this?</p>",,1,0,,2017-12-30 19:47:15.870 UTC,0,2018-11-22 15:28:05.350 UTC,,,,,8086064,1,0,java|android|google-app-engine|google-cloud-vision,189
"Python - Convert TIFF, PDF etc. to JPEG in-memory for input to Google Cloud Vision",45274013,"Python - Convert TIFF, PDF etc. to JPEG in-memory for input to Google Cloud Vision","<p>I have a number of non-JPEG image files that I want to process using Google Cloud Vision, but the API only accepts certain formats (see question <a href=""https://stackoverflow.com/questions/36728347/cloud-vision-api-pdf-ocr"">Cloud Vision API - PDF OCR</a> and answer <a href=""https://cloud.google.com/vision/docs/supported-files"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/supported-files</a>).</p>

<p>I can use PIL or some such to convert a TIFF to JPEG to be uploaded, but I'd like to avoid a temp file if possible.</p>

<p>So, in python, how do I convert a TIFF in-memory for upload to GCV? numpy array, base64, string..?</p>",45295447,1,5,,2017-07-24 06:33:57.423 UTC,,2017-07-25 06:27:04.283 UTC,,,,,1021819,1,0,python|image-processing|jpeg|python-imaging-library|google-cloud-vision,815
Is it possible to use Google vision API offline in Python?,48706864,Is it possible to use Google vision API offline in Python?,"<p>I have a simple Python application which uses Google vision API. How can I make it to work offline? I searched a lot but couldn't find anything useful. This <a href=""https://stackoverflow.com/questions/40832882/does-machine-vision-api-work-offline"">question </a> is about android app and it seems that for that case the answer is positive. Here is my code:</p>

<pre><code>from google.cloud import vision
from google.cloud.vision import types
from google.oauth2 import service_account
credentials=service_account.Credentials.from_service_account_file('key.json')
client = vision.ImageAnnotatorClient(credentials=credentials)
with io.open('my_figure.jpg', 'rb') as image_file:
     content = image_file.read()
image_context = types.ImageContext(language_hints =[""en""])
image = types.Image(content=content)
response = client.text_detection(image=image, image_context=image_context)
texts = response.text_annotations
for text in texts:
  print('\n""{}""'.format(text.description))
</code></pre>",,0,3,,2018-02-09 13:22:15.013 UTC,,2018-02-09 13:28:47.817 UTC,2018-02-09 13:28:47.817 UTC,,6115579,,6115579,1,0,python|google-api|google-api-client|google-vision,800
Cloud Vision Face Landmarks: How to Interpret Z Co-Ordinate?,55619304,Cloud Vision Face Landmarks: How to Interpret Z Co-Ordinate?,"<p>I've been using Google Cloud Vision to identify faces. So far, so good, but I've noticed that face landmarks are returned as 3-axis positions. X and Y are pixel co-ordinates in the image, which is fine.</p>

<p>The returned Z values are another question, and the API reference does not indicate their meaning. They are definitely non-zero, so they mean something. But they range both +ve and -ve, so they aren't relative to the view position but some other point (on the face?), and I have no idea what units they could be in.</p>

<p>Can anyone (especially from Google) shed light on this?</p>

<p>The API reference for Google Cloud Vision's face landmarks: <a href=""https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate#Landmark"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate#Landmark</a></p>

<hr>

<p>What I've learned about UNITS:</p>

<p>I've tested the same image at different resolutions. The returned Z values seems to scale (approximately) relative to the scale of the image. That is, the Zs for an image of 1024x1024 will be approximately 2x those of the same image scaled to 512x512.</p>

<p><a href=""https://i.stack.imgur.com/TwvWH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TwvWH.png"" alt=""Sample depths for an image at 1024sqr.""></a><a href=""https://i.stack.imgur.com/gEYRn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gEYRn.png"" alt=""Sample depths for the same image scaled to 512sqr""></a></p>

<p>This implies that the units of these Z values are proportional to the pixel size of the image... BUT the image width and height correspond to field of view and aspect of the camera (they aren't interpretable as distance) and it's not clear to me how a depth value could possibly be relative to those parameters.</p>

<hr>

<p>What I've learned about the REFERENCE POINT:</p>

<p>On inspection, it seems that the landmark Z values almost always range both -ve and +ve, implying that whatever they are relative to it is in the middle of them somewhere. But I cannot find a clear pattern (for example, it being based on the centre of the eyes or other specific point).</p>",,0,0,,2019-04-10 18:39:51.750 UTC,,2019-04-10 18:39:51.750 UTC,,,,,4406280,1,0,google-cloud-vision,10
Google Cloud Vision ImageAnnotator Google Application Credential File Not Exist Codeigniter PHP,55464809,Google Cloud Vision ImageAnnotator Google Application Credential File Not Exist Codeigniter PHP,"<p>I have try to implement the google cloud vision with API ImageAnnotator using a codeigniter PHP.</p>

<p>I have install the require google cloud vision using a composer to my third party directory in codeigniter.</p>

<p>This is the code looks like in my controller :</p>

<pre><code>defined('BASEPATH') OR exit('No direct script access allowed');

use Google\Auth\ApplicationDefaultCredentials;
use GuzzleHttp\Client;
use GuzzleHttp\HandlerStack;
use Google\Cloud\Vision\V1\ImageAnnotatorClient;        

class Manage_center extends CI_Controller {

    function __construct() {
        parent::__construct();

        include APPPATH . 'third_party/vendor/autoload.php';
    }

    public function index()
    {
        $this-&gt;load-&gt;view('index');
    }


    function upload_ocr_image()
    {               
        //img_data contain image =&gt; i just shorten the code.
        $img_data = $this-&gt;upload-&gt;data();                                          

        // Authenticating with a keyfile path.
        putenv('GOOGLE_APPLICATION_CREDENTIALS='.base_url().'assets/google_cloud_vision/credentials.json');
        $scopes = ['https://www.googleapis.com/auth/cloud-vision'];

        // create middleware
        $middleware = ApplicationDefaultCredentials::getMiddleware($scopes);
        $stack = HandlerStack::create();
        $stack-&gt;push($middleware);

        $imageAnnotator = new ImageAnnotatorClient();

        # annotate the image                
        $response = $imageAnnotator-&gt;textDetection($img_data['full_path']);             
        $texts = $response-&gt;getTextAnnotations();

        printf('%d texts found:' . PHP_EOL, count($texts));
        foreach ($texts as $text) {
            print($text-&gt;getDescription() . PHP_EOL);

            # get bounds
            $vertices = $text-&gt;getBoundingPoly()-&gt;getVertices();
            $bounds = [];
            foreach ($vertices as $vertex) {
                $bounds[] = sprintf('(%d,%d)', $vertex-&gt;getX(), $vertex-&gt;getY());
            }
            print('Bounds: ' . join(', ',$bounds) . PHP_EOL);
        }

        $imageAnnotator-&gt;close();



    }
}
</code></pre>

<p>I got the error :</p>

<blockquote>
  <p><p>Type: DomainException</p> <p>Message: Unable to read the credential
  file specified by  GOOGLE_APPLICATION_CREDENTIALS: file
  <a href=""http://localhost/theseeds/assets/google_cloud_vision/credentials.json"" rel=""nofollow noreferrer"">http://localhost/theseeds/assets/google_cloud_vision/credentials.json</a>
  does not exist</p> <p>Filename:
  D:\xampp\htdocs\theseeds\application\third_party\vendor\google\auth\src\CredentialsLoader.php</p></p>
  
  <p>Line Number: 74</p>
  
  <p>File:
  D:\xampp\htdocs\theseeds\application\controllers\Manage_center.php            Line: 3188<br />            Function: getMiddleware</p>
</blockquote>

<p>I dont understand why this error occur :</p>

<p><a href=""http://localhost/theseeds/assets/google_cloud_vision/credentials.json"" rel=""nofollow noreferrer"">http://localhost/theseeds/assets/google_cloud_vision/credentials.json</a> does not exist</p></p>

<p>Because when i opened the link the file is there.</p>

<p>And this error :</p>

<blockquote>
  <p>File:
  D:\xampp\htdocs\theseeds\application\controllers\Admin_center.php            Line: 3188<br />            Function: getMiddleware</p>
</blockquote>

<p>is a line code :</p>

<p><strong>$middleware = ApplicationDefaultCredentials::getMiddleware($scopes);</strong></p>

<p>What is the proper way to use the google cloud vision ImageAnnotatorClient in codeigniter PHP ?</p>

<p>Is there a problem with the authentication to google cloud api ?</p>

<p>Thank You</p>",55465575,1,0,,2019-04-01 23:11:06.937 UTC,,2019-04-04 01:02:12.103 UTC,,,,,6786634,1,0,php|codeigniter|google-cloud-platform|google-api-php-client|google-cloud-vision,71
Why Google cloud vision auto create many Computer Engine,52149997,Why Google cloud vision auto create many Computer Engine,"<p>My application run normal until now.
But suddenly one service account (used for google cloud vision) create many Computer Engine instances(about 32 instances).
I checked log and see this line.
<a href=""https://i.stack.imgur.com/bBXr1.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>My billing increase from 10 to 200$/day :(
Anyone help me?</p>",,1,0,,2018-09-03 12:41:09.833 UTC,,2018-09-03 23:04:00.307 UTC,,,,,2411719,1,-3,google-app-engine|google-compute-engine|google-cloud-vision,40
Does Amazon Rekognition just need one image to recognise faces?,55937801,Does Amazon Rekognition just need one image to recognise faces?,"<p>I am creating a collection using the amazon rekognition create collection api call .
Does each person need only one image for him to be classified well?
Or do we need to give multiple images per class(person) as done in facenet or other deep learning implementations to extract features ?</p>

<p>I have already added all the images(multiple images per person) and it shows me it has detected someone well enough.
But can the collection cluster similar featured images to form one person ?</p>",,1,0,,2019-05-01 14:25:28.900 UTC,,2019-05-01 15:28:05.353 UTC,,,,,2856292,1,0,amazon-rekognition,21
Error in installing google-cloud,48525155,Error in installing google-cloud,"<p>I am get some errors while trying to install <strong>google-cloud</strong> by using <strong>pip</strong>.</p>

<p>I tried the following commands to install <strong>google-cloud</strong>:</p>

<pre><code>sudo pip install --upgrade google-cloud
sudo pip3 install --upgrade google-cloud
sudo pip3.6 install --upgrade google-cloud
</code></pre>

<p>But I am getting the following error by all these commands:</p>

<pre><code>Requirement already satisfied: google-cloud in /usr/lib/python3.6/si
te-packages/google_cloud-0.32.1.dev1-py3.6.egg
Requirement already satisfied: google-api-core&lt;0.2.0dev,&gt;=0.1.2 in /
usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-bigquery&lt;0.29dev,&gt;=0.28.
0 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-bigquery-datatransfer&lt;0.
2dev,&gt;=0.1.0 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-bigtable&lt;0.29dev,&gt;=0.28.
1 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-container&lt;0.2dev,&gt;=0.1.0
 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-core&lt;0.29dev,&gt;=0.28.0 in
 /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-datastore&lt;1.5dev,&gt;=1.4.0
 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-dns&lt;0.29dev,&gt;=0.28.0 in 
/usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-error-reporting&lt;0.29dev,
&gt;=0.28.0 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-firestore&lt;0.29dev,&gt;=0.28
.0 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-language&lt;1.1dev,&gt;=1.0.0 
in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-logging&lt;1.5dev,&gt;=1.4.0 i
n /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-monitoring&lt;0.29dev,&gt;=0.2
8.0 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-pubsub&lt;0.31dev,&gt;=0.30.0 
in /usr/lib/python3.6/site-packages/google_cloud_pubsub-0.30.1-py3.6
.egg (from google-cloud)
Requirement already satisfied: google-cloud-resource-manager&lt;0.29dev
,&gt;=0.28.0 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-runtimeconfig&lt;0.29dev,&gt;=
0.28.0 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-spanner&lt;0.30dev,&gt;=0.29.0
 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-speech&lt;0.31dev,&gt;=0.30.0 
in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-storage&lt;1.7dev,&gt;=1.6.0 i
n /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-trace&lt;0.18dev,&gt;=0.17.0 i
n /usr/lib/python3.6/site-packages/google_cloud_trace-0.17.0-py3.6.e
gg (from google-cloud)
Requirement already satisfied: google-cloud-translate&lt;1.4dev,&gt;=1.3.0
 in /usr/lib/python3.6/site-packages/google_cloud_translate-1.3.0-py
3.6.egg (from google-cloud)
Requirement already satisfied: google-cloud-videointelligence&lt;1.1dev
,&gt;=1.0.0 in /usr/lib/python3.6/site-packages (from google-cloud)
Requirement already satisfied: google-cloud-vision&lt;0.30dev,&gt;=0.29.0 
in /usr/lib/python3.6/site-packages/google_cloud_vision-0.29.0-py3.6
.egg (from google-cloud)
Requirement already satisfied: google-auth&lt;2.0.0dev,&gt;=0.4.0 in /usr/
lib/python3.6/site-packages (from google-api-core&lt;0.2.0dev,&gt;=0.1.2-&gt;
google-cloud)
Requirement already satisfied: pytz in /usr/lib/python3.6/site-packa
ges (from google-api-core&lt;0.2.0dev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: googleapis-common-protos&lt;2.0dev,&gt;=1.5
.3 in /usr/lib/python3.6/site-packages (from google-api-core&lt;0.2.0de
v,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: requests&lt;3.0.0dev,&gt;=2.18.0 in /usr/li
b/python3.6/site-packages (from google-api-core&lt;0.2.0dev,&gt;=0.1.2-&gt;go
ogle-cloud)
Requirement already satisfied: protobuf&gt;=3.0.0 in /usr/lib64/python3
.6/site-packages (from google-api-core&lt;0.2.0dev,&gt;=0.1.2-&gt;google-clou
d)
Requirement already satisfied: six&gt;=1.10.0 in /usr/lib/python3.6/sit
e-packages (from google-api-core&lt;0.2.0dev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: setuptools&gt;=34.0.0 in /usr/lib/python
3.6/site-packages (from google-api-core&lt;0.2.0dev,&gt;=0.1.2-&gt;google-clo
ud)
Requirement already satisfied: google-resumable-media&gt;=0.2.1 in /usr
/lib/python3.6/site-packages (from google-cloud-bigquery&lt;0.29dev,&gt;=0
.28.0-&gt;google-cloud)
Requirement already satisfied: google-gax&lt;0.16dev,&gt;=0.15.7 in /usr/l
ib/python3.6/site-packages (from google-cloud-bigtable&lt;0.29dev,&gt;=0.2
8.1-&gt;google-cloud)
Requirement already satisfied: gapic-google-cloud-datastore-v1&lt;0.16d
ev,&gt;=0.15.0 in /usr/lib/python3.6/site-packages (from google-cloud-d
atastore&lt;1.5dev,&gt;=1.4.0-&gt;google-cloud)
Requirement already satisfied: gapic-google-cloud-error-reporting-v1
beta1&lt;0.16dev,&gt;=0.15.0 in /usr/lib/python3.6/site-packages (from goo
gle-cloud-error-reporting&lt;0.29dev,&gt;=0.28.0-&gt;google-cloud)
Requirement already satisfied: gapic-google-cloud-logging-v2&lt;0.92dev
,&gt;=0.91.0 in /usr/lib/python3.6/site-packages (from google-cloud-log
ging&lt;1.5dev,&gt;=1.4.0-&gt;google-cloud)
Requirement already satisfied: grpc-google-iam-v1&lt;0.12dev,&gt;=0.11.1 i
n /usr/lib/python3.6/site-packages (from google-cloud-pubsub&lt;0.31dev
,&gt;=0.30.0-&gt;google-cloud)
Collecting psutil&lt;6.0dev,&gt;=5.2.2 (from google-cloud-pubsub&lt;0.31dev,&gt;
=0.30.0-&gt;google-cloud)
  Using cached psutil-5.4.3.tar.gz
Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/lib/pyt
hon3.6/site-packages (from google-auth&lt;2.0.0dev,&gt;=0.4.0-&gt;google-api-
core&lt;0.2.0dev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: cachetools&gt;=2.0.0 in /usr/lib/python3
.6/site-packages (from google-auth&lt;2.0.0dev,&gt;=0.4.0-&gt;google-api-core
&lt;0.2.0dev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: rsa&gt;=3.1.4 in /usr/lib/python3.6/site
-packages (from google-auth&lt;2.0.0dev,&gt;=0.4.0-&gt;google-api-core&lt;0.2.0d
ev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: urllib3&lt;1.23,&gt;=1.21.1 in /usr/lib/pyt
hon3.6/site-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-co
re&lt;0.2.0dev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/lib/python
3.6/site-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;
0.2.0dev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: idna&lt;2.7,&gt;=2.5 in /usr/lib/python3.6/
site-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;0.2.
0dev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/lib/pyt
hon3.6/site-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-co
re&lt;0.2.0dev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: dill&lt;0.3dev,&gt;=0.2.5 in /usr/lib/pytho
n3.6/site-packages (from google-gax&lt;0.16dev,&gt;=0.15.7-&gt;google-cloud-b
igtable&lt;0.29dev,&gt;=0.28.1-&gt;google-cloud)
Requirement already satisfied: future&lt;0.17dev,&gt;=0.16.0 in /usr/lib/p
ython3.6/site-packages (from google-gax&lt;0.16dev,&gt;=0.15.7-&gt;google-clo
ud-bigtable&lt;0.29dev,&gt;=0.28.1-&gt;google-cloud)
Requirement already satisfied: ply==3.8 in /usr/lib/python3.6/site-p
ackages (from google-gax&lt;0.16dev,&gt;=0.15.7-&gt;google-cloud-bigtable&lt;0.2
9dev,&gt;=0.28.1-&gt;google-cloud)
Requirement already satisfied: grpcio&lt;2.0dev,&gt;=1.0.2 in /usr/lib64/p
ython3.6/site-packages (from google-gax&lt;0.16dev,&gt;=0.15.7-&gt;google-clo
ud-bigtable&lt;0.29dev,&gt;=0.28.1-&gt;google-cloud)
Requirement already satisfied: oauth2client&lt;4.0dev,&gt;=2.0.0 in /usr/l
ib/python3.6/site-packages (from gapic-google-cloud-datastore-v1&lt;0.1
6dev,&gt;=0.15.0-&gt;google-cloud-datastore&lt;1.5dev,&gt;=1.4.0-&gt;google-cloud)
Requirement already satisfied: proto-google-cloud-datastore-v1[grpc]
&lt;0.91dev,&gt;=0.90.3 in /usr/lib/python3.6/site-packages (from gapic-go
ogle-cloud-datastore-v1&lt;0.16dev,&gt;=0.15.0-&gt;google-cloud-datastore&lt;1.5
dev,&gt;=1.4.0-&gt;google-cloud)
Requirement already satisfied: proto-google-cloud-error-reporting-v1
beta1[grpc]&lt;0.16dev,&gt;=0.15.3 in /usr/lib/python3.6/site-packages (fr
om gapic-google-cloud-error-reporting-v1beta1&lt;0.16dev,&gt;=0.15.0-&gt;goog
le-cloud-error-reporting&lt;0.29dev,&gt;=0.28.0-&gt;google-cloud)
Requirement already satisfied: proto-google-cloud-logging-v2[grpc]&lt;0
.92dev,&gt;=0.91.3 in /usr/lib/python3.6/site-packages (from gapic-goog
le-cloud-logging-v2&lt;0.92dev,&gt;=0.91.0-&gt;google-cloud-logging&lt;1.5dev,&gt;=
1.4.0-&gt;google-cloud)
Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.1 in /usr/lib/pyth
on3.6/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;2.0.0de
v,&gt;=0.4.0-&gt;google-api-core&lt;0.2.0dev,&gt;=0.1.2-&gt;google-cloud)
Requirement already satisfied: httplib2&gt;=0.9.1 in /usr/lib/python3.6
/site-packages (from oauth2client&lt;4.0dev,&gt;=2.0.0-&gt;gapic-google-cloud
-datastore-v1&lt;0.16dev,&gt;=0.15.0-&gt;google-cloud-datastore&lt;1.5dev,&gt;=1.4.
0-&gt;google-cloud)
Installing collected packages: psutil
  Running setup.py install for psutil ... error
    Complete output from command /usr/bin/python3 -u -c ""import setu
ptools, tokenize;__file__='/tmp/pip-build-1mt8_94q/psutil/setup.py';
f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\
r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install
 --record /tmp/pip-co20my3o-record/install-record.txt --single-versi
on-externally-managed --compile:
    running install
    running build
    running build_py
    creating build
    creating build/lib.linux-x86_64-3.6
    creating build/lib.linux-x86_64-3.6/psutil
    copying psutil/_exceptions.py -&gt; build/lib.linux-x86_64-3.6/psut
il

    creating build/lib.linux-x86_64-3.6/psutil/tests
    copying psutil/tests/__main__.py -&gt; build/lib.linux-x86_64-3.6/p
sutil/tests
    copying psutil/tests/test_bsd.py -&gt; build/lib.linux-x86_64-3.6/p
sutil/tests
    copying psutil/tests/test_contracts.py -&gt; build/lib.linux-x86_64
-3.6/psutil/tests
    copying psutil/tests/test_aix.py -&gt; build/lib.linux-x86_64-3.6/p
sutil/tests
    copying psutil/tests/test_memory_leaks.py -&gt; build/lib.linux-x86
_64-3.6/psutil/tests
    copying psutil/tests/test_connections.py -&gt; build/lib.linux-x86_
64-3.6/psutil/tests
    copying psutil/tests/test_system.py -&gt; build/lib.linux-x86_64-3.
6/psutil/tests
    copying psutil/tests/test_linux.py -&gt; build/lib.linux-x86_64-3.6
/psutil/tests
    copying psutil/tests/test_misc.py -&gt; build/lib.linux-x86_64-3.6/
psutil/tests
    copying psutil/tests/__init__.py -&gt; build/lib.linux-x86_64-3.6/p
sutil/tests
    copying psutil/tests/test_osx.py -&gt; build/lib.linux-x86_64-3.6/p
sutil/tests
    copying psutil/tests/test_process.py -&gt; build/lib.linux-x86_64-3
.6/psutil/tests
    copying psutil/tests/test_windows.py -&gt; build/lib.linux-x86_64-3
.6/psutil/tests
    copying psutil/tests/test_posix.py -&gt; build/lib.linux-x86_64-3.6
/psutil/tests
    copying psutil/tests/test_sunos.py -&gt; build/lib.linux-x86_64-3.6
/psutil/tests
    copying psutil/tests/test_unicode.py -&gt; build/lib.linux-x86_64-3
.6/psutil/tests
    running build_ext
    building 'psutil._psutil_linux' extension
    creating build/temp.linux-x86_64-3.6
    creating build/temp.linux-x86_64-3.6/psutil
    gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -fmessag
e-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fsta
ck-protector-strong -funwind-tables -fasynchronous-unwind-tables -g 
-DOPENSSL_LOAD_CONF -fmessage-length=0 -grecord-gcc-switches -O2 -Wa
ll -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fas
ynchronous-unwind-tables -g -fmessage-length=0 -grecord-gcc-switches
 -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tab
les -fasynchronous-unwind-tables -g -fPIC -DPSUTIL_POSIX=1 -DPSUTIL_
VERSION=543 -DPSUTIL_LINUX=1 -I/usr/include/python3.6m -c psutil/_ps
util_common.c -o build/temp.linux-x86_64-3.6/psutil/_psutil_common.o
    psutil/_psutil_common.c:9:10: fatal error: Python.h: No such fil
e or directory
     #include &lt;Python.h&gt;
              ^~~~~~~~~~
    compilation terminated.
    error: command 'gcc' failed with exit status 1

    ----------------------------------------
Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file_
_='/tmp/pip-build-1mt8_94q/psutil/setup.py';f=getattr(tokenize, 'ope
n', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();ex
ec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-co20m
y3o-record/install-record.txt --single-version-externally-managed --
compile"" failed with error code 1 in /tmp/pip-build-1mt8_94q/psutil/
</code></pre>

<p>I think that some packages are being compiled by <strong>GCC</strong> and <strong>GCC</strong> is trying to find <strong>python.h</strong>. But <strong>python.h</strong> is missing, so the Process is exiting with error code 1.</p>

<p>I am using <strong>OpenSuse tumblebeed.</strong> I want a parmanent solution because I am making a application which I will distribute to client and these packages will be installed automatically in user's machine.</p>",,1,1,,2018-01-30 15:37:17.333 UTC,,2018-01-30 15:50:50.380 UTC,,,,,8459063,1,0,python|pip|google-cloud-platform,586
"AWS compareFaces() Rekognition Javascript SDK Error: Unable to get object metadata from S3. Check object key, region and/or access permissions",53105446,"AWS compareFaces() Rekognition Javascript SDK Error: Unable to get object metadata from S3. Check object key, region and/or access permissions","<p>I'm trying to use compareFaces() function from aws Rekognition API by referencing two files in the same S3 bucket ""reconfaces"" that is in the same Region as Rekognition(I set the S3 bucket to us-east-1, and so Rekognition). I set the bucket to public for simplicity and I'm also using a user that has Full Permisions over Rekognition and S3(which wasn't necessary for this case but just to clarify it):</p>

<p>aws-rekognition-config.js</p>

<pre><code>const dotenv = require('dotenv');
dotenv.config();
const AWS = require('aws-sdk');

const rekognition = {
    ""accessKeyId"": process.env.AMAZON_DEV_ACCESS_KEY_ID,
    ""secretAccessKey"": process.env.AMAZON_DEV_SECRET_ACCESS_KEY,
    ""region"": ""us-east-1""
  };

const Recognition = new AWS.Rekognition(rekognition);

module.exports = Recognition;
</code></pre>

<p>and the index.js where I do a simple test to compare the two images in my bucket:</p>

<pre><code>let express = require('express');
let router = express.Router();


var AWS = require('aws-sdk');

var rekognition = require('../config/aws-rekognition-config');


module.exports = () =&gt; {

    router.get('/compare/:uid', async(req,res,next) =&gt; {


 var params = {
  SimilarityThreshold: 90, 
  SourceImage: {
   S3Object: {
    Bucket: ""reconfaces"", 
    Name: ""1541079978865.jpg""
   }
  }, 
  TargetImage: {
   S3Object: {
    Bucket: ""reconfaces"", 
    Name: ""1541079982272.png""   }
  }
 };

  let faceMatches;

//tried promised version here as well but without any luck
  rekognition.compareFaces(params, function(err, data) {
    if (err) console.log(err, err.stack); // an error occurred
    else     console.log(data);           // successful response
  });
/* 
   try{
      faceMatches = await rekognition.compareFaces(params);
      let a = 1;
   }catch(err){
      console.log(""Error comparing faces"",err);
      return;
   } */


    });



  /* GET home page. */
  router.use('/', (req, res) =&gt; {
        //list all routes
      res.send({""Default"":'Backend-'});
  });

  return router;
}
</code></pre>

<p>As you can see the files exist in the bucket and it's on the same region as the one specified in the rekognition config: 
<a href=""https://i.stack.imgur.com/2birQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2birQ.png"" alt=""enter image description here""></a></p>

<p>And the user credentials I'm using has more permissions than it needs to for this task:</p>

<p><a href=""https://i.stack.imgur.com/KHg0r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KHg0r.png"" alt=""enter image description here""></a></p>

<p>I also have to mention that I uploaded the files via api as well using the npm package multer-s3:</p>

<pre><code>    var multer = require('multer');
    var multerS3 = require('multer-s3');

    var s3 =  new aws.S3({
    ""accessKeyId"": process.env.AMAZON_DEV_ACCESS_KEY_ID,
    ""secretAccessKey"": process.env.AMAZON_DEV_SECRET_ACCESS_KEY,
    ""region"": ""us-east-1"",
    ""s3BucketEndpoint"": false,
    ""endpoint"": ""https://s3.amazonaws.com""
  });


    var upload = multer({
      storage: multerS3({
        s3,
        bucket: 'reconfaces',
        metadata: function (req, file, cb) {
          cb(null, {fieldName: file.fieldname});
        },
        key: function (req, file, cb) {
          cb(null, Date.now() + path.extname(file.originalname));
        }
      })
    });
</code></pre>

<p>and then it's applied as a middleware:</p>

<pre><code>router.post('/upload', upload.array('image',10), async(req,res,next)=&gt;{

  //upload picture to s3 

    console.log(""Files uploaded successfully"");
    res.json({data:""UPLOAD_SUCCESS""});
});
</code></pre>

<p>I don't know if maybe the metadata is messed up by multer-s3. But I also tried to upload both of the files from the aws console in the browser, I made both files and the bucket public and I get the same error, so I doubt it has to do with multer-s3 package. The files are not corrupted or anything since I can download them and view them without any problem...</p>

<p>I also tried using the cli and I get the same error:</p>

<pre><code>aws rekognition compare-faces  --source-image '{""S3Object"":{""Bucket"":""reconfaces"",""Name"":""11112-face1.jpg""}}' --region us-east-1 --target-image '{""S3Object"":{""Bucket"":""reconfaces"",""Name"":""11112-face2.jpg""}}'
</code></pre>

<p>The guy in this video couldn't do the same as I wanted either:</p>

<p><a href=""https://www.youtube.com/watch?v=GtknPjdlOfg"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=GtknPjdlOfg</a></p>

<p>and this guy could using the same privileges I have</p>

<p><a href=""https://www.youtube.com/watch?v=FhFs0zwCvg4"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=FhFs0zwCvg4</a></p>

<p>If I throw this other operation it works:</p>

<pre><code>aws rekognition detect-labels --image '{""S3Object"":{""Bucket"":""reconfaces"",""Name"":""11112-face1.jpg""}}' 
</code></pre>

<p>it returns: </p>

<pre><code>{
    ""Labels"": [
        {
            ""Name"": ""Human"",
            ""Confidence"": 99.17317962646484
        },
        {
            ""Name"": ""People"",
            ""Confidence"": 99.17317962646484
        },
        {
            ""Name"": ""Person"",
            ""Confidence"": 99.17317962646484
        },
        {
            ""Name"": ""Face"",
            ""Confidence"": 63.1695671081543
        },
        {
            ""Name"": ""Head"",
            ""Confidence"": 57.89347839355469
        },
        {
            ""Name"": ""Portrait"",
            ""Confidence"": 55.02056884765625
        },
        {
            ""Name"": ""Glasses"",
            ""Confidence"": 51.320011138916016
        },
        {
            ""Name"": ""Child"",
            ""Confidence"": 50.735557556152344
        },
        {
            ""Name"": ""Kid"",
            ""Confidence"": 50.735557556152344
        }
    ],
    ""OrientationCorrection"": ""ROTATE_0""
}
</code></pre>

<p>so it must be something with the compare-faces endpoint.</p>

<p>What could be the problem?. I saw a lot of people having problems with this particular API, but most of the answers that I found here and in github issues were about both resources being operating in different regions, which is not my case. </p>

<p>Thank you very much!</p>",53107529,1,0,,2018-11-01 16:29:35.543 UTC,,2018-11-01 18:48:21.933 UTC,2018-11-01 18:22:59.773 UTC,,5935343,,5935343,1,0,node.js|amazon-web-services|amazon-s3|amazon-rekognition|multer-s3,264
Why does google cloud vision Logo detection return different results on the same image?,49590288,Why does google cloud vision Logo detection return different results on the same image?,"<p>I have been playing around with the google cloud vision API, namely the logo detection feature. Basically I want to determine if an image is a logo, so I run it through the API. However, I always get different results every time I run it. Sometimes the API classifies it as a logo, and sometimes it does not. Is there any explanation for this and possibly a way to improve the accuracy?</p>

<p>EDIT: I have just determined what the problem really is. I am trying to detect logos on remote images on a public facing website, and occasionally (but not all the time) the following error is returned:</p>

<pre><code>I20180409-21:25:38.090(8)?     error: 
I20180409-21:25:38.091(8)?      { details: [],
I20180409-21:25:38.091(8)?        code: 13,
I20180409-21:25:38.091(8)?        message: 'We can not access the URL 
currently. Please download the content and pass it in.' },
</code></pre>

<p>What is the cause for this issue and is there a way around it?</p>",49787240,2,3,,2018-03-31 17:01:39.420 UTC,,2018-04-12 02:54:02.490 UTC,2018-04-09 13:28:32.227 UTC,,5855648,,5855648,1,1,javascript|node.js|google-cloud-platform|google-cloud-vision,298
Android Google cloud vision API get Json and use Json to go to another acitvity,47368685,Android Google cloud vision API get Json and use Json to go to another acitvity,"<p>Basically in the title, I've been trying to work with the Google Cloud Vision API through android as I'm trying to make an application that will allow the user to scan the name of a game for instance, and the app will detect the name of the game or object and then move to an activity or web page that corresponds with the name given back in the JSON. Only problem is, I'm unable to get the JSON in the android application and I'm not sure why, I heard that its not possible on the android app but I'm not 100% sure on that. I was wondering if anyone would be able to confirm if thats the case or if there are any alternatives to my solution as I'm starting to tear my hair out over this. </p>",,1,3,,2017-11-18 17:02:31.910 UTC,,2017-12-31 15:32:31.093 UTC,,,,,8894889,1,0,android|json|vision|google-cloud-vision,213
Is there any OCR web service that is GDPR compliant,46580171,Is there any OCR web service that is GDPR compliant,"<p>By that I mean:</p>

<ul>
<li>I can opt-out of using the images for training, or</li>
<li>My users can delete their images (if they are used for training by default).</li>
</ul>

<p>My reading of the Google Vision terms indicate that they are completely non-compliant.</p>",46581592,1,0,,2017-10-05 07:30:11.297 UTC,,2017-10-05 08:59:20.350 UTC,,,,,239558,1,1,privacy|google-cloud-vision|privacy-policy,244
How to search data without searchbar using ionic & firebase?,49062449,How to search data without searchbar using ionic & firebase?,"<pre><code>&lt;ion-card *ngFor=""let item of items | async""&gt;
&lt;img [src]=""'data:image/png;base64,' + item.imageData""/&gt;
&lt;ion-card-content&gt;
  &lt;ion-list no-lines&gt;
    &lt;ion-list-header&gt;
      Text
    &lt;/ion-list-header&gt;
   &lt;ion-item *ngFor=""let text of item.results[0].textAnnotations""(click)=""itemTapped($event,text.description)""&gt;{{text.description}}&lt;/ion-item&gt;
  &lt;/ion-list&gt;
&lt;/ion-card-content&gt;
</code></pre>

<p></p>

<p>javascript code :</p>

<pre><code>import {Component} from '@angular/core';
import {AlertController} from 'ionic-angular';
import {Camera,CameraOptions} from '@ionic-native/camera';
import {GoogleCloudVisionServiceProvider} from '../../providers/google-cloud-vision-service ';
import {AngularFireDatabase,FirebaseListObservable} from ""angularfire2/database-deprecated"";
import {DataProvider} from '../../providers/data/data';
import {IonicPage,NavController,NavParams} from 'ionic-angular';
import {FirstPage} from '../first/first';
import {FormControl} from '@angular/forms';

@Component({
    selector: 'page-camera',
    templateUrl: 'camera.html'
})

export class CameraPage {
    public item: Array &lt; any &gt; ;
    items: FirebaseListObservable &lt; any[] &gt; ;
    searchTerm: string = '';
    searchControl: FormControl;


    constructor(
        private camera: Camera,
        private vision: GoogleCloudVisionServiceProvider,
        private db: AngularFireDatabase,
        private alert: AlertController,
        public dataService: DataProvider,
        public navCtrl: NavController, public navParams: NavParams, ) {
        this.searchControl = new FormControl();
        this.items = db.list('/items');
    }

    takePhoto() {
        const options: CameraOptions = {
            quality: 100,
            targetHeight: 500,
            targetWidth: 500,
            destinationType: this.camera.DestinationType.DATA_URL,
            encodingType: this.camera.EncodingType.PNG,
            mediaType: this.camera.MediaType.PICTURE
        }

        this.camera.getPicture(options).then((imageData) =&gt; {
            this.vision.getLabels(imageData).subscribe((result) =&gt; {
                this.saveResults(imageData, result.json().responses);
            }, err =&gt; {
                this.showAlert(err);
            });
        }, err =&gt; {
            this.showAlert(err);
        });
    }

    saveResults(imageData, results) {
        this.items.push({
                imageData: imageData,
                results: results
            }).set({
                imageData: imageData,
                results: results
            })
            .then(_ =&gt; {})
            .catch(err =&gt; {
                this.showAlert(err)
            });
    }

    showAlert(message) {
        let alert = this.alert.create({
            title: 'Error',
            subTitle: message,
            buttons: ['OK']
        });
        alert.present();
    }

    declareItem(): void {
        this.item = [{
                my_name: 'Kari Ayam',
                eng_name: ""Curry Chicken"",
                ch_name: '咖喱鸡',
                des: ""A whole genre, rather than a distinct dish, you'll find curries of all 
                sorts on Malaysian tables,
                a bowl of rice usually not far away.Malaysian
                versions tend to start with a rempah,
                a complex paste of spices and
                aromatics that 's cooked together and forms the base of the curry; like so 
                many of the country 's dishes, they tend to make use of coconut milk, too. "",
                pic: ""https://firebasestorage.googleapis.com/v0/b/testing-
                227 d9.appspot.com / o / curry % 20 chicken.jpg ? alt = media &amp; token = e28e63cd - 7906 - 4923 -
                b6d8 - 1 c7726d85615 ""
            }, {
                my_name: ""Roti Canai"",
                eng_name: ""Roti Canai"",
                ch_name: ""印度煎饼"",
                des: ""A classic Malaysian breakfast of Indian derivation, though this flaky 
                finger food is good any time of day(and really good at about three in the morning).A dough of flour,
                egg,
                and ghee(clarified butter) is incredibly,
                almost unbelievably elastic;it 's stretched quickly into a tissue-thin 
                sheet,
                like pizza dough but even more dramatic,
                then folded back up and
                griddled.In its best form,
                right off the griddle,
                it 's flaky and crisp like 
                a good croissant on the outside,
                soft and steaming and a little bit chewy on
                the inside.It 's also served with curry, often lentil dal; other versions 
                are cooked with egg,
                or onion,
                or sardines.
                "",
                pic: ""https://firebasestorage.googleapis.com/v0/b/testing-
                227 d9.appspot.com / o / roti % 20 canai.jpg ? alt = media &amp; token = b852c79f - 6 da6 - 48e1 -
                9 b94 - 50512 fa32be9 ""
            },
            {
                my_name: ""Nasi goreng"",
                eng_name: ""fried rice"",
                ch_name: ""炒饭"",
                des: ""Rice stir-fried with chilis and garlic and kecap manis (sweet soy); 
                like mee goreng,
                it might have chicken or shrimp
                for a little more
                substance.(I love it with a fried egg over the top.)
                "",
                pic: ""https://firebasestorage.googleapis.com/v0/b/testing-
                227 d9.appspot.com / o / nasi % 20 goreng.jpg ? alt = media &amp; token = ea257884 - 63 f5 - 4 cd2 -
                8 c6d - 4 d1cb4462d91 ""

            }
        ];
    }

    itemTapped($event, text): void {


        let val: string = text;
        if (val.trim() !== '') {
            this.item = this.item.filter((item) =&gt; {
                return item.my_name.toLowerCase().indexOf(val.toLowerCase()) &gt; -1;
            })
            this.navCtrl.push(FirstPage, text);
        }

    }
}
</code></pre>

<p>I use google text detection, it will store the result in firebase. [firebase screenshot][Apps Screenshot]<a href=""https://i.stack.imgur.com/bQebS.png"" rel=""nofollow noreferrer"">1</a> After it detect the word, I want to pass the text and search from my list (this.item()), but I fail to do that.My itemTapped function is not working for the filter part.</p>",,0,0,,2018-03-02 03:35:29.327 UTC,,2018-03-02 04:10:03.393 UTC,2018-03-02 04:10:03.393 UTC,,4044787,,9432166,1,1,android|firebase|ionic-framework,115
Unable to import google.cloud.vision via the reticulate package in R,46095769,Unable to import google.cloud.vision via the reticulate package in R,"<p>I am using the reticulate package to import python modules into RStudio.I was able to import packages like cv2,pandas,sklearn but was unable to import the google.cloud.vision package.I have installed this package via pip and am using  Anaconda 4.4.0  Python  2.7 version on ubuntu-trusty-14.04-amd64-server on AWS.<a href=""https://i.stack.imgur.com/R3ucb.png"" rel=""nofollow noreferrer"">Pic of the error in R-Studio Server</a>
The Installation of the vision and the language libraries was done via</p>

<pre><code>pip install --upgrade google-cloud-vision
pip install --upgrade google-cloud-language
</code></pre>

<p>I am also able to import these modules in the python interactive sessions without any error</p>

<pre><code>&gt;&gt;&gt;import google.cloud.vision
&gt;&gt;&gt;import google.cloud.language
</code></pre>

<p>Can somebody please advise me on what I am doing wrong...</p>

<p><strong>Edit 1</strong> :I have tried using the approach that <em>Yuan Tang</em> had suggested.I have installed the requests and httplib2 package in the conda environment.I have also used the use_python command to point Rstudio to the correct python environment.</p>

<pre><code>use_python(""/home/avadhut/miniconda2/bin/python"")
</code></pre>

<p>The cv2 package is installed in the conda environment and it is imported successfully which means RStudio is using the correct Python environment</p>

<p>Even after doing all this I am getting the following trace-back in the R studio console. </p>

<pre><code>Error in py_module_import(module, convert = convert) : 
  ImportError: The requests library is not installed, please install the requests package to use the requests transport.

Detailed traceback: 
File ""/home/avadhut/miniconda2/lib/python2.7/site-packages/google/cloud/vision/__init__.py"", line 36, in &lt;module&gt;
    from google.cloud.vision.client import Client
File ""/home/avadhut/miniconda2/lib/python2.7/site-packages/google/cloud/vision/client.py"", line 20, in &lt;module&gt;
    from google.cloud.client import ClientWithProject
File ""/home/avadhut/miniconda2/lib/python2.7/site-packages/google/cloud/client.py"", line 25, in &lt;module&gt;
    import google.auth.transport.requests
File ""/home/avadhut/miniconda2/lib/python2.7/site-packages/google/auth/transport/requests.py"", line 30, in &lt;module&gt;
    caught_exc,
File ""/home/avadhut/miniconda2/lib/python2.7/site-packages/six.py"", line 737, in raise_from
    raise value
</code></pre>

<p>Here is the pic of my RStudio IDE with the error displayed.
<a href=""https://i.stack.imgur.com/5DaDK.jpg"" rel=""nofollow noreferrer"">!Reticulate Package Import Error</a></p>",,1,0,,2017-09-07 11:59:01.097 UTC,,2018-03-29 07:05:08.553 UTC,2018-03-29 07:05:08.553 UTC,,5927529,,8229596,1,0,r|python-2.7|google-api-python-client|google-cloud-vision|reticulate,208
Android TextRecognizer TextBlocks out of order when 4 or more are detected,45618476,Android TextRecognizer TextBlocks out of order when 4 or more are detected,"<p>I have been using <strong>Google Vision api</strong> <em>to scan Images for text and display them in a TextView</em>, the only problem is if it detects 4 or more TextBlocks within the image the blocks appear out of order. I have been looking for a solution to the problem but haven't found one so far, any help would be greatly appreciated.</p>

<p>Here is my code for translating the image:</p>

<pre><code>   public void RunOCR(TextView scanResults, Context context) {
    detector = new TextRecognizer.Builder(context).build();
    imageUri = Uri.fromFile(new File(Environment.getExternalStorageDirectory() + ""/pic.jpg""));
        try {
            Bitmap bitmap = decodeBitmapUri(context, imageUri);
            if (detector.isOperational() &amp;&amp; bitmap != null) {
                Frame frame = new Frame.Builder().setBitmap(bitmap).build();
                SparseArray&lt;TextBlock&gt; textBlocks = detector.detect(frame);
                String blocks = """";
                String lines = """";
                String words = """";
                int number = 0;
                boolean goneThroughLoop = false;
                //if(textBlocks.size() &gt;= 4){number = textBlocks.size() - 2;}
                for (int index = number; index &lt; textBlocks.size(); index++) {
                    //extract scanned text blocks here
                    if(index == textBlocks.size() - 2 &amp;&amp; goneThroughLoop){break;}
                    TextBlock tBlock = textBlocks.valueAt(index);
                    blocks = blocks + tBlock.getValue() + ""\n"" + ""\n"";
                    for (Text line : tBlock.getComponents()) {
                        //extract scanned text lines here
                        lines = lines + line.getValue() + ""\n"";
                        for (Text element : line.getComponents()) {
                            //extract scanned text words here
                            words = words + element.getValue() + "", "";
                        }
                    }
                    /*if(index == textBlocks.size() - 1 &amp;&amp; !goneThroughLoop){index = 0; goneThroughLoop = true; ;
                        tBlock = textBlocks.valueAt(index);
                        blocks = blocks + tBlock.getValue() + ""\n"" + ""\n"";
                        for (Text line : tBlock.getComponents()) {
                            //extract scanned text lines here
                            lines = lines + line.getValue() + ""\n"";
                            for (Text element : line.getComponents()) {
                                //extract scanned text words here
                                words = words + element.getValue() + "", "";
                            }
                        }*/

                    }
                }
                if (textBlocks.size() == 0) {
                    scanResults.setText(""Scan Failed: Found nothing to scan"");
                } else {
                    scanResults.setText(scanResults.getText() + blocks + ""\n"");
                }
            } else {
                scanResults.setText(""Could not set up the detector!"");
            }
        } catch (Exception e) {
            //Toast.makeText(this, ""Failed to load Image"", Toast.LENGTH_SHORT).show();
            Log.e(LOG_TAG, e.toString());
        }
    }
</code></pre>

<p>edit: The recent fix of using if statements(commented out in code) to rearrange the blocks no longer works. Links below are screenshots of the problem I've been having.</p>

<p><a href=""https://drive.google.com/file/d/0B_I_5fURAbCgNGhQLTIxY3NfWlE/view?usp=drivesdk"" rel=""nofollow noreferrer"">Scanned image</a></p>

<p><a href=""https://drive.google.com/file/d/0B_I_5fURAbCgSFFLbXc3MFJZNU0/view?usp=drivesdk"" rel=""nofollow noreferrer"">Resulting text</a></p>",,0,2,,2017-08-10 16:00:51.887 UTC,,2017-11-23 12:58:13.430 UTC,2017-08-22 22:01:56.703 UTC,,8437684,,8437684,1,1,javascript|android|android-studio,283
Match an image to bucket of images using Amazon Rekognition,46281898,Match an image to bucket of images using Amazon Rekognition,"<p>I am using <a href=""https://aws.amazon.com/rekognition/"" rel=""nofollow noreferrer"">Amazon Rekognition</a> in a project. My requirement is to upload a set of products to the bucket initially and when a user uploads an image to my portal he/she should get matching(similar) image/images from my bucket as a result. Is this possible?</p>",46282288,1,0,,2017-09-18 14:31:17.240 UTC,,2017-09-18 15:51:17.983 UTC,2017-09-18 15:51:17.983 UTC,,1562662,,4854309,1,1,amazon-web-services|amazon-s3|aws-lambda|aws-sdk|amazon-rekognition,211
Can't connect to AWS rekognition API from IntelliJ,41186458,Can't connect to AWS rekognition API from IntelliJ,"<p>I've been playing with the new rekognition API from Amazon and I am having trouble running their <a href=""http://docs.aws.amazon.com/rekognition/latest/dg/get-started-exercise-detect-faces.html"" rel=""nofollow noreferrer"">example</a> Java application from IntelliJ.  I'm using Maven to build the project and have included the AWS SDK in my <code>pom.xml</code> as follows:</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;
    &lt;artifactId&gt;aws-java-sdk&lt;/artifactId&gt;
    &lt;version&gt;${aws-java-sdk.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>From what I can tell, my application seems to be failing somewhere around here:</p>

<pre><code>AmazonRekognitionClient rekognitionClient = new AmazonRekognitionClient(credentials)
        .withEndpoint(""s3-us-west-2.amazonaws.com"");
rekognitionClient.setSignerRegionOverride(""us-west-2"");
try {
    DetectFacesResult result = rekognitionClient.detectFaces(request);
    ObjectMapper objectMapper = new ObjectMapper();
    System.out.println(""Result = "" + objectMapper.writeValueAsString(result));
} catch (AmazonRekognitionException e) {
    e.printStackTrace();
}
</code></pre>

<p>...And the error that I'm getting is:</p>

<pre><code>com.amazonaws.services.rekognition.model.AmazonRekognitionException: null (Service: AmazonRekognition; Status Code: 400; Error Code: null; Request ID: null)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1545)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1183)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:964)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:676)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:650)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:633)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$300(AmazonHttpClient.java:601)
    at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:583)
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:447)
    at com.amazonaws.services.rekognition.AmazonRekognitionClient.doInvoke(AmazonRekognitionClient.java:1130)
    at com.amazonaws.services.rekognition.AmazonRekognitionClient.invoke(AmazonRekognitionClient.java:1106)
    at com.amazonaws.services.rekognition.AmazonRekognitionClient.detectFaces(AmazonRekognitionClient.java:599)
    at com.github.jhenningsgaard.DetectFaces.main(DetectFaces.java:39)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
</code></pre>

<p>I should also note that I ran the operation (see below) in AWS CLI and was successful.</p>

<pre><code>aws rekognition detect-faces \
--image '{""S3Object"":{""Bucket"":""Bucketname"",""Name"":""s3ObjectKey""}}' \
--attributes ""ALL"" \
--region us-east-1 \
--profile adminuser
</code></pre>",,1,3,,2016-12-16 14:24:54.763 UTC,,2017-01-29 21:30:26.617 UTC,2016-12-16 16:51:52.477 UTC,,4122925,,4122925,1,1,java|maven|amazon-web-services|intellij-idea|amazon-rekognition,569
How to Create a ID-Scanner and OCR mobile app,56006562,How to Create a ID-Scanner and OCR mobile app,"<p>I need to create a Driving license scanner, but I don't know how to started. </p>

<p>The process that I need:</p>

<p>1 - Open Camera
2 - Detect Driving license 
3 - Autofocus, autocrop and take the picture
4 - Extract and OCR data from the driving license</p>

<p>I already create an application where I crop and use google vision to OCR. My problem is detect, autofocus and autocrop only the detected document.</p>

<p>Reference apps: </p>

<p>Regula: <a href=""https://play.google.com/store/apps/details?id=com.regula.documentreader"" rel=""nofollow noreferrer"">https://play.google.com/store/apps/details?id=com.regula.documentreader</a></p>

<p>BlinkID: <a href=""https://play.google.com/store/apps/details?id=com.microblink.blinkidapp"" rel=""nofollow noreferrer"">https://play.google.com/store/apps/details?id=com.microblink.blinkidapp</a></p>",,0,1,,2019-05-06 13:47:39.380 UTC,,2019-05-06 13:47:39.380 UTC,,,,,11434077,1,-2,android|mobile|camera|ocr|detection,33
Bundle can't find gem that is clearly installed,50805898,Bundle can't find gem that is clearly installed,"<p>I'm having a really weird issue in which I can clearly see that a gem file is installed and so can bundle, but then when I try to run it I get an error that bundle can't find it.</p>

<p>Gem File: </p>

<pre><code>source ""https://rubygems.org/""

gem 'kitchen-terraform'
</code></pre>

<p>Bundle env:</p>

<pre><code>## Environment

```
Bundler       1.16.2
  Platforms   ruby, x64-mingw32
Ruby          2.5.1p57 (2018-03-29 revision 63029) [x64-mingw32]
  Full Path   C:/Ruby25-x64/bin/ruby.exe
  Config Dir  C:/ProgramData
RubyGems      2.7.6
  Gem Home    C:/Ruby25-x64/lib/ruby/gems/2.5.0
  Gem Path    C:/Users/rperkins/.gem/ruby/2.5.0;C:/Ruby25-x64/lib/ruby/gems/2.5.0
  User Path   C:/Users/rperkins/.gem/ruby/2.5.0
  Bin Dir     C:/Ruby25-x64/bin
Tools
  Git         2.7.2.windows.1
  RVM         not installed
  rbenv       not installed
  chruby      not installed
```

## Bundler Build Metadata

```
Built At          2018-05-16
Git SHA           9f7bf0ac3
Released Version  true
```

## Gemfile

### Gemfile

```ruby
source ""https://rubygems.org/""

gem 'kitchen-terraform'
```

### Gemfile.lock

```
GEM
  remote: https://rubygems.org/
  specs:
    addressable (2.5.2)
      public_suffix (&gt;= 2.0.2, &lt; 4.0)
    aws-sdk (2.11.65)
      aws-sdk-resources (= 2.11.65)
    aws-sdk-core (2.11.65)
      aws-sigv4 (~&gt; 1.0)
      jmespath (~&gt; 1.0)
    aws-sdk-resources (2.11.65)
      aws-sdk-core (= 2.11.65)
    aws-sigv4 (1.0.2)
    azure_mgmt_resources (0.16.0)
      ms_rest_azure (~&gt; 0.10.0)
    builder (3.2.3)
    coderay (1.1.2)
    concurrent-ruby (1.0.5)
    declarative (0.0.10)
    declarative-option (0.1.0)
    diff-lcs (1.3)
    digest-crc (0.4.1)
    docker-api (1.34.2)
      excon (&gt;= 0.47.0)
      multi_json
    domain_name (0.5.20180417)
      unf (&gt;= 0.0.5, &lt; 1.0.0)
    dry-configurable (0.7.0)
      concurrent-ruby (~&gt; 1.0)
    dry-container (0.6.0)
      concurrent-ruby (~&gt; 1.0)
      dry-configurable (~&gt; 0.1, &gt;= 0.1.3)
    dry-core (0.4.6)
      concurrent-ruby (~&gt; 1.0)
    dry-equalizer (0.2.1)
    dry-inflector (0.1.2)
    dry-logic (0.4.2)
      dry-container (~&gt; 0.2, &gt;= 0.2.6)
      dry-core (~&gt; 0.2)
      dry-equalizer (~&gt; 0.2)
    dry-types (0.13.2)
      concurrent-ruby (~&gt; 1.0)
      dry-container (~&gt; 0.3)
      dry-core (~&gt; 0.4, &gt;= 0.4.4)
      dry-equalizer (~&gt; 0.2)
      dry-inflector (~&gt; 0.1, &gt;= 0.1.2)
      dry-logic (~&gt; 0.4, &gt;= 0.4.2)
    dry-validation (0.12.0)
      concurrent-ruby (~&gt; 1.0)
      dry-configurable (~&gt; 0.1, &gt;= 0.1.3)
      dry-core (~&gt; 0.2, &gt;= 0.2.1)
      dry-equalizer (~&gt; 0.2)
      dry-logic (~&gt; 0.4, &gt;= 0.4.0)
      dry-types (~&gt; 0.13.1)
    erubis (2.7.0)
    excon (0.62.0)
    faraday (0.15.2)
      multipart-post (&gt;= 1.2, &lt; 3)
    faraday-cookie_jar (0.0.6)
      faraday (&gt;= 0.7.4)
      http-cookie (~&gt; 1.0.0)
    faraday_middleware (0.12.2)
      faraday (&gt;= 0.7.4, &lt; 1.0)
    ffi (1.9.25-x64-mingw32)
    google-api-client (0.19.8)
      addressable (~&gt; 2.5, &gt;= 2.5.1)
      googleauth (&gt;= 0.5, &lt; 0.7.0)
      httpclient (&gt;= 2.8.1, &lt; 3.0)
      mime-types (~&gt; 3.0)
      representable (~&gt; 3.0)
      retriable (&gt;= 2.0, &lt; 4.0)
    google-cloud (0.51.1)
      google-cloud-bigquery (~&gt; 1.1)
      google-cloud-bigquery-data_transfer (~&gt; 0.1)
      google-cloud-container (~&gt; 0.1)
      google-cloud-dataproc (~&gt; 0.1)
      google-cloud-datastore (~&gt; 1.4)
      google-cloud-dlp (~&gt; 0.1)
      google-cloud-dns (~&gt; 0.28)
      google-cloud-error_reporting (~&gt; 0.30)
      google-cloud-firestore (~&gt; 0.21)
      google-cloud-language (~&gt; 0.30)
      google-cloud-logging (~&gt; 1.5)
      google-cloud-monitoring (~&gt; 0.27)
      google-cloud-os_login (~&gt; 0.1)
      google-cloud-pubsub (~&gt; 0.30)
      google-cloud-resource_manager (~&gt; 0.29)
      google-cloud-spanner (~&gt; 1.3)
      google-cloud-speech (~&gt; 0.29)
      google-cloud-storage (~&gt; 1.10)
      google-cloud-trace (~&gt; 0.31)
      google-cloud-translate (~&gt; 1.2)
      google-cloud-video_intelligence (~&gt; 1.0)
      google-cloud-vision (~&gt; 0.28)
    google-cloud-bigquery (1.5.0)
      concurrent-ruby (~&gt; 1.0)
      google-api-client (~&gt; 0.19.8)
      google-cloud-core (~&gt; 1.2)
      googleauth (~&gt; 0.6.2)
    google-cloud-bigquery-data_transfer (0.1.0)
      google-gax (~&gt; 1.0)
    google-cloud-container (0.1.0)
      google-gax (~&gt; 1.0.1)
    google-cloud-core (1.2.0)
      google-cloud-env (~&gt; 1.0)
    google-cloud-dataproc (0.1.0)
      google-gax (~&gt; 1.0.0)
    google-cloud-datastore (1.4.0)
      google-cloud-core (~&gt; 1.2)
      google-gax (~&gt; 1.0)
      google-protobuf (~&gt; 3.3)
    google-cloud-dlp (0.4.0)
      google-gax (~&gt; 1.0)
    google-cloud-dns (0.28.0)
      google-api-client (~&gt; 0.19.0)
      google-cloud-core (~&gt; 1.2)
      googleauth (~&gt; 0.6.2)
      zonefile (~&gt; 1.04)
    google-cloud-env (1.0.1)
      faraday (~&gt; 0.11)
    google-cloud-error_reporting (0.30.0)
      google-cloud-core (~&gt; 1.2)
      google-gax (~&gt; 1.0)
      stackdriver-core (~&gt; 1.3)
    google-cloud-firestore (0.21.1)
      concurrent-ruby (~&gt; 1.0)
      google-cloud-core (~&gt; 1.2)
      google-gax (~&gt; 1.0)
    google-cloud-language (0.30.0)
      google-gax (~&gt; 1.0)
    google-cloud-logging (1.5.0)
      google-cloud-core (~&gt; 1.2)
      google-gax (~&gt; 1.0)
      stackdriver-core (~&gt; 1.3)
    google-cloud-monitoring (0.28.0)
      google-gax (~&gt; 1.0)
    google-cloud-os_login (0.1.0)
      google-gax (~&gt; 1.0.0)
    google-cloud-pubsub (0.30.2)
      concurrent-ruby (~&gt; 1.0)
      google-cloud-core (~&gt; 1.2)
      google-gax (~&gt; 1.0)
      grpc-google-iam-v1 (~&gt; 0.6.9)
    google-cloud-resource_manager (0.29.0)
      google-api-client (~&gt; 0.19.8)
      google-cloud-core (~&gt; 1.2)
      googleauth (~&gt; 0.6.2)
    google-cloud-spanner (1.4.0)
      concurrent-ruby (~&gt; 1.0)
      google-cloud-core (~&gt; 1.2)
      google-gax (~&gt; 1.0)
      grpc-google-iam-v1 (~&gt; 0.6.9)
    google-cloud-speech (0.29.0)
      google-cloud-core (~&gt; 1.2)
      google-gax (~&gt; 1.0)
    google-cloud-storage (1.12.0)
      digest-crc (~&gt; 0.4)
      google-api-client (~&gt; 0.19.0)
      google-cloud-core (~&gt; 1.2)
      googleauth (~&gt; 0.6.2)
    google-cloud-trace (0.33.0)
      google-cloud-core (~&gt; 1.2)
      google-gax (~&gt; 1.0)
      stackdriver-core (~&gt; 1.3)
    google-cloud-translate (1.2.0)
      faraday (~&gt; 0.13)
      google-cloud-core (~&gt; 1.2)
      googleauth (~&gt; 0.6.2)
    google-cloud-video_intelligence (1.0.0)
      google-gax (~&gt; 1.0)
    google-cloud-vision (0.28.0)
      google-cloud-core (~&gt; 1.2)
      google-gax (~&gt; 1.0)
    google-gax (1.0.1)
      google-protobuf (~&gt; 3.2)
      googleapis-common-protos (&gt;= 1.3.5, &lt; 2.0)
      googleauth (~&gt; 0.6.2)
      grpc (&gt;= 1.7.2, &lt; 2.0)
      rly (~&gt; 0.2.3)
    google-protobuf (3.5.1.2-x64-mingw32)
    googleapis-common-protos (1.3.7)
      google-protobuf (~&gt; 3.0)
      googleapis-common-protos-types (~&gt; 1.0)
      grpc (~&gt; 1.0)
    googleapis-common-protos-types (1.0.1)
      google-protobuf (~&gt; 3.0)
    googleauth (0.6.2)
      faraday (~&gt; 0.12)
      jwt (&gt;= 1.4, &lt; 3.0)
      logging (~&gt; 2.0)
      memoist (~&gt; 0.12)
      multi_json (~&gt; 1.11)
      os (~&gt; 0.9)
      signet (~&gt; 0.7)
    grpc (1.12.0-x64-mingw32)
      google-protobuf (~&gt; 3.1)
      googleapis-common-protos-types (~&gt; 1.0.0)
      googleauth (&gt;= 0.5.1, &lt; 0.7)
    grpc-google-iam-v1 (0.6.9)
      googleapis-common-protos (&gt;= 1.3.1, &lt; 2.0)
      grpc (~&gt; 1.0)
    gssapi (1.2.0)
      ffi (&gt;= 1.0.1)
    gyoku (1.3.1)
      builder (&gt;= 2.1.2)
    hashie (3.5.7)
    htmlentities (4.3.4)
    http-cookie (1.0.3)
      domain_name (~&gt; 0.5)
    httpclient (2.8.3)
    inifile (3.0.0)
    inspec (2.2.10)
      addressable (~&gt; 2.4)
      faraday (&gt;= 0.9.0)
      faraday_middleware (~&gt; 0.12.2)
      hashie (~&gt; 3.4)
      htmlentities
      json (&gt;= 1.8, &lt; 3.0)
      method_source (~&gt; 0.8)
      mixlib-log
      parallel (~&gt; 1.9)
      parslet (~&gt; 1.5)
      pry (~&gt; 0)
      rspec (~&gt; 3)
      rspec-its (~&gt; 1.2)
      rubyzip (~&gt; 1.1)
      semverse
      sslshake (~&gt; 1.2)
      thor (~&gt; 0.20)
      tomlrb (~&gt; 1.2)
      train (~&gt; 1.4.11)
    jmespath (1.4.0)
    json (2.1.0)
    jwt (2.1.0)
    kitchen-inspec (0.23.1)
      hashie (~&gt; 3.4)
      inspec (&gt;= 0.34.0, &lt; 3.0.0)
      test-kitchen (~&gt; 1.6)
    kitchen-terraform (3.3.1)
      dry-types (~&gt; 0.9)
      dry-validation (~&gt; 0.10)
      kitchen-inspec (~&gt; 0.18)
      mixlib-shellout (~&gt; 2.2)
      test-kitchen (~&gt; 1.16)
    little-plugger (1.1.4)
    logging (2.2.2)
      little-plugger (~&gt; 1.1)
      multi_json (~&gt; 1.10)
    memoist (0.16.0)
    method_source (0.9.0)
    mime-types (3.1)
      mime-types-data (~&gt; 3.2015)
    mime-types-data (3.2016.0521)
    mixlib-install (3.10.0)
      mixlib-shellout
      mixlib-versioning
      thor
    mixlib-log (2.0.4)
    mixlib-shellout (2.3.2-universal-mingw32)
      win32-process (~&gt; 0.8.2)
      wmi-lite (~&gt; 1.0)
    mixlib-versioning (1.2.2)
    ms_rest (0.7.2)
      concurrent-ruby (~&gt; 1.0)
      faraday (~&gt; 0.9)
      timeliness (~&gt; 0.3)
    ms_rest_azure (0.10.8)
      concurrent-ruby (~&gt; 1.0)
      faraday (~&gt; 0.9)
      faraday-cookie_jar (~&gt; 0.0.6)
      ms_rest (~&gt; 0.7.2)
    multi_json (1.13.1)
    multipart-post (2.0.0)
    net-scp (1.2.1)
      net-ssh (&gt;= 2.6.5)
    net-ssh (4.2.0)
    net-ssh-gateway (1.3.0)
      net-ssh (&gt;= 2.6.5)
    nori (2.6.0)
    os (0.9.6)
    parallel (1.12.1)
    parslet (1.8.2)
    pry (0.11.3)
      coderay (~&gt; 1.1.0)
      method_source (~&gt; 0.9.0)
    public_suffix (3.0.2)
    representable (3.0.4)
      declarative (&lt; 0.1.0)
      declarative-option (&lt; 0.2.0)
      uber (&lt; 0.2.0)
    retriable (3.1.2)
    rly (0.2.3)
    rspec (3.7.0)
      rspec-core (~&gt; 3.7.0)
      rspec-expectations (~&gt; 3.7.0)
      rspec-mocks (~&gt; 3.7.0)
    rspec-core (3.7.1)
      rspec-support (~&gt; 3.7.0)
    rspec-expectations (3.7.0)
      diff-lcs (&gt;= 1.2.0, &lt; 2.0)
      rspec-support (~&gt; 3.7.0)
    rspec-its (1.2.0)
      rspec-core (&gt;= 3.0.0)
      rspec-expectations (&gt;= 3.0.0)
    rspec-mocks (3.7.0)
      diff-lcs (&gt;= 1.2.0, &lt; 2.0)
      rspec-support (~&gt; 3.7.0)
    rspec-support (3.7.1)
    rubyntlm (0.6.2)
    rubyzip (1.2.1)
    semverse (2.0.0)
    signet (0.8.1)
      addressable (~&gt; 2.3)
      faraday (~&gt; 0.9)
      jwt (&gt;= 1.5, &lt; 3.0)
      multi_json (~&gt; 1.10)
    sslshake (1.2.0)
    stackdriver-core (1.3.0)
      google-cloud-core (~&gt; 1.2)
    test-kitchen (1.21.2)
      mixlib-install (~&gt; 3.6)
      mixlib-shellout (&gt;= 1.2, &lt; 3.0)
      net-scp (~&gt; 1.1)
      net-ssh (&gt;= 2.9, &lt; 5.0)
      net-ssh-gateway (~&gt; 1.2)
      thor (~&gt; 0.19)
      winrm (~&gt; 2.0)
      winrm-elevated (~&gt; 1.0)
      winrm-fs (~&gt; 1.1)
    thor (0.20.0)
    timeliness (0.3.8)
    tomlrb (1.2.6)
    train (1.4.11)
      aws-sdk (~&gt; 2)
      azure_mgmt_resources (~&gt; 0.15)
      docker-api (~&gt; 1.26)
      google-api-client (~&gt; 0.19.8)
      google-cloud (~&gt; 0.51.1)
      googleauth (~&gt; 0.6.2)
      inifile
      json (&gt;= 1.8, &lt; 3.0)
      mixlib-shellout (~&gt; 2.0)
      net-scp (~&gt; 1.2)
      net-ssh (&gt;= 2.9, &lt; 5.0)
      winrm (~&gt; 2.0)
      winrm-fs (~&gt; 1.0)
    uber (0.1.0)
    unf (0.1.4)
      unf_ext
    unf_ext (0.0.7.5-x64-mingw32)
    win32-process (0.8.3)
      ffi (&gt;= 1.0.0)
    winrm (2.2.3)
      builder (&gt;= 2.1.2)
      erubis (~&gt; 2.7)
      gssapi (~&gt; 1.2)
      gyoku (~&gt; 1.0)
      httpclient (~&gt; 2.2, &gt;= 2.2.0.2)
      logging (&gt;= 1.6.1, &lt; 3.0)
      nori (~&gt; 2.0)
      rubyntlm (~&gt; 0.6.0, &gt;= 0.6.1)
    winrm-elevated (1.1.0)
      winrm (~&gt; 2.0)
      winrm-fs (~&gt; 1.0)
    winrm-fs (1.2.0)
      erubis (~&gt; 2.7)
      logging (&gt;= 1.6.1, &lt; 3.0)
      rubyzip (~&gt; 1.1)
      winrm (~&gt; 2.0)
    wmi-lite (1.0.0)
    zonefile (1.06)

PLATFORMS
  x64-mingw32

DEPENDENCIES
  kitchen-terraform

BUNDLED WITH
   1.16.2
```
</code></pre>

<p>Bundle Doctor:</p>

<pre><code>The Gemfile's dependencies are satisfied
No issues found with the installed bundle
</code></pre>

<p>Alright, awesome everything is clearly installed, so lets try this.</p>

<p><strong>Bundle exec kitchen test</strong>: </p>

<pre><code>Could not find aws-sdk-core-2.11.65 in any of the sources
Run `bundle install` to install missing gems.
</code></pre>

<p>How can this be, it was clearly installed when we ran bundle install and we can see that in the bundle env above. </p>

<p>Bundle info aws-sdk-core:</p>

<pre><code>  * aws-sdk-core (2.11.65)
        Summary: AWS SDK for Ruby - Core
        Homepage: http://github.com/aws/aws-sdk-ruby
        Path: C:/Ruby25-x64/lib/ruby/gems/2.5.0/gems/aws-sdk-core-2.11.65
</code></pre>

<p>Bundle show aws-sdk-core:</p>

<pre><code>C:/Ruby25-x64/lib/ruby/gems/2.5.0/gems/aws-sdk-core-2.11.65
</code></pre>",,3,2,,2018-06-11 20:57:06.973 UTC,1,2018-07-06 10:08:04.707 UTC,,,,,39143,1,5,ruby|rubygems|bundle,1188
In Android portrait mode google vision Camera Source preview has padding,49518796,In Android portrait mode google vision Camera Source preview has padding,"<p>I am Using Google vision API and developing BARcodes/QRcodes scanning APP. I am also trying to use Overlay to make the scanning square area. But when I am trying to execute I am getting some margin space at right side of the screen, though I provided only Match-parent for both height and weight. Sometimes while rotating the screen looks fine but the camera focus is somewhat Elongated. </p>

<pre><code>   public CameraSourcePreview(Context context, AttributeSet attrs) {
    super(context, attrs);
    mContext = context;
    mStartRequested = false;
    mSurfaceAvailable = false;

    mSurfaceView = new SurfaceView(context);
    mSurfaceView.getHolder().addCallback(new SurfaceCallback());
    addView(mSurfaceView);
}

@RequiresPermission(Manifest.permission.CAMERA)
public void start(CameraSource cameraSource) throws IOException, SecurityException {
    if (cameraSource == null) {
        stop();
    }

    mCameraSource = cameraSource;

    if (mCameraSource != null) {
        mStartRequested = true;
        startIfReady();
    }
}

@RequiresPermission(Manifest.permission.CAMERA)
public void start(CameraSource cameraSource, GraphicOverlay overlay) throws IOException, SecurityException {
    mOverlay = overlay;
    start(cameraSource);
}

public void stop() {
    if (mCameraSource != null) {
        mCameraSource.stop();
    }
}

public void release() {
    if (mCameraSource != null) {
        mCameraSource.release();
        mCameraSource = null;
    }
}

@RequiresPermission(Manifest.permission.CAMERA)
private void startIfReady() throws IOException, SecurityException {
    if (mStartRequested &amp;&amp; mSurfaceAvailable) {
        mCameraSource.start(mSurfaceView.getHolder());
        if (mOverlay != null) {
            Size size = mCameraSource.getPreviewSize();
            int min = Math.min(size.getWidth(), size.getHeight());
            int max = Math.max(size.getWidth(), size.getHeight());
            if (isPortraitMode()) {
                // Swap width and height sizes when in portrait, since it will be rotated by
                // 90 degrees
                mOverlay.setCameraInfo(min, max, mCameraSource.getCameraFacing());
            } else {
                mOverlay.setCameraInfo(max, min, mCameraSource.getCameraFacing());
            }
            mOverlay.clear();
        }
        mStartRequested = false;
   }
}

private class SurfaceCallback implements SurfaceHolder.Callback {
    @Override
    public void surfaceCreated(SurfaceHolder surface) {
        mSurfaceAvailable = true;
        try {
            startIfReady();
        } catch (SecurityException se) {
            Log.e(TAG, ""Do not have permission to start the camera"", se);
        } catch (IOException e) {
            Log.e(TAG, ""Could not start camera source."", e);
        }
    }

    @Override
    public void surfaceDestroyed(SurfaceHolder surface) {
        mSurfaceAvailable = false;
    }

    @Override
    public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {
    }
}

@Override
protected void onLayout (boolean changed, int left, int top, int right, int bottom)
{
    int width = getWidth();
    int height = getHeight();

    if (mCameraSource != null) {
        Size size = mCameraSource.getPreviewSize();
        if (size != null) {
            width = size.getWidth();
            height = size.getHeight();
        }
    }

    int layoutWidth = right - left;
    int layoutHeight = bottom - top;

    // Swap width and height sizes when in portrait, since it will be rotated 90 degrees
    if (isPortraitMode()) {
        int tmp = width;
        //noinspection SuspiciousNameCombination
        width = height;
        height = tmp;
    }

    int childWidth = layoutWidth;
    int childHeight = (int) (((float) layoutWidth / (float) width) * height);
  if (isPortraitMode()) {

        childHeight = layoutHeight;
        childWidth = (int) (((float) layoutHeight / (float) height) * width);
    }

    for (int i = 0; i &lt; getChildCount(); ++i) {
      getChildAt(i).layout(0, 0, childWidth, childHeight);
    }

    try {
        startIfReady();
    } catch (SecurityException se) {
        Log.e(TAG, ""Do not have permission to start the camera"", se);
    } catch (IOException e) {
        Log.e(TAG, ""Could not start camera source."", e);
    }
}

/*@Override
protected void onLayout(boolean changed, int left, int top, int right, int bottom) {
    int width = 320;
    int height = 240;
    if (mCameraSource != null) {
        Size size = mCameraSource.getPreviewSize();
        if (size != null) {
            width = size.getWidth();
            height = size.getHeight();
        }
    }

    // Swap width and height sizes when in portrait, since it will be rotated 90 degrees
 if (isPortraitMode()) {
        int tmp = width;
        //noinspection SuspiciousNameCombination
        width = height;
        height = tmp;
    }

    final int layoutWidth = right - left;
    final int layoutHeight = bottom - top;

    // Computes height and width for potentially doing fit width.
    int childWidth = layoutWidth;
    int childHeight = (int) (((float) layoutWidth / (float) width) * height);

    // If height is too tall using fit width, does fit height instead.
    if (childHeight &gt; layoutHeight) {
        childHeight = layoutHeight;
        childWidth = (int) (((float) layoutHeight / (float) height) * width);
    }

    for (int i = 0; i &lt; getChildCount(); ++i) {
        getChildAt(i).layout(0, 0, childWidth, childHeight);
    }

    try {
        startIfReady();
    } catch (SecurityException se) {
        Log.e(TAG, ""Do not have permission to start the camera"", se);
    } catch (IOException e) {
        Log.e(TAG, ""Could not start camera source."", e);
    }
}*/

private boolean isPortraitMode() {
    int orientation = mContext.getResources().getConfiguration().orientation;
    if (orientation == Configuration.ORIENTATION_LANDSCAPE) {
        return false;
    }
    if (orientation == Configuration.ORIENTATION_PORTRAIT) {
        return true;
    }

    Log.d(TAG, ""isPortraitMode returning false by default"");
    return false;
}
</code></pre>

<p>}</p>",,0,2,,2018-03-27 17:13:25.900 UTC,,2018-03-28 05:35:42.793 UTC,2018-03-28 05:35:42.793 UTC,,6903105,,6903105,1,0,android|qr-code|barcode-scanner|google-vision|image-scanner,213
Are the Cloud Vision API limits in documentation correct?,36655630,Are the Cloud Vision API limits in documentation correct?,"<p>I cross posted this to the google group for Cloud Vision...
and have added some additional findings.</p>

<p>Here are all of the specifics I believe are relevant:</p>

<ul>
<li>Using VB.NET 2010</li>
<li>Using service account authentication</li>
<li>Limited to .NET 4.0</li>
<li>Using these Google libs: Google.Api  v1.10.0, Google.Apis.Auth v1.10.0, Google.Apis.Vision.v1  v1.12.0.45</li>
<li>Performing Text and Safe Search Analysis </li>
<li>Passing image content in request (not using Google Drive)</li>
</ul>

<p>When just sending through 4 images or so per request, things work as expected... I get responses, and annotations. </p>

<p>If I up the number of images to 8 files per request,   The response back from Execute contains no results..  No errors, no exceptions either.  </p>

<p>Just a Google.Apis.Vision.v1.Data.BatchAnnotateImagesResponse object with zero responses.   Using a network traffic monitoring tool, I see the connection to google vision - and the service returns a 200 server response.  But is otherwise empty.</p>

<p>Further research showed that I'm able to successfully send about a total 1MB of base64 content to the API per overall request  Anything more, I get the odd condition described.</p>

<p>According to the API documentation, the following limits apply to Google Cloud Vision API usage.</p>

<p>I'm not seeing any way I could be breaking the documented limits:    8 files per each request, total way less than 8MB, and no file even close to 4MB.</p>

<p>Any thoughts as to what I might be missing?  Are the documented limitations below correct?</p>

<ul>
<li>MB per image 4 MB </li>
<li>MB per request 8 MB </li>
<li>Requests per second 10 </li>
<li>Requests per feature per day 700,000 </li>
<li>Requests per feature per month 20,000,000 </li>
<li>Images per second 8 </li>
<li>Images per request 16 </li>
</ul>",,2,3,,2016-04-15 19:43:49.327 UTC,,2018-05-15 06:15:37.160 UTC,,,,,6210900,1,2,google-cloud-vision,603
Google Cloud Vision fails at batch annotate images. Getting Netty Shaded ClosedChannelException error,51142340,Google Cloud Vision fails at batch annotate images. Getting Netty Shaded ClosedChannelException error,"<p>I'm using the Java Google Vision API to run through a batch of images to find the Web properties and that's been working fine the last couple months.I'm now getting a channel closed and ClosedChannelException error on the request. I created a new project with just the <a href=""https://cloud.google.com/vision/docs/libraries#client-libraries-install-java"" rel=""nofollow noreferrer"">Google provided example</a> to test and I run into the same issue.</p>

<p>I updated my Maven dependency version to the most current and still get the issue on the original application and the new test project.</p>

<p>It errors on the request here:</p>

<pre><code>BatchAnnotateImagesResponse response = vision.batchAnnotateImages(requests);
</code></pre>

<p>This is the error message I'm getting: </p>

<pre><code>Exception in thread ""main"" com.google.api.gax.rpc.UnknownException: io.grpc.StatusRuntimeException: UNKNOWN: channel closed
    at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:47)
    at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:72)
    at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:60)
    at com.google.api.gax.grpc.GrpcExceptionCallable$ExceptionTransformingFuture.onFailure(GrpcExceptionCallable.java:95)
    at com.google.api.core.ApiFutures$1.onFailure(ApiFutures.java:61)
    at com.google.common.util.concurrent.Futures$4.run(Futures.java:1123)
    at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:435)
    at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:900)
    at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:811)
    at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:675)
    at io.grpc.stub.ClientCalls$GrpcFuture.setException(ClientCalls.java:492)
    at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:467)
    at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:41)
    at io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:684)
    at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:41)
    at io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:391)
    at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:475)
    at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:557)
    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:478)
    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:590)
    at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
    at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
    at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
    at java.util.concurrent.FutureTask.run(Unknown Source)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(Unknown Source)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: io.grpc.StatusRuntimeException: UNKNOWN: channel closed
    at io.grpc.Status.asRuntimeException(Status.java:526)
    ... 19 more
Caused by: java.nio.channels.ClosedChannelException
    at io.grpc.netty.shaded.io.grpc.netty.Utils.statusFromThrowable(Utils.java:161)
    at io.grpc.netty.shaded.io.grpc.netty.NettyClientTransport$5.operationComplete(NettyClientTransport.java:254)
    at io.grpc.netty.shaded.io.grpc.netty.NettyClientTransport$5.operationComplete(NettyClientTransport.java:248)
    at io.grpc.netty.shaded.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)
    at io.grpc.netty.shaded.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481)
    at io.grpc.netty.shaded.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)
    at io.grpc.netty.shaded.io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:113)
    at io.grpc.netty.shaded.io.netty.channel.DefaultChannelPromise.setFailure(DefaultChannelPromise.java:87)
    at io.grpc.netty.shaded.io.grpc.netty.ProtocolNegotiators$AbstractBufferingHandler.fail(ProtocolNegotiators.java:558)
    at io.grpc.netty.shaded.io.grpc.netty.ProtocolNegotiators$BufferUntilTlsNegotiatedHandler.userEventTriggered(ProtocolNegotiators.java:652)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:329)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:315)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.fireUserEventTriggered(AbstractChannelHandlerContext.java:307)
    at io.grpc.netty.shaded.io.netty.handler.ssl.SslUtils.notifyHandshakeFailure(SslUtils.java:317)
    at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.notifyHandshakeFailure(SslHandler.java:1533)
    at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.setHandshakeFailure(SslHandler.java:1518)
    at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1004)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)
    at io.grpc.netty.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1354)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)
    at io.grpc.netty.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:917)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:822)
    at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
    at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
    at io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
    at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
    at io.grpc.netty.shaded.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
    ... 1 more
Caused by: java.nio.channels.ClosedChannelException
    at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.channelInactive(...)(Unknown Source)
</code></pre>",,1,0,,2018-07-02 19:05:33.003 UTC,,2018-10-10 20:23:31.363 UTC,,,,,7064840,1,0,java|netty|google-cloud-vision,135
Android(Mobile) vision camera brightness is very low,38147675,Android(Mobile) vision camera brightness is very low,"<p>I have integrated google vision in my project as shown in below post:
<a href=""http://code.tutsplus.com/tutorials/reading-qr-codes-using-the-mobile-vision-api--cms-24680"" rel=""noreferrer"">http://code.tutsplus.com/tutorials/reading-qr-codes-using-the-mobile-vision-api--cms-24680</a></p>

<p>Everything looks fine except the camera view brightness . The camera view here is very dark when comparing with my actual android camera app.</p>

<p>Please let me know if i can increase the brightness of the camera and turn on any low light settings. Thanks .</p>

<p>Pictures : 
<a href=""http://i.stack.imgur.com/QHqIL.png"" rel=""noreferrer"">Camera picture</a>
,
<a href=""http://i.stack.imgur.com/vOSEX.png"" rel=""noreferrer"">App Camera view picture</a></p>",45150651,1,1,,2016-07-01 14:14:33.487 UTC,,2017-07-17 17:53:39.960 UTC,2016-07-01 15:39:20.050 UTC,,2912386,,2912386,1,7,android|camera|android-camera|android-vision,913
Google Play Services Crashes on Android 2.3 Vision API,32458351,Google Play Services Crashes on Android 2.3 Vision API,"<p>I'm using the Android Vision API for a QR-code-scanner which works fine on several Android devices and versions, except for Android 2.3.x devices. When I open the QR-code-scanner a dialog is shown that the google-play-services crashes. In the Logcat I see the following error.</p>

<pre><code>FATAL EXCEPTION: AsyncOperationService[VisionDependencyIntentService]
java.lang.NoSuchMethodError: android.content.SharedPreferences.getStringSet
        at com.google.android.gms.vision.service.VisionDependencyIntentService.b(SourceFile:185)
        at com.google.android.gms.vision.service.VisionDependencyIntentService.a(SourceFile:174)
        at com.google.android.gms.vision.service.a.a.a(SourceFile:45)
        at com.google.android.gms.chimera.f.run(SourceFile:179)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1088)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:581)
        at java.lang.Thread.run(Thread.java:1019)
</code></pre>

<p>This is the code of my fragment</p>

<pre><code>@Override
public void onViewCreated(View view, Bundle savedInstanceState)
{
  super.onViewCreated(view, savedInstanceState);

  Activity activity = getActivity();

  preview = (CameraSourcePreview) view.findViewById(R.id.preview);

  int connectionResult = GoogleApiAvailability.getInstance().isGooglePlayServicesAvailable(activity);
  if (connectionResult == ConnectionResult.SUCCESS)
  {
     // create a barcode detector.
     BarcodeDetector barcodeDetector = new BarcodeDetector.Builder(activity).setBarcodeFormats(Barcode.QR_CODE)
           .build();

     // create a processor to filter qr-codes and a tracker to handle the selected qr-code.
     barcodeDetector.setProcessor(new QrCodeProcessor(barcodeDetector, new QrCodeTracker(this)));

     if (barcodeDetector.isOperational())
     {
        // Creates and starts the camera.
        cameraSource = new CameraSource.Builder(activity, barcodeDetector)
              .setFacing(CameraSource.CAMERA_FACING_BACK).setRequestedPreviewSize(1600, 1024)
              .setRequestedFps(15.0f).build();
     }
     else
     {
        showAlert(R.string.QrCodeScanner_alert_play_services_not_operational_header,
              R.string.QrCodeScanner_alert_play_services_not_operational_body);
     }
  }
  else
  {
     PlatformUtil.handlePlayServicesError(activity, connectionResult);
  }
}
</code></pre>

<p>according to the Google Play Services guide Android 2.3 should be supported:
<a href=""https://developers.google.com/android/guides/setup"" rel=""nofollow noreferrer"">https://developers.google.com/android/guides/setup</a></p>

<p>I also tried freeing up some space and did a factory reset as suggested in: <a href=""https://stackoverflow.com/questions/32099530/google-vision-barcode-library-not-found/32100941#32100941"">Google Vision barcode library not found</a> without any succes. </p>

<p>Does anybody know what i'm doing wrong?</p>",32458544,1,1,,2015-09-08 12:50:48.303 UTC,,2015-09-08 13:00:50.200 UTC,2017-05-23 12:06:40.913 UTC,,-1,,2212770,1,2,android|google-play-services|android-2.3-gingerbread|google-vision|android-vision,731
str object has no attribute batch annotate images,56326059,str object has no attribute batch annotate images,"<p>I want to extract text from the image and using google vision API but getting an error ""str object has no attribute batch annotate images"".</p>

<pre><code>import io
import os

# Imports the Google Cloud client library
from google.cloud import vision
from google.cloud.vision import types

# Instantiates a client
client = vision.ImageAnnotatorClient('OCR and voice-bd78adad8bd9.json')

# The name of the image file to annotate
file_name = 'b1.jpg'

# Loads the image into memory
with io.open(file_name, 'rb') as image_file:
    content = image_file.read()

image = types.Image(content=content)

# Performs label detection on the image file
response = client.label_detection(image=image)
labels = response.label_annotations

print('Labels:')
for label in labels:
    print(label.description)
</code></pre>

<p>The error showing to me is the following</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-1-c65657163dd3&gt; in &lt;module&gt;
     19 
     20 # Performs label detection on the image file
---&gt; 21 response = client.label_detection(image=image)
     22 labels = response.label_annotations
     23 

~\Anaconda3\lib\site-packages\google\cloud\vision_helpers\decorators.py in inner(self, image, max_results, retry, timeout, **kwargs)
     99             copied_features[""max_results""] = max_results
    100         request = dict(image=image, features=[copied_features], **kwargs)
--&gt; 101         response = self.annotate_image(request, retry=retry, timeout=timeout)
    102         return response
    103 

~\Anaconda3\lib\site-packages\google\cloud\vision_helpers\__init__.py in annotate_image(self, request, retry, timeout)
     70         # of them.
     71         protobuf.setdefault(request, ""features"", self._get_all_features())
---&gt; 72         r = self.batch_annotate_images([request], retry=retry, timeout=timeout)
     73         return r.responses[0]
     74 

~\Anaconda3\lib\site-packages\google\cloud\vision_v1\gapic\image_annotator_client.py in batch_annotate_images(self, requests, retry, timeout, metadata)
    224                 ""batch_annotate_images""
    225             ] = google.api_core.gapic_v1.method.wrap_method(
--&gt; 226                 self.transport.batch_annotate_images,
    227                 default_retry=self._method_configs[""BatchAnnotateImages""].retry,
    228                 default_timeout=self._method_configs[""BatchAnnotateImages""].timeout,

AttributeError: 'str' object has no attribute 'batch_annotate_images'
</code></pre>

<p>I will be good if you could provide me with some source link where I could learn more about the GOOGLE API</p>",,1,0,,2019-05-27 12:26:21.260 UTC,,2019-05-31 01:53:01.843 UTC,2019-05-29 09:05:16.167 UTC,,7691943,,7691943,1,0,python-3.x|google-cloud-vision,28
Amazon Rekognition API - S3 MetaData Issue,41523004,Amazon Rekognition API - S3 MetaData Issue,"<p>I am trying to detect faces in an image using AWS Image Rekognition API. But getting the following Error: </p>

<p>Error1: </p>

<pre><code>ClientError: An error occurred (InvalidS3ObjectException) when calling the DetectFaces operation: Unable to get image metadata from S3.  Check object key, region and/or access permissions.
</code></pre>

<p>Python Code1: </p>

<pre><code>def detect_faces(object_name=""path/to/image/001.jpg""):
    client = get_aws_client('rekognition')

    response = client.detect_faces(
        Image={
            # 'Bytes': source_bytes,
            'S3Object': {
                'Bucket': ""bucket-name"",
                'Name': object_name,
                'Version': 'string'
            }
        },
        Attributes=[
            'ALL',
        ]
    )

    return response
</code></pre>

<p>The Object ""path/to/image/001.jpg"" exists in the AWS S3 Bucket ""bucket-name"". And the region Name is also correct. </p>

<p>The Permissions for this object '001.jpg' is: Everyone is granted Open/Download/view Permission.
MetaData for the Object: Content-Type: image/jpeg</p>

<p>Not sure how to debug this. Any Suggestion to resolve this please ?</p>

<p>Thanks,</p>",41524598,2,0,,2017-01-07 15:22:46.733 UTC,,2018-06-07 18:30:14.630 UTC,2017-01-08 02:07:55.107 UTC,,4804716,,4804716,1,4,python|amazon-web-services|amazon-s3|metadata,1865
how to store personal information for an image using amazon rekognition,52393200,how to store personal information for an image using amazon rekognition,"<p>i am trying to create a app which makes use of amazon rekogition in aws for identification of a person and retrieving the personal information for an internal storage system.i wanted to know how to connect the amazon rekognition part
and the information stored in the database.The face detection  part will be done by amazon rekognition but how will store and retrieve the personal information after detection of face </p>",,1,1,,2018-09-18 19:13:04.443 UTC,,2018-09-21 21:17:49.003 UTC,,,,,8319130,1,0,amazon-rekognition,34
Extract data from HTTP_Request2_Response object using PHP,49134014,Extract data from HTTP_Request2_Response object using PHP,"<p>I am currently working on the response from the <a href=""https://docs.microsoft.com/en-us/azure/cognitive-services/emotion/quickstarts/php"" rel=""nofollow noreferrer"">Microsoft Emotion API</a>. In the API, <code>HTTP/Request2.php</code> is used and I successfully get the HTTP response message by using <code>getBody()</code> in HTTP_Request2_Response<a href=""http://pear.php.net/manual/en/package.http.http-request2.response.php"" rel=""nofollow noreferrer"">(Manual here)</a>. 
Just like the API manual, I do the following:</p>

<pre><code>try
{
    $response = $request-&gt;send();
    echo $response-&gt;getBody();
}
</code></pre>

<p>What I got is like this:</p>

<pre><code>[
  {
    ""faceRectangle"": {
      ""height"":264,
      ""left"":162,
      ""top"":523,
      ""width"":264,

    },
    ""scores"": {
      ""anger"":1.38974974E-06,
      ""contempt"":2.75673688E-08,
      ""disgust"":3.75520244E-06,
      ""fear"":5.69216375E-11,
      ""happiness"":0.999994636,
      ""neutral"":1.77765841E-07,
      ""sadness"":3.03275627E-09,
""surprise"":2.25669652E-08
    }
  }
]
</code></pre>

<p>However, I only want some aspect of the ""scores"", like ""anger"" or ""happiness"". I tried to use json_decode on <code>$response</code> but I got an error:<code>json_decode() expects parameter 1 to be string, object given</code>. It seems that HTTP_Request2_Response provide me an object. How do I extract the data I want from the response?</p>

<p>Here is the <code>var_dump</code> of <code>$response</code> for your reference:</p>

<pre><code>object(HTTP_Request2_Response)#5 (9) { 
[""version"":protected]=&gt; string(3) ""1.1"" 
[""code"":protected]=&gt; int(200) 
[""reasonPhrase"":protected]=&gt; string(2) ""OK"" 
[""effectiveUrl"":protected]=&gt; string(65) ""https://westus.api.cognitive.microsoft.com/emotion/v1.0/recognize"" 
[""headers"":protected]=&gt; array(10) { 
[""cache-control""]=&gt; string(8) ""no-cache"" 
[""pragma""]=&gt; string(8) ""no-cache"" 
[""content-length""]=&gt; string(3) ""274"" 
[""content-type""]=&gt; string(31) ""application/json; charset=utf-8"" 
[""expires""]=&gt; string(2) ""-1"" 
[""x-powered-by""]=&gt; string(7) ""ASP.NET"" 
[""apim-request-id""]=&gt; string(36) ""96fgh4z1-62z0-43r3-82z7-8823hdg5g86d"" 
[""strict-transport-security""]=&gt; string(44) ""max-age=31536000; includeSubDomains; preload"" 
[""x-content-type-options""]=&gt; string(7) ""nosniff"" 
[""date""]=&gt; string(29) ""Tue, 06 Mar 2018 14:55:20 GMT"" } 
[""cookies"":protected]=&gt; array(0) { } 
[""lastHeader"":protected]=&gt; string(4) ""date"" 
[""body"":protected]=&gt; string(274) ""
[{""faceRectangle"":{""height"":264,""left"":162,""top"":523,""width"":264},
""scores"":{""anger"":1.38974974E-06,""contempt"":2.75673688E-08,""disgust"":3.75520244E-06,""fear"":5.69216375E-11,""happiness"":0.999994636,""neutral"":1.77765841E-07,""sadness"":3.03275627E-09,""surprise"":2.25669652E-08}}]"" 
[""bodyEncoded"":protected]=&gt; bool(true) 
</code></pre>",49134637,2,0,,2018-03-06 15:08:47.697 UTC,,2018-03-06 15:38:27.663 UTC,,,,,9284327,1,0,php|json|api|pear|http-request2,154
An example of calling AWS Rekognition HTTP API from Python,41388926,An example of calling AWS Rekognition HTTP API from Python,"<p>I'd like to try <a href=""http://docs.aws.amazon.com/rekognition/latest/dg/API_CompareFaces.html"" rel=""noreferrer"">Rekognition's CompareFaces</a>, but I don't see a full example of the syntax for using the HTTP API. Assuming I have two images, how would I call this API from Python to retrieve a similarity score?</p>",41390797,1,1,,2016-12-30 00:07:57.880 UTC,6,2016-12-31 23:34:10.810 UTC,,,,,1084865,1,9,python|amazon-web-services|amazon-rekognition,5961
Access denied due to invalid subscription key (Face API),42123633,Access denied due to invalid subscription key (Face API),"<p>I am having trouble using Microsoft Face API. Below is my sample request:</p>

<pre><code>curl -v -X POST ""https://westus.api.cognitive.microsoft.com/face/v1.0/detect?returnFaceId=true&amp;returnFaceLandmarks=false&amp;returnFaceAttributes=age,gender"" -H ""Content-Type: application/json"" -H ""Ocp-Apim-Subscription-Key: 1xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxd"" --data-ascii ""{\""url\"":\""http://www.mrbeantvseries.co.uk/bean3.jpg\""}""
</code></pre>

<p>I use the subscription id from my cognitive services account and I got below response:</p>

<pre><code>{
  ""error"": {
    ""code"": ""Unspecified"",
    ""message"": ""Access denied due to invalid subscription key. Make sure you are subscribed to an API you are trying to call and provide the right key.""
  }
}
</code></pre>

<p>Not sure if I've missed out anything there. Can someone help me on this? Very much appreciated.</p>",42127319,5,1,,2017-02-08 21:08:41.743 UTC,1,2018-12-20 12:56:37.737 UTC,,,,,4029205,1,11,face-detection|microsoft-cognitive|face-api,10343
Calling Azure face detect api from NodeJs with binary image data coming from Postman client,50598790,Calling Azure face detect api from NodeJs with binary image data coming from Postman client,"<p>I am trying to build a REST middle-ware in nodejs which will call azure face api's, like the picture shown below. <a href=""https://i.stack.imgur.com/TjqX1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TjqX1.png"" alt=""enter image description here""></a></p>

<p>When I call my node js endpoint for face detection with required data and an image file then the I successfully receive the request and the binary data of the image in request.body.</p>

<p>Since I received the request in my middle-ware  now I am supposed to call Azure face detection end point with the received data from my node js middle-ware.
[now here is the problem]  </p>

<p>//---------------call made from node js to azure face detection api----</p>

<pre><code>Request.post({
    ""headers"": {
      ""content-type"": reqBinaryContentType,     
      ""Ocp-Apim-Subscription-Key"": azureSubscriptionKey,     
      'Content-Length': req.headers['content-length']
    },  

      ""body"":req.body,
  }, (error, response, body) =&gt; {
    if (error) {     
      return console.dir(error);
    }   
    res.send(JSON.parse(body))
  })
</code></pre>

<p>//---------------</p>

<p>Error:
<code>{""error"":{""code"":""InvalidImageSize"",""message"":""Image size is too small.""}}</code></p>

<p>Thanks</p>",,0,2,,2018-05-30 07:29:15.010 UTC,,2018-05-30 08:49:35.203 UTC,2018-05-30 08:49:35.203 UTC,,9455659,,1733765,1,0,node.js|azure|azure-cognitive-services|face-api,227
Image resize/compression with CameraSource.takePicture,52539984,Image resize/compression with CameraSource.takePicture,"<p>I am a newbie to Android Development, so please excuse if this question has already been answered. I have an application, where i can scan a qr-code and the camera takes a picture and uploads it to a server. That all works fine, but the issue i'm having is with phones that have a high resolution, hence the time out cuts off the connection and i keep getting an error. With some phones i get a size of 180KB and on other phones i get something like 2.9MB. My question is, how can i say, i want a fixed resolution, regardless of the resolution on a certain phone? I have the following code:</p>

<pre><code>cameraSource = new CameraSource
            .Builder(this, barcodeDetector)
            .setRequestedPreviewSize(800, 800)
            .setRequestedFps(30.0f)
            .setAutoFocusEnabled(true)
            .build();

@Override
        public void receiveDetections(Detector.Detections&lt;Barcode&gt; detections) {
            final SparseArray&lt;Barcode&gt; qrcodes = detections.getDetectedItems();
            if (qrcodes.size() != 0 &amp;&amp; !qrfound)
            {
                cameraSource.takePicture(null, jpegCallback);
                qrfound = true;
                codeInfo.post(new Runnable() {
                    @Override
                    public void run() {
                        //Create vibrate
                        Vibrator vibrator = (Vibrator)getApplicationContext().getSystemService(Context.VIBRATOR_SERVICE);
                        vibrator.vibrate(500);
                        codeInfo.setText(qrcodes.valueAt(0).displayValue);
                        hash = qrcodes.valueAt(0).displayValue;

                    }
                });
            }
        }
</code></pre>

<p>The image is converted to a string using base64:</p>

<pre><code>jpegCallback = new CameraSource.PictureCallback() {
        @Override
        public void onPictureTaken(byte[] bytes) {

            scannedImage = Base64.encodeToString(bytes, Base64.DEFAULT);
            uploadScannedImage();
        }
    };
</code></pre>

<p>In the above code, i would like something that compresses the image size, but i am unsure how i can do that. I have seen a similar post (<a href=""https://stackoverflow.com/questions/43999829/set-the-camera-resolution-with-google-vision-android"">Set the camera resolution with google vision android</a>), the answer seems plausible, but i want to set the resolution myself, so the size is kept at minimal (maybe upto 1MB). Can someone please help me?</p>",,0,0,,2018-09-27 15:02:54.583 UTC,,2018-09-27 15:06:27.850 UTC,2018-09-27 15:06:27.850 UTC,,2649012,,7451476,1,0,android|android-camera|resolution|image-resizing,36
Error: Property 'onChange' does not exist on type 'ObjectBuilder',51691313,Error: Property 'onChange' does not exist on type 'ObjectBuilder',"<p>I need help with my Ionic typescript code.</p>

<p>This uses a Google Cloud Vision API to tag photos that you upload to Firebase Storage. </p>

<p>My problem: (returns the following error)</p>

<blockquote>
  <p>error TS2339: Property 'onChange' does not exist on type 'ObjectBuilder'. 
  npm ERR! code ELIFECYCLE
  npm ERR! functions@ build: 'tsc'</p>
</blockquote>

<p>My ts file:</p>

<pre><code>import * as functions from 'firebase-functions';

import * as admin from 'firebase-admin';
admin.initializeApp(functions.config().firebase);

import * as vision from '@google-cloud/vision';
const visionClient = new vision.ImageAnnotatorClient();

//may need to delete, use default
const bucketName = 'outsidehax.appspot.com';

export const imageTagger = functions.storage
.bucket(bucketName)
.object()
.onChange( async event =&gt; {
    const object = event.data;
    const filePath = object.name;

    const imageUri = 'gs://'+bucketName+'/'+filePath;

    const docId = filePath.split('.jpg')[0];

    const docRef = admin.firestore().collection('photos').doc(docId);

    const results = await visionClient.labelDetection(imageUri);

    const labels = results[0].labelAnnotations.map(obj =&gt; obj.description);
    const sad = labels.includes('sad');

    return docRef.set({ sad, labels })

});
</code></pre>

<p>This index.ts file is located at AppName/functions/src/index.ts.</p>",,1,0,,2018-08-05 04:09:27.630 UTC,,2018-08-11 01:41:38.457 UTC,2018-08-11 01:41:38.457 UTC,,9983127,,9983127,1,-2,typescript|firebase|google-cloud-firestore,397
Google vision APi labels decrease the labels percentages,46594701,Google vision APi labels decrease the labels percentages,"<p>I would like  to use the Google Vision API for label detection. But I want to decrease the labels percentages and I do not know how I can do this. Could someone help me? I am using a sample. I'm using a sample for android that google makes available
This is the code:</p>

<p><a href=""https://github.com/GoogleCloudPlatform/cloud-vision/blob/master/android/CloudVision/app/src/main/java/com/google/sample/cloudvision/MainActivity.java"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/cloud-vision/blob/master/android/CloudVision/app/src/main/java/com/google/sample/cloudvision/MainActivity.java</a></p>

<p>And this and that aside it displays the results:</p>

<pre><code> // add the features we want
     annotateImageRequest.setFeatures(new ArrayList&lt;Feature&gt;() {{
                            Feature labelDetection = new Feature();
                            labelDetection.setType(""LABEL_DETECTION"");
                            labelDetection.setMaxResults(10);
                            add(labelDetection);
                        }});
</code></pre>",,1,0,,2017-10-05 20:59:51.833 UTC,,2018-04-18 15:49:56.927 UTC,,,,,8728823,1,1,java|android|google-vision|vision-api,67
Google Video Intelligence API query,42761695,Google Video Intelligence API query,"<p>I am trying to implement VideoIntelligence API in my 'personal-project'. but I am not able to do so. 
[I have the access permissions for VideoIntelligence API for my personal-project]</p>

<p>Please provide some suggestions to make it work.</p>

<p>I tried the following commands:</p>

<pre><code>(venv) naveen@naveen:~/Desktop/personal-project$ python manage.py shell
Python 2.7.12 (default, Nov 19 2016, 06:48:10) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
(InteractiveConsole)
&gt;&gt;&gt; from google.cloud.gapic.videointelligence.v1beta1 import video_intelligence_service_client
&gt;&gt;&gt; video_client = video_intelligence_service_client.VideoIntelligenceServiceClient()
&gt;&gt;&gt; path = 'gs://demomaker/google_gmail.mp4'
&gt;&gt;&gt; features = [2]
&gt;&gt;&gt; operation = video_client.annotate_video(path, features)
</code></pre>

<p>But I am getting this as the Error:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;console&gt;"", line 1, in &lt;module&gt;
  File ""personal-project/venv/local/lib/python2.7/site-packages/google/cloud/gapic/videointelligence/v1beta1/video_intelligence_service_client.py"", line 234, in annotate_video
    self._annotate_video(request, options), self.operations_client,
  File ""personal-project/venv/local/lib/python2.7/site-packages/google/gax/api_callable.py"", line 419, in inner
    return api_caller(api_call, this_settings, request)
  File ""personal-project/venv/local/lib/python2.7/site-packages/google/gax/api_callable.py"", line 407, in base_caller
    return api_call(*args)
  File ""personal-project/venv/local/lib/python2.7/site-packages/google/gax/api_callable.py"", line 368, in inner
    return a_func(*args, **kwargs)
  File ""personal-project/venv/local/lib/python2.7/site-packages/google/gax/retry.py"", line 126, in inner
    ' classified as transient', exception)
RetryError: GaxError(Exception occurred in retry method that was not classified as transient, caused by &lt;_Rendezvous of RPC that terminated with (StatusCode.PERMISSION_DENIED, Google Cloud Video Intelligence API has not been used in project usable-auth-library before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/videointelligence.googleapis.com/overview?project=usable-auth-library then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.)&gt;)
</code></pre>

<p>It is searching inside 'usable-auth-library' project. Whereas It should search/use permission for my 'personal-project'.
[since I have access for 'personal-project' and not 'usable-auth-library']</p>

<p>How can I make this work ? Any Suggestions please ?</p>

<p>Thanks</p>",,2,3,,2017-03-13 10:46:28.037 UTC,,2017-04-26 14:09:30.260 UTC,2017-03-18 07:27:51.213 UTC,,4804716,,4804716,1,0,python|google-api|google-api-client|google-api-python-client|video-intelligence-api,534
Emotion Recognition using Google Cloud Vision API?,40503587,Emotion Recognition using Google Cloud Vision API?,"<p>I wish to use Google Cloud Vision API to generate features from images, that I will further use to train my SVM for emotion recognition problem. Please provide a detailed procedure for how to write a script in python that can use Google Cloud Vision API to generate features that I can directly feed into SVM.</p>",40505734,1,0,,2016-11-09 09:29:27.240 UTC,1,2016-11-09 11:19:23.513 UTC,,,,,4923477,1,1,computer-vision|google-cloud-vision,604
Google Vision API request size limitation (text detection),52471113,Google Vision API request size limitation (text detection),"<p>I'm using Google Vision API via curl (image is sent as base64-encoded payload within JSON). I can get correct results back only when my request sent via CURL is under 16k or so. As soon as it's over ~16k I'm getting no response at all:</p>

<p><img src=""https://i.imgur.com/u1a86wi.png"" alt=""no response returned""></p>

<p>Exactly the same request but with a smaller image 
<img src=""https://i.imgur.com/u0czK65.png"" alt=""all good here""></p>

<p>I have added the request over 16k to pastebin:</p>

<pre><code>{
  ""requests"": [
    {
      ""image"": {
           ""content"": ...base64...
 ....
}
</code></pre>

<p>Failing request is here:
<a href=""https://pastebin.com/dl/vL4Ahfw7"" rel=""nofollow noreferrer"">https://pastebin.com/dl/vL4Ahfw7</a></p>

<p>I could only find a 20MB limitation in the docs (<a href=""https://cloud.google.com/vision/docs/supported-files?hl=th"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/supported-files?hl=th</a>) but nothing like the weird issue I have. Thanks.</p>",,0,3,,2018-09-23 23:25:12.800 UTC,,2018-09-23 23:25:12.800 UTC,,,,,1552175,1,0,google-cloud-platform|google-vision,72
Unable to identify people using Amazon Rekognition AWS Java SDK,47571678,Unable to identify people using Amazon Rekognition AWS Java SDK,"<p>Helo everyone,</p>

<p>I am trying to run a face detection on one image based on a collection created from portrait images of few people. the approach used is as below:</p>

<ol>
<li>Create Collection name ""DATABASE""</li>
<li>Index faces from individual pictures and store them in collection ""DATABASE"".</li>
<li>run index faces on target image and store all faces in a separate collection ""toBeDetected"".</li>
<li>Use SearchFaces API call to identify all the faces from the target images against Database collection.</li>
</ol>

<p>however when i try to do that i get invalid parameter exception. I am very new to this and have tried to find the solution to the problem however i have nothing yet. Please help. I have attached the code as below.</p>

<pre><code>public class FRInvoker {

    public static final String COLLECTION_ID_DATABASE = ""collectionDatabase"";
//  public static final String COLLECTION_ID_TARGET = ""toBeDetected"";
public static Map&lt;String, String&gt; names = new HashMap&lt;&gt;(); 
private static AmazonRekognition amazonRekognition;


//Configure Credentials
public FRInvoker() {
    AWSCredentials credentials;
    try {
        credentials = new BasicAWSCredentials(""XXXXXXXXXXXXXXXXXXXXX"", ""XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"");
    } catch (Exception e) {
        throw new AmazonClientException(""Cannot load the credentials from the credential profiles file. ""
                + ""Please make sure that your credentials file is at the correct ""
                + ""location (/Users/userid/.aws/credentials), and is in a valid format."", e);
    }

    amazonRekognition = AmazonRekognitionClientBuilder.standard().withRegion(Regions.US_WEST_2)
            .withCredentials(new AWSStaticCredentialsProvider(credentials)).build();
}



public static void main(String[] args) {
    FRInvoker invoker = new FRInvoker();
    invoker.invokeSystem();
}


private void invokeSystem(){
    AddFacesToRekognitionCollection faceRecognition = new AddFacesToRekognitionCollection(amazonRekognition);
    faceRecognition.addFacesToRecognition(amazonRekognition);

    DetectMultipleFaceHelper detectMultipleFaceHelper = new DetectMultipleFaceHelper(); 
    detectMultipleFaceHelper.detectAllPossibleFaces(amazonRekognition);

    MatchAllFacesInCollection matchFacesInCollection = new MatchAllFacesInCollection();
    matchFacesInCollection.matchAllFacesInTargetCollection(amazonRekognition);
}
}
</code></pre>

<p>RekognitionCollectionCreateHelper</p>

<pre><code>public class RekognitionCollectionCreateHelper {

        public void createCollections(AmazonRekognition amazonRekognition, String collectionName) {
            DeleteCollectionRequest request = new DeleteCollectionRequest().withCollectionId(collectionName);
            amazonRekognition.deleteCollection(request);

            try {
                amazonRekognition.createCollection(new CreateCollectionRequest().withCollectionId(collectionName));
            } catch (com.amazonaws.services.rekognition.model.ResourceAlreadyExistsException e) {
                System.out.println(collectionName + ""Already Exists"");
                System.out.println(""Listing Existing Collections : \n"");
                this.printCollectionList(amazonRekognition);
            }
        }

        private ListCollectionsResult callListCollections(String paginationToken, int limit,
                AmazonRekognition amazonRekognition) {
            ListCollectionsRequest listCollectionsRequest = new ListCollectionsRequest().withMaxResults(limit)
                    .withNextToken(paginationToken);
            return amazonRekognition.listCollections(listCollectionsRequest);
        }

        private void printCollectionList(AmazonRekognition amazonRekognition){
            int limit = 1;
            ListCollectionsResult listCollectionsResult = null;
            String paginationToken = null;
            do {
                if (listCollectionsResult != null) {
                    paginationToken = listCollectionsResult.getNextToken();
                }
                listCollectionsResult = callListCollections(paginationToken, limit, amazonRekognition);

                List&lt;String&gt; collectionIds = listCollectionsResult.getCollectionIds();
                for (String resultId : collectionIds) {
                    System.out.println(resultId);
                }
            } while (listCollectionsResult != null &amp;&amp; listCollectionsResult.getNextToken() != null);
        }

        public void printContentOfCollection(AmazonRekognition amazonRekognition, String collectionName){
            ObjectMapper objectMapper = new ObjectMapper();
            ListFacesResult listFacesResult = null;
              System.out.println(""Faces in collection "" + collectionName);

              String paginationToken = null;
              do {
                 if (listFacesResult != null) {
                    paginationToken = listFacesResult.getNextToken();
                 }

                 ListFacesRequest listFacesRequest = new ListFacesRequest()
                         .withCollectionId(collectionName)
                         .withMaxResults(1)
                         .withNextToken(paginationToken);

                 listFacesResult =  amazonRekognition.listFaces(listFacesRequest);
                 List&lt;Face&gt; faces = listFacesResult.getFaces();
                 for (Face face: faces) {
                    try {
                        System.out.println(objectMapper.writerWithDefaultPrettyPrinter()
                           .writeValueAsString(face));
                    } catch (JsonProcessingException e) {
                        // TODO Auto-generated catch block
                        e.printStackTrace();
                    }
                 }
              } while (listFacesResult != null &amp;&amp; listFacesResult.getNextToken() !=
                 null);
        }

    }
</code></pre>

<p>AddFacesToRekognitionCollection</p>

<pre><code>public AddFacesToRekognitionCollection(AmazonRekognition amazonRekognition) {
            RekognitionCollectionCreateHelper newCollectionCreator = new 
RekognitionCollectionCreateHelper();
    //      newCollectionCreator.deleteAllAwsCollections(amazonRekognition);
            newCollectionCreator.createCollections(amazonRekognition, FRInvoker.COLLECTION_ID_DATABASE);
        }

        public void addFacesToRecognition(AmazonRekognition amazonRekognition) {
            File[] files = getAllImageFiles();
            for (int i = 0; i &lt; files.length; i++) {
                Image image = new 
Image().withBytes(AddFacesToRekognitionCollection.getImageBytes(files[i]));
                String externalImageId = files[i].getName();
                IndexFacesResult indexFacesResult = callIndexFaces(FRInvoker.COLLECTION_ID_DATABASE, externalImageId, ""ALL"", image,
                        amazonRekognition);
                List&lt;FaceRecord&gt; faceRecords = indexFacesResult.getFaceRecords();
                for (FaceRecord faceRecord : faceRecords) {
                    System.out.println(""Image name: "" + files[i].getName() + "" ::::::::: Faceid is "" + faceRecord.getFace().getFaceId());
                    FRInvoker.names.put(faceRecord.getFace().getFaceId(), files[i].getName());
                }
            }

        }


        //Private Helper Methods
        public static ByteBuffer getImageBytes(File file) {
            ByteBuffer imageBytes = null;
            try (InputStream inputStream = new FileInputStream(file)) {
                imageBytes = ByteBuffer.wrap(IOUtils.toByteArray(inputStream));
            } catch (IOException e) {
                e.printStackTrace();
            }
            return imageBytes;
        }

        private IndexFacesResult callIndexFaces(String collectionId, String externalImageId, String attributes, Image image,
                AmazonRekognition amazonRekognition) {
            IndexFacesRequest indexFacesRequest = new IndexFacesRequest().withImage(image).withCollectionId(collectionId);
            return amazonRekognition.indexFaces(indexFacesRequest);

        }

        public static File[] getAllImageFiles() {
            File dir = new File(System.getProperty(""user.dir"") + ""/imageDatabase/"");
            System.out.println(dir.getAbsolutePath());
            File[] files = dir.listFiles(new FilenameFilter() {
                public boolean accept(File dir, String name) {
                    return name.toLowerCase().endsWith("".jpg"") || name.toLowerCase().endsWith("".png"");
                }
            });
            return files;
        }

        //Private Helper Methods
    }
</code></pre>

<p>MatchAllFacesInCollection </p>

<pre><code>public class MatchAllFacesInCollection {

        public void matchAllFacesInTargetCollection(AmazonRekognition amazonRekognition) {

            ListFacesRequest request = new ListFacesRequest().withCollectionId(FRInvoker.COLLECTION_ID_TARGET)
                    .withMaxResults(50);
            ListFacesResult response = amazonRekognition.listFaces(request);
            for (Face face : response.getFaces()) {
                SearchFacesRequest searchFaceRequest = new SearchFacesRequest()
                        .withCollectionId(FRInvoker.COLLECTION_ID_DATABASE).withFaceId(face.getFaceId())
                        .withMaxFaces(1).withFaceMatchThreshold(90f);
                SearchFacesResult searchFaceResponse = null;
                try{
                    searchFaceResponse = amazonRekognition.searchFaces(searchFaceRequest);
                    System.out.println(searchFaceResponse.getFaceMatches().get(0).getFace().getFaceId() + "" matches best with Highest Matching rate of"" + 
                            searchFaceResponse.getFaceMatches().get(0).getSimilarity());
                }catch(com.amazonaws.services.rekognition.model.InvalidParameterException e){
                    e.printStackTrace();
                    System.out.println(""Face Not Found :::::: "" + face.getFaceId());
                }
            }
        }
    }
</code></pre>

<p>DetectMultipleFaceHelper </p>

<pre><code>public class DetectMultipleFaceHelper {

public void detectAllPossibleFaces(AmazonRekognition amazonRekognition) {

    RekognitionCollectionCreateHelper collectionCreaterHelper = new RekognitionCollectionCreateHelper();

    collectionCreaterHelper.createCollections(amazonRekognition, FRInvoker.COLLECTION_ID_TARGET);

    IndexFacesRequest request = new IndexFacesRequest().withCollectionId(FRInvoker.COLLECTION_ID_TARGET)
            .withImage(new Image().withBytes(AddFacesToRekognitionCollection.getImageBytes(new File(System.getProperty(""user.dir"") + ""/ImageToRekognize/target.jpg""))));
    amazonRekognition.indexFaces(request);
}
</code></pre>

<p>}</p>

<blockquote>
  <p>com.amazonaws.services.rekognition.model.InvalidParameterException: faceId was not found in the collection. (Service: AmazonRekognition; Status Code: 400; Error Code: InvalidParameterException; Request ID: e28de8f9-d5b4-11e7-b9db-4fe55f28a54b)
  Face Not Found :::::: 1becc904-b4b8-417a-92bf-7ade964838c0
      at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1638)
      at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1303)
      at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1055)
      at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:743)
      at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717)
      at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)
      at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)
      at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)
      at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)
      at com.amazonaws.services.rekognition.AmazonRekognitionClient.doInvoke(AmazonRekognitionClient.java:1458)
      at com.amazonaws.services.rekognition.AmazonRekognitionClient.invoke(AmazonRekognitionClient.java:1434)
      at com.amazonaws.services.rekognition.AmazonRekognitionClient.executeSearchFaces(AmazonRekognitionClient.java:1309)
      at com.amazonaws.services.rekognition.AmazonRekognitionClient.searchFaces(AmazonRekognitionClient.java:1285)
      at com.siemens.aws.recognition.MatchAllFacesInCollection.matchAllFacesInTargetCollection(MatchAllFacesInCollection.java:23)
      at com.siemens.aws.recognition.FRInvoker.invokeSystem(FRInvoker.java:79)
      at com.siemens.aws.recognition.FRInvoker.main(FRInvoker.java:67)</p>
</blockquote>

<p>Please help. Thank you!</p>",,1,0,,2017-11-30 10:53:48.680 UTC,,2018-06-14 14:50:33.690 UTC,,,,,5659314,1,0,amazon-web-services|amazon-rekognition,394
Error occurs while using Google Cloud Vision Face Detection API,55063440,Error occurs while using Google Cloud Vision Face Detection API,"<p>In my application, I am uploading image to get emotion response from Google Cloud Vision API. But while sending the captured image to cloud, I am getting error as</p>

<p>Error </p>

<pre><code>401-1982/com.emotionrecognition W/System.err: java.io.FileNotFoundException: https://vision.googleapis.com/v1/images:annotate?key=AIzaSyAbr--TMSjNERLbwHB4p6a0EnYvVaFlqus
03-08 16:30:41.588 1401-1982/com.emotionrecognition W/System.err:     at com.android.okhttp.internal.huc.HttpURLConnectionImpl.getInputStream(HttpURLConnectionImpl.java:255)
03-08 16:30:41.588 1401-1982/com.emotionrecognition W/System.err:     at com.android.okhttp.internal.huc.DelegatingHttpsURLConnection.getInputStream(DelegatingHttpsURLConnection.java:210)
03-08 16:30:41.588 1401-1982/com.emotionrecognition W/System.err:     at com.android.okhttp.internal.huc.HttpsURLConnectionImpl.getInputStream(Unknown Source:0)
03-08 16:30:41.588 1401-1982/com.emotionrecognition W/System.err:     at com.emotionrecognition.CloudVision.send(CloudVision.java:81)
03-08 16:30:41.589 1401-1982/com.emotionrecognition W/System.err:     at com.emotionrecognition.DetectedEmotionAnalysis$DetectEmotion.doInBackground(DetectedEmotionAnalysis.java:177)
03-08 16:30:41.589 1401-1982/com.emotionrecognition W/System.err:     at com.emotionrecognition.DetectedEmotionAnalysis$DetectEmotion.doInBackground(DetectedEmotionAnalysis.java:172)
03-08 16:30:41.589 1401-1982/com.emotionrecognition W/System.err:     at android.os.AsyncTask$2.call(AsyncTask.java:333)
03-08 16:30:41.589 1401-1982/com.emotionrecognition W/System.err:     at java.util.concurrent.FutureTask.run(FutureTask.java:266)
03-08 16:30:41.589 1401-1982/com.emotionrecognition W/System.err:     at android.os.AsyncTask$SerialExecutor$1.run(AsyncTask.java:245)
03-08 16:30:41.590 1401-1982/com.emotionrecognition W/System.err:     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)
03-08 16:30:41.590 1401-1982/com.emotionrecognition W/System.err:     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:636)
03-08 16:30:41.590 1401-1982/com.emotionrecognition W/System.err:     at java.lang.Thread.run(Thread.java:764)
03-08 16:30:41.590 1401-1982/com.emotionrecognition D/GCV Error: java.io.FileNotFoundException: https://vision.googleapis.com/v1/images:annotate?key=AIzaSyAbr--TMSjNERLbwHB4p6a0EnYvVaFlqus
03-08 16:30:55.951 1401-1401/com.emotionrecognition V/InputMethodManager: Starting input: tba=android.view.inputmethod.EditorInfo@8a9b4a4 nm : com.emotionrecognition ic=null
03-08 16:30:55.952 1401-1401/com.emotionrecognition I/InputMethodManager: startInputInner - mService.startInputOrWindowGainedFocus
</code></pre>

<p>This is the code i have written to send image to URL,i am not getting response from Google Cloud Vision API ,when i paste that url in web browser i am getting error as 404 file not found</p>

<pre><code> Bitmap bmp = BitmapFactory.decodeFile(imagePath);
            ByteArrayOutputStream stream = new ByteArrayOutputStream();
            bmp.compress(Bitmap.CompressFormat.PNG, 100, stream);
            byte[] imageByteArray = stream.toByteArray();

            String encodedImageString = Base64.encodeToString(imageByteArray, Base64.DEFAULT);

            URL url = new URL(Config.CLOUD_VISION_URL);
            HttpURLConnection con = (HttpURLConnection) url.openConnection();

            con.setUseCaches(false);
            con.setDoInput(true);
            con.setDoOutput(true);
            con.setRequestMethod(""POST"");
            con.setRequestProperty(""Content-Type"", ""application/json"");

            // Features JSON
            JSONObject featuresJson = new JSONObject();
            featuresJson.put(""type"", ""FACE_DETECTION"");
            featuresJson.put(""maxResults"", 1);

            JSONArray featuresJSONArray = new JSONArray();
            featuresJSONArray.put(featuresJson);

            // Image JSON
            JSONObject imageJson = new JSONObject();
            imageJson.put(""content"", encodedImageString);

            // Requests JSON
            JSONObject requestsJson = new JSONObject();
            requestsJson.put(""image"", imageJson);
            requestsJson.put(""features"", featuresJSONArray);

            JSONArray requestsJSONArray = new JSONArray();
            requestsJSONArray.put(requestsJson);

            JSONObject wholeRequest = new JSONObject();
            wholeRequest.put(""requests"", requestsJSONArray);

            OutputStreamWriter out = new OutputStreamWriter(con.getOutputStream());
            out.write(wholeRequest.toString());

            out.flush();
            out.close();

            int response_code = con.getResponseCode();
            Log.d(""GCV Response Code"", """"+response_code);

            BufferedReader br = new BufferedReader(new InputStreamReader(con.getInputStream()));
            StringBuilder sb = new StringBuilder("""");
            String temp;
            while((temp = br.readLine()) != null)
                sb.append(temp);
            String response = sb.toString();

            JSONObject responseJSONObject = new JSONObject(response);

            if (response_code == 200) {
                Log.d(""GCV"", ""Successful response"");
            }

            JSONArray responsesArray = responseJSONObject.getJSONArray(""responses"");
            JSONObject faceAnnotationsJson = responsesArray.getJSONObject(0);
            JSONObject annotedEmotionsJson = faceAnnotationsJson.getJSONArray(""faceAnnotations"").getJSONObject(0);
            return annotedEmotionsJson;
</code></pre>",,1,4,,2019-03-08 12:40:12.130 UTC,,2019-03-09 20:08:42.940 UTC,2019-03-09 13:27:28.493 UTC,,9098365,,9098365,1,0,java|android|filenotfoundexception|google-cloud-vision,58
React Native Camera - Multiple Photos,55234086,React Native Camera - Multiple Photos,"<p>I am currently using react-native-camera as a library to take pictures. I managed to show and hide a one and only camera component depending on a specific state. I am working on an app that has multiple buttons to take a picture, for example:</p>

<ul>
<li>Button A (show camera -> take picture -> store value on local state A)</li>
<li>Button B (show camera -> take picture -> store value on local state B)</li>
<li>Button C (show camera -> take picture -> store value on local state C)</li>
</ul>

<p>I have been breaking my head on how to do this, but can't figure it out.</p>

<p>My code is the following:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>import React, { Component } from 'react';
import { StyleSheet, Text, TouchableOpacity, View, Button } from 'react-native';
import { RNCamera } from 'react-native-camera';

export default class BadInstagramCloneApp extends Component {
  constructor(props){
    super(props);
    this.state = {
      isVisible: false,
      value1: null,
      value2: null
    }
  }

  render() {
    return (
        &lt;View style={styles.subcontainer}&gt;
          {this.state.isVisible === true
              ?
                &lt;View style={styles.container}&gt;
                  &lt;RNCamera
                      ref={ref =&gt; {
                        this.camera = ref;
                      }}
                      style={styles.preview}
                      type={RNCamera.Constants.Type.back}
                      flashMode={RNCamera.Constants.FlashMode.on}
                      permissionDialogTitle={'Permission to use camera'}
                      permissionDialogMessage={'We need your permission to use your camera phone'}
                      onGoogleVisionBarcodesDetected={({ barcodes }) =&gt; {
                        console.log(barcodes);
                      }}
                  /&gt;
                  &lt;View style={{ flex: 0, flexDirection: 'row', justifyContent: 'center' }}&gt;
                    &lt;TouchableOpacity onPress={this.takePicture.bind(this)} style={styles.capture}&gt;
                      &lt;Text style={{ fontSize: 14 }}&gt; SNAP &lt;/Text&gt;
                    &lt;/TouchableOpacity&gt;
                  &lt;/View&gt;
                &lt;/View&gt;
              :
                &lt;View&gt;
                  &lt;Button title='PHOTO 1' onPress={this.changeState}/&gt;
                  &lt;Button title='PHOTO 2' onPress={this.changeState2}/&gt;
                  &lt;Button title='SHOW RESULTS' onPress={this.showResults}/&gt;
                &lt;/View&gt;
          }
        &lt;/View&gt;
    );
  }

  changeState = () =&gt;{
    this.setState({isVisible: true})
  }

  changeState2 = () =&gt;{
    this.setState({isVisible: true})
  }

  showResults = () =&gt; {
    console.log('VALUE1: ' + this.state.value1);
    console.log('VALUE2: ' + this.state.value2);
  }

  takePicture = async function() {
    if (this.camera) {
      const options = { quality: 0.5, base64: true };
      const data = await this.camera.takePictureAsync(options);
      console.log(data.uri);
      this.setState({isVisible: false});
    }
  };
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    flexDirection: 'column',
    backgroundColor: 'black'
  },
  subcontainer: {
    flex: 1,
    flexDirection: 'column',
  },
  preview: {
    flex: 1,
    justifyContent: 'flex-end',
    alignItems: 'center',
  },
  capture: {
    flex: 0,
    backgroundColor: '#fff',
    borderRadius: 5,
    padding: 15,
    paddingHorizontal: 20,
    alignSelf: 'center',
    margin: 20,
  },
});</code></pre>
</div>
</div>
</p>",55243689,1,0,,2019-03-19 05:17:20.027 UTC,,2019-03-19 14:51:01.620 UTC,2019-03-19 14:35:39.693 UTC,,4049851,,5921192,1,1,javascript|react-native|react-native-camera,178
Amazon Web Service Recognition error in POSTMAN,48582519,Amazon Web Service Recognition error in POSTMAN,"<p>I am IAM user and trying to hit API <strong>RekognitionService.CreateCOllection</strong> for the testing in POSTMAN, but getting this </p>

<pre><code>&lt;InvalidSignatureException&gt;
  &lt;Message&gt;The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.&lt;/Message&gt;
&lt;/InvalidSignatureException&gt;
</code></pre>

<p>My header request is</p>

<pre><code>https://rekognition.us-east-2.amazonaws.com/

Content-Type:application/x-www-form-urlencoded
Host:rekognition.us-east-2.amazonaws.com
Accept-Encoding:identity
X-Amz-Target:RekognitionService.CreateCollections
Content-Length:30
Authorization:AWS4-HMAC-SHA256 Credential=AKIAJCWZHWIIW7BH5HPA/20180202/us-east-2/rekognition/aws4_request, SignedHeaders=accept-encoding;content-length;content-type;host;x-amz-date;x-amz-target, Signature=847d47d4e323fed46020c9fb2ac32882a01edd6efa179ed241d36d038712469e
X-Amz-Date:20180202T121035Z
Content-Type:application/x-www-form-urlencoded
</code></pre>

<p>Though it works with same <strong>Access Key Id</strong> and <strong>Secret Key</strong> using <strong>CLI(Command Line Interface)</strong>.</p>

<p>Can anyone help me to solve this problem. </p>",,1,1,,2018-02-02 12:18:00.527 UTC,,2018-06-03 23:27:01.580 UTC,2018-06-03 23:27:01.580 UTC,,174777,,9147826,1,1,amazon-web-services|amazon-s3|aws-api-gateway|amazon-rekognition,381
Google Cloud Vision returns empty response,51642038,Google Cloud Vision returns empty response,"<p>I am trying to build a C# library that will act as a wrapper for a set of Google APIs. When working with Google Vision API, I have found the API returns an empty response set for certain queries. For example, when I try to run FACE_ANNOTATION on <a href=""https://cloud.google.com/vision/docs/images/car.png"" rel=""nofollow noreferrer"">car.png</a>, the response I get back is:</p>

<pre><code>{
  ""responses"": [
    {}
  ]
}
</code></pre>

<p>I have eliminated all the basic issues like storing the image in a Google Cloud bucket, public access for the image, valid API key, enabling the API from the Google API Dashboard. </p>

<p>Below is a segment of the code where I make the request:</p>

<pre><code>httpClient.DefaultRequestHeaders.Accept.Clear();
httpClient.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(""application/json""));

// The API address to which we will make the HTTP POST query
String request_query = ""v1/images:annotate?"" + $""key={APIKey}"";
HttpResponseMessage response = await httpClient.PostAsJsonAsync(request_query, imageRequests);

Stream stream = await response.Content.ReadAsStreamAsync();
StreamReader streamReader = new StreamReader(stream);
String response_str = streamReader.ReadToEnd();        
Console.WriteLine(response_str);

if (response.IsSuccessStatusCode) {
    try {
        imageResponseList = JsonConvert.DeserializeObject&lt;AnnotateImageResponseList&gt;(response_str);

     } catch (JsonSerializationException e) {
         Debug.WriteLine(e.StackTrace);
     }
}
</code></pre>

<p>Here is the request body (imageRequests as it's called in my code above) that is sent to the API:</p>

<pre><code>{
  ""requests"": [
    {
      ""image"":
       {
         ""content"":"""",
         ""source"":
         {
           ""imageUri"":""gs://&lt;google_cloud_bucket&gt;/car.png""
         }
       },
       ""features"":[
         {
           ""type"":0,
           ""maxResults"":100,
           ""model"":""builtin/stable""
         } 
       ],
       ""imageContext"":null
    }
  ]
}
</code></pre>

<p>Now, I am aware that there is already a C# client that can be used directly, but the project I am working on needs me to access the REST API through HTTP requests.</p>

<p>Any help would be appreciated.</p>",51651548,2,0,,2018-08-01 20:48:41.437 UTC,,2019-01-09 14:45:08.260 UTC,2018-08-01 21:17:13.807 UTC,,10167828,,10167828,1,0,c#|google-api|google-cloud-vision,285
Android Vision Api - Barcode detection how to get type of barcode?,45742530,Android Vision Api - Barcode detection how to get type of barcode?,"<p>I am working with Android <code>Google Vision API</code>, and have created a standard barcode reader, but I want to detect what type/format of barcode is read i.e. <code>CODE 39</code>,
<code>CODE 128</code>, <code>QR Code</code>.... etc.<br>
Is there anyway to return the type?</p>

<p>Thanks</p>",,3,0,,2017-08-17 18:26:28.270 UTC,,2019-01-16 22:02:46.703 UTC,2018-12-10 09:17:45.543 UTC,,1824361,,787247,1,3,android|api|barcode|google-vision|vision,1022
AWS Rekognition JS SDK Invalid image encoding error,43599556,AWS Rekognition JS SDK Invalid image encoding error,"<p>Building a simple AWS Rekognition demo with React, using <code>&lt;input type=""file""&gt;</code> </p>

<p>Getting <code>Invalid image encoding</code> error.</p>

<pre><code>let file = e.target.files[0];
let reader = new FileReader();

reader.readAsDataURL(file);

reader.onloadend = () =&gt; {
  let rekognition = new aws.Rekognition();

  var params = {
    Image: { /* required */
      Bytes: reader.result,
    },
    MaxLabels: 0,
    MinConfidence: 0.0
  };

  rekognition.detectLabels(params, function(err, data) {
    if (err) console.log(err, err.stack); // an error occurred
    else     console.log(data);           // successful response
  });
</code></pre>

<p><a href=""https://i.stack.imgur.com/zZHHz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zZHHz.png"" alt=""enter image description here""></a></p>

<p>GitHub repo: <a href=""https://github.com/html5cat/vision-test/"" rel=""nofollow noreferrer"">https://github.com/html5cat/vision-test/</a></p>

<p>GitHub Issue: <a href=""https://github.com/html5cat/vision-test/issues/1"" rel=""nofollow noreferrer"">https://github.com/html5cat/vision-test/issues/1</a></p>",43702620,3,0,,2017-04-25 00:15:22.347 UTC,,2017-08-02 00:07:15.227 UTC,,,,,1448435,1,3,reactjs|amazon-web-services|amazon-rekognition,2564
Image detection with Google Vison and NodeJS,47641619,Image detection with Google Vison and NodeJS,"<p>I'm using Node JS to call Google Vision Cloud API. It's working fine but I can't understand how to process the returned object. </p>

<pre><code>clinet.textDetection(fileName).then(response =&gt; {

    console.log(response);
    console.log(Type(response));
    console.log(JSON.stringify(response));

    // console.log(JSON.stringify(response.responses));
    // console.log(JSON.stringify(response.fullTextAnnotation));
    // console.log(""fullTextAnnotation: "" + response.fullTextAnnotation);
    // var jsonObj = JSON.parse(response);
    // console.log(jsonObj.key);

}).catch(err =&gt; { console.error(err); });
</code></pre>

<p>Any clue? I have to read <strong>fullTextAnnotation.text</strong> key. All the sample I tried (and left on the code sample are not working [for instance I'm getting undefined]</p>

<p>This is the execution output:</p>

<pre><code>[04/12/2017 15:41:53.780] [LOG]   [ { faceAnnotations: [],
    landmarkAnnotations: [],
    logoAnnotations: [],
    labelAnnotations: [],
    textAnnotations: 
     [ [Object],
       [Object],
       [Object],
       [Object],
       [Object],
       [Object],
       [Object],
       [Object],
       [Object],
       [Object],
       [Object],
       [Object] ],
    safeSearchAnnotation: null,
    imagePropertiesAnnotation: null,
    error: null,
    cropHintsAnnotation: null,
    fullTextAnnotation: 
     { pages: [Array],
       text: 'Stefano Vecchier\nSystem Administrator\nstefano.vecchier@domino.it\nM +39 392 93 26 453\n' },
    webDetection: null } ]
[04/12/2017 15:41:53.782] [LOG]   [Function: Array]
[04/12/2017 15:41:53.782] [LOG]   [{""faceAnnotations"":[],""landmarkAnnotations"":[],""logoAnnotations"":[],""labelAnnotations"":[],""textAnnotations"":[{""locations"":[],""properties"":[],""mid"":"""",""locale"":""sq"",""description"":""Stefano Vecchier\nSystem Administrator\nstefano.vecchier@domino.it\nM +39 392 93 26 453\n"",""score"":0,""confidence"":0,""topicality"":0,""boundingPoly"":{""vertices"":[{""x"":616,""y"":113},{""x"":1007,""y"":113},{""x"":1007,""y"":257},{""x"":616,""y"":257}]}},{""locations"":[],""properties"":[],""mid"":"""",""locale"":"""",""description"":""Stefano"",""score"":0,""confidence"":0,""topicality"":0,""boundingPoly"":{""vertices"":[{""x"":616,""y"":116},{""x"":730,""y"":115},{""x"":730,""y"":138},{""x"":616,""y"":139}]}},{""locations"":[],""properties"":[],""mid"":"""",""locale"":"""",""description"":""Vecchier"",""score"":0,""confidence"":0,""topicality"":0,""boundingPoly"":{""vertices"":[{""x"":740,""y"":114},{""x"":870,""y"":113},{""x"":870,""y"":137},{""x"":740,""y"":138}]}},{""locations"":[],""properties"":[],""mid"":"""",""locale"":"""",""description"":""System"",""score"":0,""confidence"":0,""topicality"":0,""boundingPoly"":{""vertices"":[{""x"":616,""y"":155},{""x"":726,""y"":153},{""x"":726,""y"":182},{""x"":616,""y"":184}]}},{""locations"":[],""properties"":[],""mid"":"""",""locale"":"""",""description"":""Administrator"",""score"":0,""confidence"":0,""topicality"":0,""boundingPoly"":{""vertices"":[{""x"":736,""y"":153},{""x"":943,""y"":150},{""x"":943,""y"":175},{""x"":736,""y"":178}]}},{""locations"":[],""properties"":[],""mid"":"""",""locale"":"""",""description"":""stefano.vecchier@domino.it"",""score"":0,""confidence"":0,""topicality"":0,""boundingPoly"":{""vertices"":[{""x"":617,""y"":195},{""x"":1007,""y"":190},{""x"":1007,""y"":216},{""x"":617,""y"":221}]}},{""locations"":[],""properties"":[],""mid"":"""",""locale"":"""",""description"":""M"",""score"":0,""confidence"":0,""topicality"":0,""boundingPoly"":{""vertices"":[{""x"":618,""y"":236},{""x"":640,""y"":236},{""x"":640,""y"":257},{""x"":618,""y"":257}]}},{""locations"":[],""properties"":[],""mid"":"""",""locale"":"""",""description"":""+39"",""score"":0,""confidence"":0,""topicality"":0,""boundingPoly"":{""vertices"":[{""x"":654,""y"":235},{""x"":702,""y"":234},{""x"":702,""y"":255},{""x"":654,""y"":256}]}},{""locations"":[],""properties"":[],""mid"":"""",""locale"":"""",""description"":""392"",""score"":0,""confidence"":0,""topicality"":0,""boundingPoly"":{""vertices"":[{""x"":712,""y"":234},{""x"":759,""y"":233},{""x"":759,""y"":255},{""x"":712,""y"":256}]}},{""locations"":[],""properties"":[],""mid"":"""",""locale"":"""",""description"":""93"",""score"":0,""confidence"":0,""topicality"":0,""boundingPoly"":{""vertices"":[{""x"":770,""y"":234},{""x"":801,""y"":234},{""x"":801,""y"":256},{""x"":770,""y"":256}]}},{""locations"":[],""properties"":[],""mid"":"""",""locale"":"""",""description"":""26"",""score"":0,""confidence"":0,""topicality"":0,""boundingPoly"":{""vertices"":[{""x"":812,""y"":233},{""x"":843,""y"":233},{""x"":843,""y"":255},{""x"":812,""y"":255}]}},{""locations"":[],""properties"":[],""mid"":"""",""locale"":"""",""description"":""453"",""score"":0,""confidence"":0,""topicality"":0,""boundingPoly"":{""vertices"":[{""x"":854,""y"":232},{""x"":902,""y"":231},{""x"":902,""y"":253},{""x"":854,""y"":254}]}}],""safeSearchAnnotation"":null,""imagePropertiesAnnotation"":null,""error"":null,""cropHintsAnnotation"":null,""fullTextAnnotation"":{""pages"":[{""blocks"":[{""paragraphs"":[{""words"":[{""symbols"":[{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":616,""y"":116},{""x"":630,""y"":116},{""x"":630,""y"":139},{""x"":616,""y"":139}]},""text"":""S""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":632,""y"":116},{""x"":646,""y"":116},{""x"":646,""y"":139},{""x"":632,""y"":139}]},""text"":""t""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":648,""y"":122},{""x"":663,""y"":122},{""x"":663,""y"":139},{""x"":648,""y"":139}]},""text"":""e""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":665,""y"":115},{""x"":674,""y"":115},{""x"":674,""y"":138},{""x"":665,""y"":138}]},""text"":""f""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":676,""y"":121},{""x"":691,""y"":121},{""x"":691,""y"":138},{""x"":676,""y"":138}]},""text"":""a""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":695,""y"":121},{""x"":710,""y"":121},{""x"":710,""y"":138},{""x"":695,""y"":138}]},""text"":""n""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":{""type"":""SPACE"",""isPrefix"":false}},""boundingBox"":{""vertices"":[{""x"":714,""y"":121},{""x"":730,""y"":121},{""x"":730,""y"":138},{""x"":714,""y"":138}]},""text"":""o""}],""property"":{""detectedLanguages"":[{""languageCode"":""en"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":616,""y"":116},{""x"":730,""y"":115},{""x"":730,""y"":138},{""x"":616,""y"":139}]}},{""symbols"":[{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":740,""y"":114},{""x"":759,""y"":114},{""x"":759,""y"":138},{""x"":740,""y"":138}]},""text"":""V""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":761,""y"":120},{""x"":775,""y"":120},{""x"":775,""y"":137},{""x"":761,""y"":137}]},""text"":""e""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":778,""y"":120},{""x"":794,""y"":120},{""x"":794,""y"":137},{""x"":778,""y"":137}]},""text"":""c""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":796,""y"":120},{""x"":812,""y"":120},{""x"":812,""y"":137},{""x"":796,""y"":137}]},""text"":""c""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":815,""y"":113},{""x"":830,""y"":113},{""x"":830,""y"":136},{""x"":815,""y"":136}]},""text"":""h""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":833,""y"":113},{""x"":838,""y"":113},{""x"":838,""y"":136},{""x"":833,""y"":136}]},""text"":""i""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":841,""y"":120},{""x"":855,""y"":120},{""x"":855,""y"":136},{""x"":841,""y"":136}]},""text"":""e""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":{""type"":""EOL_SURE_SPACE"",""isPrefix"":false}},""boundingBox"":{""vertices"":[{""x"":857,""y"":119},{""x"":870,""y"":119},{""x"":870,""y"":135},{""x"":857,""y"":135}]},""text"":""r""}],""property"":{""detectedLanguages"":[{""languageCode"":""fr"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":740,""y"":114},{""x"":870,""y"":113},{""x"":870,""y"":137},{""x"":740,""y"":138}]}},{""symbols"":[{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":616,""y"":155},{""x"":633,""y"":155},{""x"":633,""y"":184},{""x"":616,""y"":184}]},""text"":""S""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":635,""y"":155},{""x"":653,""y"":155},{""x"":653,""y"":184},{""x"":635,""y"":184}]},""text"":""y""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":654,""y"":156},{""x"":666,""y"":156},{""x"":666,""y"":177},{""x"":654,""y"":177}]},""text"":""s""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":668,""y"":156},{""x"":680,""y"":156},{""x"":680,""y"":177},{""x"":668,""y"":177}]},""text"":""t""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":681,""y"":160},{""x"":697,""y"":160},{""x"":697,""y"":177},{""x"":681,""y"":177}]},""text"":""e""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":{""type"":""SPACE"",""isPrefix"":false}},""boundingBox"":{""vertices"":[{""x"":699,""y"":160},{""x"":726,""y"":160},{""x"":726,""y"":177},{""x"":699,""y"":177}]},""text"":""m""}],""property"":{""detectedLanguages"":[{""languageCode"":""en"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":616,""y"":155},{""x"":726,""y"":153},{""x"":726,""y"":182},{""x"":616,""y"":184}]}},{""symbols"":[{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":736,""y"":154},{""x"":757,""y"":154},{""x"":757,""y"":176},{""x"":736,""y"":176}]},""text"":""A""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":758,""y"":154},{""x"":776,""y"":154},{""x"":776,""y"":178},{""x"":758,""y"":178}]},""text"":""d""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":779,""y"":158},{""x"":804,""y"":158},{""x"":804,""y"":175},{""x"":779,""y"":175}]},""text"":""m""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":808,""y"":152},{""x"":812,""y"":152},{""x"":812,""y"":175},{""x"":808,""y"":175}]},""text"":""i""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":815,""y"":158},{""x"":832,""y"":158},{""x"":832,""y"":175},{""x"":815,""y"":175}]},""text"":""n""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":834,""y"":153},{""x"":840,""y"":153},{""x"":840,""y"":175},{""x"":834,""y"":175}]},""text"":""i""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":842,""y"":153},{""x"":855,""y"":153},{""x"":855,""y"":176},{""x"":842,""y"":176}]},""text"":""s""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":857,""y"":153},{""x"":870,""y"":153},{""x"":870,""y"":176},{""x"":857,""y"":176}]},""text"":""t""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":870,""y"":157},{""x"":883,""y"":157},{""x"":883,""y"":174},{""x"":870,""y"":174}]},""text"":""r""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":883,""y"":158},{""x"":900,""y"":158},{""x"":900,""y"":176},{""x"":883,""y"":176}]},""text"":""a""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":900,""y"":154},{""x"":914,""y"":154},{""x"":914,""y"":176},{""x"":900,""y"":176}]},""text"":""t""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":916,""y"":153},{""x"":930,""y"":153},{""x"":930,""y"":175},{""x"":916,""y"":175}]},""text"":""o""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":{""type"":""EOL_SURE_SPACE"",""isPrefix"":false}},""boundingBox"":{""vertices"":[{""x"":932,""y"":157},{""x"":943,""y"":157},{""x"":943,""y"":175},{""x"":932,""y"":175}]},""text"":""r""}],""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":736,""y"":153},{""x"":943,""y"":150},{""x"":943,""y"":175},{""x"":736,""y"":178}]}},{""symbols"":[{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":617,""y"":198},{""x"":627,""y"":198},{""x"":627,""y"":218},{""x"":617,""y"":218}]},""text"":""s""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":629,""y"":198},{""x"":640,""y"":198},{""x"":640,""y"":218},{""x"":629,""y"":218}]},""text"":""t""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":643,""y"":201},{""x"":657,""y"":201},{""x"":657,""y"":218},{""x"":643,""y"":218}]},""text"":""e""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":659,""y"":194},{""x"":668,""y"":194},{""x"":668,""y"":217},{""x"":659,""y"":217}]},""text"":""f""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":669,""y"":200},{""x"":683,""y"":200},{""x"":683,""y"":217},{""x"":669,""y"":217}]},""text"":""a""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":686,""y"":200},{""x"":701,""y"":200},{""x"":701,""y"":217},{""x"":686,""y"":217}]},""text"":""n""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":703,""y"":200},{""x"":719,""y"":200},{""x"":719,""y"":217},{""x"":703,""y"":217}]},""text"":""o""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":723,""y"":200},{""x"":732,""y"":200},{""x"":732,""y"":217},{""x"":723,""y"":217}]},""text"":"".""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":734,""y"":199},{""x"":743,""y"":199},{""x"":743,""y"":216},{""x"":734,""y"":216}]},""text"":""v""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":746,""y"":199},{""x"":761,""y"":199},{""x"":761,""y"":216},{""x"":746,""y"":216}]},""text"":""e""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":763,""y"":199},{""x"":777,""y"":199},{""x"":777,""y"":216},{""x"":763,""y"":216}]},""text"":""c""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":781,""y"":199},{""x"":794,""y"":199},{""x"":794,""y"":216},{""x"":781,""y"":216}]},""text"":""c""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":798,""y"":192},{""x"":812,""y"":192},{""x"":812,""y"":215},{""x"":798,""y"":215}]},""text"":""h""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":816,""y"":192},{""x"":819,""y"":192},{""x"":819,""y"":215},{""x"":816,""y"":215}]},""text"":""i""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":823,""y"":198},{""x"":837,""y"":198},{""x"":837,""y"":215},{""x"":823,""y"":215}]},""text"":""e""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":840,""y"":198},{""x"":848,""y"":198},{""x"":848,""y"":215},{""x"":840,""y"":215}]},""text"":""r""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":850,""y"":194},{""x"":872,""y"":194},{""x"":872,""y"":218},{""x"":850,""y"":218}]},""text"":""@""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":876,""y"":197},{""x"":892,""y"":197},{""x"":892,""y"":214},{""x"":876,""y"":214}]},""text"":""d""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":894,""y"":197},{""x"":911,""y"":197},{""x"":911,""y"":214},{""x"":894,""y"":214}]},""text"":""o""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":913,""y"":197},{""x"":937,""y"":197},{""x"":937,""y"":214},{""x"":913,""y"":214}]},""text"":""m""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":940,""y"":191},{""x"":944,""y"":191},{""x"":944,""y"":214},{""x"":940,""y"":214}]},""text"":""i""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":948,""y"":196},{""x"":963,""y"":196},{""x"":963,""y"":214},{""x"":948,""y"":214}]},""text"":""n""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":965,""y"":196},{""x"":980,""y"":196},{""x"":980,""y"":214},{""x"":965,""y"":214}]},""text"":""o""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":985,""y"":210},{""x"":988,""y"":210},{""x"":988,""y"":214},{""x"":985,""y"":214}]},""text"":"".""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":992,""y"":191},{""x"":998,""y"":191},{""x"":998,""y"":214},{""x"":992,""y"":214}]},""text"":""i""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":{""type"":""EOL_SURE_SPACE"",""isPrefix"":false}},""boundingBox"":{""vertices"":[{""x"":1000,""y"":191},{""x"":1007,""y"":191},{""x"":1007,""y"":214},{""x"":1000,""y"":214}]},""text"":""t""}],""property"":{""detectedLanguages"":[{""languageCode"":""en"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":617,""y"":195},{""x"":1007,""y"":190},{""x"":1007,""y"":216},{""x"":617,""y"":221}]}},{""symbols"":[{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":{""type"":""SPACE"",""isPrefix"":false}},""boundingBox"":{""vertices"":[{""x"":618,""y"":236},{""x"":640,""y"":236},{""x"":640,""y"":257},{""x"":618,""y"":257}]},""text"":""M""}],""property"":{""detectedLanguages"":[{""languageCode"":""en"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":618,""y"":236},{""x"":640,""y"":236},{""x"":640,""y"":257},{""x"":618,""y"":257}]}},{""symbols"":[{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":654,""y"":241},{""x"":669,""y"":241},{""x"":669,""y"":255},{""x"":654,""y"":255}]},""text"":""+""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":671,""y"":235},{""x"":685,""y"":235},{""x"":685,""y"":256},{""x"":671,""y"":256}]},""text"":""3""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":{""type"":""SPACE"",""isPrefix"":false}},""boundingBox"":{""vertices"":[{""x"":688,""y"":235},{""x"":702,""y"":235},{""x"":702,""y"":256},{""x"":688,""y"":256}]},""text"":""9""}],""property"":{""detectedLanguages"":[{""languageCode"":""en"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":654,""y"":235},{""x"":702,""y"":234},{""x"":702,""y"":255},{""x"":654,""y"":256}]}},{""symbols"":[{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":712,""y"":234},{""x"":726,""y"":234},{""x"":726,""y"":256},{""x"":712,""y"":256}]},""text"":""3""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":729,""y"":234},{""x"":743,""y"":234},{""x"":743,""y"":256},{""x"":729,""y"":256}]},""text"":""9""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":{""type"":""SPACE"",""isPrefix"":false}},""boundingBox"":{""vertices"":[{""x"":746,""y"":234},{""x"":759,""y"":234},{""x"":759,""y"":256},{""x"":746,""y"":256}]},""text"":""2""}],""property"":{""detectedLanguages"":[{""languageCode"":""en"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":712,""y"":234},{""x"":759,""y"":233},{""x"":759,""y"":255},{""x"":712,""y"":256}]}},{""symbols"":[{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":770,""y"":234},{""x"":784,""y"":234},{""x"":784,""y"":256},{""x"":770,""y"":256}]},""text"":""9""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":{""type"":""SPACE"",""isPrefix"":false}},""boundingBox"":{""vertices"":[{""x"":787,""y"":233},{""x"":801,""y"":233},{""x"":801,""y"":255},{""x"":787,""y"":255}]},""text"":""3""}],""property"":{""detectedLanguages"":[{""languageCode"":""en"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":770,""y"":234},{""x"":801,""y"":234},{""x"":801,""y"":256},{""x"":770,""y"":256}]}},{""symbols"":[{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":812,""y"":233},{""x"":826,""y"":233},{""x"":826,""y"":255},{""x"":812,""y"":255}]},""text"":""2""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":{""type"":""SPACE"",""isPrefix"":false}},""boundingBox"":{""vertices"":[{""x"":829,""y"":233},{""x"":843,""y"":233},{""x"":843,""y"":255},{""x"":829,""y"":255}]},""text"":""6""}],""property"":{""detectedLanguages"":[{""languageCode"":""en"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":812,""y"":233},{""x"":843,""y"":233},{""x"":843,""y"":255},{""x"":812,""y"":255}]}},{""symbols"":[{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":854,""y"":232},{""x"":868,""y"":232},{""x"":868,""y"":253},{""x"":854,""y"":253}]},""text"":""4""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":871,""y"":232},{""x"":885,""y"":232},{""x"":885,""y"":254},{""x"":871,""y"":254}]},""text"":""5""},{""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":{""type"":""EOL_SURE_SPACE"",""isPrefix"":false}},""boundingBox"":{""vertices"":[{""x"":888,""y"":232},{""x"":902,""y"":232},{""x"":902,""y"":254},{""x"":888,""y"":254}]},""text"":""3""}],""property"":{""detectedLanguages"":[{""languageCode"":""en"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":854,""y"":232},{""x"":902,""y"":231},{""x"":902,""y"":253},{""x"":854,""y"":254}]}}],""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":616,""y"":113},{""x"":1007,""y"":113},{""x"":1007,""y"":257},{""x"":616,""y"":257}]}}],""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""boundingBox"":{""vertices"":[{""x"":616,""y"":113},{""x"":1007,""y"":113},{""x"":1007,""y"":257},{""x"":616,""y"":257}]},""blockType"":""TEXT""}],""property"":{""detectedLanguages"":[{""languageCode"":""sq"",""confidence"":0}],""detectedBreak"":null},""width"":1280,""height"":722}],""text"":""Stefano Vecchier\nSystem Administrator\nstefano.vecchier@domino.it\nM +39 392 93 26 453\n""},""webDetection"":null}]
</code></pre>",47657567,1,0,,2017-12-04 20:40:16.997 UTC,,2017-12-05 15:49:30.313 UTC,2017-12-05 14:18:21.483 UTC,,71410,,71410,1,0,javascript|arrays|node.js|google-cloud-vision|google-vision,147
Google Cloud Vision Backned Stuff,53814812,Google Cloud Vision Backned Stuff,"<p>I am impressed with accuracy levels of Google's Vision API for OCR.
Are they using tesseract as part of their suite?.
Can somebody guess what tools they might be using in order to improve the OCR operation?.</p>",,1,0,,2018-12-17 11:59:56.190 UTC,,2018-12-17 12:18:31.323 UTC,,,,,10120429,1,-1,computer-vision|ocr|tesseract,21
IBM Watson Visual Recognition module,38753678,IBM Watson Visual Recognition module,"<p>I have developed an Android app that send REST directives directly to the Visual Recognition service in IBM Bluemix.</p>

<p>If I send a photograph that shows a female subject, Watson responds with a proper identification.</p>

<p>But if I send one, with the very same application, of a male subject, Watson does not even analyze it.</p>

<p>I am not trying to classify. I just want to identify faces in a photo.</p>

<p>Can anybody tell me if the Watson Visual Recognition service that I am accessing is only trained to identify women?. (joking) Is there something I am missing when I send the POST Rest directive?</p>

<p>Thanks in advance for your help.</p>

<p>PS.</p>

<p>I am a registered user in the Bluemix platform and I have proper credentials to access the Visual Recognition service.</p>

<p>The app was developed in the MIT App inventor platform. It works OK for female photos, but not with male ones.</p>",,0,2,,2016-08-03 21:10:03.160 UTC,,2016-08-09 00:11:04.543 UTC,2016-08-09 00:11:04.543 UTC,,3198917,,6586831,1,0,android|rest|ibm-cloud|visual-recognition,197
Add 2D or 3D Face Filters like MSQRD/SnapChat Using Google Vision API for iOS,47413657,Add 2D or 3D Face Filters like MSQRD/SnapChat Using Google Vision API for iOS,"<p>Here's some research I have done so far:
- I have used Google Vision API to detect various face landmarks.
Here's the reference: <a href=""https://developers.google.com/vision/introduction"" rel=""noreferrer"">https://developers.google.com/vision/introduction</a></p>

<ul>
<li><p>Here's the link to Sample Code to get the facial landmarks. It uses the same Google Vision API. Here's the reference link: <a href=""https://github.com/googlesamples/ios-vision"" rel=""noreferrer"">https://github.com/googlesamples/ios-vision</a></p></li>
<li><p>I have gone through the various blogs on internet which says MSQRD based on the Google's cloud vision. Here's the link to it: <a href=""https://medium.com/@AlexioCassani/how-to-create-a-msqrd-like-app-with-google-cloud-vision-802b578b30a0"" rel=""noreferrer"">https://medium.com/@AlexioCassani/how-to-create-a-msqrd-like-app-with-google-cloud-vision-802b578b30a0</a></p></li>
<li><p>For Android here's the reference:
<a href=""https://www.raywenderlich.com/158580/augmented-reality-android-googles-face-api"" rel=""noreferrer"">https://www.raywenderlich.com/158580/augmented-reality-android-googles-face-api</a></p></li>
<li><p>There are multiple paid SDK's which full fills the purpose. But they are highly priced. So cant able to afford it.
For instance:</p></li>
</ul>

<p>1) <a href=""https://deepar.ai/contact/"" rel=""noreferrer"">https://deepar.ai/contact/</a></p>

<p>2) <a href=""https://www.luxand.com/"" rel=""noreferrer"">https://www.luxand.com/</a></p>

<p>There is possibility might have some see this <strong>question as duplicate</strong> of this:
<a href=""https://stackoverflow.com/questions/36727201/face-filter-implementation-like-msqrd-snapchat"">Face filter implementation like MSQRD/SnapChat</a></p>

<p>But the thread is almost 1.6 years old with no right answers to it. </p>

<p>I have gone through this article: 
<a href=""https://dzone.com/articles/mimic-snapchat-filters-programmatically-1"" rel=""noreferrer"">https://dzone.com/articles/mimic-snapchat-filters-programmatically-1</a></p>

<p>It describes all the essential steps to achieve the desired results. But they advice to use their own made SDK. </p>

<p>As per my research no good enough material is around which helps to full fill the desired results like <strong>MSQRD face filters</strong>.</p>

<p>One more Github repository around which has same implementation but it doesn't gives much information about same.
<a href=""https://github.com/rootkit/LiveFaceMask"" rel=""noreferrer"">https://github.com/rootkit/LiveFaceMask</a></p>

<p><strong>Now my question is:</strong></p>

<blockquote>
  <p>If we have the facial landmarks using Google Vision API (or even using
  DiLib), how I can add 2d or 3d models over it. In which format this
  needs to be done like this require some X,Y coordinates with vertices
  calculation. </p>
  
  <p>NOTE: I have gone through the Googles ""GooglyEyesDemo"" which adds the
  preview layer over eyes. It basically adds a view over the face. So I
  dont want to add UIView one dimensional preview layers over it. Image
  attached for reference :</p>
  
  <p><a href=""https://developers.google.com/vision/ios/face-tracker-tutorial"" rel=""noreferrer"">https://developers.google.com/vision/ios/face-tracker-tutorial</a></p>
  
  <p>Creating Models: I also want to know how to create models for live
  filters like MSQRD. I welcome any software or format recommendations.
  Hope the research I have done will help others and someone else
  experience helps me to achieve the desired results. Let me know if any
  more details are required.**</p>
</blockquote>

<p>Image attached for more reference:  <a href=""https://i.stack.imgur.com/0sCbE.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/0sCbE.png"" alt=""I want this result ""></a></p>

<p>Thanks
Harry</p>",,2,0,,2017-11-21 12:55:10.510 UTC,12,2019-04-09 06:32:06.903 UTC,,,,,1670825,1,16,ios|objective-c|face-detection|google-vision|snapchat,3022
Blackout number in pdf using OCR,56406581,Blackout number in pdf using OCR,"<p>Have 3 pages PDF which has scanned Id card. Id card copy can be on any page I need to blackout Id card number (Format of Id card number - 12 Digits and two spaces i.e xxxx xxxx xxxx)</p>

<p>Please suggest how can i achieve this</p>

<p>I tried microsoft computer vision OCR services but unable to integrate the code </p>

<p>Need to automate this task </p>

<p>Find the Input and expected Output file
<a href=""https://filebin.net/qwyjhkdvw72yzpgn"" rel=""nofollow noreferrer"">Input and Outputfile</a></p>",,1,0,,2019-06-01 12:05:26.197 UTC,,2019-06-01 14:23:23.447 UTC,2019-06-01 13:04:34.033 UTC,,11585767,,11585767,1,0,c#|computer-vision|ocr|tesseract|python-tesseract,33
Recognizing hashtag symbol,36452038,Recognizing hashtag symbol,<p>I am trying to use Google Cloud Vision with TEXT_DETECTION to try and do OCR on pictures with hashtags.  But I can't get the hashtag symbol recognized.  Any ideas?</p>,,0,2,,2016-04-06 13:04:14.057 UTC,,2016-04-06 13:04:14.057 UTC,,,,,6166846,1,0,ocr|google-cloud-vision,43
using google storage and google vision API,55868948,using google storage and google vision API,"<p>I have the follow function that passes a image url to google vision service and returns the letters and numbers (characters) in the image.  It works fine with general web urls but I'm calling it to access files stored in Google storage, it doesn't work.  How can i get this to work? I've looked at examples from googling but I cant work out how to do this? </p>

<p>If its not possible to use google storage, is there a way you can just upload the image rather than storing in on a file system? I have no need for storing the image, all i care about is the returned characters.  </p>

<pre><code>def detect_text_uri(uri):
    """"""Detects text in the file located in Google Cloud Storage or on the Web.
    """"""
    from google.cloud import vision
    client = vision.ImageAnnotatorClient()
    image = vision.types.Image()
    image.source.image_uri = uri
    image.source.gcs_image_uri = uri

    response = client.text_detection(image=image)
    texts = response.text_annotations
    print('Texts:')

    for text in texts:
        print('\n""{}""'.format(text.description))

        vertices = (['({},{})'.format(vertex.x, vertex.y)
                    for vertex in text.bounding_poly.vertices])


        print('bounds: {}'.format(','.join(vertices)))
    return texts
{
</code></pre>

<p>This line doesn't work which should read an image I've placed in google storage, all thats returned is a blank responce: </p>

<pre><code>detect_text_uri(""'source': {'image_uri': 'gs://ocr_storage/meter_reader.jpg'}"")
</code></pre>

<p>This line works fine : </p>

<pre><code>detect_text_uri('https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Transparent_Electricity_Meter_found_in_Israel.JPG/220px-Transparent_Electricity_Meter_found_in_Israel.JPG')
</code></pre>",,0,0,,2019-04-26 13:42:12.863 UTC,,2019-04-26 13:42:12.863 UTC,,,,,1019687,1,0,python|google-api|google-vision,13
"Cloud Vision API Client threw an OS Error ""too many open files""",50545515,"Cloud Vision API Client threw an OS Error ""too many open files""","<p>I have met an Error of ""Too many open files"" when I run label detection via Cloud Vision API Client with Python.<br>
When I asked this probrem on GitHub before this post, the maintainer gave me an advice that the problem is general Python issue rather than API.<br>
After this advice, I have not understood yet why Python threw ""too many open files"".<br>
I did logging and it showed that urllib3 had raised such errors, although I did not import that package explicitly.<br>
What I wrong? Please help me.<br>
My Environment is</p>

<ul>
<li>Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-112-generic x86_64)  </li>
<li>Python 3.5.2  </li>
<li>google-cloud-vision (0.31.1)  </li>
</ul>

<p>The error logs:  </p>

<pre><code>[2018-05-25 20:18:46,573] {label_detection.py:60} DEBUG - success open decile_data/image/src/00000814.jpg
[2018-05-25 20:18:46,573] {label_detection.py:62} DEBUG - success convert image to types.Image
[2018-05-25 20:18:46,657] {requests.py:117} DEBUG - Making request: POST https://accounts.google.com/o/oauth2/token
[2018-05-25 20:18:46,657] {connectionpool.py:824} DEBUG - Starting new HTTPS connection (1): accounts.google.com
[2018-05-25 20:18:46,775] {connectionpool.py:396} DEBUG - https://accounts.google.com:443 ""POST /o/oauth2/token HTTP/1.1"" 200 None
[2018-05-25 20:18:47,803] {label_detection.py:60} DEBUG - success open decile_data/image/src/00000815.jpg
[2018-05-25 20:18:47,803] {label_detection.py:62} DEBUG - success convert image to types.Image
[2018-05-25 20:18:47,896] {requests.py:117} DEBUG - Making request: POST https://accounts.google.com/o/oauth2/token
[2018-05-25 20:18:47,896] {connectionpool.py:824} DEBUG - Starting new HTTPS connection (1): accounts.google.com
[2018-05-25 20:18:47,902] {_plugin_wrapping.py:81} ERROR - AuthMetadataPluginCallback ""&lt;google.auth.transport.grpc.AuthMetadataPlugin object at 0x7fcd94eb7dd8&gt;"" raised exception!
Traceback (most recent call last):
  File ""/home/ishiyama/tensorflow/lib/python3.5/site-packages/urllib3/util/ssl_.py"", line 313, in ssl_wrap_socket
OSError: [Errno 24] Too many open files

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/ishiyama/tensorflow/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 601, in urlopen
  File ""/home/ishiyama/tensorflow/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 346, in _make_request
  File ""/home/ishiyama/tensorflow/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 850, in _validate_conn
  File ""/home/ishiyama/tensorflow/lib/python3.5/site-packages/urllib3/connection.py"", line 326, in connect
  File ""/home/ishiyama/tensorflow/lib/python3.5/site-packages/urllib3/util/ssl_.py"", line 315, in ssl_wrap_socket
urllib3.exceptions.SSLError: [Errno 24] Too many open files

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/ishiyama/tensorflow/lib/python3.5/site-packages/requests/adapters.py"", line 440, in send
  File ""/home/ishiyama/tensorflow/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 639, in urlopen
  File ""/home/ishiyama/tensorflow/lib/python3.5/site-packages/urllib3/util/retry.py"", line 388, in increment
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='accounts.google.com', port=443): Max retries exceeded with url: /o/oauth2/token (Caused by SSLError(OSError(24, 'Too many open files'),))
</code></pre>

<p>The script exported above errors is following:</p>

<pre><code># -*- coding: utf-8 -*-
"""""" Detecting labels of images using Google Cloud Vision. """"""

import argparse
import csv
from datetime import datetime
import os
import logging
from pathlib import Path
import sys
from google.cloud import vision
from google.cloud.vision import types


logger= logging.getLogger(__name__)


def get_commandline_args():
    parser = argparse.ArgumentParser(
        description='Detecting labels of images using Google Cloud Vision.')

    parser.add_argument('--image-dir',
                        type=str,
                        required=True,
                        help='Directory in which images are saved.')
    parser.add_argument('--output-path',
                        type=str,
                        required=True,
                        help='Path of output file. This is saved as CSV.')
    parser.add_argument('--max-results',
                        type=int,
                        required=False,
                        default=5,
                        help=('Maximum number of resulting labels.'
                              ' Default is 5.'))
    parser.add_argument('--debug',
                        type=bool,
                        required=False,
                        default=False,
                        help=('Whether running to debug.'
                              ' If True, this scripts will run on 3 files.'
                              ' Default is False.'))
    return parser.parse_args()


def load_image(path):
    """""" load image to be capable with Google Cloud Vision Clienet API.

    Args:
        path (str): a path of an image.

    Returns:
        img : an object which is google.cloud.vision.types.Image.

    Raise:
        IOError is raised when 'open' is failed to load the image.
    """"""
    with open(path, 'rb') as f:
        content = f.read()
    logger.debug('success open {}'.format(path))
    img = types.Image(content=content)
    logger.debug('success convert image to types.Image')

    return img


def detect_labels_of_image(path, max_results):
    _path = Path(path)
    client = vision.ImageAnnotatorClient()
    image = load_image(path=str(_path))
    execution_time = datetime.now()
    response = client.label_detection(image=image, max_results=max_results)
    labels = response.label_annotations
    for label in labels:
        record = (str(_path), _path.name, label.description,
                  label.score, execution_time.strftime('%Y-%m-%d %H:%M:%S'))
        yield record


def main():
    args = get_commandline_args()

    file_handler = logging.FileHandler(filename='label_detection.log')
    logging.basicConfig(
        level=logging.DEBUG,
        format='[%(asctime)s] {%(filename)s:%(lineno)s} %(levelname)s - %(message)s',
        handlers=[file_handler]
    )

    image_dir = args.image_dir

    with open(args.output_path, 'w') as fout:

        writer = csv.writer(fout, lineterminator='\n')
        header = ['path', 'filename', 'label', 'score', 'executed_at']
        writer.writerow(header)

        image_file_lists = os.listdir(image_dir)
        image_file_lists.sort()
        if args.debug:
            image_file_lists = image_file_lists[:3]

        for filename in image_file_lists:
            path = os.path.join(image_dir, filename)
            try:
                results = detect_labels_of_image(path, args.max_results)
            except Exception as e:
                logger.warning(e)
                logger.warning('skiped processing {} due to above exception.'.format(path))
            for record in results:
                writer.writerow(record)


if __name__ == '__main__':
    main()
</code></pre>",,3,2,,2018-05-26 17:46:36.243 UTC,,2018-10-30 05:54:18.667 UTC,,,,,9851702,1,3,python|ubuntu|google-cloud-vision|urllib3,636
Google Cloud Java clients: forcing HTTP when setting a custom endpoint,49167306,Google Cloud Java clients: forcing HTTP when setting a custom endpoint,"<p>I am using Google's <code>LanguageServiceClient</code> from <code>com.google.cloud:google-cloud-vision:1.16.0</code>, and <code>ImageAnnotatorClient</code> from <code>com.google.cloud:google-cloud-language:1.16.0</code>.</p>

<p>They are used in a project that runs inside a private VPN. The company's infrastructure dictates that accessing external services must be done through a forward proxy. Furthermore, all forward proxies in the VPN are mandated to be on HTTP, not HTTPS.</p>

<p>So I have a forward proxy xx.xx.xx.xx, and all requests like <a href=""http://xx.xx.xx.xx/somePath"" rel=""nofollow noreferrer"">http://xx.xx.xx.xx/somePath</a> get forwarded to <a href=""https://language.googleapis.com/somePath"" rel=""nofollow noreferrer"">https://language.googleapis.com/somePath</a>. I tested this with some curl requests and they way work correctly.</p>

<p>I have changed the endpoint as follows:</p>

<pre><code>LanguageServiceSettings serviceSettings = LanguageServiceSettings.newBuilder()
                .setEndpoint(""xx.xx.xx.xx:80"")
                //the default value of this is ""language.googleapis.com:443""
                .build();
languageServiceClient = LanguageServiceClient.create(serviceSettings);
</code></pre>

<p>However, the client seems to be hitting the new endpoint via HTTPS. I can't figure out how to set the scheme. Any help would be appreciated.</p>",49235576,1,0,,2018-03-08 07:08:36.490 UTC,,2018-03-16 06:32:06.517 UTC,2018-03-08 07:14:22.527 UTC,,3441882,,3441882,1,0,java|google-cloud-platform,104
blurred screen and focus issue in barcode scanner android 5.1,52865934,blurred screen and focus issue in barcode scanner android 5.1,"<p>barcode scanner in the samsung galaxy a6 tablet (android 5.1) device is showing blurred screen and focus issues while opening camera.I'm using google vision api.
Working fine with all other devices.Please help me out.</p>",,0,0,,2018-10-18 02:08:09.607 UTC,,2018-10-18 02:08:09.607 UTC,,,,,7315124,1,1,android|barcode-scanner|google-vision,27
Google Cloud Vision Lower Case Upper Case,56175881,Google Cloud Vision Lower Case Upper Case,"<p>Sometimes the Google Cloud Vision API will return results with upper case labels and sometimes with lower case labels. For example, it may return ""dress"" or ""Dress"". Does anyone know if this signifies any difference?</p>",,1,0,,2019-05-16 20:06:30.553 UTC,,2019-05-17 02:25:06.550 UTC,,,,,4631796,1,0,google-cloud-platform|google-cloud-vision,19
Front Facing Camera does not Auto Focus or Manually Focus,55846066,Front Facing Camera does not Auto Focus or Manually Focus,"<p><strong>Current Behaviour</strong></p>

<p>I'm using <a href=""https://github.com/react-native-community/react-native-camera"" rel=""nofollow noreferrer""><code>react-native-camera</code></a> with iPads/iPhones and I use the front-facing camera to scan barcodes (Code39, Code128, QR, etc..) However when using the front-facing camera, it does not focus on the barcode or anything I put mildly close to the camera. The rear camera works absolutely perfectly however the front camera does not.</p>

<p>I have not been able to test android as I'm not building for android purely just iOS. I can't seem to find any information on getting the front camera to focus.</p>

<p>If I was to stand in the background, hold up my Code39 close to the camera but leave a small gap at the bottom, it would not try focus on the card however stay focused on me in the background.</p>

<p>I've also raised an <a href=""https://github.com/react-native-community/react-native-camera/issues/2231"" rel=""nofollow noreferrer"">issue here</a> on their GitHub page, but came here to see if anyone has ran in to this before, has a work around etc.</p>

<p><strong>Expected Behaviour</strong></p>

<p>I expect the camera to see the code is taking up a lot more of the screen than I am, focus on it, read the code and proceed to run the code <code>onBarCodeRead</code></p>

<p><strong>What have I tried to fix it?</strong></p>

<ul>
<li>Disable the <code>autoFocus</code> as this was a fix for Android, no luck here.</li>
<li>Manually set the <code>focusDepth</code>.</li>
<li>Manually set <code>autoFocusPointOfInterest</code> to center of screen.</li>
<li>Change the <code>zoom</code> to 0.2 and slowly increase to the point of where it starts to look silly.</li>
<li>Set <code>onGoogleVisionBarcodesDetected</code> to just console.log something as this was another fix for android.</li>
<li>Update <code>react-native-camera@2.6.0</code> </li>
<li>Update to master branch at <code>react-native-camera@git+https://git@github.com/react-native-community/react-native-camera.git</code></li>
</ul>

<p><strong>How can I recreate it?</strong></p>

<ul>
<li>Create new react-native project</li>
<li><code>yarn add react-native-camera</code> / <code>npm install react-native-camera --save</code></li>
<li>set <code>type={RNCamera.Constants.Type.front}</code> to use the front camera.</li>
<li>set <code>autoFocus={RNCamera.Constants.AutoFocus.on}</code> (It's on by default anyway, this just ensures it.</li>
<li>set <code>onBarCodeRead={() =&gt; alert('barcode found')}</code></li>
<li>Try to scan a Code39 / Code128 - <a href=""http://online-barcode-generator.net/"" rel=""nofollow noreferrer"">(creatable here)</a></li>
<li>Try to scan it and you'll find the camera will not focus on it however stay focused on the background. This is also true if you cover the camera with your finger, when you pull your finger away you expect the camera to be out of focus to the background and try and re-focus. This is not the case, it will stay focused at a medium / long distance.</li>
</ul>

<p><strong>Software Used &amp; Versions</strong></p>

<ul>
<li>iOS: 12.1.4</li>
<li>react-native-camera: ^2.1.1 / 2.6.0</li>
<li>react-native: 0.57.7</li>
<li>react: 16.6.1</li>
</ul>

<p><strong>Code</strong></p>

<p>I render the camera in a <a href=""https://github.com/react-native-community/react-native-modal"" rel=""nofollow noreferrer""><code>react-native-modal</code></a> and I've put my code below.</p>

<pre><code>&lt;RNCamera 
  style={styles.camera}
  type={RNCamera.Constants.Type.front}
  flashMode={RNCamera.Constants.FlashMode.off}
  autoFocus={RNCamera.Constants.AutoFocus.on}
  captureAudio={false}
  onBarCodeRead={(barcode) =&gt; {
    if (this.state.isModalVisible) {
      this.setState({
        isModalVisible : false
      }, () =&gt; this.captureQR(barcode.data));
    }
}}&gt;
</code></pre>

<p><strong>Relevant Package Code</strong></p>

<p>I found some code that seems relevant:</p>

<p>at <code>RNCamera.m</code> <a href=""https://github.com/react-native-community/react-native-camera/blob/master/ios/RN/RNCamera.m#L275"" rel=""nofollow noreferrer"">method <code>updateFocusDepth</code></a></p>

<pre><code>- (void)updateFocusDepth
{
    AVCaptureDevice *device = [self.videoCaptureDeviceInput device];
    NSError *error = nil;

    if (device == nil || self.autoFocus &lt; 0 || device.focusMode != RNCameraAutoFocusOff || device.position == RNCameraTypeFront) {
        return;
    }

    if (![device respondsToSelector:@selector(isLockingFocusWithCustomLensPositionSupported)] || ![device isLockingFocusWithCustomLensPositionSupported]) {
        RCTLogWarn(@""%s: Setting focusDepth isn't supported for this camera device"", __func__);
        return;
    }

    if (![device lockForConfiguration:&amp;error]) {
        if (error) {
            RCTLogError(@""%s: %@"", __func__, error);
        }
        return;
    }

    __weak __typeof__(device) weakDevice = device;
    [device setFocusModeLockedWithLensPosition:self.focusDepth completionHandler:^(CMTime syncTime) {
        [weakDevice unlockForConfiguration];
    }];
}
</code></pre>

<p>More specifically just this section here: </p>

<p>If <code>device.position == RNCameraTypeFront</code> it will just return providing it doesn't meet any of the other criteria.</p>

<pre><code>    if (device == nil || self.autoFocus &lt; 0 || device.focusMode != RNCameraAutoFocusOff || device.position == RNCameraTypeFront) {
        return;
    }
</code></pre>",55966437,1,0,,2019-04-25 09:32:35.170 UTC,,2019-05-04 09:50:13.370 UTC,2019-05-03 08:19:24.543 UTC,,7295772,,5283119,1,2,javascript|ios|react-native|react-native-camera,158
Perform operations on text scanned from image using google vision library,55281456,Perform operations on text scanned from image using google vision library,"<p>I am working on a project in which I have to read text from image using google vision library. I have to decide that whether the text obtained from image is email, contact number, location coordinates, math problem or simple string. I am passing scanned text to a function which decides in what category this text falls. I also want to check if portion of the scanned text matches with the above mentioned categories. The problem is text falls in category other than it is supposed to fall in or none of any categories. I have gone through multiple solutions on stack overflow but nothing is sorting the problem. Below is the function I have written for text categories. Thanks</p>

<pre><code> private void checkString(String string){
        Pattern mathPattern = Pattern.compile(""^(\\d+)\\s*([\\+\\*\\-\\/])\\s*(\\d+)\\s*"");
        Pattern paragraphPattern = Pattern.compile(""^[a-zA-Z]\\n?"");
        Pattern locationPattern = Pattern.compile(""^(\\w*)\\s*(\\d+)\\.(\\d+)\\,?(\\d+)\\.(\\d+)\\s*(\\w*)"");
        Matcher coordinateMatcher = locationPattern.matcher(string);
        Matcher paragraphMatcher = paragraphPattern.matcher(string);
        Matcher expressionMatcher = mathPattern.matcher(string);
        isLocationCoordinates=coordinateMatcher.find();
        isParagraph=paragraphMatcher.find();
        isMathExp=expressionMatcher.find();
        if (isLocationCoordinates){
            while (coordinateMatcher.find()){
                for (int i=0; i&lt;coordinateMatcher.groupCount(); i++){
                    Log.v(TAG, coordinateMatcher.group(i));
                }
            }
        }
        if (isParagraph){
            while (paragraphMatcher.find()){
                for (int i=0; i&lt;paragraphMatcher.groupCount(); i++){
                    Log.v(TAG, paragraphMatcher.group(i));
                }
            }
        }
        if (isMathExp){
            btn.setText(""Expression"");
            while (expressionMatcher.find()){
                for (int i=0; i&lt;expressionMatcher.groupCount(); i++ ) {
                    Log.v(TAG, expressionMatcher.group(i));
                }
            }
        }
//        else {
//            Log.v(TAG, ""No category matched"");
//        }
    }
</code></pre>",,0,0,,2019-03-21 13:23:45.770 UTC,,2019-03-21 13:25:53.903 UTC,2019-03-21 13:25:53.903 UTC,,5082285,,9161046,1,0,java|android,19
"Ruby - ""Do"" loop and ""rescue""",34310579,"Ruby - ""Do"" loop and ""rescue""","<p>I'm using the Microsoft computer vision API. The API can recognise faces and gives data on how many people are in an image, what estimated age they are, and what estimated gender. However, I have a ""do"" loop which I can't ""rescue."" Here's the code below: </p>

<pre><code> values = json_data['faces'].map do |result| 
</code></pre>

<p>Here's the error I receive:</p>

<pre><code>C:/Users/KVadher/Desktop/micr1.rb:122:in `block in &lt;main&gt;': undefined method `[]' for nil:NilClass (NoMethodError)
</code></pre>

<p>I want my code to look something like this:</p>

<pre><code> begin
  values = json_data['faces'].map do |result| 
 rescue
 end
</code></pre>

<p>However, when I do this, I get the following error:</p>

<pre><code>C:/Users/USERNAME/Desktop/micr1.rb:123: syntax error, unexpected keyword_rescue
</code></pre>

<p>How do I pass my code if a request doesn't apply to it? </p>",34310681,2,0,,2015-12-16 11:15:08.437 UTC,,2015-12-16 11:35:43.477 UTC,2015-12-16 11:16:44.003 UTC,,1342402,,5080273,1,-2,ruby|loops|rescue,293
ImportError: cannot import name JSONClient,43584965,ImportError: cannot import name JSONClient,"<p>Whenever I request to GoogleVision api's, this error pops up. Even cannot install/uninstall any of the package</p>

<p>Sample Output:</p>

<pre><code>engineer@engineer:~$ sudo pip install --upgrade google-cloud
</code></pre>

<p>[sudo] password for engineer: 
Traceback (most recent call last):</p>

<pre><code>File ""/usr/local/bin/pip"", line 7, in &lt;module&gt;
    from pip import main
  File ""/usr/local/lib/python2.7/dist-packages/pip/__init__.py"", line 26, in &lt;module&gt;
    from pip.utils import get_installed_distributions, get_prog
  File ""/usr/local/lib/python2.7/dist-packages/pip/utils/__init__.py"", line 27, in &lt;module&gt;
    from pip._vendor import pkg_resources
  File ""/usr/local/lib/python2.7/dist-packages/pip/_vendor/pkg_resources/__init__.py"", line 3018, in &lt;module&gt;
    @_call_aside
  File ""/usr/local/lib/python2.7/dist-packages/pip/_vendor/pkg_resources/__init__.py"", line 3004, in _call_aside
    f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/pip/_vendor/pkg_resources/__init__.py"", line 3046, in _initialize_master_working_set
    dist.activate(replace=False)
  File ""/usr/local/lib/python2.7/dist-packages/pip/_vendor/pkg_resources/__init__.py"", line 2578, in activate
    declare_namespace(pkg)
  File ""/usr/local/lib/python2.7/dist-packages/pip/_vendor/pkg_resources/__init__.py"", line 2152, in declare_namespace
    _handle_ns(packageName, path_item)
  File ""/usr/local/lib/python2.7/dist-packages/pip/_vendor/pkg_resources/__init__.py"", line 2091, in _handle_ns
    loader.load_module(packageName)
  File ""/usr/lib/python2.7/pkgutil.py"", line 246, in load_module
    mod = imp.load_module(fullname, self.file, self.filename, self.etc)
  File ""/home/engineer/.local/lib/python2.7/site-packages/google/cloud/logging/__init__.py"", line 18, in &lt;module&gt;
    from pkg_resources import get_distribution
  File ""/home/engineer/.local/lib/python2.7/site-packages/pkg_resources/__init__.py"", line 3036, in &lt;module&gt;
    @_call_aside
  File ""/home/engineer/.local/lib/python2.7/site-packages/pkg_resources/__init__.py"", line 3020, in _call_aside
    f(*args, **kwargs)
  File ""/home/engineer/.local/lib/python2.7/site-packages/pkg_resources/__init__.py"", line 3064, in _initialize_master_working_set
    for dist in working_set
  File ""/home/engineer/.local/lib/python2.7/site-packages/pkg_resources/__init__.py"", line 3064, in &lt;genexpr&gt;
    for dist in working_set
  File ""/home/engineer/.local/lib/python2.7/site-packages/pkg_resources/__init__.py"", line 2594, in activate
    declare_namespace(pkg)
  File ""/home/engineer/.local/lib/python2.7/site-packages/pkg_resources/__init__.py"", line 2162, in declare_namespace
    _handle_ns(packageName, path_item)
  File ""/home/engineer/.local/lib/python2.7/site-packages/pkg_resources/__init__.py"", line 2097, in _handle_ns
    loader.load_module(packageName)
  File ""/usr/lib/python2.7/pkgutil.py"", line 246, in load_module
    mod = imp.load_module(fullname, self.file, self.filename, self.etc)
  File ""/usr/local/lib/python2.7/dist-packages/google/cloud/logging/__init__.py"", line 18, in &lt;module&gt;
    from google.cloud.logging.client import Client
  File ""/usr/local/lib/python2.7/dist-packages/google/cloud/logging/client.py"", line 32, in &lt;module&gt;
    from google.cloud.client import JSONClient
</code></pre>

<p>ImportError: cannot import name JSONClient</p>",,1,4,,2017-04-24 09:53:24.817 UTC,,2017-04-25 08:52:24.197 UTC,,,,,6250659,1,1,machine-learning|google-cloud-platform|google-cloud-vision|google-cloud-ml,749
Google Vision API similar files,45505722,Google Vision API similar files,"<p>I prepare some solution for grouping documents using Google Vision API. I would like grouping documents by something like template of document.</p>

<p>If i firsty scan invoice from one company and a few days after a scan additional other invoice from the same company, can I check they are simlar?</p>",,1,1,,2017-08-04 11:31:18.370 UTC,2,2018-03-08 16:36:21.823 UTC,2018-03-08 16:36:21.823 UTC,,1033581,,182823,1,1,google-cloud-platform|google-cloud-vision|google-vision,49
Google Vision OCR,51646954,Google Vision OCR,"<p>I am trying to read handwritten text from an image using Google Vision API. But the problem is, every time I scan the document (in which I need to recognize handwritten text) and pass it to Google API, the text comes up in a different block. Even though I am scanning the same page. For, eg, the First time the text will come up in Block 8 &amp; next time I scan the document, the text is coming up in Block 10. There is no inconsistency. </p>

<p>I understand the position of text in blocks depends on the scanned document. But is there a better way of going and reading the text? </p>

<p>I know where the handwritten text will be on the scanned doc, but how to determine the position of that text using this google API. </p>",,1,0,,2018-08-02 06:34:54.877 UTC,,2018-08-02 06:53:32.373 UTC,2018-08-02 06:53:32.373 UTC,user10160670,,,10169363,1,2,c#|google-vision,185
Extracting handwritten text from an application form using Google Cloud Vision API,56065772,Extracting handwritten text from an application form using Google Cloud Vision API,"<p>I want to extract handwritten text from an application form using Google Vision API's text detection feature.  It greatly extracts the handwritten text but gives very unorganized JSON type response, which I don't know how to parse because I want to extract only the specific fields like name, contact number, email, etc. and store them into MySQL database.</p>

<p>Code (<a href=""https://cloud.google.com/vision/docs/detecting-fulltext#vision-document-text-detection-python"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/detecting-fulltext#vision-document-text-detection-python</a>):</p>

<pre><code>def detect_document(path):
    """"""Detects document features in an image.""""""
    from google.cloud import vision
    client = vision.ImageAnnotatorClient()

    with io.open(path, 'rb') as image_file:
        content = image_file.read()

    image = vision.types.Image(content=content)

    response = client.document_text_detection(image=image)

    for page in response.full_text_annotation.pages:
        for block in page.blocks:
            print('\nBlock confidence: {}\n'.format(block.confidence))

            for paragraph in block.paragraphs:
                print('Paragraph confidence: {}'.format(
                    paragraph.confidence))

                for word in paragraph.words:
                    word_text = ''.join([
                        symbol.text for symbol in word.symbols
                    ])
                    print('Word text: {} (confidence: {})'.format(
                        word_text, word.confidence))

                    #for symbol in word.symbols:
                    #    print('\tSymbol: {} (confidence: {})'.format(
                    #        symbol.text, symbol.confidence))
</code></pre>

<p><a href=""https://i.stack.imgur.com/M5Ovx.jpg"" rel=""nofollow noreferrer"">Input Image</a></p>

<p>Response from API:</p>

<pre><code>'Block confidence: 0.9900000095367432

Paragraph confidence: 0.9900000095367432
Word text: A (confidence: 0.9900000095367432)
Word text: . (confidence: 0.9900000095367432)
Word text: Bank (confidence: 0.9900000095367432)
Word text: Challan (confidence: 0.9900000095367432)
Paragraph confidence: 0.9900000095367432
Word text: Bank (confidence: 0.9900000095367432)
Word text: Branch (confidence: 0.9800000190734863)

Block confidence: 0.44999998807907104

Paragraph confidence: 0.44999998807907104
Word text: ca (confidence: 0.44999998807907104)

Block confidence: 0.7099999785423279

Paragraph confidence: 0.7099999785423279
Word text: ABC (confidence: 0.9900000095367432)
Word text: muitce (confidence: 0.5699999928474426)

Block confidence: 0.7400000095367432

Paragraph confidence: 0.7400000095367432
Word text: Deposit (confidence: 0.8700000047683716)
Word text: ID (confidence: 0.6700000166893005)
Word text: VOSSÁETM (confidence: 0.5400000214576721)
Word text: - (confidence: 0.7900000214576721)
Word text: 0055 (confidence: 0.9300000071525574)

Block confidence: 0.800000011920929

Paragraph confidence: 0.800000011920929
Word text: Deposit (confidence: 0.9800000190734863)
Word text: Date (confidence: 0.9200000166893005)
Word text: 14 (confidence: 0.47999998927116394)
Word text: al (confidence: 0.27000001072883606)
Word text: 19 (confidence: 0.800000011920929)

Block confidence: 0.9900000095367432

Paragraph confidence: 0.9900000095367432
Word text: ate (confidence: 0.9900000095367432)

Block confidence: 0.9900000095367432

Paragraph confidence: 0.9900000095367432
Word text: B (confidence: 0.9900000095367432)
Word text: . (confidence: 0.9900000095367432)
Word text: Personal (confidence: 0.9900000095367432)
Word text: Information (confidence: 0.9900000095367432)
Word text: : (confidence: 0.9900000095367432)
Word text: Use (confidence: 0.9900000095367432)
Word text: CAPITAL (confidence: 0.9900000095367432)
Word text: letters (confidence: 0.9900000095367432)
Word text: and (confidence: 0.9900000095367432)
Word text: leave (confidence: 0.9900000095367432)
Word text: spaces (confidence: 0.9900000095367432)
Word text: between (confidence: 0.9900000095367432)
Word text: words (confidence: 0.9900000095367432)
Word text: . (confidence: 0.9900000095367432)

Block confidence: 0.9100000262260437

Paragraph confidence: 0.8999999761581421
Word text: Name (confidence: 0.9900000095367432)
Word text: : (confidence: 0.9700000286102295)
Word text: MUHAMMAD (confidence: 0.9599999785423279)
Word text: HANIE (confidence: 0.9200000166893005)
Word text: Father (confidence: 0.9900000095367432)
Word text:  (confidence: 0.9900000095367432)
Word text: s (confidence: 1.0)
Word text: Name (confidence: 0.9900000095367432)
Word text: : (confidence: 0.9900000095367432)
Word text: MUHAMMAD (confidence: 0.9100000262260437)
Word text: Y (confidence: 0.8399999737739563)
Word text: AQOOB (confidence: 0.8500000238418579)
Word text: Computerized (confidence: 0.9800000190734863)
Word text: NIC (confidence: 0.9900000095367432)
Word text: No (confidence: 0.5400000214576721)
Word text: . (confidence: 0.9100000262260437)
Word text: 77 (confidence: 0.8899999856948853)
Word text: 356 (confidence: 0.8100000023841858)
Word text: - (confidence: 0.699999988079071)
Word text: 5 (confidence: 0.8600000143051147)
Word text: 284 (confidence: 0.5699999928474426)
Word text: 345 (confidence: 0.800000011920929)
Word text: - (confidence: 0.41999998688697815)
Word text: 3 (confidence: 0.8199999928474426)
Paragraph confidence: 0.8999999761581421
Word text: D (confidence: 0.699999988079071)
Word text: D (confidence: 0.5600000023841858)
Word text: M (confidence: 0.6700000166893005)
Word text: m (confidence: 0.6600000262260437)
Word text: rrrr (confidence: 0.6600000262260437)
Word text: Gender (confidence: 0.9900000095367432)
Word text: : (confidence: 1.0)
Word text: Male (confidence: 0.9900000095367432)
Word text: Age (confidence: 0.9800000190734863)
Word text: : (confidence: 0.9700000286102295)
Word text: ( (confidence: 0.9399999976158142)
Word text: in (confidence: 0.9700000286102295)
Word text: years (confidence: 0.9900000095367432)
Word text: ) (confidence: 0.9599999785423279)
Word text: 24 (confidence: 0.6499999761581421)
Word text: Date (confidence: 0.9900000095367432)
Word text: of (confidence: 0.9900000095367432)
Word text: Birth (confidence: 0.9900000095367432)
Word text: ( (confidence: 0.12999999523162842)
Word text: 4 (confidence: 0.9399999976158142)
Word text: - (confidence: 0.8999999761581421)
Word text: 06 (confidence: 0.9100000262260437)
Word text: - (confidence: 0.7400000095367432)
Word text: 1999 (confidence: 0.5099999904632568)
Word text: Domicile (confidence: 0.9900000095367432)
Word text: ( (confidence: 0.949999988079071)
Word text: District (confidence: 0.9900000095367432)
Word text: ) (confidence: 0.9900000095367432)
Word text: : (confidence: 0.9599999785423279)
Word text: Mirpuskhas (confidence: 0.8399999737739563)
Word text: Contact (confidence: 0.9399999976158142)
Word text: No (confidence: 0.9100000262260437)
Word text: . (confidence: 0.9900000095367432)
Word text: 0333 (confidence: 0.9900000095367432)
Word text: - (confidence: 0.9800000190734863)
Word text: 7072258 (confidence: 0.9900000095367432)
Paragraph confidence: 0.9200000166893005
Word text: ( (confidence: 0.9900000095367432)
Word text: Please (confidence: 0.9900000095367432)
Word text: do (confidence: 0.9800000190734863)
Word text: not (confidence: 1.0)
Word text: mention (confidence: 0.9900000095367432)
Word text: converted (confidence: 0.949999988079071)
Word text: No (confidence: 0.9900000095367432)
Word text: . (confidence: 0.9900000095367432)
Word text: ) (confidence: 0.9800000190734863)
Word text: Postal (confidence: 0.9900000095367432)
Word text: Address (confidence: 0.9800000190734863)
Word text: : (confidence: 0.9900000095367432)
Word text: Wasdev (confidence: 0.9300000071525574)
Word text: Book (confidence: 0.8799999952316284)
Word text: Depo (confidence: 0.9599999785423279)
Word text: Digri (confidence: 0.9599999785423279)
Word text: Taluka (confidence: 0.9900000095367432)
Word text: jhuddo (confidence: 0.9700000286102295)
Word text: Disstri (confidence: 0.7599999904632568)
Word text: mes (confidence: 0.38999998569488525)
Word text: . (confidence: 0.1899999976158142)

Block confidence: 0.9399999976158142

Paragraph confidence: 0.9399999976158142
Word text: Sindh (confidence: 0.9700000286102295)
Word text: . (confidence: 0.75)

Block confidence: 0.9800000190734863

Paragraph confidence: 0.9800000190734863
Word text: Are (confidence: 0.9900000095367432)
Word text: You (confidence: 0.9900000095367432)
Word text: Government (confidence: 0.9800000190734863)
Word text: Servant (confidence: 0.9900000095367432)
Word text: : (confidence: 0.9900000095367432)
Word text: Yes (confidence: 0.9900000095367432)
Word text: ( (confidence: 0.9900000095367432)
Word text: If (confidence: 0.7599999904632568)
Word text: yes (confidence: 0.9700000286102295)
Word text: , (confidence: 0.9900000095367432)
Word text: please (confidence: 0.9900000095367432)
Word text: attach (confidence: 0.9900000095367432)
Word text: NOC (confidence: 0.9900000095367432)
Word text: ) (confidence: 0.949999988079071)

Block confidence: 0.9900000095367432

Paragraph confidence: 0.9900000095367432
Word text: No (confidence: 0.9900000095367432)

Block confidence: 0.20999999344348907

Paragraph confidence: 0.20999999344348907
Word text: ✓ (confidence: 0.20999999344348907)

Block confidence: 0.9700000286102295

Paragraph confidence: 0.9700000286102295
Word text: Religion (confidence: 0.9599999785423279)
Word text: : (confidence: 0.9900000095367432)
Word text: Muslim (confidence: 0.9900000095367432)

Block confidence: 0.3799999952316284

Paragraph confidence: 0.3799999952316284
Word text: ✓ (confidence: 0.3799999952316284)

Block confidence: 0.9599999785423279

Paragraph confidence: 0.9599999785423279
Word text: Non (confidence: 0.9300000071525574)
Word text: - (confidence: 0.9399999976158142)
Word text: Muslimo (confidence: 0.9700000286102295)'
</code></pre>",56073558,1,0,,2019-05-09 18:45:50.953 UTC,,2019-05-10 09:04:37.080 UTC,,,,,11456782,1,0,python|google-cloud-platform|google-cloud-vision,61
Trying detect image Values using Google Cloud Vision using c# asp.net c#,50715542,Trying detect image Values using Google Cloud Vision using c# asp.net c#,"<p>Trying detect image Values using Google Cloud Vision using c# asp.net c# but i am getting below error.</p>

<pre><code>Error loading native library. Not found in any of the possible locations: C:\Users\mazharkhan\Documents\Visual Studio 2013\WebSites\googleapi\bin\grpc_csharp_ext.x86.dll,C:\Users\mazharkhan\Documents\Visual Studio 2013\WebSites\googleapi\bin\runtimes/win/native\grpc_csharp_ext.x86.dll,C:\Users\mazharkhan\Documents\Visual Studio 2013\WebSites\googleapi\bin\../..\runtimes/win/native\grpc_csharp_ext.x86.dll
</code></pre>

<p>I am getting error in below line. And tried to open this url is not working: <a href=""http://vision.googleapis.com"" rel=""nofollow noreferrer"">http://vision.googleapis.com</a></p>

<pre><code>var channel = new Grpc.Core.Channel(@""http://vision.googleapis.com"", credential.ToChannelCredentials()); // &lt;-- Getting error in this line 
</code></pre>

<p>Below is my design code.</p>

<pre><code>&lt;form id=""form1"" runat=""server""&gt;
&lt;div&gt;
    &lt;asp:Label ID=""Label1"" runat=""server"" Text=""Label""&gt;&lt;/asp:Label&gt;
&lt;/div&gt;
    &lt;asp:Button ID=""Button1"" runat=""server"" Text=""Button"" OnClick=""Button1_Click"" /&gt;
&lt;/form&gt;
</code></pre>

<p>Below is my code which worte in button click for display in label</p>

<pre><code>protected void Button1_Click(object sender, EventArgs e)
{
    var image = Google.Cloud.Vision.V1.Image.FromFile(@""C:\!\cat.jpg"");
    var credential = GoogleCredential.FromFile(@""C:\!\Tutorials-0a2efaf1b53c.json"");
    var channel = new Grpc.Core.Channel(@""http://vision.googleapis.com"", credential.ToChannelCredentials()); // &lt;-- Getting error in this line 
    var client = ImageAnnotatorClient.Create(channel);
    var response = client.DetectText(image); 
    foreach (var annotation in response)
    {
        if (annotation.Description != null)
            //    Console.WriteLine(annotation.Description);
            Label1.Text += annotation.Description + ""\r\n"";

    }

}
</code></pre>

<p>I used below example url:</p>

<p><a href=""https://stackoverflow.com/questions/48999636/detecting-content-in-google-cloud-vision-for-net-does-nothing-hangs-app"">Detecting content in Google Cloud Vision for .NET does nothing/hangs app</a></p>

<p>I created service key account also in google for json file.</p>

<p><a href=""https://i.stack.imgur.com/076aJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/076aJ.png"" alt=""enter image description here""></a></p>",50715652,1,0,,2018-06-06 08:22:07.343 UTC,,2018-07-08 04:44:33.030 UTC,2018-07-08 04:44:33.030 UTC,,9110142,,7036981,1,0,c#|asp.net|google-cloud-vision,765
Can amazon lex chatbot accept image as input ?,50210568,Can amazon lex chatbot accept image as input ?,"<p>I want to build a deep learning chatbot application which accepts image as input. 
I have built a lambda function integrating AWS rekognition that accepts image.Now, i want to extend this lambda function, and connect it to Amazon Lex bot , where user can upload the image for analysis. </p>",,1,0,,2018-05-07 09:05:02.347 UTC,2,2018-05-11 07:02:18.663 UTC,,,,,7889181,1,0,aws-lambda|chatbot|amazon-lex|amazon-rekognition,345
App can't find Application Default Credentials no matter what,47257476,App can't find Application Default Credentials no matter what,"<p>Running on Windows 10 with Android studio 3 using <code>com.google.cloud:google-cloud-vision:0.28.0-beta</code></p>

<p>with these package options in the build.gradle</p>

<pre><code>packagingOptions {
    exclude 'META-INF/DEPENDENCIES'
    exclude 'META-INF/LICENSE'
    exclude 'META-INF/LICENSE.txt'
    exclude 'META-INF/license.txt'
    exclude 'META-INF/NOTICE'
    exclude 'META-INF/NOTICE.txt'
    exclude 'META-INF/notice.txt'
    exclude 'META-INF/ASL2.0'

    exclude 'project.properties'
    exclude 'META-INF/io.netty.versions.properties'
    exclude 'META-INF/INDEX.LIST'
}
</code></pre>

<p>After building the app using the local image sample code found in <a href=""https://cloud.google.com/vision/docs/detecting-fulltext"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/detecting-fulltext</a></p>

<p>I've followed the directions from
<a href=""https://cloud.google.com/vision/docs/auth"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/auth</a></p>

<p>and down paths 1 &amp; 2 in ""How the Application Default Credentials work"" from
<a href=""https://developers.google.com/identity/protocols/application-default-credentials"" rel=""nofollow noreferrer"">https://developers.google.com/identity/protocols/application-default-credentials</a></p>

<p>As well as other SO stuff like <a href=""https://stackoverflow.com/questions/44925170/google-application-credentials-error"">GOOGLE_APPLICATION_CREDENTIALS error</a></p>

<p>following step 1 combined with the linked SO and running <code>gcloud auth application-default print-access-token</code> prints the access token</p>

<p>following step 2 with <code>gcloud auth application-default login</code> gives</p>

<blockquote>
  <p>Credentials saved to file: [C:\Users\USER\AppData\Roaming\gcloud\application_default_credentials.json]</p>
  
  <p>These credentials will be used by any library that requests
  Application Default Credentials.</p>
</blockquote>

<p>yet no matter what i do, the app always crashes with <strong>java.io.IOException: The Application Default Credentials are not available</strong>.  This is additionally verified outside of executing an <code>AnnotateImageRequest</code> by running the code linked after paths 1&amp;2 to see if i can fetch credentials programatically.</p>

<pre><code>    return Single.fromCallable(new Callable&lt;Object&gt;() {
        @Override
        public Object call() throws Exception {
            GoogleCredential credential = GoogleCredential.getApplicationDefault();

            return new Object();
        }
    }).subscribeOn(Schedulers.io()).observeOn(AndroidSchedulers.mainThread());
</code></pre>

<p>Where the GoogleCredential object is available through <code>implementation 'com.google.api-client:google-api-client:1.23.0'</code> and </p>

<pre><code>android {
...
    configurations.all {
        resolutionStrategy.force 'com.google.code.findbugs:jsr305:1.3.9'
    }
...
}
</code></pre>

<p>I'm 50 tabs deep with no answer working and I'm hoping someone here has run into this and can help me because otherwise I'm completely stuck</p>",,0,2,,2017-11-13 05:14:33.190 UTC,,2017-11-13 06:00:07.107 UTC,2017-11-13 06:00:07.107 UTC,,2584521,,2584521,1,2,android|google-app-engine|google-authentication|google-cloud-vision,452
How to make a horizontal color scheme in android,42735068,How to make a horizontal color scheme in android,"<p>I am using the google vision API and it can return the color scheme of a picture like this: </p>

<p><a href=""https://i.stack.imgur.com/i1pcEm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i1pcEm.png"" alt=""google vision color scheme""></a></p>

<p>The API itself returns values to calculate the exact color from the RGB values and to calculate how much of the image contains that color in %.</p>

<p>I am trying to create something like in the first picture. But I have no clue how to do that, so far I just have a listview that gives an overview shown like here. </p>

<p><a href=""https://i.stack.imgur.com/vR0nIl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vR0nIl.png"" alt=""listview overview""></a></p>

<p>Does anyone have an idea how I can create a horizontal color scheme in android where I specify all the colors myself? Even a horizontal listview might work with dynamic widths for each color to reflect the percentage.</p>

<p>Thank you!</p>",42735285,2,0,,2017-03-11 12:14:37.973 UTC,,2017-03-11 14:16:08.293 UTC,2017-03-11 14:16:08.293 UTC,,4985359,,3801533,1,0,android|listview|colors|color-scheme,64
google vision for image analysis in r (multiple imgaes and saving the result),54132185,google vision for image analysis in r (multiple imgaes and saving the result),"<p>I am trying image analysis with google vision in R, able to do it for a single image stored in folder, I have to choose the image and then run googlevisionresponse.</p>

<p>getGoogleVisionResponse(file.choose(),feature = ""LABEL_DETECTION"")</p>

<p>I have 100+ images, i want to do the analysis for all the images in that folder, do not want to choose each image at a time, any ways to do the analysis for all image at same time and save the result in a file.</p>

<p>Any help regarding this?</p>",,0,0,,2019-01-10 15:44:25.233 UTC,,2019-01-10 15:44:25.233 UTC,,,,,8233109,1,0,r|google-vision,23
OCR not working when background color & text color almost simillar,48806569,OCR not working when background color & text color almost simillar,"<p>We are facing one issue when we try to scan bag. The main reason behind that the text color on the bag which are embroidered are almost same as bag's color. So it can not scan exact text which is written on the bag.</p>

<p>To get actual idea I have attached image.</p>

<p>In attached image we want to scan bag's id (D1 150491). Let me know if we have to do extra effort to scan this type of image.</p>

<p><strong>Note: We have tried two SDK</strong></p>

<ul>
<li><a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">Google Vision Api</a> </li>
<li><a href=""https://rtrsdk.com/"" rel=""nofollow noreferrer"">Abbyy</a> </li>
</ul>

<p><a href=""https://i.stack.imgur.com/BRpWk.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BRpWk.jpg"" alt=""enter image description here""></a></p>",48830052,3,4,,2018-02-15 11:49:39.937 UTC,1,2018-06-12 05:52:45.293 UTC,2018-04-24 05:52:12.133 UTC,,1457813,,1457813,1,1,android|ocr|google-cloud-vision|abbyy,412
Get Response Headers from Angular HttpClient call Async method azure AI computer vision,54722619,Get Response Headers from Angular HttpClient call Async method azure AI computer vision,"<p>I am trying to implement <a href=""https://westcentralus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/587f2c6a154055056008f200"" rel=""nofollow noreferrer"">Azure Computer Vision Recognize text AI</a> using Angular. I need to find a particular header from the response of the first Http call and then call the second one. But I am unable to find the header. Can you please help me find what I am missing here? You can see the things I had already tried in the code below.</p>

<pre><code>async post(url: string): Promise&lt;any&gt; {
    const body = {
      url: url,
      observe: 'response'
    };
    const options = {
      headers: new HttpHeaders({
        'Content-Type': 'application/json',
        'Ocp-Apim-Subscription-Key': config.api.apiKey,
        'Access-Control-Expose-Headers': 'allow',
        'resolveWithFullResponse': 'true',
        'responseType': 'text'
      })
    };
    const result = await this.http.post(config.api.baseUrl, body, options)
      .subscribe(async (res: Response) =&gt; {
        console.log(res);
        const operationLocation = res.headers.get('Operation-Location');
        return await this.http.get(operationLocation, options).toPromise();
      });
    return result;
  }
</code></pre>

<p>I am able to see the response headers in the browser network, but the <code>res</code> object is always null.</p>

<p><a href=""https://i.stack.imgur.com/ahlxV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ahlxV.png"" alt=""enter image description here""></a></p>

<p>The Azure documentation says ""The service has accepted the request and will start processing later. 
It will return Accepted immediately and include an “<em>Operation-Location” header. Client side should further query the operation status using the URL specified in this header. The operation ID will expire in 48 hours.</em>""</p>",54723292,2,0,,2019-02-16 11:33:31.780 UTC,,2019-02-16 13:20:45.283 UTC,2019-02-16 12:36:53.077 UTC,,5550507,,5550507,1,0,angular|azure|async-await|computer-vision|azure-ai,82
"google Cloud Vision API: node.js and an image URI, how to invoke vision.detectText()?",44373968,"google Cloud Vision API: node.js and an image URI, how to invoke vision.detectText()?","<p>I'm trying to detect text in a remote image with the google Cloud Vision API, but can't seem to get the vision.detectText() syntax right. </p>

<p>How do I use vision.detectText() when there is no cloud storage bucket?</p>

<p>I'm thinking I can/should ignore the reference to storage.bucket() indicated on <a href=""https://cloud.google.com/vision/docs/detecting-text"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/detecting-text</a> </p>

<p>I have:</p>

<pre><code> vision.detectText('https://drive.google.com/file
   /d/0Bw4DMtLCtPMkWVlIVXE5a2ZpQlU/view?usp=drivesdk')
          .then((results) =&gt; {
            const detections = results[0];
            console.log('Text:');
            detections.forEach((text) =&gt; console.log(text));
          })
          .catch((err) =&gt; {
            console.error('ERROR:', err);
          });
</code></pre>

<p>the console reports:</p>

<pre><code>ERROR: { PartialFailureError: A failure occurred during this request.
at /Users/node_modules/@google-cloud/vision/src/index.js:434:15
at /Users/node_modules/@google-cloud/vision/src/index.js:126:5
at _combinedTickCallback (internal/process/next_tick.js:80:11)
at process._tickCallback (internal/process/next_tick.js:104:9)
errors: 
[ { image:  'https://drive.google.com/file/d
 /0Bw4DMtLCtPMkNFFselFhU0RMV2c/view?usp=drivesdk',
   errors: [Object] } ],
 response: { responses: [ [Object] ] },
 message: 'A failure occurred during this request.' }
</code></pre>

<p>I have tried using:   </p>

<pre><code>vision.detectText(storage.bucket().file('https://......
</code></pre>

<p>but the error is:</p>

<pre><code>Error: A bucket name is needed to use Cloud Storage.
</code></pre>",44375527,1,0,,2017-06-05 17:00:52.970 UTC,,2017-06-05 19:53:06.873 UTC,,,,,1405141,1,0,google-cloud-vision,639
Vision api not supporting camera autofocus,46549047,Vision api not supporting camera autofocus,"<p>I am using <strong>google vision api</strong> for scanning QR code and barcode. It is not supporting camera autofocus and remains blurred when detecting a barcode. Although my device supports autofocus. I am using autofocus feature provided by vision api but its not working </p>

<pre><code>cameraSource = new CameraSource
            .Builder(getActivity(), barcodeDetector)
            .setRequestedPreviewSize(1600, 1024)
            .setAutoFocusEnabled(true)
            .build();
</code></pre>

<p>Log is showing:</p>

<pre><code>I/CameraSource: Camera auto focus is not supported on this device.
</code></pre>

<p>Please Help. How can I resolve this?</p>",,2,4,,2017-10-03 16:12:45.647 UTC,,2017-10-05 07:29:03.437 UTC,2017-10-04 12:17:50.363 UTC,,8231841,,8231841,1,3,java|android|qr-code|barcode-scanner|vision-api,497
Google Cloud API not returning any response,50714894,Google Cloud API not returning any response,"<p>Background information:<br>
I'm trying to create a PoC for Google Cloud Vision API using their <a href=""https://github.com/GoogleCloudPlatform/google-cloud-dotnet"" rel=""nofollow noreferrer"">.NET library</a>.</p>

<p>What I have done:<br>
Create a simple console apps with the following code for Vision API.  </p>

<pre><code>GoogleCredential credential = GoogleCredential.FromFile(ConfigurationManager.AppSettings[""GoogleCredentialFile""]);
Grpc.Core.Channel channel = new Grpc.Core.Channel(Google.Cloud.Vision.V1.ImageAnnotatorClient.DefaultEndpoint.ToString(), credential.ToChannelCredentials());
var client = Google.Cloud.Vision.V1.ImageAnnotatorClient.Create(channel);

var image = Google.Cloud.Vision.V1.Image.FromFile(@""C:\Users\u065340\Documents\sample.jpg"");
var response = client.DetectLabels(image);
foreach (var annotation in response)
{
    if (annotation.Description != null)
    result = annotation.Description;
}
</code></pre>

<p>Problem:<br>
The line <code>client.DetectLabels(image)</code> gets stuck for a long time before ultimately throwing the error <code>Deadline Exceeded</code>.<br>
My code sits behind a corporate proxy, but I have validated that it is not blocking internet access because I can call <a href=""http://https://vision.googleapis.com/$discovery/rest?version=v1"" rel=""nofollow noreferrer"">https://vision.googleapis.com/$discovery/rest?version=v1</a> from the same apps and get its JSON response just fine.<br>
Any suggestions?</p>",50736959,1,10,,2018-06-06 07:46:01.787 UTC,,2018-06-08 02:01:37.777 UTC,,,,,860864,1,0,c#|.net|ocr|google-cloud-vision,441
Possible to reorder an array?,53224704,Possible to reorder an array?,"<p>So I am using Google Vision TEXT_DETECTION and the basis of it is - it reads a numberplate then covers it with a polygon using PHPGD. now that's all great but it seems the array of co-ordinates are in the wrong order and im smashing my head against the wall I hope you can help :)</p>

<p><a href=""https://i.stack.imgur.com/3q2Ry.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3q2Ry.png"" alt=""numberplate ""></a></p>

<p>In the image above you can see the number plate and the polygon that surrounds it. You can see that it should be 2 squares but it is one square and a cross</p>

<p>Here is my code where I get the coordiantes and use them to place a polygon</p>

<pre><code>$response = json_decode($json_response, true);

//dd($response);
$red =0;
$green =0;
$blue = 0;
$i =0;
foreach($response['responses'][0]['fullTextAnnotation']['pages'] as $box) {
    $points = array();
    foreach ($box['blocks'] as $block) {
        foreach ($block['paragraphs'] as $paragraph) {
            foreach ($paragraph['words'] as $word) {
                foreach ($word['boundingBox']['vertices'] as $vertex) {
                    array_push($points, $vertex['x'], $vertex['y']);
                }
                $count_points = count($points) / 2;
                $color = imagecolorallocate($im, round(0), round(0), round(0));
                imagefilledpolygon($im, $points, $count_points, $color);
                var_dump($points);
            }

        }

    }

}
</code></pre>

<p>Here is a var_dump of $points(The coordintes)</p>

<pre><code>    array(8) { 
[0]=&gt; int(424) 
[1]=&gt; int(224) 
[2]=&gt; int(446) 
[3]=&gt; int(218) 
[4]=&gt; int(451) 
[5]=&gt; int(235) 
[6]=&gt; int(429) 
[7]=&gt; int(241) }
 array(16) { 
[0]=&gt; int(424)
 [1]=&gt; int(224)
 [2]=&gt; int(446)
 [3]=&gt; int(218)
 [4]=&gt; int(451) 
[5]=&gt; int(235) 
[6]=&gt; int(429) 
[7]=&gt; int(241) 
[8]=&gt; int(454) 
[9]=&gt; int(216)
 [10]=&gt; int(472)
 [11]=&gt; int(211)
 [12]=&gt; int(477)
 [13]=&gt; int(228)
 [14]=&gt; int(459)
 [15]=&gt; int(233)
 }
</code></pre>",,1,0,,2018-11-09 11:16:35.227 UTC,1,2018-11-10 11:08:31.523 UTC,2018-11-09 12:09:06.670 UTC,,5260982,,10101320,1,1,php|arrays|google-vision,83
I want implement face recognition as part of authentication,54017318,I want implement face recognition as part of authentication,"<p>Currently I am using <a href=""https://azure.microsoft.com/en-in/services/cognitive-services/face/"" rel=""nofollow noreferrer"">Microsoft Face api</a> to match photo during login 
(capturing using webcam while login and compared with already uploaded photo).</p>

<p>Issue I am facing is people can use photos from mobile or some photos to share account.</p>

<p>Is there any full proof way to verify face?</p>

<p>Thanks,</p>",,1,4,,2019-01-03 06:27:53.407 UTC,,2019-01-04 09:23:01.187 UTC,,,,,432128,1,0,api|authentication|face-recognition|azure-cognitive-services,25
API to extract text from a image for Android Mobile,36976312,API to extract text from a image for Android Mobile,<p>I am trying to extract text from a picture taken in Android Mobile through some API. Will Google Vision help me with that? I used OCR too but I felt that the output is not accurate. Any suggestions?</p>,,1,0,,2016-05-02 06:36:51.133 UTC,,2016-06-29 05:49:51.720 UTC,2016-05-02 07:50:55.167 UTC,,3919155,,2189252,1,-2,android|eclipse,638
YuvImage conversion to Bitmap is resulting in green images on android Pie (using pixel mobile),53955998,YuvImage conversion to Bitmap is resulting in green images on android Pie (using pixel mobile),"<p>I am using google vision to get camera frames and overlay bitmap on each camera frame and make it a video file but when converting frame using YuvImage to bitmap.</p>

<p>I am getting green images in Google Pixel (Android P). I tried in lollipop and Marshmallow and it is working fine.</p>",,0,0,,2018-12-28 09:05:43.600 UTC,,2018-12-28 09:41:22.370 UTC,2018-12-28 09:41:22.370 UTC,,8757883,,10324430,1,0,bitmap|yuv|android-9.0-pie,25
Google vision Text Detection response to be line by line,46548182,Google vision Text Detection response to be line by line,"<p>I am using the Google vision api to perform text recognition on receipt images. I am getting some nice results returned but the format in which the return is quite unreliable. If there is a large gap between text the readout will print the line below instead of the line next to it.</p>

<p>For example, with the following <a href=""https://i.stack.imgur.com/TRTXo.png"" rel=""nofollow noreferrer"">Recipt Image</a> i get the below response: </p>

<pre><code>    4x Löwenbräu Original a 3,00 12,00 1
    8x Weissbier dunkel a 3,30 26,401
    3x Hefe-Weissbier a 3,30 9,90 1
    1x Saft 0,25
    1x Grosses Wasser
    1x Vegetarische Varia
    1x Gyros
    1x Baby Kalamari Gefu
    2x Gyros Folie
    1x Schafskäse Ofen
    1x Bifteki Metaxa
    1x Schweinefilet Meta
    1x St ifado
    1x Tee
    2,50 1
    2,40 1
    9,90 1
    8,90 1
    12,90
    a 9,9019,80 1
    6,90 1
    11,90 1
    13,90 1
    14,90 1
    2,10 1
</code></pre>

<p>Which starts of well and as expected but then becomes fairly un helpful when trying to connect prices to text etc. The ideal response would be as follows:</p>

<pre><code>    4x Löwenbräu Original a 3,00 12,00 1
    8x Weissbier dunkel    a 3,30 26,401
    3x Hefe-Weissbier      a 3,30 9,90 1
    1x Saft 0,25                  2,50 1
    1x Grosses Wasser             2,40 1
    1x Vegetarische Varia         9,90 1
    1x Gyros                      8,90 1
    1x Baby Kalamari Gefu        12,90 1
    2x Gyros Folie         a 9,9019,80 1
    1x Schafskäse Ofen            6,90 1
    1x Bifteki Metaxa            11,90 1
    1x Schweinefilet Meta        13,90 1
    1x St ifado                  14,90 1
    1x Tee                        2,10 1
</code></pre>

<p>Or close to that. </p>

<p>Is there a formatting request you can add to the api to get different responses? I have had success when using tessereact where you can change the output format to achieve this result and was wondering if the vision api has something similar.</p>

<p>I understand the api returns letter coordinates which could be used but i was hoping not to have to go into that kind of depth.</p>",46552248,2,0,,2017-10-03 15:27:15.143 UTC,1,2018-01-16 10:20:09.630 UTC,2017-10-04 03:33:54.887 UTC,,322020,,8238879,1,5,swift|google-cloud-vision,1022
Authenticating app with Google Vision API,51811837,Authenticating app with Google Vision API,"<p>I'm trying to upgrade my app to google-cloud-vision:1.35.0 but I can't authenticate with my api key.</p>

<p>Previously it was as simple as adding my key to the method before calling it. It went something like this :</p>

<pre><code>VisionRequestInitializer requestInitializer = new VisionRequestInitializer(CLOUD_VISION_API_KEY);
</code></pre>

<p>I don't think there's a method like that anymore. I was trying to run the steps here: <a href=""https://cloud.google.com/docs/authentication/getting-started"" rel=""nofollow noreferrer"">https://cloud.google.com/docs/authentication/getting-started</a>
,including creating a service account and using that export command to export the json to my project.</p>

<p>And yet, I still keep getting the same error :</p>

<blockquote>
  <p>The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See <a href=""https://developers.google.com/accounts/docs/application-default-credentials"" rel=""nofollow noreferrer"">https://developers.google.com/accounts/docs/application-default-credentials</a> for more information</p>
</blockquote>

<p>Is there a simpler way to add my authorize my app? Whether it be with my api key or with the service account json. I've been stuck on this for several days.</p>",,1,0,,2018-08-12 18:46:19.330 UTC,,2018-08-12 18:53:44.637 UTC,,,,,4092656,1,0,android|google-cloud-platform|google-vision,108
node.js / google drive sdk v3: access to webContentLink (and all image metadata),44355403,node.js / google drive sdk v3: access to webContentLink (and all image metadata),"<p>I'm creating an image file in drive v3 successfully, but cannot retrieve the URI for the file; I'm expecting to find it in the success response_object.</p>

<p>I do find the file ID in the response_object and I can use that to construct a URI that displays the image: </p>

<p><a href=""https://drive.google.com/file/d/0Bw4DMtLCtPMkNERiTnpVNDdQV2c/view"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/0Bw4DMtLCtPMkNERiTnpVNDdQV2c/view</a></p>

<p>I would like to have 'an official/correct' URI to hand to the google Cloud Vision API as an image source. I think I'm looking for the webContentLink v3-file metadata. I guess, more generally, I can't see how to get all the metadata (as described on <a href=""https://developers.google.com/drive/v3/reference/files"" rel=""nofollow noreferrer"">https://developers.google.com/drive/v3/reference/files</a>) for the file that I have just created.</p>

<pre><code>        var newFileMetadata = {
          'name': unique_file_name,
          description: options.multipart,
          useContentAsIndexableText: false,
          parents: [ file.id ]
        };

        var media = {
          mimeType: 'image/jpg',
          body: fs.createReadStream(options.src_dir + '/' + sourceFile),
          viewersCanCopyContent: true,
          writersCanShare: true
        };

        var request_object = drive.files.create({
            auth: auth,
            resource: newFileMetadata,
            media: media,
            fields: ['id']
            },
            function(err, response) {
              if (err) {
                console.log('drive.files.create error: %s %s', err, response);
                return;
              } else {
                // file create success; get response
                console.log('dump the response\n %s', JSON.stringify(response));
              }
            }
          );  // end of drive.files.create()
</code></pre>

<p>and the console output is (with cr for readability):</p>

<p>dump the request_object</p>

<p>{""uri"":
 {""protocol"":""https:"",
 ""slashes"":true,
 ""auth"":null,
 ""host"":""www.googleapis.com"",
 ""port"":null,
 ""hostname"":""www.googleapis.com"",
 ""hash"":null,
 ""search"":""?fields=id&amp;uploadType=multipart"",
 ""query"":""fields=id&amp;uploadType=multipart"",
 ""pathname"":""/upload/drive/v3/files"",
 ""path"":""/upload/drive/v3/files?fields=id&amp;uploadType=multipart"",
 ""href"":""<a href=""https://www.googleapis.com/upload/drive/v3/files?fields=id&amp;uploadType=multipart"" rel=""nofollow noreferrer"">https://www.googleapis.com/upload/drive/v3/files?fields=id&amp;uploadType=multipart</a>""},
 ""method"":""POST"",
 ""headers"":{""Authorization"":""Bearer ya29.GlxfB18oVtG6A3j7cXzSq17-RSmj9-2BlQm4zHD5sPzKkfhRM1FxlxUHc9mxWaka1N2fBiTJun-SYLB8ewuc63XVbUo01q0bS2FiS6iJTq9O1h9FQWfoO5r8E6z_6Q"",
 ""User-Agent"":""google-api-nodejs-client/0.10.0"",""host"":""www.googleapis.com"",
 ""transfer-encoding"":""chunked"",
 ""content-type"":""multipart/related; 
 boundary=d6cef3f5-2246-5654-ac1f-d00561be5e8a""}
 }</p>

<p>dump the response
 {""id"":""0Bw4DMtLCtPMkNERiTnpVNDdQV2c""}</p>

<p>to recap:</p>

<ul>
<li>how do I get (in Node.js) a google Cloud Vision appropriate URI for a newly created google Drive v3 image file?</li>
<li>how do I get (in Node.js) the metadata for a newly created google Drive v3 image file?</li>
</ul>

<p>Many thanks.</p>

<p>new information: I have tried a separate drive.files.get()</p>

<pre><code>                drive.files.get({
                  auth: auth,
                  fileId: response.id
                },
                function(err, response) {
                  if (err) {
                  console.log('drive.files.get error: %s %s', err, response);
                  return;
                  } else {
                  // file create success; get properties
                  console.log('get properties\n %s', JSON.stringify(response));
                  }
                });
</code></pre>

<p>and I do receive some metadata (but not the big picture), console output, new upload:</p>

<p>get properties
 {""kind"":""drive#file"",""id"":""0Bw4DMtLCtPMkWFdTMUVwY2tRZ2c"",""name"":""2017-06-04T17:57:42.189Zlovelock.png"",""mimeType"":""image/png""}</p>",44359493,1,0,,2017-06-04 14:55:41.467 UTC,,2017-06-04 22:39:56.820 UTC,2017-06-04 18:04:34.690 UTC,,1405141,,1405141,1,0,google-drive-api|google-cloud-vision,396
Reading Different Font types with Vision API,49877255,Reading Different Font types with Vision API,"<p>I am trying to extract text from such images but Google Vision API does not seem to recognise majority of the text, Can someone suggest a better alternative? <a href=""https://i.stack.imgur.com/oQV2B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oQV2B.png"" alt=""enter image description here""></a></p>

<p>Results from Google OCR
<a href=""https://i.stack.imgur.com/XvSUs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XvSUs.png"" alt=""enter image description here""></a></p>",,1,7,,2018-04-17 11:47:25.427 UTC,,2018-04-17 14:33:58.713 UTC,2018-04-17 14:08:06.403 UTC,,7617328,,7617328,1,-1,image-processing|google-cloud-platform|ocr|text-extraction|vision-api,95
"I got the ""image-annotator::Bad image data.: Image processing error!"" from Google Cloud Vision",41299413,"I got the ""image-annotator::Bad image data.: Image processing error!"" from Google Cloud Vision","<p>I am trying to call Google Cloud Vision from a PHP Script. And also, I want to get the image data from a Web page and try to send image data from JavaScript to PHP script.</p>

<p>But, I got the error message from Google Cloud Vision.</p>

<pre><code>{
    ""responses"": [
    {
        ""error"": {
            ""code"": 3,
        ""message"": ""image-annotator::Bad image data.: Image processing error!""
        }
    }]
}
</code></pre>

<p>This is my javascript code fragment.</p>

<pre><code>    var b64 = ImageToBase64(img, ""image/jpeg"");
    $.ajax ({
        type: ""POST"",
        url: ""php/ocr.php"",
        data: ""data="" + b64,
        contentType: false,
        processData: false,

        // Method when calling ocr.php was successed. 
        success: function(data, dataType)
        {
            // Show the data
            console.log(data);
            $(""#source_text"").html(data);
            var text = ""It is snow today"";
            translateText(text);
        },
        // Method when calling ocr.php was failed.
        error: function(XMLHttpRequest, textStatus, errorThrown)
        {
            // Display error message.
            alert('Error : ' + errorThrown);
        }
    });

function ImageToBase64(img, mime_type) {
    // New Canvas
    var canvas = document.createElement('canvas');
    canvas.width  = img.width;
    canvas.height = img.height;
    // Draw Image
    var ctx = canvas.getContext('2d');
    ctx.drawImage(img, 0, 0);
    // To Base64
    return canvas.toDataURL(mime_type);
}
</code></pre>

<p>And my PHP script is below.</p>

<pre><code>$api_key = ""my-api-key"" ;

$image_data = $_POST[""data""];
$image = base64_decode($image_data);

// Feature Type
$feature = ""TEXT_DETECTION"";

$param = array(""requests"" =&gt; array());
// $item[""image""] = array(""content"" =&gt; base64_encode($image));
$item[""image""] = array(""content"" =&gt; $image_data);
$item[""features""] = array(array(""type"" =&gt; $feature, ""maxResults"" =&gt; 1));
$param[""requests""][] = $item;

$json = json_encode($param);

$curl = curl_init() ;
curl_setopt($curl, CURLOPT_URL, ""https://vision.googleapis.com/v1/images:annotate?key="" . $api_key);
curl_setopt($curl, CURLOPT_HEADER, true);
curl_setopt($curl, CURLOPT_CUSTOMREQUEST, ""POST"");
curl_setopt($curl, CURLOPT_HTTPHEADER, array(""Content-Type: application/json""));
curl_setopt($curl, CURLOPT_SSL_VERIFYPEER, false);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);
curl_setopt($curl, CURLOPT_TIMEOUT, 15);
curl_setopt($curl, CURLOPT_POSTFIELDS, $json);
$res1 = curl_exec($curl);
$res2 = curl_getinfo($curl);
curl_close($curl);

$json = substr($res1, $res2[""header_size""]);
$array = json_decode($json, true);

echo $json;
</code></pre>

<p>I guess image processing must be wrong. But I have no idea what should I do about this. 
Would you give me an advice? </p>",,2,1,,2016-12-23 10:14:11.217 UTC,,2017-03-27 21:52:50.443 UTC,,,,,7157015,1,2,javascript|php,1014
Groovy Spock mock calling real method of mocked class,55568798,Groovy Spock mock calling real method of mocked class,"<p>I am trying to write a unit test for a class that uses Google's vision API with the <code>AnnotatorImageClient</code> from the <code>google-cloud-vision</code> lib.
The problem is that my mocked <code>AnnotatorImageClient</code> for some reason still calls the real <code>batchAnnotateImages</code> method and then throws a NPE, which breaks my test. 
I have never seen this behavior on a mock before and I'm wondering if I'm doing something wrong, if there is a bug in spock/groovy or if it has something to do with that Google lib?</p>

<p>I have already checked if the object used in my class is really a mock, which it is. I have tried with Spock version 1.2-groovy-2.5 and 1.3-groovy.2.5</p>

<p>The class that is tested:</p>

<pre><code>public class VisionClient {

    private final ImageAnnotatorClient client;

    @Autowired
    public VisionClient(final ImageAnnotatorClient client) {
        this.client = client;
    }

    public Optional&lt;BatchAnnotateImagesResponse&gt; getLabelsForImage(final Image image) {
        var feature = Feature.newBuilder().setType(LABEL_DETECTION).build();

        var request = AnnotateImageRequest.newBuilder()
                .addFeatures(feature)
                .setImage(image)
                .build();

        return Optional.ofNullable(client.batchAnnotateImages(singletonList(request)));
}
</code></pre>

<p>The test:</p>

<pre><code>class VisionClientSpec extends Specification {
    def ""The client should use Google's client to call Vision API""() {
        given:
        def googleClientMock = Mock(ImageAnnotatorClient)
        def visionClient = new VisionClient(googleClientMock)
        def imageMock = Image.newBuilder().build()

        when:
        def resultOpt = visionClient.getLabelsForImage(imageMock)

        then:
        1 * googleClientMock.batchAnnotateImages(_ as List) &gt;&gt; null
        !resultOpt.isPresent()
    }
}
</code></pre>

<p>I would expect the mock to simply return <code>null</code> (I know that this test doesn't make a lot of sense). Instead, it calls <code>com.google.cloud.vision.v1.ImageAnnotatorClient.batchAnnotateImages</code> which throws an NPE.</p>",,1,2,,2019-04-08 08:05:36.077 UTC,,2019-04-08 10:24:10.770 UTC,,,,,4997884,1,2,java|groovy|mocking|spock|vision-api,113
Android AWS RekognitionClient error,45300613,Android AWS RekognitionClient error,"<p>I am trying to use Amazon Web Services Recognition in Android but I get a Problem with the RekognitionClient. As I try to initialize it, I get the error: </p>

<blockquote>
  <p>No instance field endpointPrefix of type Ljava/lang/String; in class Lcom/amazonaws/services/rekognition/AmazonRekognitionClient; or its superclasses (declaration of 'com.amazonaws.services.rekognition.AmazonRekognitionClient' appears in /data/app/com.amazonaws.husebnerbot-2/base.apk)</p>
</blockquote>

<p>I have tried everything but I can not find my error. Can you help me?</p>

<pre><code>private void initializeRekognitionSDK() {
    Log.d(TAG, ""Rekognition Client"");

    CognitoCredentialsProvider credentialsProvider = new CognitoCachingCredentialsProvider(
            getApplicationContext(),
            appContext.getResources().getString(R.string.identity_id_test),
            Regions.fromName(""us-east-1"")
    );

    amazonRekognitionClient = new AmazonRekognitionClient(credentialsProvider);
}
</code></pre>

<p>Thank you!</p>",,1,3,,2017-07-25 10:28:53.580 UTC,,2018-01-08 12:57:03.733 UTC,2017-07-25 10:33:36.017 UTC,,5908465,,8020659,1,0,android|amazon-web-services|amazon-rekognition,185
I need to get torch turn on by toggle Button at on going Camera Activity By adding some more lines of code without touching the existing code,38890861,I need to get torch turn on by toggle Button at on going Camera Activity By adding some more lines of code without touching the existing code,"<p>I do have a Barcode Decoding Applcation :  <a href=""https://play.google.com/store/apps/details?id=com.barcodereader"" rel=""nofollow"">https://play.google.com/store/apps/details?id=com.barcodereader</a></p>

<p>I have Used Zbar library and Google Vision API for Scanning</p>

<p>Now what i want is while scanning the Barcode if user taps on a button at appbar for turning on Torch (Flash) then it should be turn on and off vice-versa.</p>

<p>But the Problem is the camera is already on with its all parameters so when user taps the button to turn on torch we need to interrupt the ongoing Camera parameters and I don't want to do that,</p>

<p>I am searching the other way to get torch on without changing the existing Camera Parameters..</p>

<p>Below is The Camera Activity of ZBar And Google Vision this both use some other Camera Classes for Camera Preview.</p>

<pre><code>@SuppressWarnings(""deprecation"")
public class ZBarFirstScannerActivity extends AppCompatActivity{

//TextView tv;
ImageView iv;
LinearLayout ll;
private Camera mCamera;
private CameraPreview mPreview;
private Handler autoFocusHandler;
private ImageScanner scanner;
private boolean barcodeScanned = false;
private boolean previewing = true;
TextView tv;

static {
    System.loadLibrary(""iconv"");
}
static {
    System.loadLibrary(""zbarjni"");
}



public void onCreate(Bundle savedInstanceState)
{
    super.onCreate(savedInstanceState);


    setContentView(R.layout.barcode_capture1d);


    tv = (TextView) findViewById(R.id.textVertical);
    tv.setRotation(90);

    initToolbar();


    autoFocusHandler = new Handler();
    mCamera = getCameraInstance();
    // Instance barcode scanner

    scanner = new ImageScanner();
    scanner.setConfig(0, Config.X_DENSITY, 1);
    scanner.setConfig(0, Config.Y_DENSITY, 1);
    scanner.setConfig(Symbol.CODE128, Config.ENABLE,1);
    scanner.setConfig(Symbol.EAN13, Config.ENABLE,1);


    mPreview = new CameraPreview(this, mCamera, previewCb, autoFocusCB);
    FrameLayout preview = (FrameLayout)findViewById(R.id.cameraPreview);
    preview.addView(mPreview);


}

private void initToolbar() {

    final Toolbar toolbar = (Toolbar) findViewById(R.id.toolbar);
    setSupportActionBar(toolbar);
    final ActionBar actionBar = getSupportActionBar();

    if (actionBar != null) {


        actionBar.setHomeButtonEnabled(true);
        actionBar.setHomeAsUpIndicator(ContextCompat.getDrawable(this, R.drawable.abc_ic_ab_back_mtrl_am_alpha));

        actionBar.setDisplayHomeAsUpEnabled(true);
    }
}
/** A safe way to get an instance of the Camera object. */
public static Camera getCameraInstance()
{
    Camera c = null;
    try
    {
        c = Camera.open();
    } catch (Exception e)
    {
        //nada
    }
    return c;
}

private void releaseCamera()
{
    if (mCamera != null)
    {
        previewing = false;
        mCamera.setPreviewCallback(null);
        mCamera.release();
        mCamera = null;
    }
}



PreviewCallback previewCb = new PreviewCallback()
{
    public void onPreviewFrame(byte[] data, Camera camera)
    {
        Camera.Parameters parameters = camera.getParameters();
        Size size = parameters.getPreviewSize();

        Image barcode = new Image(size.width, size.height, ""Y800"");
        barcode.setData(data);

        int result = scanner.scanImage(barcode);
        if (result != 0)
        {
            previewing = false;
            mCamera.setPreviewCallback(null);
            mCamera.stopPreview();
            SymbolSet syms = scanner.getResults();
            for (Symbol sym : syms)
            {
                barcodeScanned = true;

                Intent returnIntent = new Intent();
                returnIntent.putExtra(""BARCODE"", sym.getData());
                setResult(MainActivity.BAR_CODE_TYPE_128,returnIntent);
                releaseCamera();
                finish();
                break;
            }
        }
    }
};

// Mimic continuous auto-focusing
AutoFocusCallback autoFocusCB = new AutoFocusCallback()
{
    public void onAutoFocus(boolean success, Camera camera)
    {
        autoFocusHandler.postDelayed(doAutoFocus, 3000);
    }
};

private Runnable doAutoFocus = new Runnable()
{
    public void run()
    {
        if (previewing)
            mCamera.autoFocus(autoFocusCB);
    }
};

public void onPause() {
    super.onPause();
    releaseCamera();
}

public void onResume(){
    super.onResume();
    new ZBarFirstScannerActivity();

}

@Override
public void onBackPressed() {

    releaseCamera();
    finish();
}

@Override
public boolean onOptionsItemSelected(MenuItem item) {
    int id = item.getItemId();

    if (id == android.R.id.home) {
        onBackPressed();
        return true;
    }
    return super.onOptionsItemSelected(item);
}
}
</code></pre>

<p>And GoogleScanner</p>

<pre><code>public final class GoogleScannerActivity extends AppCompatActivity {
private static final String TAG = ""Barcode-reader"";

// intent request code to handle updating play services if needed.
private static final int RC_HANDLE_GMS = 9001;

// permission request codes need to be &lt; 256
private static final int RC_HANDLE_CAMERA_PERM = 2;

// constants used to pass extra data in the intent
public static final String AutoFocus = ""AutoFocus"";
public static final String UseFlash = ""UseFlash"";
public static final String BarcodeObject = ""Barcode"";
Bitmap bmp;
FileOutputStream fos = null;
private Camera c;

Switch aSwitch;
private CameraSource mCameraSource;
private CameraSourcePreview mPreview;
private GraphicOverlay&lt;BarcodeGraphic&gt; mGraphicOverlay;

// helper objects for detecting taps and pinches.
private ScaleGestureDetector scaleGestureDetector;
private GestureDetector gestureDetector;

/**
 * Initializes the UI and creates the detector pipeline.
 */
@Override
public void onCreate(Bundle icicle) {
    super.onCreate(icicle);
    setContentView(R.layout.barcode_capture2d);
    initToolbar();
    ActivitySource.caller = this;
    mPreview = (CameraSourcePreview) findViewById(R.id.preview);
    mGraphicOverlay = (GraphicOverlay&lt;BarcodeGraphic&gt;)findViewById(R.id.graphicOverlay);

    boolean autoFocus = true;
    boolean useFlash = false;

    // Check for the camera permission before accessing the camera.  If the
    // permission is not granted yet, request permission.
    int rc = ActivityCompat.checkSelfPermission(this, Manifest.permission.CAMERA);
    if (rc == PackageManager.PERMISSION_GRANTED) {
        createCameraSource(autoFocus, useFlash);
    } else {
        requestCameraPermission();
    }

    gestureDetector = new GestureDetector(this, new CaptureGestureListener());
    scaleGestureDetector = new ScaleGestureDetector(this, new ScaleListener());

    /*Snackbar.make(mGraphicOverlay, ""Tap to capture. Pinch/Stretch to zoom"",
            Snackbar.LENGTH_LONG)
            .show();*/
}

private void initToolbar() {


    final Toolbar toolbar = (Toolbar) findViewById(R.id.toolbar);
    setSupportActionBar(toolbar);
    final ActionBar actionBar = getSupportActionBar();

    if (actionBar != null) {

        actionBar.setHomeButtonEnabled(true);
        actionBar.setHomeAsUpIndicator(ContextCompat.getDrawable(this, R.drawable.abc_ic_ab_back_mtrl_am_alpha));

        actionBar.setDisplayHomeAsUpEnabled(true);
    }
}

private Camera.Size getBestPreviewSize(int width, int height, Camera.Parameters parameters){
    Camera.Size bestSize = null;
    List&lt;Camera.Size&gt; sizeList = parameters.getSupportedPreviewSizes();

    bestSize = sizeList.get(0);

    for(int i = 1; i &lt; sizeList.size(); i++){
        if((sizeList.get(i).width * sizeList.get(i).height) &gt;
                (bestSize.width * bestSize.height)){
            bestSize = sizeList.get(i);
        }
    }

    return bestSize;
}
/**
 * Handles the requesting of the camera permission.  This includes
 * showing a ""Snackbar"" message of why the permission is needed then
 * sending the request.
 */
private void requestCameraPermission() {
    Log.w(TAG, ""Camera permission is not granted. Requesting permission"");

    final String[] permissions = new String[]{Manifest.permission.CAMERA};

    if (!ActivityCompat.shouldShowRequestPermissionRationale(this,
            Manifest.permission.CAMERA)) {
        ActivityCompat.requestPermissions(this, permissions, RC_HANDLE_CAMERA_PERM);
        return;
    }

    final Activity thisActivity = this;

    View.OnClickListener listener = new View.OnClickListener() {
        @Override
        public void onClick(View view) {
            ActivityCompat.requestPermissions(thisActivity, permissions,
                    RC_HANDLE_CAMERA_PERM);
        }
    };

    Snackbar.make(mGraphicOverlay, R.string.permission_camera_rationale,
            Snackbar.LENGTH_INDEFINITE)
            .setAction(R.string.ok, listener)
            .show();
}

@Override
public boolean onTouchEvent(MotionEvent e) {
    boolean b = scaleGestureDetector.onTouchEvent(e);

    boolean c = gestureDetector.onTouchEvent(e);

    return b || c || super.onTouchEvent(e);
}

/**
 * Creates and starts the camera.  Note that this uses a higher resolution in comparison
 * to other detection examples to enable the barcode detector to detect small barcodes
 * at long distances.
 *
 * Suppressing InlinedApi since there is a check that the minimum version is met before using
 * the constant.
 */
@SuppressLint(""InlinedApi"")
private void createCameraSource(boolean autoFocus, boolean useFlash) {
    Context context = getApplicationContext();

    // A barcode detector is created to track barcodes.  An associated multi-processor instance
    // is set to receive the barcode detection results, track the barcodes, and maintain
    // graphics for each barcode on screen.  The factory is used by the multi-processor to
    // create a separate tracker instance for each barcode.

    BarcodeDetector barcodeDetector = new BarcodeDetector.Builder(context).setBarcodeFormats(Barcode.CODE_128 | Barcode.DATA_MATRIX | Barcode.QR_CODE).build();
    BarcodeTrackerFactory barcodeFactory = new BarcodeTrackerFactory(mGraphicOverlay);
    barcodeDetector.setProcessor(
            new MultiProcessor.Builder&lt;&gt;(barcodeFactory).build());

    if (!barcodeDetector.isOperational()) {
        // Note: The first time that an app using the barcode or face API is installed on a
        // device, GMS will download a native libraries to the device in order to do detection.
        // Usually this completes before the app is run for the first time.  But if that
        // download has not yet completed, then the above call will not detect any barcodes
        // and/or faces.
        //
        // isOperational() can be used to check if the required native libraries are currently
        // available.  The detectors will automatically become operational once the library
        // downloads complete on device.
        Log.w(TAG, ""Detector dependencies are not yet available."");

        // Check for low storage.  If there is low storage, the native library will not be
        // downloaded, so detection will not become operational.
        IntentFilter lowstorageFilter = new IntentFilter(Intent.ACTION_DEVICE_STORAGE_LOW);
        boolean hasLowStorage = registerReceiver(null, lowstorageFilter) != null;

        if (hasLowStorage) {
            Toast.makeText(this, R.string.low_storage_error, Toast.LENGTH_LONG).show();
            Log.w(TAG, getString(R.string.low_storage_error));
        }
    }

    // Creates and starts the camera.  Note that this uses a higher resolution in comparison
    // to other detection examples to enable the barcode detector to detect small barcodes
    // at long distances.
    CameraSource.Builder builder = new CameraSource.Builder(getApplicationContext(), barcodeDetector)
            .setFacing(CameraSource.CAMERA_FACING_BACK)
            .setRequestedPreviewSize(1100, 844)
            .setRequestedFps(15.0f);
    // make sure that auto focus is an available option
    if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.ICE_CREAM_SANDWICH) {
        builder = builder.setFocusMode(
                autoFocus ? Camera.Parameters.FOCUS_MODE_CONTINUOUS_PICTURE : null);
    }

    mCameraSource = builder
            .setFlashMode(useFlash ? Camera.Parameters.FLASH_MODE_TORCH : null)
            .build();
}


/**
 * Restarts the camera.
 */
@Override
protected void onResume() {
    super.onResume();
    startCameraSource();
}

/**
 * Stops the camera.
 */
@Override
protected void onPause() {
    super.onPause();
    if (mPreview != null) {
        mPreview.stop();
    }
}

/**
 * Releases the resources associated with the camera source, the associated detectors, and the
 * rest of the processing pipeline.
 */
@Override
protected void onDestroy() {
    super.onDestroy();
    if (mPreview != null) {
        mPreview.release();
    }
}


@Override
public void onRequestPermissionsResult(int requestCode,
                                       @NonNull String[] permissions,
                                       @NonNull int[] grantResults) {
    if (requestCode != RC_HANDLE_CAMERA_PERM) {
        Log.d(TAG, ""Got unexpected permission result: "" + requestCode);
        super.onRequestPermissionsResult(requestCode, permissions, grantResults);
        return;
    }

    if (grantResults.length != 0 &amp;&amp; grantResults[0] == PackageManager.PERMISSION_GRANTED) {
        Log.d(TAG, ""Camera permission granted - initialize the camera source"");
        // we have permission, so create the camerasource
        boolean autoFocus = getIntent().getBooleanExtra(AutoFocus,false);
        boolean useFlash = getIntent().getBooleanExtra(UseFlash, false);
        createCameraSource(autoFocus, useFlash);
        return;
    }

    Log.e(TAG, ""Permission not granted: results len = "" + grantResults.length +
            "" Result code = "" + (grantResults.length &gt; 0 ? grantResults[0] : ""(empty)""));

    DialogInterface.OnClickListener listener = new DialogInterface.OnClickListener() {
        public void onClick(DialogInterface dialog, int id) {
            finish();
        }
    };

    AlertDialog.Builder builder = new AlertDialog.Builder(this);
    builder.setTitle(""Multitracker sample"")
            .setMessage(R.string.no_camera_permission)
            .setPositiveButton(R.string.ok, listener)
            .show();
}

/**
 * Starts or restarts the camera source, if it exists.  If the camera source doesn't exist yet
 * (e.g., because onResume was called before the camera source was created), this will be called
 * again when the camera source is created.
 */
private void startCameraSource() throws SecurityException {
    // check that the device has play services available.
    int code = GoogleApiAvailability.getInstance().isGooglePlayServicesAvailable(
            getApplicationContext());
    if (code != ConnectionResult.SUCCESS) {
        Dialog dlg =
                GoogleApiAvailability.getInstance().getErrorDialog(this, code, RC_HANDLE_GMS);
        dlg.show();
    }

    if (mCameraSource != null) {
        try {
            mPreview.start(mCameraSource, mGraphicOverlay);
        } catch (IOException e) {
            Log.e(TAG, ""Unable to start camera source."", e);
            mCameraSource.release();
            mCameraSource = null;
        }
    }
}

/**
 * onTap is called to capture the oldest barcode currently detected and
 * return it to the caller.
 *
 * @param rawX - the raw position of the tap
 * @param rawY - the raw position of the tap.
 * @return true if the activity is ending.
 */

private boolean onTap(float rawX, float rawY) {
    //TODO: use the tap position to select the barcode.
    BarcodeGraphic graphic = mGraphicOverlay.getFirstGraphic();
    Barcode barcode = null;
    if (graphic != null) {
        barcode = graphic.getBarcode();
        if (barcode != null) {
            Intent data = new Intent();
            data.putExtra(BarcodeObject, barcode);
            setResult(CommonStatusCodes.SUCCESS, data);
            finish();
        }
        else {
            Log.d(TAG, ""barcode data is null"");
        }
    }
    else {
        Log.d(TAG,""no barcode detected"");
    }
    return barcode != null;
}

private class CaptureGestureListener extends GestureDetector.SimpleOnGestureListener {

    @Override
    public boolean onSingleTapConfirmed(MotionEvent e) {

        return onTap(e.getRawX(), e.getRawY()) || super.onSingleTapConfirmed(e);
    }
}

private class ScaleListener implements ScaleGestureDetector.OnScaleGestureListener {

    /**
     * Responds to scaling events for a gesture in progress.
     * Reported by pointer motion.
     *
     * @param detector The detector reporting the event - use this to
     *                 retrieve extended info about event state.
     * @return Whether or not the detector should consider this event
     * as handled. If an event was not handled, the detector
     * will continue to accumulate movement until an event is
     * handled. This can be useful if an application, for example,
     * only wants to update scaling factors if the change is
     * greater than 0.01.
     */
    @Override
    public boolean onScale(ScaleGestureDetector detector) {
        return false;
    }

    /**
     * Responds to the beginning of a scaling gesture. Reported by
     * new pointers going down.
     *
     * @param detector The detector reporting the event - use this to
     *                 retrieve extended info about event state.
     * @return Whether or not the detector should continue recognizing
     * this gesture. For example, if a gesture is beginning
     * with a focal point outside of a region where it makes
     * sense, onScaleBegin() may return false to ignore the
     * rest of the gesture.
     */
    @Override
    public boolean onScaleBegin(ScaleGestureDetector detector) {
        return true;
    }

    /**
     * Responds to the end of a scale gesture. Reported by existing
     * pointers going up.
     * &lt;p/&gt;
     * Once a scale has ended, {@link ScaleGestureDetector#getFocusX()}
     * and {@link ScaleGestureDetector#getFocusY()} will return focal point
     * of the pointers remaining on the screen.
     *
     * @param detector The detector reporting the event - use this to
     *                 retrieve extended info about event state.
     */
    @Override
    public void onScaleEnd(ScaleGestureDetector detector) {
        mCameraSource.doZoom(detector.getScaleFactor());
    }
}

@Override
public boolean onOptionsItemSelected(MenuItem item) {
    int id = item.getItemId();

    if (id == android.R.id.home) {
        onBackPressed();
        return true;
    }
    return super.onOptionsItemSelected(item);
}
}
</code></pre>",,0,1,,2016-08-11 08:16:12.443 UTC,1,2016-08-11 08:16:12.443 UTC,,,,,5502638,1,2,android|barcode-scanner|flashlight|zbar|google-vision,326
Google API setting environment variable,46838135,Google API setting environment variable,"<p>When I load my application I get this:</p>

<pre><code>Fatal error: Uncaught exception 'Google\Cloud\Core\Exception\ServiceException' with message 'Could not load the default credentials. Browse to https://developers.google.com/accounts/docs/application-default-credentials for more information' in C:\xampp\htdocs\CarLookup\vendor\google\cloud-core\RequestWrapper.php:253
</code></pre>

<p>I am trying to follow this: <a href=""https://cloud.google.com/vision/docs/reference/libraries"" rel=""nofollow noreferrer"">Google Vision Doc</a></p>

<p>I have run the command: <code>composer require google/cloud-vision</code></p>

<p>Then on the Client Libary it is saying I have to set up a Client Library? I have done this with all he correct things then it says to-:</p>

<blockquote>
  <p>Next, provide the credentials to your application code by setting the environment variable GOOGLE_APPLICATION_CREDENTIALS to point to the JSON file you downloaded in the previous step.</p>
</blockquote>

<p>And the execute this: <code>set GOOGLE_APPLICATION_CREDENTIALS=&lt;path_to_service_account_file&gt;</code></p>

<p>Where I am stuck is, where do I execute this, how do I set the environment variable?</p>",,1,0,,2017-10-19 20:09:32.260 UTC,,2018-08-25 17:57:37.557 UTC,,,,,8563360,1,1,php|api|google-api|google-vision,293
How can I insert images of new brands or logos and be detected with the LOGO_DETECTION annotation of Google Cloud Vision Api?,47833305,How can I insert images of new brands or logos and be detected with the LOGO_DETECTION annotation of Google Cloud Vision Api?,"<p>I'm using Google Cloud Vision API to detect logos of brands or companies, when testing everything works correctly. However, in the application that I am developing I need to upload images or logos of brands that are not so popular (<b> new companies, new logos or new brands e </b>). </p>

<p><b> Example </b>: it is necessary for a new company to upload logos and images and remain in the google database to be able to scan or upload an image and the <b> API REST  I get the name of the  new </b> company.</p>",,2,1,,2017-12-15 13:24:07.397 UTC,,2017-12-21 11:37:32.630 UTC,,,,,9100911,1,0,google-cloud-vision,459
Where to find Google cloud platform id?,35326807,Where to find Google cloud platform id?,"<p>I wanted to create an account to new <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">google vision api</a> to possibly integrate its service in the app <a href=""http://www.openbricks.io"" rel=""nofollow noreferrer"">http://www.openbricks.io</a> I am working on.</p>

<p>But the <a href=""https://services.google.com/fb/forms/visionapialpha/"" rel=""nofollow noreferrer"">form</a> to have access to this new api ask for an account on Google Cloud platform, it is my case, but also a mystical Google Cloud platform user account Id, I cannot find no where Google Cloud help is a real maze and to have support we need to pay.</p>

<p>Is anyone using Google Cloud Platform? And know how to get this id ?</p>",,2,0,,2016-02-10 22:00:40.487 UTC,1,2019-04-19 23:15:41.040 UTC,2019-04-19 23:15:41.040 UTC,,11293327,,1453811,1,6,google-cloud-platform,1207
Android Studio ZXsing QR SCANNER not working,47365217,Android Studio ZXsing QR SCANNER not working,"<p>So I've been doing this for 2 days still it won't work. I tried some github advises and those on stack overflow but neither worked for me, the google vision one and the zebra crossing one(ZXing) can somebody please help me? It shows an irritating whitescreen, I tried these in versions 4.4.4 and 4.1.2. HEre's my code, I'm currently using the Zxing one.</p>

<p>Here's my class where number of buttons are lineup. From here I go to the class where the scanner is, so I can scan the item, and then from there I will do make the other feature. For now my goal is to be able to Scan QR Codes and get its input. I've found some examples on how to get the input so it's not a problem. btu yeah It's not showing!</p>

<p><strong>Normal.java</strong></p>

<pre><code>public class Normal extends AppCompatActivity {

    Button suite, normal, room1, room2, room3, room4, room5, room6, room7, room8, room9, room10;
    private DrawerLayout mDrawerLayout;
    private ActionBarDrawerToggle mToggle;
    private DatabaseReference mFirebaseDatabase1;
    private FirebaseDatabase mFirebaseInstance;


    //FIREBASE AUTH FIELDS

    DatabaseReference mSearchedLocationReference;
    DatabaseReference mSearchedLocationReference1;




    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_normal);

        mFirebaseInstance = FirebaseDatabase.getInstance();
        //FIREBASE
        mFirebaseDatabase1 = mFirebaseInstance.getReference(""Rooms"");



        //FIREBASE PINPOINT

        //DRAWER LAYOUT
        mDrawerLayout = (DrawerLayout) findViewById(R.id.drawerLayout);
        mToggle = new ActionBarDrawerToggle(this, mDrawerLayout, R.string.open, R.string.close);
        //navigation Drawer
        mDrawerLayout.addDrawerListener(mToggle);
        mToggle.syncState();

        getSupportActionBar().setDisplayHomeAsUpEnabled(true);

        NavigationView mNavigationView = (NavigationView) findViewById(R.id.nav_menu);
        mNavigationView.setNavigationItemSelectedListener(new NavigationView.OnNavigationItemSelectedListener() {
            @Override
            public boolean onNavigationItemSelected(MenuItem menuItem) {
                switch (menuItem.getItemId()) {

                    case (R.id.nav_logout):
                        Intent accountActivity4 = new Intent(getApplicationContext(), login1.class);
                        finish();
                        startActivity(accountActivity4);
                        break;


                }
                return true;
            }
        });

        //Navigation Drawer

        //ASSIGN ID's

        room1 = (Button) findViewById(R.id.room2);
        room2 = (Button) findViewById(R.id.room3);
        room3 = (Button) findViewById(R.id.room4);
        room4 = (Button) findViewById(R.id.room5);
        room5 = (Button) findViewById(R.id.room6);
        room6 = (Button) findViewById(R.id.room7);
        room7 = (Button) findViewById(R.id.room8);
        room8 = (Button) findViewById(R.id.room9);
        room9 = (Button) findViewById(R.id.room10);
        room10 = (Button) findViewById(R.id.room11);

        //ASSIGN ID's
        suite = (Button) findViewById(R.id.button2);
        normal = (Button) findViewById(R.id.button);

        suite.setOnClickListener(new View.OnClickListener() {

            @Override
            public void onClick(View view) {


                Intent next = new Intent(Normal.this, Suite.class);
                startActivity(next);

            }
        });

        normal.setOnClickListener(new View.OnClickListener() {

            @Override
            public void onClick(View view) {

                Intent next1 = new Intent(Normal.this, Normal.class);
                startActivity(next1);


            }

        });


        room1.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                startActivity(new Intent(Normal.this, room2.class));


            }
        });

        room2.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                IntentIntegrator integrator = new IntentIntegrator(this);
                integrator.initiateScan();
            }
        });

        room5.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                startActivity(new Intent(Normal.this, room5.class));
            }
        });



        mSearchedLocationReference.addValueEventListener(new ValueEventListener() { //attach listener

            @Override
            public void onDataChange(DataSnapshot dataSnapshot) { //something changed!


               for (DataSnapshot locationSnapshot : dataSnapshot.getChildren()) {
                  String location = locationSnapshot.getValue().toString();



                  Log.d(""Locations updated"", ""location: "" + location); //log
                    if ( location.equals(""Green"")){

                        room1.setBackgroundColor(Color.GREEN);
                    }else if ( location.equals(""Red"")){
                        room1.setBackgroundColor(Color.RED);

                    }
                    else{
                        room1.setBackgroundColor(Color.YELLOW);

                    }
                }
            }

            @Override
            public void onCancelled(DatabaseError databaseError) { //update UI here if error occurred.

            }
        });
        mSearchedLocationReference1.addValueEventListener(new ValueEventListener() { //attach listener

            @Override
            public void onDataChange(DataSnapshot dataSnapshot) { //something changed!


                for (DataSnapshot locationSnapshot : dataSnapshot.getChildren()) {
                    String location = locationSnapshot.getValue().toString();



                    Log.d(""Locations updated"", ""location: "" + location); //log
                    if ( location.equals(""Green"")){

                        room2.setBackgroundColor(Color.GREEN);
                    }else if ( location.equals(""Red"")){
                        room2.setBackgroundColor(Color.RED);
                    }
                    else{
                        room2.setBackgroundColor(Color.YELLOW);

                    }
                }
            }

            @Override
            public void onCancelled(DatabaseError databaseError) { //update UI here if error occurred.

            }
        });


    }

}

whenever i click room1, it won't got to the room2 class where I put the scanner:
</code></pre>

<p><strong>room2.class</strong></p>

<pre><code>import me.dm7.barcodescanner.zxing.ZXingScannerView;

public class room2 extends AppCompatActivity implements ZXingScannerView.ResultHandler{
    private ZXingScannerView zXingScannerView;
    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_room2);
    }

    public void scan(View view){
        zXingScannerView =new ZXingScannerView(getApplicationContext());
        setContentView(zXingScannerView);
        zXingScannerView.setResultHandler(this);
        zXingScannerView.startCamera();

    }

    @Override
    protected void onPause() {
        super.onPause();
        zXingScannerView.stopCamera();
    }

    @Override
    public void handleResult(Result result) {
        Toast.makeText(getApplicationContext(),result.getText(),Toast.LENGTH_SHORT).show();


    }
}
</code></pre>

<p><strong>Here's my manifest, it's got services-vision since I have been trying the two, but yes permission for camera was indeed requested.</strong></p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;manifest xmlns:android=""http://schemas.android.com/apk/res/android""
    package=""com.example.chanty.hoteltwoway""&gt;



    &lt;uses-feature android:name=""android.hardware.camera"" /&gt;
    &lt;uses-feature android:name=""android.hardware.autofocus"" /&gt;
    &lt;uses-permission android:name=""android.permission.CAMERA""/&gt;
    &lt;uses-permission android:name=""android.permission.VIBRATE""/&gt;
    &lt;uses-permission android:name=""android.permission.WRITE_EXTERNAL_STORAGE""/&gt;


    &lt;application
        android:allowBackup=""true""
        android:icon=""@mipmap/ic_launcher""
        android:label=""@string/app_name""
        android:roundIcon=""@mipmap/ic_launcher_round""
        android:supportsRtl=""true""
        android:theme=""@style/AppTheme""
        android:name=""android.support.multidex.MultiDexApplication"" &gt;
        &lt;activity android:name="".login""&gt;
            &lt;intent-filter&gt;
                &lt;action android:name=""android.intent.action.MAIN"" /&gt;

                &lt;category android:name=""android.intent.category.LAUNCHER"" /&gt;
            &lt;/intent-filter&gt;
        &lt;/activity&gt;
        &lt;activity android:name="".MainActivity"" /&gt;
        &lt;activity android:name="".Room"" /&gt;
        &lt;activity android:name="".Suite"" /&gt;
        &lt;activity android:name="".Normal"" /&gt;
        &lt;activity android:name="".rooms.room1"" /&gt;
        &lt;activity android:name="".rooms.room2"" /&gt;
        &lt;activity android:name="".rooms.room3"" /&gt;
        &lt;activity android:name="".rooms.room5"" /&gt;
        &lt;activity android:name="".rooms.room6"" /&gt;
        &lt;activity android:name="".rooms.room7"" /&gt;
        &lt;activity android:name="".rooms.room8"" /&gt;
        &lt;activity android:name="".rooms.room10"" /&gt;
        &lt;activity android:name="".suites.suite1"" /&gt;
        &lt;activity android:name="".suites.suite2"" /&gt;
        &lt;activity android:name="".suites.suite3"" /&gt;
        &lt;activity android:name="".suites.suite4"" /&gt;
        &lt;activity android:name="".suites.suite5"" /&gt;
        &lt;activity android:name="".suites.suite6"" /&gt;
        &lt;activity android:name="".suites.suite7"" /&gt;
        &lt;activity android:name="".suites.suite9"" /&gt;
        &lt;activity android:name="".suites.suite8"" /&gt;
        &lt;activity android:name="".suites.suite10"" /&gt;
        &lt;activity android:name="".login1"" /&gt;
        &lt;activity android:name="".MainActivity1"" /&gt;
        &lt;activity android:name="".login2"" /&gt;
        &lt;activity android:name="".Room1"" /&gt;
        &lt;activity android:name="".Normal1"" /&gt;
        &lt;activity android:name="".Suite1"" /&gt;
        &lt;activity android:name="".suprooms.suproom1"" /&gt;
        &lt;activity android:name="".suprooms.suproom2"" /&gt;
        &lt;activity android:name="".suprooms.suproom3"" /&gt;
        &lt;activity android:name="".suprooms.suproom4"" /&gt;
        &lt;activity android:name="".suprooms.suproom5"" /&gt;
        &lt;activity android:name="".suprooms.suproom6"" /&gt;
        &lt;activity android:name="".suprooms.suproom7"" /&gt;
        &lt;activity android:name="".suprooms.suproom8"" /&gt;
        &lt;activity android:name="".suprooms.suproom9"" /&gt;
        &lt;activity android:name="".suprooms.suproom10"" /&gt;
        &lt;activity android:name="".supsuites.supsuite1""&gt;&lt;/activity&gt;
        &lt;meta-data android:name=""com.google.android.gms.vision.DEPENDENCIES"" android:value=""barcode""/&gt;
    &lt;/application&gt;

&lt;/manifest&gt;
</code></pre>

<p><strong>Lot's of depencies, since yeah I have been testing out things since day 1
Here's my gradle</strong></p>

<pre><code>apply plugin: 'com.android.application'

android {
    compileSdkVersion 26
    buildToolsVersion ""26.0.2""
    defaultConfig {
        applicationId ""com.example.chanty.hoteltwoway""
        minSdkVersion 15
        targetSdkVersion 26
        versionCode 1
        versionName ""1.0""
        testInstrumentationRunner ""android.support.test.runner.AndroidJUnitRunner""
        multiDexEnabled true
    }
    buildTypes {
        release {
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
        }
    }
}

dependencies {
    compile fileTree(dir: 'libs', include: ['*.jar'])
    androidTestCompile('com.android.support.test.espresso:espresso-core:2.2.2', {
        exclude group: 'com.android.support', module: 'support-annotations'
    })


    compile 'me.dm7.barcodescanner:zxing:1.7.2'
    compile 'com.google.android.gms:play-services-vision:10.0.1'
    compile 'com.android.support.constraint:constraint-layout:1.0.2'
    compile 'com.google.firebase:firebase-database:10.0.1'
    compile 'com.google.firebase:firebase-auth:10.0.1'
    compile 'com.android.support:multidex:1.0.1'
    compile 'com.android.support:design:26.+'
    compile 'com.android.support:appcompat-v7:26.+'
    compile 'com.google.firebase:firebase-core:10.0.1'
    testCompile 'junit:junit:4.12'
}

// Add to the bottom of the file
apply plugin: 'com.google.gms.google-service
</code></pre>

<p>I tried downgrading the zebra crossing dependencies according to some sources so it might work. but yeah, it didn't work, can you guys help me figure out what I did wrong with this one?</p>",,1,0,,2017-11-18 10:54:35.550 UTC,,2017-11-21 23:59:39.273 UTC,2017-11-19 04:12:58.137 UTC,,7661057,,8720798,1,0,android-studio|zebra,58
lambda task timed out error while using rekognition,53910973,lambda task timed out error while using rekognition,"<p>I'm building a React native app with serverless framework using AWS services.</p>

<p>I created a  RESTapi with lambda function (nodeJs8.10 environment) and API gateway to use rekognition services such as indexFaces, listCollection, etc. My lambda is in VPC with RDS( later I'll Aurora) to store faceID and other data.
Everything works fine except rekognition services.</p>

<p>When I call any rekognition services it shows <code>Task timed out after 270.04 seconds</code>.But it works when I call locally using <code>serverless-offline-plugin</code>
I attach all necessary permissions to my <code>lambda</code> like <code>AmazonRekognitionFullAccess</code></p>

<p>Here is my code</p>

<p><strong>index.js</strong></p>

<pre><code>app.post('/myapi', function (req, res) {
    var params = {
        MaxResults: 3,
    };
    const rekognition = aws_config(); &lt;-- rekognition configuration
    rekognition.listCollections(params, function(err, data) {
        if (err) {
            res.json(err.stack);
            console.log(err, err.stack);
        }
        else{
            res.json(data);
            console.log(data);
        }
    });
});

function aws_config(){
    const $options = {
        'region'            : 'ap-southeast-2',
        'version'           : '2016-06-27',
        'accessKeyId '    : config.ENV.aws_key,
        'secretAccessKey ' : config.ENV.aws_secret,

    };
    return new AWS.Rekognition($options);
}
</code></pre>

<p>How to solve this timeout error as it doesn't show any error on <code>CloudWatch logs</code>?</p>",,0,14,,2018-12-24 08:26:09.473 UTC,,2018-12-26 17:24:39.377 UTC,2018-12-26 17:24:39.377 UTC,,3386310,,4992190,1,0,amazon-web-services|react-native|aws-lambda|serverless-framework|amazon-rekognition,81
Download image from ImageID and collection using Amazon Rekognition,51907473,Download image from ImageID and collection using Amazon Rekognition,"<p>I am building a face recogition app using AWS Rekognition. I also want to show the image to user that matches with my input image. Does AWS also saves the image when we index them? If yes, how can I get that image? </p>

<p>If no I am thinking to do it this way. I can save the image into S3 with same name as ExternalImageId so that when ever a face is detected, I read the external id from Rekognition and fetch that from S3.</p>

<p>If there is a better approach than this, please let me know.</p>

<p>I am using following code to index an image in a collection:</p>

<pre><code>import boto3
from PIL import Image
import io
import time

image1 = 'images/haad2.jpg'

client = boto3.client('rekognition')
image = Image.open(image1)

stream = io.BytesIO()
image.save(stream, format=""JPEG"")
image_binary = stream.getvalue()

response = client.index_faces(
    Image={
        'Bytes': image_binary
    },
    CollectionId='root_faces_data',
    ExternalImageId=str(time.time()),
    DetectionAttributes=[
        'ALL',
    ]
)

print(response)
</code></pre>

<p>And the following code to see if a face exists in a collection:</p>

<pre><code>import boto3
import io
from PIL import Image

client = boto3.client('rekognition')

image1 = 'images/haad2.jpg'

client = boto3.client('rekognition')
image = Image.open(image1)

stream = io.BytesIO()
image.save(stream, format=""JPEG"")
image_binary = stream.getvalue()

response = client.search_faces_by_image(
    CollectionId='root_faces_data',
    Image={
        'Bytes': image_binary
    },
    MaxFaces=10,
    FaceMatchThreshold=90
)

print(response)
</code></pre>

<p>This is the output of search_faces_by_image:</p>

<pre><code>{
  'SearchedFaceBoundingBox': {
    'Width': 0.2646464705467224,
    'Height': 0.39817628264427185,
    'Left': 0.3186868727207184,
    'Top': 0.23252280056476593
  },
  'SearchedFaceConfidence': 99.9957275390625,
  'FaceMatches': [
    {
      'Similarity': 99.98405456542969,
      'Face': {
        'FaceId': '5bc98595-7d30-4447-b430-4c0fd8f1b926',
        'BoundingBox': {
          'Width': 0.2646459937095642,
          'Height': 0.39817601442337036,
          'Left': 0.31868699193000793,
          'Top': 0.23252299427986145
        },
        'ImageId': '8e631731-4a0c-513d-be32-dbfe3ae5e813',
        'ExternalImageId': '1534576206.8314612',
        'Confidence': 99.9957046508789
      }
    }
  ],
  'FaceModelVersion': '3.0',
  'ResponseMetadata': {
    'RequestId': 'eca4bea6-a2b5-11e8-9345-a5eddf19f47f',
    'HTTPStatusCode': 200,
    'HTTPHeaders': {
      'content-type': 'application/x-amz-json-1.1',
      'date': 'Sat, 18 Aug 2018 07:12:09 GMT',
      'x-amzn-requestid': 'eca4bea6-a2b5-11e8-9345-a5eddf19f47f',
      'content-length': '553',
      'connection': 'keep-alive'
    },
    'RetryAttempts': 0
  }
}
</code></pre>",51912778,1,0,,2018-08-18 10:10:02.160 UTC,,2018-08-18 21:42:37.500 UTC,2018-08-18 10:51:17.007 UTC,,4395264,,4395264,1,1,amazon-web-services|amazon-s3|aws-sdk|amazon-rekognition,174
How to shrink or manage an image's size in bytes,51776654,How to shrink or manage an image's size in bytes,"<p>Python 3.6.6, Pillow 5.2.0</p>

<p>The Google Vision API has a size limit of 10485760 bytes.</p>

<p>When I'm working with a PIL Image, and save it to Bytes, it is hard to predict what the size will be.  Sometimes when I try to resize it to have smaller height and width, the image size as bytes gets bigger.</p>

<p>I've tried experimenting with modes and formats, to understand their impact on size, but I'm not having much luck getting consistent results.</p>

<p>So I start out with a rawImage that is Bytes obtained from some user uploading an image (meaning I don't know much about what I'm working with yet).</p>

<pre><code>rawImageSize = sys.getsizeof(rawImage)
if rawImageSize &gt;= 10485760:
   imageToShrink = Image.open(io.BytesIO(rawImage))

   ## do something to the image here to shrink it
   # ... mystery code ...
   ## ideally, the minimum amount of shrinkage necessary to get it under 10485760

   rawBuffer = io.BytesIO()

   # possibly convert to RGB first
   shrunkImage.save(rawBuffer, format='JPEG') # PNG files end up bigger after this resizing (!?)
   rawImage = rawBuffer.getvalue()

   print(sys.getsizeof(rawImage))
</code></pre>

<p>To shrink it I've tried getting a shrink ratio and then simply resizing it:</p>

<pre><code>    shrinkRatio =  10485760.0  / float(rawImageSize)

    imageWidth, imageHeight = pilImage.size
    shrunkImage = imageToShrink.resize((int(imageWidth * shrinkRatio),
                                        int(imageHeight * shrinkRatio)), Image.LANCZOS)
</code></pre>

<p>Of course I could use a sufficiently small and somewhat arbitrary thumbnail size instead. I've thought about iterating thumbnail sizes until a combination takes me below the maximum bytes size threshold.  I'm guessing the bytes size varies based on the color depth and mode and (?) I got from the end user that uploaded the original image.  And that brings me to my questions:</p>

<p>Can I predict the size in bytes a PIL Image will be before I convert it for consumption by Google Vision?  What is the best way to manage that size in bytes before I convert it?</p>",,1,1,,2018-08-09 22:39:58.830 UTC,,2018-11-17 03:31:59.697 UTC,,,,,3297954,1,1,python-imaging-library|google-vision,82
Defining a new promise in express.js,49741658,Defining a new promise in express.js,"<p>I am working on posting an image to the Google Vision API, and returning the labels of what the picture contains. </p>

<p>I am using express.js and the Google Cloud Vision module, and I am having trouble creating a promise from the results of the Google Vision API. When I post an image to the API, it will return an array of labels. How do I ensure that after the vision API helper function runs that I store those results and return it to my react front end? </p>

<p>Server.js </p>

<pre><code>const express = require('express');
const bodyParser = require('body-parser')
const path = require('path');
const app = express();
const multer  = require('multer');
const upload = multer({ dest: './uploads' });
const storage = multer.diskStorage({
    destination: function (req, file, cb) {
        cb(null, './uploads')
    },
    filename: function (req, file, cb) {
        cb(null, Date.now() + '-' + file.originalname)
    }
})
const google = require('../helpers/googleVisionAPI.js');

//Hitting file limits on size, increased to 50mb
app.use(bodyParser.json({limit: ""50mb""}));
app.use(bodyParser.urlencoded({limit: ""50mb"", extended: true, parameterLimit:50000}));
app.use(require('express-promise')());

app.post('/upload', upload.single('image'), (req, res) =&gt; {
  google.visionAPI(req.file.path).then(labels =&gt; {
    //SEND LABELS BACK TO REACT FRONTEND
    console.log(labels);
    res.send(labels);
  })
});

app.listen(8080);
</code></pre>

<p>GoogleVisionAPI.js helper</p>

<pre><code>// Imports the Google Cloud client library
const vision = require('@google-cloud/vision');
// Creates a client
const client = new vision.ImageAnnotatorClient();

let visionAPI = function(image) {
  client
    .labelDetection(image)
    .then(results =&gt; {
      // Pull all labels from POST request
      const labels = [];
      results[0].labelAnnotations.forEach(function(element) {
        labels.push(element.description);
      });
      return labels;
    })

    // ERROR from Cloud Vision API
    .catch(err =&gt; {
      console.log(err);
      res.end('Cloud Vision Error:' , err);
    });
}

exports.visionAPI = visionAPI;
</code></pre>

<p>Can someone have the Vision API run, and then store the returned results to a label variable to have sent back to my react frontend?</p>",,1,1,,2018-04-09 21:04:30.113 UTC,1,2018-04-10 02:26:43.827 UTC,,,,,4041447,1,0,javascript|node.js|express,55
App crashing due to two differnet Async Tasks?,44652637,App crashing due to two differnet Async Tasks?,"<p>So I have been working on this code for a while now trying to implement Google Visions into my prior app that displays an image from pixabay then tells me the tags of the photo.I had both the google vision app and pixabay app work just fine on their own. In this new version it should give me tags and the labels found by Google Visions but, whenever I activate the UP command on the sensors it crashes.</p>

<p>Here is my code:</p>

<pre><code>import android.annotation.TargetApi;
import android.content.Context;
import android.content.Intent;
import android.graphics.Bitmap;
import android.graphics.BitmapFactory;
import android.hardware.Sensor;
import android.hardware.SensorEvent;
import android.hardware.SensorEventListener;
import android.hardware.SensorManager;
import android.media.MediaPlayer;
import android.os.AsyncTask;
import android.os.Build;
import android.os.Bundle;
import android.os.Vibrator;
import android.speech.tts.TextToSpeech;
import android.support.v7.app.AppCompatActivity;
import android.support.v7.widget.Toolbar;
import android.util.Log;
import android.view.Menu;
import android.view.MenuItem;
import android.widget.ImageView;
import android.widget.TextView;

import com.google.api.client.extensions.android.http.AndroidHttp;
import com.google.api.client.googleapis.json.GoogleJsonResponseException;
import com.google.api.client.http.HttpTransport;
import com.google.api.client.json.JsonFactory;
import com.google.api.client.json.gson.GsonFactory;
import com.google.api.services.vision.v1.Vision;
import com.google.api.services.vision.v1.VisionRequest;
import com.google.api.services.vision.v1.VisionRequestInitializer;
import com.google.api.services.vision.v1.model.AnnotateImageRequest;
import com.google.api.services.vision.v1.model.BatchAnnotateImagesRequest;
import com.google.api.services.vision.v1.model.BatchAnnotateImagesResponse;
import com.google.api.services.vision.v1.model.EntityAnnotation;
import com.google.api.services.vision.v1.model.Feature;
import com.google.api.services.vision.v1.model.Image;

import java.io.BufferedInputStream;
import java.io.BufferedReader;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.net.HttpURLConnection;
import java.net.URL;
import java.util.ArrayList;
import java.util.List;
import java.util.Locale;

import static edu.ggc.lutz.recipe.pixabaysamplerwalkthrough.R.id.tvLabels;
import static edu.ggc.lutz.recipe.pixabaysamplerwalkthrough.R.id.tvTags;

public class MainActivity extends AppCompatActivity implements SensorEventListener {

    public static final String PIXABAY = ""Pixabay"";
    private ImageView imageView;
    private static PixabayQueryResult result;
    private String tags;
    long numberOfHits;
    long selected;

    float[] gravity = new float[3];
    float[] accel = new float[3];
    private static final float ALPHA = 0.80f; // weighing factor used by the low pass filter
    private static final String TAG = ""OMNI"";
    private static final float VERTICAL_TOL = 0.3f;
    private SensorManager manager;
    private long lastUpdate;
    private MediaPlayer popPlayer;
    private MediaPlayer backgroundPlayer;
    private TextToSpeech tts;
    private TextView[] tvGravity;
    private TextView[] tvAcceleration;

    private boolean isDown = false;
    private boolean isUp = false;

    private static final String CLOUD_VISION_API_KEY = ""AIzaSyCt35MZjvD_3ynTbYmeUuBFyMbYrjXUmzs"";
    private static final String ANDROID_CERT_HEADER = ""X-Android-Cert"";
    private static final String ANDROID_PACKAGE_HEADER = ""X-Android-Package"";

    private static final String TAGgoogle = MainActivity.class.getSimpleName();

    private TextView pixtags;
    private TextView googlelab;

    private String urlString;

    private Bitmap bitmapT;


    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);
        Toolbar toolbar = (Toolbar) findViewById(R.id.toolbar);
        setSupportActionBar(toolbar);

        pixtags= (TextView) findViewById(tvTags);

        googlelab= (TextView) findViewById(tvLabels);

       /* FloatingActionButton fab = (FloatingActionButton) findViewById(fab);
        fab.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                PixabayFetchTask task = new PixabayFetchTask();
                String service = ""https://pixabay.com/api/"";
                String key = ""5535853-23bc4a5e307cd5d1a5e16ebcc"";
                String query_params = ""&amp;editor_choice=true&amp;safesearch=true&amp;image_type=photo"";
                String urlString = service + ""?key="" + key + query_params;
                task.execute(urlString);


            }
        });*/

    imageView= (ImageView) findViewById(R.id.imageView);

        tts = new TextToSpeech(this, new TextToSpeech.OnInitListener() {
            @Override
            public void onInit(int status) {
                int result1=0;
                if(status == TextToSpeech.SUCCESS) {
                    result1 = tts.setLanguage(Locale.US);
                }
                if( result1 == TextToSpeech.LANG_MISSING_DATA || result1== TextToSpeech.LANG_NOT_SUPPORTED){
                    Log.e(""TTS"", ""This Language is not supported"");
                }
                else
                {
                    Log.e(""TTS"", ""Inizalization Failed"");
                }
            }
        });

        //////////////////////////
        manager = (SensorManager) getSystemService(SENSOR_SERVICE);
        lastUpdate = System.currentTimeMillis();

        backgroundPlayer = MediaPlayer.create(this, R.raw.mistsoftime4tmono);

        //////////////////////////
      //callCloudVision(""https://pixabay.com/get/eb36b90f2df1053ed95c4518b7494395e67fe7d604b0154892f2c67da7eabc_640.jpg"");
    }

    ///////////////////////////////
    @Override
    protected void onResume() {
        super.onResume();

        manager.registerListener(this, manager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER),
                SensorManager.SENSOR_DELAY_UI);
        backgroundPlayer.start();

    }

    //////////////////////////////
    @Override
    protected void onPause() {
        super.onPause();
        manager.unregisterListener(this);
        backgroundPlayer.pause();
    }

    @Override
    public boolean onCreateOptionsMenu(Menu menu) {
        // Inflate the menu; this adds items to the action bar if it is present.
        getMenuInflater().inflate(R.menu.menu_main, menu);

        return true;
    }

    @Override
    public boolean onOptionsItemSelected(MenuItem item) {
        // Handle action bar item clicks here. The action bar will
        // automatically handle clicks on the Home/Up button, so long
        // as you specify a parent activity in AndroidManifest.xml.
        int id = item.getItemId();
        Intent intent = new Intent(this, About.class);

        //noinspection SimplifiableIfStatement
        if (id == R.id.action_settings) {
            startActivity(intent);
            return true;
        }

        return super.onOptionsItemSelected(item);
    }

    public static long getRandomLong(long minimum, long maximum)
    {
        return (long) (Math.random()* (maximum- minimum))+ minimum;
    }

    @Override
    public void onAccuracyChanged(Sensor sensor, int accuracy) {

    }

    @Override
    public void onSensorChanged(SensorEvent event) {

        gravity[0] = lowPass(event.values[0], gravity[0]);
        gravity[1] = lowPass(event.values[1], gravity[1]);
        gravity[2] = lowPass(event.values[2], gravity[2]);

        accel[0] = highPass(event.values[0], accel[0]);
        accel[1] = highPass(event.values[1], accel[1]);
        accel[2] = highPass(event.values[2], accel[2]);



        long actualTime = System.currentTimeMillis();

        if (actualTime - lastUpdate &gt; 100) {
            if (inRange(gravity[2], -9.81f, VERTICAL_TOL)) {

                Log.i(TAG, ""Down"");

                if (!isDown) {
                    Vibrator v = (Vibrator) this.getApplicationContext().getSystemService(Context.VIBRATOR_SERVICE);
                    v.vibrate(500);
                    PixabayFetchTask task = new PixabayFetchTask();
                    String service = ""https://pixabay.com/api/"";
                    String key = ""5535853-23bc4a5e307cd5d1a5e16ebcc"";
                    String query_params = ""&amp;editor_choice=true&amp;safesearch=true&amp;image_type=photo"";
                    urlString = service + ""?key="" + key + query_params;
                    task.execute(urlString);
                    backgroundPlayer.setVolume(0.1f, 0.1f);
                    tts.speak(""The device is pointing down"", TextToSpeech.QUEUE_FLUSH, null);
                    backgroundPlayer.setVolume(1.0f, 1.0f);
                    isDown = true;
                    isUp = false;

                }

            } else if (inRange(gravity[2], 9.81f, VERTICAL_TOL)) {
                if (!isUp) {
                    try {
                        callCloudVision(urlString);
                    } catch (IOException e) {
                        e.printStackTrace();
                    }
                    backgroundPlayer.setVolume(0.1f, 0.1f);
                    Log.i(TAG, ""Up"");
                    tags= (String) result.getTags((int)selected);
                    pixtags.setText(""Tags: ""+tags, null);
                 /*   Snackbar.make(imageView, tags, Snackbar.LENGTH_LONG)
                            .setAction(""Action"", null).show();*/
                    tts.speak(tags.toString(), TextToSpeech.QUEUE_ADD, null);


                    //tts.speak(""up"", TextToSpeech.QUEUE_FLUSH, null);
                    backgroundPlayer.setVolume(1.0f, 1.0f);
                    isUp = true;
                    isDown = false;
                }

            } else {
                Log.i(TAG, ""In between"");
                //isDown = false;  // Rubbish!
                //isUp = false;
            }
            lastUpdate = actualTime;
        }

    }

    private boolean inRange(float value, float target,  float tol) {
        return value &gt;= target-tol &amp;&amp; value &lt;= target+tol;
    }

    // de-emphasize transient forces
    private float lowPass(float current, float gravity) {
        return current * (1-ALPHA) + gravity * ALPHA; // ALPHA indicates the influence of past observations
    }

    // de-emphasize constant forces
    private float highPass(float current, float gravity) {
        return current - gravity;
    }




    class PixabayFetchTask extends AsyncTask&lt;String, Void, PixabayQueryResult&gt; {

        /**
         * Override this method to perform a computation on a background thread. The
         * specified parameters are the parameters passed to {@link #execute}
         * by the caller of this task.
         * &lt;p&gt;
         * This method can call {@link #publishProgress} to publish updates
         * on the UI thread.
         *
         * @param params The parameters of the task.
         * @return A result, defined by the subclass of this task.
         * @see #onPreExecute()
         * @see #onPostExecute
         * @see #publishProgress
         */
        @Override
        protected PixabayQueryResult doInBackground(String... params) {

            Log.v(PIXABAY,""String[0] ="" + params[0]);

            if(result==null || result.isExpired()) {


                try {
                    String line;
                    URL u = new URL(params[0]);
                    HttpURLConnection conn = (HttpURLConnection) u.openConnection();
                    InputStream in = new BufferedInputStream(conn.getInputStream());
                    BufferedReader reader = new BufferedReader(new InputStreamReader(in));
                    StringBuilder json = new StringBuilder();
                    while ((line = reader.readLine()) != null) json.append(line);
                    result = new PixabayQueryResult(json.toString());

                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
            return result;
        }



        /**
         * &lt;p&gt;Runs on the UI thread after {@link #doInBackground}. The
         * specified result is the value returned by {@link #doInBackground}.&lt;/p&gt;
         * &lt;p&gt;
         * &lt;p&gt;This method won't be invoked if the task was cancelled.&lt;/p&gt;
         *
         * @param bitmap The result of the operation computed by {@link #doInBackground}.
         * @see #onPreExecute
         * @see #doInBackground
         * @see #onCancelled(Object)
         */
        @Override
        protected void onPostExecute(PixabayQueryResult result) {
            super.onPostExecute(result);
           numberOfHits= result.size();
             selected = getRandomLong(0, numberOfHits);
             Bitmap bitmap= result.getBitmap((int)selected);
            imageView.setImageBitmap(bitmap);
           /* try {
                callCloudVision(urlString);
            } catch (IOException e) {
                e.printStackTrace();
            }*/

        }
    }

    private void callCloudVision(final String loc) throws IOException {
        // Switch text to loading
        googlelab.setText(R.string.loading_message);

        // Do the real work in an async task, because we need to use the network anyway
        new AsyncTask&lt;Object, Bitmap, String&gt;() {
            @Override
            protected String doInBackground(Object... params) {
                try {
                    HttpTransport httpTransport = AndroidHttp.newCompatibleTransport();
                    JsonFactory jsonFactory = GsonFactory.getDefaultInstance();

                    VisionRequestInitializer requestInitializer =
                            new VisionRequestInitializer(CLOUD_VISION_API_KEY) {
                                /**
                                 * We override this so we can inject important identifying fields into the HTTP
                                 * headers. This enables use of a restricted cloud platform API key.
                                 */
                                @Override
                                protected void initializeVisionRequest(VisionRequest&lt;?&gt; visionRequest)
                                        throws IOException {
                                    super.initializeVisionRequest(visionRequest);

                                    String packageName = getPackageName();
                                    visionRequest.getRequestHeaders().set(ANDROID_PACKAGE_HEADER, packageName);

                                    String sig = PackageManagerUtils.getSignature(getPackageManager(), packageName);

                                    visionRequest.getRequestHeaders().set(ANDROID_CERT_HEADER, sig);
                                }
                            };

                    Vision.Builder builder = new Vision.Builder(httpTransport, jsonFactory, null);
                    builder.setVisionRequestInitializer(requestInitializer);

                    Vision vision = builder.build();

                    BatchAnnotateImagesRequest batchAnnotateImagesRequest =
                            new BatchAnnotateImagesRequest();
                    batchAnnotateImagesRequest.setRequests(new ArrayList&lt;AnnotateImageRequest&gt;() {{
                        AnnotateImageRequest annotateImageRequest = new AnnotateImageRequest();

                        Bitmap bitmap = null;
                        try {
                            InputStream stream = new URL(loc).openStream();
                            bitmap = BitmapFactory.decodeStream(stream);
                            publishProgress(bitmap);
                        } catch (IOException e) {
                            e.printStackTrace();
                        }

                        // Add the image
                        Image base64EncodedImage = new Image();
                        // Convert the bitmap to a JPEG
                        // Just in case it's a format that Android understands but Cloud Vision

                        ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();

                       bitmap.compress(Bitmap.CompressFormat.JPEG, 90, byteArrayOutputStream);

                        byte[] imageBytes = byteArrayOutputStream.toByteArray();


                        // Base64 encode the JPEG
                        base64EncodedImage.encodeContent(imageBytes);
                        annotateImageRequest.setImage(base64EncodedImage);

                        // add the features we want
                        annotateImageRequest.setFeatures(new ArrayList&lt;Feature&gt;() {{
                            Feature labelDetection = new Feature();
                            labelDetection.setType(""LABEL_DETECTION"");
                            labelDetection.setMaxResults(10);
                            add(labelDetection);
                        }});

                        // Add the list of one thing to the request
                        add(annotateImageRequest);
                    }});

                    Vision.Images.Annotate annotateRequest =
                            vision.images().annotate(batchAnnotateImagesRequest);
                    // Due to a bug: requests to Vision API containing large images fail when GZipped.
                    annotateRequest.setDisableGZipContent(true);
                    Log.d(TAGgoogle, ""created Cloud Vision request object, sending request"");

                    BatchAnnotateImagesResponse response = annotateRequest.execute();
                    return convertResponseToString(response);

                } catch (GoogleJsonResponseException e) {
                    Log.d(TAGgoogle, ""failed to make API request because "" + e.getContent());
                } catch (IOException e) {
                    Log.d(TAGgoogle, ""failed to make API request because of other IOException "" +
                            e.getMessage());
                }
                return ""Cloud Vision API request failed. Check logs for details."";
            }

            /**
             * Runs on the UI thread after {@link #publishProgress} is invoked.
             * The specified values are the values passed to {@link #publishProgress}.
             *
             * @param bitmaps The values indicating progress.
             * @see #publishProgress
             * @see #doInBackground
             */
            @Override
            protected void onProgressUpdate(Bitmap... bitmaps) {
                super.onProgressUpdate(bitmaps);
                imageView.setImageBitmap(bitmaps[0]);
            }

            protected void onPostExecute(String result) {

                googlelab.setText(result);

            }
        }.execute();
    }
    private String convertResponseToString(BatchAnnotateImagesResponse response) {
        String message = ""Labels:\n\n"";

        List&lt;EntityAnnotation&gt; labels = response.getResponses().get(0).getLabelAnnotations();
        if (labels != null) {
            for (EntityAnnotation label : labels) {
                message += String.format(Locale.US, ""%.3f: %s"", label.getScore(), label.getDescription());
                message += ""\n"";
            }
        } else {
            message += ""nothing"";
        }

        return message;
    }


}
</code></pre>

<p>Here is the it gives me error:</p>

<pre><code>E/AndroidRuntime: FATAL EXCEPTION: AsyncTask #3
Process: edu.ggc.lutz.recipe.pixabaysamplerwalkthrough, PID: 21223    java.lang.RuntimeException: An error occurred while executing doInBackground()  
      at android.os.AsyncTask$3.done(AsyncTask.java:309)
    at java.util.concurrent.FutureTask.finishCompletion(FutureTask.java:354)
          at java.util.concurrent.FutureTask.setException(FutureTask.java:223)
           at java.util.concurrent.FutureTask.run(FutureTask.java:242)
           at android.os.AsyncTask$SerialExecutor$1.run(AsyncTask.java:234) 
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1113)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:588)
                 at java.lang.Thread.run(Thread.java:818)
              Caused by: java.lang.NullPointerException: Attempt to invoke virtual method 'boolean android.graphics.Bitmap.compress(android.graphics.Bitmap$CompressFormat, int, java.io.OutputStream)' on a null object reference
           at edu.ggc.lutz.recipe.pixabaysamplerwalkthrough.MainActivity$2$2.&lt;init&gt;(MainActivity.java:419)
    at edu.ggc.lutz.recipe.pixabaysamplerwalkthrough.MainActivity$2.doInBackground(MainActivity.java:400)
              at edu.ggc.lutz.recipe.pixabaysamplerwalkthrough.MainActivity$2.doInBackground(MainActivity.java:366) 
             at android.os.AsyncTask$2.call(AsyncTask.java:295)                at java.util.concurrent.FutureTask.run(FutureTask.java:237)
             at android.os.AsyncTask$SerialExecutor$1.run(AsyncTask.java:234) 
             at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1113)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:588) 
            at java.lang.Thread.run(Thread.java:818)
</code></pre>

<p>There is another error that says something about the text to speech but I think that is the result of this error. </p>

<p>I believe it has something to do with running two different Async tasks at the same time overloading it or that fact a null value it getting passed in causing the error. </p>",,0,3,,2017-06-20 12:07:56.097 UTC,,2017-06-20 12:16:02.900 UTC,2017-06-20 12:16:02.900 UTC,,2308683,,8188248,1,0,android|android-asynctask|nullpointerexception|android-bitmap,144
Google Vision api :How to detect face detected is either an image face or real live face?,52549743,Google Vision api :How to detect face detected is either an image face or real live face?,"<p>I am using google vision api for face detection in my app. its working fine but in my case i need to deal with only real human faces. but my app is considering there faces in photo as a face. but i want to detect which is photo and which is live image.</p>

<p>below is the class of face graphics</p>

<pre><code> private class GraphicFaceTracker extends Tracker&lt;Face&gt; {
    private GraphicOverlay mOverlay;
    private FaceGraphic mFaceGraphic;

    GraphicFaceTracker(GraphicOverlay overlay) {
        mOverlay = overlay;
        mFaceGraphic = new FaceGraphic(overlay);
    }

    /**
     * Start tracking the detected face instance within the face overlay.
     */
    @Override
    public void onNewItem(int faceId, Face item) {
        FaceTrackerActivity.faceId = faceId;
        mFaceGraphic.setId(faceId);
    }

    /**
     * Update the position/characteristics of the face within the overlay.
     */
    @Override
    public void onUpdate(FaceDetector.Detections&lt;Face&gt; detectionResults, Face face) {
        mOverlay.add(mFaceGraphic);
        mFaceGraphic.updateFace(face);
       //here face detected live or image

    }

    /**
     * Hide the graphic when the corresponding face was not detected.  This can happen for
     * intermediate frames temporarily (e.g., if the face was momentarily blocked from
     * view).
     */
    @Override
    public void onMissing(FaceDetector.Detections&lt;Face&gt; detectionResults) {
        mOverlay.remove(mFaceGraphic);
    }

    /**
     * Called when the face is assumed to be gone for good. Remove the graphic annotation from
     * the overlay.
     */
    @Override
    public void onDone() {
        mOverlay.remove(mFaceGraphic);
        Log.d(""Gajanand"", ""onDone: "");
    }

}
</code></pre>

<p>any help?</p>",,0,4,,2018-09-28 06:40:41.270 UTC,1,2018-09-28 06:40:41.270 UTC,,,,,9634090,1,2,android|image-processing|face-detection|google-vision,177
Google cloud vision face detection API is returns only 10 responses or accepts only files from the Internet,48607548,Google cloud vision face detection API is returns only 10 responses or accepts only files from the Internet,"<p>I am integrating my camera with Google cloud vision API so that I can count the total number of people in a room. But the API is returning only 10 responses.</p>

<p>In order to get more responses I added the field <code>max_results</code> in <code>features</code>. After adding the <code>max_results</code> field it is returning more than 10 responses, but then I get the problem that it is only accepting an image with a 'URI' and I am unable to give it an image present on my system. It is only accepting images present on the internet with an image address like in the piece of code below. Now how can I specify an image present on my system instead of giving URI? </p>

<p>My python code for taking image and features:</p>

<blockquote>
  <p>response = client.annotate_image({'image': {'source': {'image_uri':'<a href=""http://im.rediff.com/news/2016/jan/26republic-day1.jpg"" rel=""nofollow noreferrer"">http://im.rediff.com/news/2016/jan/26republic-day1.jpg</a>'}},  'features': [{'type': vision.enums.Feature.Type.FACE_DETECTION,'max_results':40}],})</p>
</blockquote>",,1,9,,2018-02-04 11:26:08.157 UTC,0,2018-05-05 20:31:19.587 UTC,2018-02-04 11:58:17.163 UTC,,3077495,,9312140,1,1,python|google-cloud-vision,367
Error when trying to use detectText function in Amazon Rekognition,49187806,Error when trying to use detectText function in Amazon Rekognition,"<p>I tried to use Amazon Rekognition in one of my projects which involves in detecting the text content in a given image(ocr).I tried using AWS SDK and I used the method detectText function under Rekongtion service. But every time I tried to run my script I am getting a </p>

<p>ProvisionedThroughputExceededException </p>

<p>error as the result. I tried the Amazon Rekogntion provided demo page as well, and I got an 
<a href=""https://i.stack.imgur.com/clgbG.png"" rel=""nofollow noreferrer"">unknown error </a>
as shown in the image attached. But when I looked at my browser console I noticed that it was the same ProvisionedThroughputExceededException that I've got previously. The only help that I found regarding the problem is this thread (which isn't directly related but the person is getting the same exception),</p>

<p><a href=""https://forums.aws.amazon.com/thread.jspa?messageID=824338&amp;#824338"" rel=""nofollow noreferrer"">https://forums.aws.amazon.com/thread.jspa?messageID=824338&amp;#824338</a></p>

<p>and as mentioned in the answers I tried to increase my request limit but I couldn't found the DetectText method under any of the APIs provided. Any help would appreciate in this matter. Thanks in advance </p>",,0,2,,2018-03-09 06:29:02.977 UTC,,2018-03-09 11:33:10.933 UTC,2018-03-09 11:33:10.933 UTC,,3985738,,3985738,1,0,amazon-web-services|amazon-rekognition,226
Give Google Cloud Vision API access to Firebase Storage,38211578,Give Google Cloud Vision API access to Firebase Storage,"<p>I'm using Firebase on iOS, and I want to let users upload a photo to Firebase Storage. After that, I want to analyze the photo using Google Cloud Vision APIs.</p>

<p>Uploading works fine.</p>

<p>To analyze the photo, I'm specifying it using</p>

<pre><code>image = {
    source = {
        gcsImageUri = ""gs://app.name.from.firebase.console/path.to.photo"";
    };
};
</code></pre>

<p>The problem is that I get the following error</p>

<pre><code>error = {
    code = 7;
    message = ""image-annotator::User lacks permission.: Calling GetObjectMetadata with file \""/bigstore/gs://app.name.from.firebase.console/path.to.photo\"": cloud.bigstore.ResponseCode.ErrorCode::ACCESS_DENIED: ACCESS_DENIED: gaiaUser/0 does not have OBJECTS_GET access to object gs://app.name.from.firebase.console/path.to.photo."";
};
</code></pre>

<p>Do you have any suggestion w.r.t. what permissions I need to set?</p>

<p>Thanks!</p>",,2,0,,2016-07-05 19:49:59.740 UTC,0,2017-04-03 20:56:26.663 UTC,2016-07-05 23:51:06.787 UTC,,209103,,901043,1,1,firebase|firebase-storage|google-cloud-vision,685
AWS Athena not being recognized in Boto3?,44615959,AWS Athena not being recognized in Boto3?,"<p>I am trying to use AWS Athena from both the CLI and through boto3 but for some reason it is not being recognized. I have upgraded to the newest version of boto3</p>

<pre><code>boto3.__version__
&gt;&gt;'1.4.4'

aws --version
&gt;&gt;aws-cli/1.11.56 Python/3.6.0 Darwin/15.6.0 botocore/1.5.19
</code></pre>

<p>When I go to do <code>client = boto3.client('athena')</code> I am greeted with:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/boto3/__init__.py"", line 83, in client
    return _get_default_session().client(*args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/boto3/session.py"", line 263, in client
    aws_session_token=aws_session_token, config=config)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/botocore/session.py"", line 836, in create_client
    client_config=config, api_version=api_version)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/botocore/client.py"", line 63, in create_client
    service_model = self._load_service_model(service_name, api_version)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/botocore/client.py"", line 93, in _load_service_model
    api_version=api_version)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/botocore/loaders.py"", line 132, in _wrapper
    data = func(self, *args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/botocore/loaders.py"", line 378, in load_service_model
    known_service_names=', '.join(sorted(known_services)))
botocore.exceptions.UnknownServiceError: Unknown service: 'athena'. Valid service names are: acm, apigateway, application-autoscaling, appstream, autoscaling, batch, budgets, clouddirectory, cloudformation, cloudfront, cloudhsm, cloudsearch, cloudsearchdomain, cloudtrail, cloudwatch, codebuild, codecommit, codedeploy, codepipeline, cognito-identity, cognito-idp, cognito-sync, config, cur, datapipeline, devicefarm, directconnect, discovery, dms, ds, dynamodb, dynamodbstreams, ec2, ecr, ecs, efs, elasticache, elasticbeanstalk, elastictranscoder, elb, elbv2, emr, es, events, firehose, gamelift, glacier, health, iam, importexport, inspector, iot, iot-data, kinesis, kinesisanalytics, kms, lambda, lex-runtime, lightsail, logs, machinelearning, marketplacecommerceanalytics, meteringmarketplace, mturk, opsworks, opsworkscm, organizations, pinpoint, polly, rds, redshift, rekognition, route53, route53domains, s3, sdb, servicecatalog, ses, shield, sms, snowball, sns, sqs, ssm, stepfunctions, storagegateway, sts, support, swf, waf, waf-regional, workspaces, xray
</code></pre>

<p>Same thing for the CLI, when I do <code>aws athena help</code> I get an invalid option. Any idea why this is happening? I am trying to automate a task as opposed to sitting in the GUI repeatedly entering queries.</p>",44616066,1,0,,2017-06-18 14:24:08.253 UTC,,2017-06-18 14:39:12.860 UTC,,,,,6817835,1,1,python|amazon-web-services|boto3|amazon-athena,1045
Google Cloud Vision API - How to enable a service account,42176137,Google Cloud Vision API - How to enable a service account,"<p>I am trying to get my head round GoogleVision API Java library.</p>

<p>I have created a service account, downloaded the json and set this environment variable.</p>

<pre><code>GOOGLE_APPLICATION_CREDENTIALS=C:\GoogleAPI\keys\translate-41428d4d1ec6.json
</code></pre>

<p>I have set Application Default Credentials using:</p>

<pre><code>gcloud beta auth application-default login
</code></pre>

<p>And I am following the example here:
<a href=""https://cloud.google.com/vision/docs/reference/libraries"" rel=""nofollow noreferrer"">https://cloud.google.com/vision/docs/reference/libraries</a></p>

<p>I am assuming now that all calls made will use the service account as if by magic, as I am not doing any authentication in the code (as per the sample).</p>

<p>However, I cannot see where I authorised the service account to use the Google Vision API, which I assume I have to. So, I could possible not be using the service account at all...</p>

<p>When I go into the IAM, and try to assign a ""role"" to the service account, there is nothing related to the vision API?  There are roles such as </p>

<ul>
<li>How can I be sure I am using the service account to make the call?</li>
<li>What do I need to to explicitly to enable a service account to access a specific API such as GoogleVision when it isnt listed in IAM...or can ALL service accounts related to the project access the APIs?</li>
</ul>

<p>The example in the documentation shows how </p>

<p>Any help appreciated.</p>

<p>Also I am interested in how I would adapt the sample to not use Application Default Credentials, but actually create a specific Credential instance and use this to call Google Vision, as that is not clear.  The example give is for calling GoogleStorage, but I can't translate this to Google Vision.</p>

<pre><code>public class StorageFactory {
  private static Storage instance = null;

  public static synchronized Storage getService() throws IOException, GeneralSecurityException {
    if (instance == null) {
      instance = buildService();
    }
    return instance;
  }

  private static Storage buildService() throws IOException, GeneralSecurityException {
    HttpTransport transport = GoogleNetHttpTransport.newTrustedTransport();
    JsonFactory jsonFactory = new JacksonFactory();
    GoogleCredential credential = GoogleCredential.getApplicationDefault(transport, jsonFactory);

    // Depending on the environment that provides the default credentials (for
    // example: Compute Engine, App Engine), the credentials may require us to
    // specify the scopes we need explicitly.  Check for this case, and inject
    // the Cloud Storage scope if required.
    if (credential.createScopedRequired()) {
      Collection&lt;String&gt; scopes = StorageScopes.all();
      credential = credential.createScoped(scopes);
    }

    return new Storage.Builder(transport, jsonFactory, credential)
        .setApplicationName(""GCS Samples"")
        .build();
  }
}
</code></pre>

<p>And don't get me started on the 2 hours I just wasted getting ""Access Denied""...which apparently was due to the image size being over 4mb, and nothing to do with credentials!</p>",42184379,1,4,,2017-02-11 12:58:23.397 UTC,,2017-02-13 17:04:27.503 UTC,2017-02-13 17:04:27.503 UTC,,5231007,,1754307,1,5,java|google-cloud-platform|google-cloud-vision,1377
Limiting the response from Google Cloud Vision API,53056817,Limiting the response from Google Cloud Vision API,"<p>Currently using the google cloud vision api for pulling text from images of documents.</p>

<p><strong>Current situation</strong> - the API works great, and returns tons of data including the bounding boxes of where the words are located.</p>

<p><strong>Desired outcome</strong> - to query only the words pulled from the image and not all the meta data about where the bounding boxes and vertices of the words are (it's like 99% of the response and comes out to be about 250k which is a huge waste when all I want are just the words)</p>

<pre><code>const vision = require('@google-cloud/vision');
const client = new vision.ImageAnnotatorClient();

// Performs label detection on the image file
client
  .documentTextDetection('../assets/images_to_ocr/IMG_0942-min.jpg')
  .then(results =&gt; {
    console.log('result:', result);

  })
  .catch(err =&gt; {
    console.error('ERROR:', err);
  });
</code></pre>",53088141,1,1,,2018-10-30 03:08:22.060 UTC,,2018-10-31 16:32:25.750 UTC,,,,,2307240,1,0,google-cloud-vision,86
Google Natural Language API permission denied error,42431787,Google Natural Language API permission denied error,"<p>Google Natural Language API has been working in my iOS app up until yesterday. The API started returning ""permission denied"" errors as of this morning. E.g:</p>

<pre><code>{
    ""error"": {
        ""code"": 403,
        ""message"": ""The caller does not have permission"",
        ""status"": ""PERMISSION_DENIED""
    }
}
</code></pre>

<p>Example request:</p>

<pre><code>POST /v1/documents:analyzeEntities?key=..... HTTP/1.1
Host: language.googleapis.com
Content-Type: application/json
Connection: keep-alive
X-Ios-Bundle-Identifier: .....
Accept: */*
Accept-Language: en-us
Content-Length: 291
Accept-Encoding: gzip, deflate
User-Agent: CardScanner/1 CFNetwork/808.2.16 Darwin/15.6.0

{""encodingType"":""UTF8"",""document"":{""type"":""PLAIN_TEXT"",""content"":"".....""}}
</code></pre>

<p>Billing is enabled for the account (with a balance of $0). The account also has 36 days left on the trial period.</p>

<p>The key matches the value in the Google Cloud Platform API dashboard. I have also tried regenerating the key, and using the new key in the app.</p>

<p>I have also tried enabling key restrictions for iOS devices, and included the ""X-Ios-Bundle-Identifier"" header with the app bundle identifier.</p>

<p>The app also uses the Google Vision API which works without issues. Calls to the Vision API do respond to changes to the key restrictions.</p>

<p>Calls made from the <a href=""https://cloud.google.com/natural-language/"" rel=""nofollow noreferrer"">demo page</a> also show a permissions error message. Calls from the <a href=""https://developers.google.com/apis-explorer/?hl=en_US#p/language/v1/language.documents.analyzeEntities"" rel=""nofollow noreferrer"">API explorer</a> do work however.</p>

<p>Edit:</p>

<p>The error is also happening on the <a href=""https://cloud.google.com/natural-language/"" rel=""nofollow noreferrer"">demo on the product web page</a>. Tracing the error in Charles shows the same ""permission denied"" response being returned to the web page: </p>

<p><a href=""https://i.stack.imgur.com/JyOdf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JyOdf.png"" alt=""enter image description here""></a></p>

<p>Edit:</p>

<p>Below is an example of the HTTP request and response captured from the demo page. The request and resulting error is almost identical to my app, except that the demo seems to be using http 2, whereas my app is using http 1. </p>

<p>HTTP request:</p>

<pre><code>:method: POST
:authority: language.googleapis.com
:scheme: https
:path: /v1/documents:analyzeEntities?key=.....
content-length: 250
origin: https://cloud.google.com
user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36
content-type: text/plain;charset=UTF-8
accept: */*
referer: https://cloud.google.com/natural-language/
accept-encoding: gzip, deflate, br
accept-language: en-US,en;q=0.8

{""document"":{""type"":""PLAIN_TEXT"",""content"":""Google, headquartered in Mountain View, unveiled the new Android phone at the Consumer Electronic Show.  Sundar Pichai said in his keynote that users love their new Android phones.""},""encodingType"":""UTF16""}
</code></pre>

<p>HTTP response:</p>

<pre><code>:status: 403
vary: Origin
vary: X-Origin
vary: Referer
content-type: application/json; charset=UTF-8
content-encoding: gzip
date: Sun, 26 Feb 2017 14:52:24 GMT
server: ESF
cache-control: private
content-length: 128
x-xss-protection: 1; mode=block
x-frame-options: SAMEORIGIN
x-content-type-options: nosniff
access-control-allow-origin: https://cloud.google.com
access-control-expose-headers: content-encoding,date,server,content-length
alt-svc: quic="":443""; ma=2592000; v=""35,34""

{
  ""error"": {
    ""code"": 403,
    ""message"": ""The caller does not have permission"",
    ""status"": ""PERMISSION_DENIED""
  }
}
</code></pre>",,3,0,,2017-02-24 05:56:46.557 UTC,2,2017-02-28 07:00:03.820 UTC,2017-02-27 04:55:29.043 UTC,,762377,,762377,1,2,ios|google-cloud-nl,458
Determine which rectangle on picturebox is clicked,49520402,Determine which rectangle on picturebox is clicked,"<p>I have a c# WinForms project with a picture box that contains a document with text.  I am gathering the OCR data for the document using the Google Cloud Vision API, which works great.  Using the bounding rectangles returned from the Google API, I am drawing rectangles around each word using DrawRectangle, and in the process I am associating that rectangle with the underlying word.  What do I need to do to be able to just click on any given rectangle and know exactly which rectangle it is without having to take the point clicked and loop through all the coordinates of all the rectangles until I find it.</p>",49520511,1,3,,2018-03-27 18:50:30.420 UTC,,2018-03-27 19:22:06.390 UTC,,,,,9492455,1,0,c#,48
QR Code Scanning inside a limit area,49256697,QR Code Scanning inside a limit area,"<p>I'm new to android. I'm using Google vision-api dependency for Qr-code scanning by following the link below :</p>

<p><a href=""https://www.youtube.com/watch?v=INVNULTm56o"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=INVNULTm56o</a> .</p>

<p>But it scanning the whole screen. If i want to read Qr-code from a limit area or a Boundary without minimize the screen is it possible? Let me Know Thank you.</p>

<p>I design this boundary in my app and i want read Qr Code from only from boundary:-</p>

<p><a href=""https://i.stack.imgur.com/G9bzc.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G9bzc.jpg"" alt=""enter image description here""></a></p>",49257112,1,2,,2018-03-13 12:50:44.697 UTC,,2018-03-13 14:22:06.190 UTC,2018-03-13 14:22:06.190 UTC,,7319704,,9482391,1,0,android,781
How To Convert Google Client Vision Library Response To Json,50874265,How To Convert Google Client Vision Library Response To Json,"<p>I am trying to convert the response from Google Cloud Vision API Client Library to a json format. However i get the following error:</p>

<blockquote>
  <p>AttributeError: 'google.protobuf.pyext._message.RepeatedCompositeCo'
  object has no attribute 'DESCRIPTOR</p>
</blockquote>

<p><strong>Resource</strong></p>

<pre><code>from flask_restful import Resource
from flask import request
from flask import json
from util.GoogleVision import GoogleVision
from util.Convert import Convert

import base64
import requests

import os


class Vision(Resource):

    def post(self):

        googleVision = GoogleVision()

        req = request.get_json()

        url = req['url']

        result = googleVision.detectLabels(url)

        return result
</code></pre>

<p><strong>GoogleVision.py</strong></p>

<pre><code>import os

from google.cloud import vision
from google.cloud.vision import types
from google.protobuf.json_format import MessageToJson

class GoogleVision():

    def detectLabels(self, uri):

        client = vision.ImageAnnotatorClient()
        image = types.Image()
        image.source.image_uri = uri

        response = client.label_detection(image=image)
        labels = response.label_annotations

        res = MessageToJson(labels)

        return res
</code></pre>

<p>the labels variable is of type <code>&lt;class'google.protobuf.pyext._message.RepeatedCompositeContainer'&gt;</code></p>

<p>As you can see i am using message to json function on the labels response. But i am getting the above error.</p>

<p>Is there a way to convert the result to a json format?</p>",,1,3,,2018-06-15 11:03:15.903 UTC,,2018-11-07 17:38:03.230 UTC,2018-11-07 17:38:03.230 UTC,,8456296,,1548432,1,2,python|json|google-api|protocol-buffers|google-cloud-vision,463